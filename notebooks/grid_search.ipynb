{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293957c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "\n",
    "\n",
    "def train_and_evaluate_final_model(\n",
    "    X,\n",
    "    y,\n",
    "    groups,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,\n",
    "    run_cross_validation=True,\n",
    "    **hyperparameters,\n",
    "):\n",
    "    \"\"\"Implementation of the final training strategy.\"\"\"\n",
    "    # Create held-out test set separating EFO/Gene pairs between train/test\n",
    "    train_test_split = GroupShuffleSplit(\n",
    "        n_splits=1, test_size=test_size, random_state=42\n",
    "    )\n",
    "    for train_idx, test_idx in train_test_split.split(X, y, groups):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        groups_train = groups[train_idx]\n",
    "\n",
    "    # Cross-validation\n",
    "    if run_cross_validation:\n",
    "        cv_scores = []\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        print(\"Performing cross-validation...\")\n",
    "        for fold, (train_idx, val_idx) in enumerate(\n",
    "            gkf.split(X_train, y_train, groups_train)\n",
    "        ):\n",
    "            # Split data\n",
    "            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            model = GradientBoostingClassifier(\n",
    "                **hyperparameters, random_state=42, loss=\"log_loss\"\n",
    "            )\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            y_pred_proba = model.predict_proba(X_fold_val)[:, 1]\n",
    "            avg_precision = average_precision_score(y_fold_val, y_pred_proba)\n",
    "            cv_scores.append(avg_precision)\n",
    "\n",
    "            print(f\"Fold {fold + 1}: Average Precision = {avg_precision:.3f}\")\n",
    "\n",
    "        print(\n",
    "            f\"\\nCross-validation Average Precision: {np.mean(cv_scores):.3f} (+/- {np.std(cv_scores) * 2:.3f})\"\n",
    "        )\n",
    "\n",
    "    # Train final model on full training set\n",
    "    print(\"\\nTraining final model on full training set...\")\n",
    "    final_model = GradientBoostingClassifier(\n",
    "        **hyperparameters, random_state=42, loss=\"log_loss\"\n",
    "    )\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate once on test set\n",
    "    print(\"\\nEvaluating on held-out test set...\")\n",
    "    test_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    test_avg_precision = average_precision_score(y_test, test_pred_proba)\n",
    "\n",
    "    print(f\"Test Set Average Precision: {test_avg_precision:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"final_model\": final_model,\n",
    "        \"cv_scores\": cv_scores if run_cross_validation else None,\n",
    "        \"cv_mean\": np.mean(cv_scores) if run_cross_validation else None,\n",
    "        \"cv_std\": np.std(cv_scores) if run_cross_validation else None,\n",
    "        \"test_score\": test_avg_precision,\n",
    "        \"model\": model\n",
    "    }\n",
    "\n",
    "\n",
    "def create_param_grid(param_dict):\n",
    "    \"\"\"Convert a dictionary of parameter lists into a list of all possible combinations.\n",
    "\n",
    "    Args:\n",
    "        param_dict (dict): Dictionary where keys are parameter names and values are lists of parameter values\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries, each containing one combination of parameters\n",
    "    \"\"\"\n",
    "    keys = param_dict.keys()\n",
    "    values = param_dict.values()\n",
    "    combinations = list(product(*values))\n",
    "    return [dict(zip(keys, combo)) for combo in combinations]\n",
    "\n",
    "\n",
    "def run_single_parameter_set(args):\n",
    "    \"\"\"Helper function to run a single parameter combination.\n",
    "\n",
    "    Unpacks arguments and calls the training function.\n",
    "\n",
    "    Args:\n",
    "        args (tuple): Tuple containing (X, y, groups, params, param_id)\n",
    "\n",
    "    Returns:\n",
    "        dict: Results dictionary including parameters and scores\n",
    "    \"\"\"\n",
    "    X, y, groups, params, param_id = args\n",
    "\n",
    "    try:\n",
    "        results = train_and_evaluate_final_model(X=X, y=y, groups=groups, **params)\n",
    "\n",
    "        return {\n",
    "            \"param_id\": param_id,\n",
    "            \"parameters\": params,\n",
    "            \"cv_mean\": results[\"cv_mean\"],\n",
    "            \"cv_std\": results[\"cv_std\"],\n",
    "            \"test_score\": results[\"test_score\"],\n",
    "            \"status\": \"completed\",\n",
    "            \"error\": None,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"param_id\": param_id,\n",
    "            \"parameters\": params,\n",
    "            \"cv_mean\": None,\n",
    "            \"cv_std\": None,\n",
    "            \"test_score\": None,\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "\n",
    "\n",
    "def run_parallel_grid_search(\n",
    "    X, y, groups, param_grid, n_workers=None, results_dir=\"grid_search_results\"\n",
    "):\n",
    "    \"\"\"Run grid search in parallel using ProcessPoolExecutor.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Feature matrix\n",
    "        y (array-like): Target vector\n",
    "        groups (array-like): Groups for cross-validation\n",
    "        param_grid (dict): Dictionary of parameters to search\n",
    "        n_workers (int, optional): Number of parallel workers. Defaults to None (CPU count)\n",
    "        results_dir (str, optional): Directory to save results. Defaults to 'grid_search_results'\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all results\n",
    "    \"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Generate all parameter combinations\n",
    "    param_combinations = create_param_grid(param_grid)\n",
    "    print(f\"Total parameter combinations to test: {len(param_combinations)}\")\n",
    "\n",
    "    # Create timestamp for this run\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Prepare arguments for parallel execution\n",
    "    args_list = [\n",
    "        (X, y, groups, params, i) for i, params in enumerate(param_combinations)\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Run parallel execution\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        for result in executor.map(run_single_parameter_set, args_list):\n",
    "            results.append(result)\n",
    "\n",
    "            # Save intermediate results\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(f\"{results_dir}/grid_search_results_{timestamp}.csv\", index=False)\n",
    "\n",
    "            # Print progress\n",
    "            completed = len([r for r in results if r[\"status\"] == \"completed\"])\n",
    "            failed = len([r for r in results if r[\"status\"] == \"failed\"])\n",
    "            print(\n",
    "                f\"\\rProgress: {completed + failed}/{len(param_combinations)} \"\n",
    "                f\"(Completed: {completed}, Failed: {failed})\",\n",
    "                end=\"\",\n",
    "            )\n",
    "\n",
    "    print(\"\\nGrid search completed!\")\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Sort by CV mean score (descending)\n",
    "    results_df = results_df.sort_values(\"cv_mean\", ascending=False)\n",
    "\n",
    "    # Save final results\n",
    "    final_path = f\"{results_dir}/grid_search_results_{timestamp}_final.csv\"\n",
    "    results_df.to_csv(final_path, index=False)\n",
    "    print(f\"\\nFinal results saved to: {final_path}\")\n",
    "\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a061623",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [\n",
    "            \"eQtlColocClppMaximum\",\n",
    "            \"pQtlColocClppMaximum\",\n",
    "            \"sQtlColocClppMaximum\",\n",
    "            \"eQtlColocH4Maximum\",\n",
    "            \"pQtlColocH4Maximum\",\n",
    "            \"sQtlColocH4Maximum\",\n",
    "            \"eQtlColocClppMaximumNeighbourhood\",\n",
    "            \"pQtlColocClppMaximumNeighbourhood\",\n",
    "            \"sQtlColocClppMaximumNeighbourhood\",\n",
    "            \"eQtlColocH4MaximumNeighbourhood\",\n",
    "            \"pQtlColocH4MaximumNeighbourhood\",\n",
    "            \"sQtlColocH4MaximumNeighbourhood\",\n",
    "            \"distanceSentinelFootprint\",\n",
    "            \"distanceSentinelFootprintNeighbourhood\",\n",
    "            \"distanceFootprintMean\",\n",
    "            \"distanceFootprintMeanNeighbourhood\",\n",
    "            \"distanceTssMean\",\n",
    "            \"distanceTssMeanNeighbourhood\",\n",
    "            \"distanceSentinelTss\",\n",
    "            \"distanceSentinelTssNeighbourhood\",\n",
    "            \"vepMaximum\",\n",
    "            \"vepMaximumNeighbourhood\",\n",
    "            \"vepMean\",\n",
    "            \"vepMeanNeighbourhood\",\n",
    "            \"geneCount500kb\",\n",
    "            \"proteinGeneCount500kb\",\n",
    "            \"credibleSetConfidence\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "gs = spark.read.json(\"/Users/irenelopez/EBI/repos/gentropy/2506_traing_set.json\")\n",
    "\n",
    "print(gs.count())\n",
    "print(gs.filter(f.size(\"diseaseIds\") > 1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "gs = spark.read.json(\"/Users/irenelopez/EBI/repos/gentropy/2506_traing_set.json\") # gs://genetics-portal-dev-analysis/yt4/2506_release/training_set/2506_traing_set.json\n",
    "fm = spark.read.parquet(\"/Users/irenelopez/EBI/repos/gentropy/l2g_feature_matrix\") # gs://open-targets-pipeline-runs/szsz/25.06-testrun-4/intermediate/l2g_feature_matrix\")\n",
    "\n",
    "data_df = gs.join(fm, on=[\"geneId\", \"studyLocusId\"], how=\"inner\").coalesce(1).toPandas()\n",
    "\n",
    "\n",
    "data_df[\"goldStandardSet\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf110123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels in `goldStandardSet` to a numeric value\n",
    "label_encoder: dict[str, int] = {\n",
    "            \"negative\": 0,\n",
    "            \"positive\": 1,\n",
    "        }\n",
    "data_df[\"goldStandardSet\"] = data_df[\"goldStandardSet\"].map(label_encoder)\n",
    "\n",
    "# Define sets\n",
    "X = data_df[features_list].apply(pd.to_numeric).values\n",
    "y = data_df[\"goldStandardSet\"].apply(pd.to_numeric).values\n",
    "# traits come now as arrays, I will convert it to a string column to create the groups\n",
    "data_df[\"diseaseIds\"] = [\",\".join(map(str, l)) for l in data_df[\"diseaseIds\"]]\n",
    "gene_groups = data_df[\"geneId\"].astype(str)  # Group identifier has to be a single string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.01],\n",
    "    \"subsample\": [0.5],\n",
    "    \"max_depth\": [3],\n",
    "    \"ccp_alpha\": [0.0],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"min_samples_split\": [2]\n",
    "}\n",
    "\n",
    "results = run_parallel_grid_search(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    groups=gene_groups,\n",
    "    param_grid=param_grid,\n",
    "    n_workers=4  # Adjust based on your CPU cores\n",
    ")\n",
    "\n",
    "# Get top 5 parameter combinations\n",
    "print(\"\\nTop 5 parameter combinations:\")\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"ccp_alpha\": 0.0,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"subsample\": 0.7,\n",
    "    \"tol\": 0.01,\n",
    "}\n",
    "\n",
    "\n",
    "result = train_and_evaluate_final_model(\n",
    "    X,\n",
    "    y,\n",
    "    gene_groups,\n",
    "    run_cross_validation=True,\n",
    "    **hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eee300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = result[\"final_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a50cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Assuming 'model' is your trained GradientBoostingClassifier\n",
    "# Visualize the first tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(model.estimators_[0][0],\n",
    "               filled=True,\n",
    "               rounded=True,\n",
    "               proportion=True,\n",
    "               fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d282fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skops.io as sio\n",
    "\n",
    "sio.dump(model, \"split_by_gene_model.skops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cadf54",
   "metadata": {},
   "source": [
    "## Evaluating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gentropy.common.session import Session\n",
    "from gentropy.dataset.l2g_feature_matrix import L2GFeatureMatrix\n",
    "from gentropy.dataset.l2g_prediction import L2GPrediction\n",
    "\n",
    "session = Session()\n",
    "\n",
    "# credible_set = StudyLocus.from_parquet(\n",
    "#     session, \"/Users/irenelopez/EBI/repos/gentropy/credible_set\"\n",
    "# )\n",
    "feature_matrix = L2GFeatureMatrix(\n",
    "    _df=session.spark.read.parquet(\n",
    "        \"/Users/irenelopez/EBI/repos/gentropy/l2g_feature_matrix\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726685c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = L2GPrediction.from_credible_set(\n",
    "                session,\n",
    "                credible_set,\n",
    "                feature_matrix,\n",
    "                model_path=\"/Users/irenelopez/EBI/repos/gentropy/notebooks\",\n",
    "                features_list=features_list,\n",
    "                hf_token=None,\n",
    "                hf_model_version=None,\n",
    "                download_from_hub=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = session.load_data(\"/Users/irenelopez/EBI/repos/gentropy/predictions_old_gs_hier\")\n",
    "pred = session.load_data(\"/Users/irenelopez/EBI/repos/gentropy/l2g_prediction\").withColumnRenamed(\"score\", \"score_priv\")\n",
    "\n",
    "joined_df = new_pred.join(pred, [\"studyLocusId\", \"geneId\"], \"inner\").select(\"studyLocusId\", \"geneId\", \"score\", \"score_priv\")\n",
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_model_score(cv_precision_mean, cv_precision_std, test_precision, selectivity_2_count,\n",
    "                         cv_weight=0.35, stability_weight=0.35, generalization_weight=0.1, selectivity_weight=0.4):\n",
    "    \"\"\"Calculate a composite score for model evaluation based on:\n",
    "    1. CV precision performance\n",
    "    2. CV precision stability (low variance)\n",
    "    3. Generalization (CV vs Test precision difference)\n",
    "    4. Selectivity (preference for ~20K in category 2)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cv_precision_mean : float\n",
    "        Mean cross-validation precision\n",
    "    cv_precision_std : float\n",
    "        Standard deviation of cross-validation precision\n",
    "    test_precision : float\n",
    "        Test set precision\n",
    "    selectivity_2_count : int\n",
    "        Count of category 2 in selectivity (target ~20,000)\n",
    "    cv_weight : float\n",
    "        Weight for CV precision component (default 0.2)\n",
    "    stability_weight : float\n",
    "        Weight for stability component (default 0.2)\n",
    "    generalization_weight : float\n",
    "        Weight for generalization component (default 0.2)\n",
    "    selectivity_weight : float\n",
    "        Weight for selectivity component (default 0.4)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing individual scores and composite score\n",
    "    \"\"\"\n",
    "    # 1. CV Precision Score (0-1, higher is better)\n",
    "    cv_score = cv_precision_mean\n",
    "\n",
    "    # 2. Stability Score (0-1, higher is better - penalize high variance)\n",
    "    # Use exponential decay to heavily penalize high variance\n",
    "    stability_score = np.exp(-10 * cv_precision_std)\n",
    "\n",
    "    # 3. Generalization Score (0-1, higher is better - penalize large CV-Test gap)\n",
    "    precision_gap = abs(cv_precision_mean - test_precision)\n",
    "    generalization_score = np.exp(-20 * precision_gap)\n",
    "\n",
    "    # 4. Selectivity Score (0-1, higher is better - target ~20K for category 2)\n",
    "    target_selectivity = 20000\n",
    "    selectivity_ratio = min(selectivity_2_count, target_selectivity) / target_selectivity\n",
    "    # Bonus if very close to target, penalty if too far\n",
    "    if selectivity_2_count > target_selectivity:\n",
    "        selectivity_score = selectivity_ratio * np.exp(-0.5 * (selectivity_2_count - target_selectivity) / target_selectivity)\n",
    "    else:\n",
    "        selectivity_score = selectivity_ratio\n",
    "\n",
    "    # Composite Score (weighted average)\n",
    "    composite_score = (\n",
    "        cv_weight * cv_score +\n",
    "        stability_weight * stability_score +\n",
    "        generalization_weight * generalization_score +\n",
    "        selectivity_weight * selectivity_score\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"cv_score\": cv_score,\n",
    "        \"stability_score\": stability_score,\n",
    "        \"generalization_score\": generalization_score,\n",
    "        \"selectivity_score\": selectivity_score,\n",
    "        \"composite_score\": composite_score\n",
    "    }\n",
    "\n",
    "def evaluate_models_from_table():\n",
    "    \"\"\"Evaluate the models from your table\n",
    "    \"\"\"\n",
    "    # Your data (extracted from the table)\n",
    "    models_data = [\n",
    "        {\"cv_mean\": 0.832, \"cv_std\": 0.103, \"test_precision\": 0.835, \"selectivity_2\": 21728},\n",
    "        {\"cv_mean\": 0.704, \"cv_std\": 0.124, \"test_precision\": 0.74, \"selectivity_2\": None},  # Missing data\n",
    "        {\"cv_mean\": 0.6997, \"cv_std\": 0.0934, \"test_precision\": 0.7725, \"selectivity_2\": 42000},\n",
    "        {\"cv_mean\": 0.8206, \"cv_std\": 0.0589, \"test_precision\": 0.8459, \"selectivity_2\": 70484},\n",
    "        {\"cv_mean\": 0.875, \"cv_std\": 0.034, \"test_precision\": 0.898, \"selectivity_2\": 110000},\n",
    "        {\"cv_mean\": 0.844, \"cv_std\": 0.071, \"test_precision\": 0.778, \"selectivity_2\": 90000}\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for i, model in enumerate(models_data):\n",
    "        if model[\"selectivity_2\"] is not None:  # Skip model with missing data\n",
    "            scores = calculate_model_score(\n",
    "                model[\"cv_mean\"],\n",
    "                model[\"cv_std\"],\n",
    "                model[\"test_precision\"],\n",
    "                model[\"selectivity_2\"]\n",
    "            )\n",
    "            scores[\"model_id\"] = i + 1\n",
    "            results.append(scores)\n",
    "\n",
    "    # Convert to DataFrame for easy viewing\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = df_results[[\"model_id\", \"cv_score\", \"stability_score\", \"generalization_score\",\n",
    "                            \"selectivity_score\", \"composite_score\"]]\n",
    "    df_results = df_results.round(4)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_models_from_table()\n",
    "print(\"Model Evaluation Results (sorted by composite score):\")\n",
    "print(\"=\" * 80)\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Score Interpretation:\")\n",
    "print(\"- CV Score: Direct CV precision (higher = better)\")\n",
    "print(\"- Stability Score: Exponential penalty for high variance (higher = better)\")\n",
    "print(\"- Generalization Score: Exponential penalty for CV-Test gap (higher = better)\")\n",
    "print(\"- Selectivity Score: Preference for ~20K category 2 hits (higher = better)\")\n",
    "print(\"- Composite Score: Weighted combination (higher = better)\")\n",
    "\n",
    "# Show the best model\n",
    "best_model = results.iloc[0]\n",
    "print(f\"\\nBest Model: Model {int(best_model['model_id'])} with composite score: {best_model['composite_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcc5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Collect the data into a Pandas DataFrame\n",
    "pandas_df = joined_df.select(\"score\", \"score_priv\").toPandas()\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pandas_df[\"score\"], pandas_df[\"score_priv\"], alpha=0.005)\n",
    "plt.title(\"Scatter Plot of Score vs. Priv Score\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Priv Score\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between score and priv_score\n",
    "correlation = joined_df.corr(\"score\", \"score_priv\")\n",
    "\n",
    "# Show the correlation\n",
    "print(f\"Correlation between score and priv_score: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806423e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77123d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    new_pred.filter(f.col(\"score\") >= 0.5)\n",
    "    .groupBy(\"studyLocusId\")\n",
    "    .agg(f.collect_set(\"geneId\").alias(\"geneIds\"))\n",
    "    .withColumn(\n",
    "        \"hits\",\n",
    "        f.when(f.size(\"geneIds\") == 1, f.lit(\"1\"))\n",
    "        .when((f.size(\"geneIds\") == 2), f.lit(\"2\"))\n",
    "        .otherwise(f.lit(\">2\")),\n",
    "    )\n",
    "    .groupBy(\"hits\")\n",
    "    .count()\n",
    "    .orderBy(f.col(\"count\").desc())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cedf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix._df.filter(\n",
    "    (f.col(\"distanceTssMean\") > 1) | (f.col(\"distanceSentinelTss\") > 1)\n",
    ").select(\"distanceTssMean\", \"distanceSentinelTss\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_json(\"/Users/irenelopez/EBI/repos/gentropy/cttv008_2024-08-09.json.gz\", lines=True).query(\"studyId == 'NCT05329194'\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab499d9",
   "metadata": {},
   "source": [
    "## More intelligent splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96edfa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"goldStandardSet\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ae954",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_X = data_df[data_df[\"goldStandardSet\"] == 1][features_list].reset_index(drop=True)\n",
    "negative_X = data_df[data_df[\"goldStandardSet\"] == 0][features_list].reset_index(drop=True)\n",
    "positive_y = data_df[data_df[\"goldStandardSet\"] == 1][\"goldStandardSet\"].reset_index(drop=True)\n",
    "negative_y = data_df[data_df[\"goldStandardSet\"] == 0][\"goldStandardSet\"].reset_index(drop=True)\n",
    "positive_groups = data_df[data_df[\"goldStandardSet\"] == 1][\"geneId\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Positive X: {positive_X.shape[0]}\")\n",
    "print(f\"Negative X: {negative_X.shape[0]}\")\n",
    "print(f\"Positive y: {positive_y.shape[0]}\")\n",
    "print(f\"Negative y: {negative_y.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_train_test_split = GroupShuffleSplit(\n",
    "        n_splits=1, test_size=0.2, random_state=42\n",
    "    )\n",
    "for train_idx, test_idx in positives_train_test_split.split(positive_X, positive_y, positive_groups):\n",
    "    X_train_pos, X_test_pos = positive_X[train_idx], positive_X[test_idx]\n",
    "    y_train_pos, y_test_pos = positive_y[train_idx], positive_y[test_idx]\n",
    "    groups_train_pos = positive_groups[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776cdb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = data_df[data_df[\"goldStandardSet\"] == 1].copy()\n",
    "negatives = data_df[data_df[\"goldStandardSet\"] == 0].copy()\n",
    "\n",
    "print(f\"Total samples: {len(data_df)}\")\n",
    "print(f\"Positives: {len(positives)}\")\n",
    "print(f\"Negatives: {len(negatives)}\")\n",
    "print(f\"Unique genes in positives: {positives['geneId'].nunique()}\")\n",
    "print(f\"Unique studyLocusIds in positives: {positives['studyLocusId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96614a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group positives by geneId and split genes between train/test\n",
    "gene_groups = positives.groupby(\"geneId\").size().reset_index(name=\"count\")\n",
    "gene_groups = gene_groups.sort_values(\"count\", ascending=False)\n",
    "gene_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f10c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "genes_train, genes_test = train_test_split(\n",
    "        gene_groups[\"geneId\"].tolist(),\n",
    "        test_size=0.15,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "print(\"\\nGene-level split:\")\n",
    "print(f\"Genes in train: {len(genes_train)}\")\n",
    "print(f\"Genes in test: {len(genes_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split by studyLocusId within each gene group\n",
    "train_study_loci = set()\n",
    "test_study_loci = set()\n",
    "\n",
    "# Get studyLocusIds for train genes\n",
    "train_gene_positives = positives[positives[\"geneId\"].isin(genes_train)]\n",
    "train_study_loci.update(train_gene_positives[\"studyLocusId\"].unique())\n",
    "\n",
    "# Get studyLocusIds for test genes\n",
    "test_gene_positives = positives[positives[\"geneId\"].isin(genes_test)]\n",
    "test_study_loci.update(test_gene_positives[\"studyLocusId\"].unique())\n",
    "\n",
    "assert len(train_study_loci.intersection(test_study_loci)) == 0, \"Overlapping studyLocusIds found in both gene groups\"\n",
    "\n",
    "# Final positive splits\n",
    "train_positives = positives[positives[\"studyLocusId\"].isin(train_study_loci)]\n",
    "test_positives = positives[positives[\"studyLocusId\"].isin(test_study_loci)]\n",
    "\n",
    "print(\"\\nStudyLocusId-level split:\")\n",
    "print(f\"StudyLocusIds in train: {len(train_study_loci)}\")\n",
    "print(f\"StudyLocusIds in test: {len(test_study_loci)}\")\n",
    "print(f\"Positive samples in train: {len(train_positives)}\")\n",
    "print(f\"Positive samples in test: {len(test_positives)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Augment with negatives based on studyLocusId\n",
    "train_negatives = negatives[negatives[\"studyLocusId\"].isin(train_study_loci)]\n",
    "test_negatives = negatives[negatives[\"studyLocusId\"].isin(test_study_loci)]\n",
    "\n",
    "train_df = pd.concat([train_positives, train_negatives], ignore_index=True)\n",
    "test_df = pd.concat([test_positives, test_negatives], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation\n",
    "train_genes = set(train_df[\"geneId\"].unique())\n",
    "test_genes = set(test_df[\"geneId\"].unique())\n",
    "train_loci = set(train_df[\"studyLocusId\"].unique())\n",
    "test_loci = set(test_df[\"studyLocusId\"].unique())\n",
    "\n",
    "gene_overlap = train_genes.intersection(test_genes)\n",
    "loci_overlap = train_loci.intersection(test_loci)\n",
    "\n",
    "print(\"\\nFinal split statistics:\")\n",
    "print(f\"Train set: {len(train_df)} samples ({train_df['goldStandardSet'].sum()} positives)\")\n",
    "print(f\"Test set: {len(test_df)} samples ({test_df['goldStandardSet'].sum()} positives)\")\n",
    "print(f\"Gene overlap between splits (expected): {len(gene_overlap)}\")\n",
    "print(f\"StudyLocusId overlap between splits: {len(loci_overlap)}\")\n",
    "\n",
    "if loci_overlap:\n",
    "    print(\"Data leakage detected! Overlapping studyLocusIds between splits.\")\n",
    "else:\n",
    "    print(\"✓ No data leakage detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462467a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_split(data_df: pd.DataFrame,\n",
    "                      test_size: float = 0.15,\n",
    "                      random_state: int = 42,\n",
    "                      verbose: bool = True) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Implements hierarchical splitting strategy to prevent data leakage.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Split ALL genes (from both positives and negatives) into train/test groups\n",
    "    2. Assign studyLocusIds based on positive gene assignments\n",
    "    3. Filter all samples (positives and negatives) based on gene assignments\n",
    "    4. Resolve any studyLocusId conflicts to prevent leakage\n",
    "    \n",
    "    Args:\n",
    "        data_df (pd.DataFrame): Input dataframe with goldStandardSet column (1=positive, 0=negative)\n",
    "        test_size (float): Proportion of data for test set. Defaults to 0.15.\n",
    "        random_state (int): Random seed for reproducibility. Defaults to 42.\n",
    "        verbose (bool): Print splitting statistics. Defaults to True.\n",
    "        \n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: Training and test dataframes\n",
    "    \"\"\"\n",
    "    # Separate positives and negatives\n",
    "    positives = data_df[data_df[\"goldStandardSet\"] == 1].copy()\n",
    "    negatives = data_df[data_df[\"goldStandardSet\"] == 0].copy()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total samples: {len(data_df)}\")\n",
    "        print(f\"Positives: {len(positives)}\")\n",
    "        print(f\"Negatives: {len(negatives)}\")\n",
    "        print(f\"Unique genes in positives: {positives['geneId'].nunique()}\")\n",
    "        print(f\"Unique genes in negatives: {negatives['geneId'].nunique()}\")\n",
    "        print(f\"Unique genes overall: {data_df['geneId'].nunique()}\")\n",
    "        print(f\"Unique studyLocusIds in positives: {positives['studyLocusId'].nunique()}\")\n",
    "\n",
    "    # Step 1: Group positives by geneId and split genes between train/test\n",
    "    gene_groups = positives.groupby(\"geneId\").size().reset_index(name=\"count\")\n",
    "    gene_groups = gene_groups.sort_values(\"count\", ascending=False)\n",
    "\n",
    "    # Split genes maintaining approximate test_size proportion\n",
    "    genes_train, genes_test = train_test_split(\n",
    "        gene_groups[\"geneId\"].tolist(),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nGene-level split:\")\n",
    "        print(f\"Genes in train: {len(genes_train)}\")\n",
    "        print(f\"Genes in test: {len(genes_test)}\")\n",
    "\n",
    "    # Step 2: Split by studyLocusId within each gene group\n",
    "    train_study_loci = set()\n",
    "    test_study_loci = set()\n",
    "\n",
    "    # Get studyLocusIds for train genes\n",
    "    train_gene_positives = positives[positives[\"geneId\"].isin(genes_train)]\n",
    "    train_study_loci.update(train_gene_positives[\"studyLocusId\"].unique())\n",
    "\n",
    "    # Get studyLocusIds for test genes\n",
    "    test_gene_positives = positives[positives[\"geneId\"].isin(genes_test)]\n",
    "    test_study_loci.update(test_gene_positives[\"studyLocusId\"].unique())\n",
    "\n",
    "    # Check for overlap in studyLocusIds between train and test\n",
    "    assert len(train_study_loci.intersection(test_study_loci)) == 0, \"Overlapping studyLocusIds found in both gene groups\"\n",
    "\n",
    "    # Final positive splits\n",
    "    train_positives = positives[positives[\"studyLocusId\"].isin(train_study_loci)]\n",
    "    test_positives = positives[positives[\"studyLocusId\"].isin(test_study_loci)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nStudyLocusId-level split:\")\n",
    "        print(f\"StudyLocusIds in train: {len(train_study_loci)}\")\n",
    "        print(f\"StudyLocusIds in test: {len(test_study_loci)}\")\n",
    "        print(f\"Positive samples in train: {len(train_positives)}\")\n",
    "        print(f\"Positive samples in test: {len(test_positives)}\")\n",
    "\n",
    "    # Step 3: Augment with negatives based on studyLocusId\n",
    "    train_negatives = negatives[negatives[\"studyLocusId\"].isin(train_study_loci)]\n",
    "    test_negatives = negatives[negatives[\"studyLocusId\"].isin(test_study_loci)]\n",
    "\n",
    "    # Combine positives and negatives for final splits\n",
    "    train_df = pd.concat([train_positives, train_negatives], ignore_index=True)\n",
    "    test_df = pd.concat([test_positives, test_negatives], ignore_index=True)\n",
    "\n",
    "    # Final validation\n",
    "    train_genes = set(train_df[\"geneId\"].unique())\n",
    "    test_genes = set(test_df[\"geneId\"].unique())\n",
    "    train_loci = set(train_df[\"studyLocusId\"].unique())\n",
    "    test_loci = set(test_df[\"studyLocusId\"].unique())\n",
    "\n",
    "    gene_overlap = train_genes.intersection(test_genes)\n",
    "    loci_overlap = train_loci.intersection(test_loci)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nFinal split statistics:\")\n",
    "        print(f\"Train set: {len(train_df)} samples ({train_df['goldStandardSet'].sum()} positives)\")\n",
    "        print(f\"Test set: {len(test_df)} samples ({test_df['goldStandardSet'].sum()} positives)\")\n",
    "        print(f\"Gene overlap between splits (expected): {len(gene_overlap)}\")\n",
    "        print(f\"StudyLocusId overlap between splits (not expected): {len(loci_overlap)}\")\n",
    "\n",
    "        if loci_overlap:\n",
    "            print(\"Data leakage detected! Overlapping studyLocusIds between splits.\")\n",
    "        else:\n",
    "            print(\"✓ No data leakage detected!\")\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1633b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_split_quality(train_df: pd.DataFrame, test_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Analyze the quality of the hierarchical split.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df, test_df : pd.DataFrame\n",
    "        Training and test dataframes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary containing split quality metrics\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "\n",
    "    # Basic statistics\n",
    "    analysis[\"train_size\"] = len(train_df)\n",
    "    analysis[\"test_size\"] = len(test_df)\n",
    "    analysis[\"train_positives\"] = train_df[\"goldStandardSet\"].sum()\n",
    "    analysis[\"test_positives\"] = test_df[\"goldStandardSet\"].sum()\n",
    "    analysis[\"train_pos_ratio\"] = analysis[\"train_positives\"] / len(train_df)\n",
    "    analysis[\"test_pos_ratio\"] = analysis[\"test_positives\"] / len(test_df)\n",
    "\n",
    "    # Gene-level analysis\n",
    "    train_genes = set(train_df[\"geneId\"].unique())\n",
    "    test_genes = set(test_df[\"geneId\"].unique())\n",
    "    analysis[\"unique_genes_train\"] = len(train_genes)\n",
    "    analysis[\"unique_genes_test\"] = len(test_genes)\n",
    "    analysis[\"gene_overlap\"] = len(train_genes.intersection(test_genes))\n",
    "\n",
    "    # StudyLocusId-level analysis\n",
    "    train_loci = set(train_df[\"studyLocusId\"].unique())\n",
    "    test_loci = set(test_df[\"studyLocusId\"].unique())\n",
    "    analysis[\"unique_loci_train\"] = len(train_loci)\n",
    "    analysis[\"unique_loci_test\"] = len(test_loci)\n",
    "    analysis[\"loci_overlap\"] = len(train_loci.intersection(test_loci))\n",
    "\n",
    "    # Data leakage check\n",
    "    analysis[\"no_leakage\"] = (analysis[\"gene_overlap\"] == 0 and analysis[\"loci_overlap\"] == 0)\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "analysis = analyze_split_quality(train_df, test_df)\n",
    "print(\"\\nSplit Quality Analysis:\")\n",
    "for key, value in analysis.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12550ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = hierarchical_split(data_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e957ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_final_model(\n",
    "    X,\n",
    "    y,\n",
    "    groups,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,\n",
    "    run_cross_validation=True,\n",
    "    **hyperparameters,\n",
    "):\n",
    "    \"\"\"Implementation of the final training strategy.\"\"\"\n",
    "    # Create held-out test set separating EFO/Gene pairs between train/test\n",
    "    train_test_split = GroupShuffleSplit(\n",
    "        n_splits=1, test_size=test_size, random_state=42\n",
    "    )\n",
    "    for train_idx, test_idx in train_test_split.split(X, y, groups):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        groups_train = groups[train_idx]\n",
    "\n",
    "    # Cross-validation\n",
    "    if run_cross_validation:\n",
    "        cv_scores = []\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        print(\"Performing cross-validation...\")\n",
    "        for fold, (train_idx, val_idx) in enumerate(\n",
    "            gkf.split(X_train, y_train, groups_train)\n",
    "        ):\n",
    "            # Split data\n",
    "            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            model = GradientBoostingClassifier(\n",
    "                **hyperparameters, random_state=42, loss=\"log_loss\"\n",
    "            )\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            y_pred_proba = model.predict_proba(X_fold_val)[:, 1]\n",
    "            avg_precision = average_precision_score(y_fold_val, y_pred_proba)\n",
    "            cv_scores.append(avg_precision)\n",
    "\n",
    "            print(f\"Fold {fold + 1}: Average Precision = {avg_precision:.3f}\")\n",
    "\n",
    "        print(\n",
    "            f\"\\nCross-validation Average Precision: {np.mean(cv_scores):.3f} (+/- {np.std(cv_scores) * 2:.3f})\"\n",
    "        )\n",
    "\n",
    "    # Train final model on full training set\n",
    "    print(\"\\nTraining final model on full training set...\")\n",
    "    final_model = GradientBoostingClassifier(\n",
    "        **hyperparameters, random_state=42, loss=\"log_loss\"\n",
    "    )\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate once on test set\n",
    "    print(\"\\nEvaluating on held-out test set...\")\n",
    "    test_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    test_avg_precision = average_precision_score(y_test, test_pred_proba)\n",
    "\n",
    "    print(f\"Test Set Average Precision: {test_avg_precision:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"final_model\": final_model,\n",
    "        \"cv_scores\": cv_scores if run_cross_validation else None,\n",
    "        \"cv_mean\": np.mean(cv_scores) if run_cross_validation else None,\n",
    "        \"cv_std\": np.std(cv_scores) if run_cross_validation else None,\n",
    "        \"test_score\": test_avg_precision,\n",
    "        \"model\": model\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hierarchical_cross_validation_split(train_df, n_splits=5, random_state=42):\n",
    "    \"\"\"Creates cross-validation splits that maintain hierarchical gene/studyLocusId constraints.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df : pd.DataFrame\n",
    "        Training dataframe\n",
    "    n_splits : int, default=5\n",
    "        Number of CV folds\n",
    "    random_state : int, default=42\n",
    "        Random seed\n",
    "        \n",
    "    Yields:\n",
    "    -------\n",
    "    tuple\n",
    "        (train_indices, val_indices) for each fold\n",
    "    \"\"\"\n",
    "    # Get unique genes from positives only (maintaining hierarchy)\n",
    "    positives = train_df[train_df[\"goldStandardSet\"] == 1]\n",
    "    unique_genes = positives[\"geneId\"].unique()\n",
    "\n",
    "    # Split genes into folds\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(unique_genes)\n",
    "    gene_folds = np.array_split(unique_genes, n_splits)\n",
    "\n",
    "    for fold_idx in range(n_splits):\n",
    "        # Validation genes for this fold\n",
    "        val_genes = set(gene_folds[fold_idx])\n",
    "\n",
    "        # Training genes for this fold (all others)\n",
    "        train_genes = set()\n",
    "        for other_fold_idx in range(n_splits):\n",
    "            if other_fold_idx != fold_idx:\n",
    "                train_genes.update(gene_folds[other_fold_idx])\n",
    "\n",
    "        # Get studyLocusIds for each gene group (from positives only)\n",
    "        train_gene_positives = positives[positives[\"geneId\"].isin(train_genes)]\n",
    "        val_gene_positives = positives[positives[\"geneId\"].isin(val_genes)]\n",
    "\n",
    "        train_study_loci = set(train_gene_positives[\"studyLocusId\"].unique())\n",
    "        val_study_loci = set(val_gene_positives[\"studyLocusId\"].unique())\n",
    "\n",
    "        # Resolve conflicts (assign overlapping studyLocusIds to train)\n",
    "        overlapping_loci = train_study_loci.intersection(val_study_loci)\n",
    "        if overlapping_loci:\n",
    "            val_study_loci = val_study_loci - overlapping_loci\n",
    "            train_study_loci = train_study_loci.union(overlapping_loci)\n",
    "\n",
    "        # Get final indices for this fold\n",
    "        train_mask = (\n",
    "            (train_df[\"geneId\"].isin(train_genes)) &\n",
    "            (train_df[\"studyLocusId\"].isin(train_study_loci))\n",
    "        )\n",
    "        val_mask = (\n",
    "            (train_df[\"geneId\"].isin(val_genes)) &\n",
    "            (train_df[\"studyLocusId\"].isin(val_study_loci))\n",
    "        )\n",
    "\n",
    "        train_indices = train_df.index[train_mask].tolist()\n",
    "        val_indices = train_df.index[val_mask].tolist()\n",
    "\n",
    "        yield train_indices, val_indices\n",
    "\n",
    "\n",
    "def train_and_evaluate_with_presplit(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    feature_columns,\n",
    "    n_splits=5,\n",
    "    run_cross_validation=True,\n",
    "    target_column=\"goldStandardSet\",\n",
    "    **hyperparameters,\n",
    "):\n",
    "    \"\"\"Implementation of training strategy with hierarchical cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df : pd.DataFrame\n",
    "        Training dataframe from hierarchical split\n",
    "    test_df : pd.DataFrame\n",
    "        Test dataframe from hierarchical split\n",
    "    feature_columns : list\n",
    "        List of feature column names\n",
    "    n_splits : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    run_cross_validation : bool, default=True\n",
    "        Whether to perform cross-validation\n",
    "    target_column : str, default='goldStandardSet'\n",
    "        Name of target column\n",
    "    **hyperparameters : dict\n",
    "        Hyperparameters for GradientBoostingClassifier\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing model and evaluation results\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    X_train = train_df[feature_columns].values\n",
    "    y_train = train_df[target_column].values\n",
    "    X_test = test_df[feature_columns].values\n",
    "    y_test = test_df[target_column].values\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Training positives: {y_train.sum()}/{len(y_train)} ({y_train.mean():.3f})\")\n",
    "    print(f\"Test positives: {y_test.sum()}/{len(y_test)} ({y_test.mean():.3f})\")\n",
    "\n",
    "    # Cross-validation with hierarchical splits\n",
    "    cv_scores = []\n",
    "    if run_cross_validation:\n",
    "        print(f\"\\nPerforming hierarchical cross-validation ({n_splits} folds)...\")\n",
    "\n",
    "        for fold, (train_indices, val_indices) in enumerate(\n",
    "            hierarchical_cross_validation_split(train_df, n_splits=n_splits)\n",
    "        ):\n",
    "            if len(val_indices) == 0:\n",
    "                print(f\"Fold {fold + 1}: Skipped (no validation samples)\")\n",
    "                continue\n",
    "\n",
    "            # Convert DataFrame indices to array indices\n",
    "            train_array_idx = [train_df.index.get_loc(idx) for idx in train_indices]\n",
    "            val_array_idx = [train_df.index.get_loc(idx) for idx in val_indices]\n",
    "\n",
    "            # Split data for this fold\n",
    "            X_fold_train, X_fold_val = X_train[train_array_idx], X_train[val_array_idx]\n",
    "            y_fold_train, y_fold_val = y_train[train_array_idx], y_train[val_array_idx]\n",
    "\n",
    "            # Validate no gene/studyLocusId overlap\n",
    "            fold_train_genes = set(train_df.iloc[train_array_idx][\"geneId\"])\n",
    "            fold_val_genes = set(train_df.iloc[val_array_idx][\"geneId\"])\n",
    "            fold_train_loci = set(train_df.iloc[train_array_idx][\"studyLocusId\"])\n",
    "            fold_val_loci = set(train_df.iloc[val_array_idx][\"studyLocusId\"])\n",
    "\n",
    "            gene_overlap = len(fold_train_genes.intersection(fold_val_genes))\n",
    "            loci_overlap = len(fold_train_loci.intersection(fold_val_loci))\n",
    "\n",
    "            # Train model on fold\n",
    "            model = GradientBoostingClassifier(\n",
    "                **hyperparameters, random_state=42, loss=\"log_loss\"\n",
    "            )\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "            # Evaluate on validation fold\n",
    "            y_pred_proba = model.predict_proba(X_fold_val)[:, 1]\n",
    "            avg_precision = average_precision_score(y_fold_val, y_pred_proba)\n",
    "            cv_scores.append(avg_precision)\n",
    "\n",
    "            print(f\"Fold {fold + 1}: AP = {avg_precision:.3f} | \"\n",
    "                  f\"Train: {len(X_fold_train)} samples | Val: {len(X_fold_val)} samples | \"\n",
    "                  f\"Gene overlap: {gene_overlap} | Loci overlap: {loci_overlap}\")\n",
    "\n",
    "        if cv_scores:\n",
    "            print(\n",
    "                f\"\\nCross-validation Average Precision: {np.mean(cv_scores):.3f} (+/- {np.std(cv_scores) * 2:.3f})\"\n",
    "            )\n",
    "\n",
    "    # Train final model on full training set\n",
    "    print(\"\\nTraining final model on full training set...\")\n",
    "    final_model = GradientBoostingClassifier(\n",
    "        **hyperparameters, random_state=42, loss=\"log_loss\"\n",
    "    )\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on held-out test set\n",
    "    print(\"\\nEvaluating on held-out test set...\")\n",
    "    test_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    test_avg_precision = average_precision_score(y_test, test_pred_proba)\n",
    "\n",
    "    print(f\"Test Set Average Precision: {test_avg_precision:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"final_model\": final_model,\n",
    "        \"cv_scores\": cv_scores if run_cross_validation else None,\n",
    "        \"cv_mean\": np.mean(cv_scores) if cv_scores else None,\n",
    "        \"cv_std\": np.std(cv_scores) if cv_scores else None,\n",
    "        \"test_score\": test_avg_precision,\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_data_for_training(train_df, test_df, feature_columns, target_column=\"goldStandardSet\", group_column=\"studyLocusId\"):\n",
    "    \"\"\"Helper function to prepare data from hierarchical split for training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df : pd.DataFrame\n",
    "        Training dataframe from hierarchical_split\n",
    "    test_df : pd.DataFrame\n",
    "        Test dataframe from hierarchical_split\n",
    "    feature_columns : list\n",
    "        List of column names to use as features\n",
    "    target_column : str, default='goldStandardSet'\n",
    "        Name of target column\n",
    "    group_column : str, default='studyLocusId'\n",
    "        Name of group column for cross-validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (X_train, y_train, X_test, y_test, groups_train)\n",
    "    \"\"\"\n",
    "    # Prepare training data\n",
    "    X_train = train_df[feature_columns].values\n",
    "    y_train = train_df[target_column].values\n",
    "    groups_train = train_df[group_column].values\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = test_df[feature_columns].values\n",
    "    y_test = test_df[target_column].values\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Training positives: {y_train.sum()}/{len(y_train)} ({y_train.mean():.3f})\")\n",
    "    print(f\"Test positives: {y_test.sum()}/{len(y_test)} ({y_test.mean():.3f})\")\n",
    "    print(f\"Unique groups in training: {len(np.unique(groups_train))}\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, groups_train\n",
    "\n",
    "\n",
    "# Example usage workflow:\n",
    "def complete_training_workflow(data_df, feature_columns, hyperparameters, test_size=0.2, cv_folds=5):\n",
    "    \"\"\"Complete workflow: hierarchical split -> train and evaluate with hierarchical CV\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_df : pd.DataFrame\n",
    "        Full dataset\n",
    "    feature_columns : list\n",
    "        List of feature column names\n",
    "    hyperparameters : dict\n",
    "        Model hyperparameters\n",
    "    test_size : float, default=0.2\n",
    "        Test set proportion\n",
    "    cv_folds : int, default=5\n",
    "        Number of CV folds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (results_dict, train_df, test_df)\n",
    "    \"\"\"\n",
    "    # Step 1: Hierarchical split\n",
    "    print(\"=== Performing Hierarchical Split ===\")\n",
    "    train_df, test_df = hierarchical_split(data_df, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Step 2: Train and evaluate with hierarchical CV\n",
    "    print(\"\\n=== Training and Evaluating Model with Hierarchical CV ===\")\n",
    "    results = train_and_evaluate_with_presplit(\n",
    "        train_df, test_df, feature_columns,\n",
    "        n_splits=cv_folds,\n",
    "        **hyperparameters\n",
    "    )\n",
    "\n",
    "    return results, train_df, test_df\n",
    "\n",
    "hyperparameters = {\n",
    "    \"ccp_alpha\": 0.0,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"subsample\": 0.7,\n",
    "    \"tol\": 0.01,\n",
    "}\n",
    "\n",
    "results, train_df, test_df = complete_training_workflow(\n",
    "    data_df,\n",
    "    features_list,\n",
    "    hyperparameters,\n",
    "    test_size=0.15,\n",
    "    cv_folds=5\n",
    ")\n",
    "\n",
    "print(results)\n",
    "\n",
    "\"\"\"\n",
    "# OR use step by step:\n",
    "# 1. Split data hierarchically\n",
    "train_df, test_df = hierarchical_split(data_df, test_size=0.2)\n",
    "\n",
    "# 2. Train and evaluate with hierarchical CV\n",
    "results = train_and_evaluate_with_presplit(\n",
    "    train_df, test_df, feature_columns,\n",
    "    n_splits=5,\n",
    "    **hyperparameters\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def train_and_evaluate_final_model(\n",
    "    data_df,  # Changed from separate X, y, groups to full dataframe\n",
    "    feature_columns,  # List of feature column names\n",
    "    target_column=\"goldStandardSet\",  # Target column name\n",
    "    test_size=0.2,\n",
    "    n_splits=5,\n",
    "    run_cross_validation=True,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "    **hyperparameters,\n",
    "):\n",
    "    \"\"\"Implementation of the final training strategy using hierarchical splitting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_df : pd.DataFrame\n",
    "        Full dataframe containing features, target, geneId, and studyLocusId\n",
    "    feature_columns : list\n",
    "        List of column names to use as features\n",
    "    target_column : str, default='goldStandardSet'\n",
    "        Name of target column\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of data for test set\n",
    "    n_splits : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    run_cross_validation : bool, default=True\n",
    "        Whether to perform cross-validation\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "    verbose : bool, default=True\n",
    "        Print detailed information\n",
    "    **hyperparameters : dict\n",
    "        Additional hyperparameters for GradientBoostingClassifier\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing model, scores, and evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"=== Hierarchical Train/Test Split ===\")\n",
    "    # Create held-out test set using hierarchical splitting\n",
    "    train_df, test_df = hierarchical_split(\n",
    "        data_df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Prepare train and test data\n",
    "    X_train = train_df[feature_columns].values\n",
    "    y_train = train_df[target_column].values\n",
    "    X_test = test_df[feature_columns].values\n",
    "    y_test = test_df[target_column].values\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nTraining set: {len(X_train)} samples, {y_train.sum()} positives\")\n",
    "        print(f\"Test set: {len(X_test)} samples, {y_test.sum()} positives\")\n",
    "\n",
    "    # Cross-validation using hierarchical splitting\n",
    "    cv_scores = []\n",
    "    if run_cross_validation:\n",
    "        print(f\"\\n=== {n_splits}-Fold Cross-Validation with Hierarchical Splitting ===\")\n",
    "\n",
    "        # Create CV folds using hierarchical splitting\n",
    "        cv_train_dfs = []\n",
    "        cv_val_dfs = []\n",
    "\n",
    "        for fold in range(n_splits):\n",
    "            # Use different random seeds for each fold\n",
    "            fold_seed = random_state + fold\n",
    "\n",
    "            # Split training data hierarchically for this fold\n",
    "            cv_train, cv_val = hierarchical_split(\n",
    "                train_df,\n",
    "                test_size=1.0/n_splits,  # Approximate validation size\n",
    "                random_state=fold_seed,\n",
    "                verbose=False  # Reduce verbosity for CV folds\n",
    "            )\n",
    "\n",
    "            cv_train_dfs.append(cv_train)\n",
    "            cv_val_dfs.append(cv_val)\n",
    "\n",
    "        # Train and evaluate each fold\n",
    "        for fold in range(n_splits):\n",
    "            if verbose:\n",
    "                print(f\"\\nFold {fold + 1}/{n_splits}:\")\n",
    "\n",
    "            # Get fold data\n",
    "            fold_train_df = cv_train_dfs[fold]\n",
    "            fold_val_df = cv_val_dfs[fold]\n",
    "\n",
    "            X_fold_train = fold_train_df[feature_columns].values\n",
    "            y_fold_train = fold_train_df[target_column].values\n",
    "            X_fold_val = fold_val_df[feature_columns].values\n",
    "            y_fold_val = fold_val_df[target_column].values\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"  Train: {len(X_fold_train)} samples, {y_fold_train.sum()} positives\")\n",
    "                print(f\"  Val: {len(X_fold_val)} samples, {y_fold_val.sum()} positives\")\n",
    "\n",
    "                # Check for data leakage in this fold\n",
    "                train_genes = set(fold_train_df[\"geneId\"].unique())\n",
    "                val_genes = set(fold_val_df[\"geneId\"].unique())\n",
    "                train_loci = set(fold_train_df[\"studyLocusId\"].unique())\n",
    "                val_loci = set(fold_val_df[\"studyLocusId\"].unique())\n",
    "\n",
    "                gene_overlap = len(train_genes.intersection(val_genes))\n",
    "                loci_overlap = len(train_loci.intersection(val_loci))\n",
    "\n",
    "                if gene_overlap > 0 or loci_overlap > 0:\n",
    "                    print(f\"  Warning: Fold {fold + 1} has leakage - Gene overlap: {gene_overlap}, Loci overlap: {loci_overlap}\")\n",
    "\n",
    "            # Train model for this fold\n",
    "            fold_model = GradientBoostingClassifier(\n",
    "                **hyperparameters,\n",
    "                random_state=random_state,\n",
    "                loss=\"log_loss\"\n",
    "            )\n",
    "\n",
    "            fold_model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "            # Predict and evaluate\n",
    "            y_pred_proba = fold_model.predict_proba(X_fold_val)[:, 1]\n",
    "            avg_precision = average_precision_score(y_fold_val, y_pred_proba)\n",
    "            cv_scores.append(avg_precision)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"  Average Precision: {avg_precision:.4f}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nCross-validation Results:\")\n",
    "            print(f\"  Mean Average Precision: {np.mean(cv_scores):.4f}\")\n",
    "            print(f\"  Std Average Precision: {np.std(cv_scores):.4f}\")\n",
    "            print(f\"  95% CI: {np.mean(cv_scores):.4f} ± {np.std(cv_scores) * 1.96:.4f}\")\n",
    "\n",
    "    # Train final model on full training set\n",
    "    print(\"\\n=== Training Final Model ===\")\n",
    "    final_model = GradientBoostingClassifier(\n",
    "        **hyperparameters,\n",
    "        random_state=random_state,\n",
    "        loss=\"log_loss\"\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Training on {len(X_train)} samples with {len(feature_columns)} features...\")\n",
    "\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on held-out test set\n",
    "    print(\"\\n=== Final Test Set Evaluation ===\")\n",
    "    test_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    test_avg_precision = average_precision_score(y_test, test_pred_proba)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Test Set Average Precision: {test_avg_precision:.4f}\")\n",
    "\n",
    "    # Return comprehensive results\n",
    "    results = {\n",
    "        \"final_model\": final_model,\n",
    "        \"train_df\": train_df,\n",
    "        \"test_df\": test_df,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"test_predictions\": test_pred_proba,\n",
    "        \"cv_scores\": cv_scores if run_cross_validation else None,\n",
    "        \"cv_mean\": np.mean(cv_scores) if run_cross_validation and cv_scores else None,\n",
    "        \"cv_std\": np.std(cv_scores) if run_cross_validation and cv_scores else None,\n",
    "        \"test_score\": test_avg_precision,\n",
    "        \"feature_columns\": feature_columns,\n",
    "        \"hyperparameters\": hyperparameters\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def simple_train_and_evaluate(\n",
    "    data_df,\n",
    "    feature_columns,\n",
    "    target_column=\"goldStandardSet\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    **hyperparameters\n",
    "):\n",
    "    \"\"\"Simplified version for quick model training and evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_df : pd.DataFrame\n",
    "        Full dataframe containing features, target, geneId, and studyLocusId\n",
    "    feature_columns : list\n",
    "        List of column names to use as features\n",
    "    target_column : str, default='goldStandardSet'\n",
    "        Name of target column\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of data for test set\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "    **hyperparameters : dict\n",
    "        Additional hyperparameters for GradientBoostingClassifier\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing model and test score\n",
    "    \"\"\"\n",
    "    return train_and_evaluate_final_model(\n",
    "        data_df=data_df,\n",
    "        feature_columns=feature_columns,\n",
    "        target_column=target_column,\n",
    "        test_size=test_size,\n",
    "        n_splits=0,  # Skip cross-validation\n",
    "        run_cross_validation=False,\n",
    "        random_state=random_state,\n",
    "        verbose=True,\n",
    "        **hyperparameters\n",
    "    )\n",
    "\n",
    "\n",
    "results = train_and_evaluate_final_model(\n",
    "    data_df=data_df,\n",
    "    feature_columns=features_list,\n",
    "    **hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcf3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = results[\"final_model\"]\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skops.io as sio\n",
    "\n",
    "sio.dump(model, \"classifier.skops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db966258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
