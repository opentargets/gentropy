{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Targets Gentropy","text":"<p>Open Targets Gentropy is a Python package to facilitate the interpretation and analysis of GWAS and functional genomic studies for target identification. This package contains a toolkit for the harmonisation, statistical analysis and prioritisation of genetic signals to assist drug discovery.</p>"},{"location":"#key-features","title":"Key Features:","text":"<ul> <li>Specialized Datatypes: Introduces essential genetics datatypes like StudyLocus, LocusToGene, and SummaryStatistics.</li> <li>Performance-Oriented: Optimized for large-scale genetic data analysis, including locus-to-gene scoring, fine mapping, and colocalization analysis.</li> <li>User-Friendly: The package is designed to be intuitive, allowing both beginners and experienced researchers to conduct complex genetic with ease.</li> </ul>"},{"location":"#about-open-targets","title":"About Open Targets","text":"<p>Open Targets is a pre-competitive, public-private partnership that uses human genetics and genomics data to systematically identify and prioritise drug targets. Through large-scale genomic experiments and the development of innovative computational techniques, the partnership aims to help researchers select the best targets for the development of new therapies. For more information, visit the Open Targets website.</p>"},{"location":"installation/","title":"Installation","text":"<p>Note</p> <p>In the early stages of development, we are using Python version 3.10. We recommend using pyenv or similar tools to manage your local Python version. We intend to support more Python versions in the future.</p>"},{"location":"installation/#pypi","title":"Pypi","text":"<p>We recommend installing Open Targets Gentropy using Pypi:</p> <pre><code>pip install gentropy\n</code></pre>"},{"location":"installation/#source","title":"Source","text":"<p>Alternatively, you can install Open Targets Gentropy from source. Check the contributing section for more information.</p>"},{"location":"installation/#uninstall","title":"Uninstall","text":"<pre><code>pip uninstall gentropy -y\n</code></pre> <p>For any issues with the installation, check the troubleshooting section.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>The Open Targets core team is working on refactoring Open Targets Genetics, aiming to:</p> <ul> <li>Re-focus the product around Target ID</li> <li>Create a gold standard toolkit for post-GWAS analysis</li> <li>Faster/robust addition of new datasets and datatypes</li> <li>Reduce computational and financial cost</li> </ul> <p>See here for a list of open issues for this project.</p> <p>Schematic diagram representing the drafted process:</p> <p></p>"},{"location":"development/_development/","title":"Development","text":"<p>This section contains various technical information on how to develop and run the code.</p>"},{"location":"development/contributing/","title":"Contributing guidelines","text":""},{"location":"development/contributing/#one-time-configuration","title":"One-time configuration","text":"<p>The steps in this section only ever need to be done once on any particular system.</p> <p>For Google Cloud configuration:</p> <ol> <li> <p>Install Google Cloud SDK: https://cloud.google.com/sdk/docs/install.</p> </li> <li> <p>Log in to your work Google Account: run <code>gcloud auth login</code> and follow instructions.</p> </li> <li> <p>Obtain Google application credentials: run <code>gcloud auth application-default login</code> and follow instructions.</p> </li> </ol> <p>Check that you have the <code>make</code> utility installed, and if not (which is unlikely), install it using your system package manager.</p> <p>Java support</p> <p>Check that you have <code>java</code> installed. To be able to use all features including hail support use java 11 (for handling multiple java versions, consider using <code>sdkman</code>).</p>"},{"location":"development/contributing/#environment-configuration","title":"Environment configuration","text":"<p>Run <code>make setup-dev</code> to install/update the necessary packages (including required python version for development) and activate the development environment. You need to do it just once.</p> <p>It is recommended to use VS Code as an IDE for development.</p>"},{"location":"development/contributing/#how-to-create-gentropy-step","title":"How to create gentropy step","text":"<p>All gentropy steps can be invoked after successful environment configuration by running</p> <pre><code>uv run gentropy step=&lt;step_name&gt;\n</code></pre> <ol> <li> <p>Create a new step config in the <code>src/gentropy/config.py</code> that inherits from <code>StepConfig</code> class.</p> </li> <li> <p>Register new step configuration to <code>ConfigStore</code>.</p> </li> <li> <p>Create a step class that holds the business logic in new file in the <code>src/gentropy/{your_step_name}.py</code>.</p> </li> </ol>"},{"location":"development/contributing/#contributing-checklist","title":"Contributing checklist","text":"<p>When making changes, and especially when implementing a new module or feature, it's essential to ensure that all relevant sections of the code base are modified.</p> <ul> <li> Run <code>make check</code>. This will run the linter and formatter to ensure that the code is compliant with the project conventions.</li> <li> Develop unit tests for your code and run <code>make test</code>. This will run all unit tests in the repository, including the examples appended in the docstrings of some methods.</li> <li> Update the configuration if necessary.</li> <li> Update the documentation and check it with <code>make build-documentation</code>. This will start a local server to browse it (URL will be printed, usually <code>http://127.0.0.1:8000/</code>)</li> </ul> <p>For more details on each of these steps, see the sections below.</p>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<ul> <li>If during development you had a question which wasn't covered in the documentation, and someone explained it to you, add it to the documentation. The same applies if you encountered any instructions in the documentation which were obsolete or incorrect.</li> <li>Documentation autogeneration expressions start with <code>:::</code>. They will automatically generate sections of the documentation based on class and method docstrings. Be sure to update them for:</li> <li>Datasource main page, for example: <code>docs/python_api/datasources/finngen/_finngen.md</code></li> <li>Dataset definitions, for example: <code>docs/python_api/datasources/finngen/study_index.md</code></li> <li>Step definition, for example: <code>docs/python_api/steps/finngen_sumstat_preprocess.md</code></li> </ul>"},{"location":"development/contributing/#configuration","title":"Configuration","text":"<ul> <li>step default configuration in the <code>src/gentropy/config/</code> <code>StepConfig</code> derived classes.</li> </ul>"},{"location":"development/contributing/#classes","title":"Classes","text":"<ul> <li>Datasource init, for example: <code>src/gentropy/datasource/finngen/__init__.py</code></li> <li>Dataset classes, for example: <code>src/gentropy/datasource/finngen/study_index.py</code> \u2192 <code>FinnGenStudyIndex</code></li> <li>Step main running class, for example: <code>src/gentropy/finngen_sumstat_preprocess.py</code></li> </ul>"},{"location":"development/contributing/#tests","title":"Tests","text":"<ul> <li>Test study fixture in <code>tests/conftest.py</code>, for example: <code>mock_study_index_finngen</code> in that module</li> <li>Test sample data, for example: <code>tests/gentropy/data_samples/finngen_studies_sample.json</code></li> <li>Test definition, for example: <code>tests/dataset/test_study_index.py</code> \u2192 <code>test_study_index_finngen_creation</code>)</li> </ul>"},{"location":"development/contributing/#airflow-dags","title":"Airflow dags","text":"<ul> <li>Upstream of version 2.0.0 airflow orchestration layer was moved to the orchestration repository</li> </ul>"},{"location":"development/contributing/#support-for-python-versions","title":"Support for python versions","text":"<p>As of version 2.1.X gentropy supports multiple python versions. To ensure compatibility with all supported versions, unit tests are run for each of the minor python release from 3.10 to 3.12. Make sure your changes are compatible with all supported versions.</p>"},{"location":"development/contributing/#development-process","title":"Development process","text":"<p>The development follows simplified Git Flow process that includes usage of</p> <ul> <li><code>dev</code> (development branch)</li> <li><code>feature</code> branches</li> <li><code>main</code> (production branch)</li> </ul> <p>The development starts with creating new <code>feature</code> branch based on the <code>dev</code> branch. Once the feature is ready, the Pull Request for the <code>dev</code> branch is created and CI/CD Checks are performed to ensure that the code is compliant with the project conventions. Once the PR is approved, the feature branch is merged into the <code>dev</code> branch.</p>"},{"location":"development/contributing/#development-releases","title":"Development releases","text":"<p>One can create the dev release tagged by <code>vX.Y.Z-dev.V</code> tag. This release will not trigger the CI/CD pipeline to publish the package to the PyPi repository. The release is done by triggering the <code>Release</code> GitHub action.</p>"},{"location":"development/contributing/#production-releases","title":"Production releases","text":"<p>Once per week, the <code>Trigger PR for release</code> github action creates a Pull Request from <code>dev</code> to <code>main</code> branch, when the PR is approved, the <code>Release</code> GitHub action is triggered to create a production release tagged by <code>vX.Y.Z</code> tag. This release triggers the CI/CD pipeline to publish the package to the TestPyPi repository. If it is successful, then the actual deployment to the PyPI repository is done. The deployment to the PyPi repository must be verified by the gentropy maintainer.</p> <p>Below you can find a simplified diagram of the development process.</p>"},{"location":"development/troubleshooting/","title":"Troubleshooting","text":""},{"location":"development/troubleshooting/#blaslapack","title":"BLAS/LAPACK","text":"<p>If you see errors related to BLAS/LAPACK libraries, see this StackOverflow post for guidance.</p>"},{"location":"development/troubleshooting/#uv","title":"UV","text":"<p>The default python version and gentropy dependencies are managed by uv. To perform a fresh installation run <code>make setup-dev</code>.</p>"},{"location":"development/troubleshooting/#adding-new-dependencies-or-updating-existing-ones","title":"Adding new dependencies or updating existing ones","text":"<p>To add new dependencies or update existing ones, you need to update the <code>pyproject.toml</code> file. This can be done automatically with <code>uv add ${package}</code> command. Refer to the uv documentation for more information.</p>"},{"location":"development/troubleshooting/#java","title":"Java","text":"<p>Officially, PySpark requires Java version 8, or 11, 17. To support hail (gentropy dependency) it is recommended to use Java 11.</p>"},{"location":"development/troubleshooting/#pre-commit","title":"Pre-commit","text":"<p>If you see an error message thrown by pre-commit, which looks like this (<code>SyntaxError: Unexpected token '?'</code>), followed by a JavaScript traceback, the issue is likely with your system NodeJS version.</p> <p>One solution which can help in this case is to upgrade your system NodeJS version. However, this may not always be possible. For example, Ubuntu repository is several major versions behind the latest version as of July 2023.</p> <p>Another solution which helps is to remove Node, NodeJS, and npm from your system entirely. In this case, pre-commit will not try to rely on a system version of NodeJS and will install its own, suitable one.</p> <p>On Ubuntu, this can be done using <code>sudo apt remove node nodejs npm</code>, followed by <code>sudo apt autoremove</code>. But in some cases, depending on your existing installation, you may need to also manually remove some files. See this StackOverflow answer for guidance.</p>"},{"location":"development/troubleshooting/#macos","title":"MacOS","text":"<p>Some functions on MacOS may throw a java error:</p> <p><code>python3.10/site-packages/py4j/protocol.py:326: Py4JJavaError</code></p> <p>This can be resolved by adding the follow line to your <code>~/.zshrc</code>:</p> <p><code>export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES</code></p>"},{"location":"development/troubleshooting/#creating-development-dataproc-cluster-ot-users-only","title":"Creating development dataproc cluster (OT users only)","text":"<p>Requirements</p> <p>To create the cluster, you need to auth to the google cloud</p> <pre><code>gcloud auth login\n</code></pre> <p>To start dataproc cluster in the development mode run.</p> <pre><code>make create-dev-cluster REF=dev\n</code></pre> <p><code>REF</code> - remote branch available at the gentropy repository</p> <p>During cluster initialization actions the <code>utils/install_dependencies_on_cluster.sh</code> script is run, that installs <code>gentropy</code> package from the remote repository by using VCS support, hence it does not require the gentropy package whl artifact to be prepared in the Google Cloud Storage before the make command can be run.</p> <p>Check details how to make a package installable by VCS in pip documentation.</p> <p>How <code>create-dev-cluster</code> works</p> <p>This command will work, provided you have done one of:</p> <ul> <li>run <code>make create-dev-cluster REF=dev</code>, since the REF is requested, the cluster will attempt to install it from the remote repository.</li> <li>run <code>make create-dev-cluster</code> without specifying the REF or specifying REF that points to your local branch will request branch name you are checkout on your local repository, if any changes are pending locally, the cluster can not be created, it requires stashing or pushing the changes to the remote.</li> </ul> <p>The command will create a new dataproc cluster with the following configuration:</p> <ul> <li>package installed from the requested REF (for example <code>dev</code> or <code>feature/xxx</code>)</li> <li>uv installed in the cluster (to speed up the installation and dependency resolution process)</li> <li>cli script to run gentropy steps</li> </ul> <p>Dataproc cluster timeout</p> <p>By default the cluster will delete itself when running for 60 minutes after the last submitted job to the cluster was successfully completed (running jobs interactively via Jupyter or Jupyter lab is not treated as submitted job). To preserve the cluster for arbitrary period (for instance when the cluster is used only for interactive jobs) increase the cluster timeout:</p> <pre><code>make create-dev-cluster CLUSTER_TIMEOUT=1d REF=dev # 60m 1h 1d (by default 60m)\n</code></pre> <p>For the reference on timeout format check gcloud documentation</p>"},{"location":"howto/_howto/","title":"How-to","text":"<p>This page contains a collection of how-to guides for the project.</p> <ul> <li>Command line interface: Learn how to use the Gentropy CLI.</li> <li>Python API: Learn how to use the Gentropy Python package.</li> </ul> <p>For additional information please visit https://community.opentargets.org/</p>"},{"location":"howto/command_line/_command_line/","title":"Command line interface","text":"<p>Gentropy steps can be run using the command line interface (CLI). This section contains a collection of how-to guides for the CLI.</p>"},{"location":"howto/command_line/run_step_in_cli/","title":"Run step in CLI","text":"<p>To run a step in the command line interface (CLI), you need to know the step's name. To list what steps are avaiable in your current environment, simply run <code>gentropy</code> with no arguments. This will list all the steps:</p> <pre><code>You must specify 'step', e.g, step=&lt;OPTION&gt;\nAvailable options:\n        clump\n        colocalisation\n        eqtl_catalogue\n        finngen_studies\n        finngen_sumstat_preprocess\n        gwas_catalog_ingestion\n        gwas_catalog_sumstat_preprocess\n        ld_index\n        locus_to_gene\n        overlaps\n        pics\n        ukbiobank\n        variant_annotation\n        variant_index\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n</code></pre> <p>As indicated, you can run a step by specifying the step's name with the <code>step</code> argument. For example, to run the <code>gwas_catalog_sumstat_preprocess</code> step, you can run:</p> <pre><code>gentropy step=gwas_catalog_sumstat_preprocess\n</code></pre> <p>In most occassions, some mandatory values will be required to run the step. For example, the <code>gwas_catalog_sumstat_preprocess</code> step requires the <code>step.raw_sumstats_path</code> and <code>step.out_sumstats_path</code> argument to be specified. You can complete the necessary arguments by adding them to the command line:</p> <pre><code>gentropy step=gwas_catalog_sumstat_preprocess step.raw_sumstats_path=/path/to/raw_sumstats step.out_sumstats_path=/path/to/out_sumstats\n</code></pre> <p>You can find more about the available steps in the documentation.</p>"},{"location":"howto/command_line/run_step_using_config/","title":"Run step using YAML config","text":"<p>It's possible to parametrise the functionality of a step using a YAML configuration file. This is useful when you want to run a step multiple times with different parameters or simply to avoid having to specify the same parameters every time you run a step.</p> <p>Info</p> <p>The package uses Hydra to handle configuration files. For more information, please visit the Hydra documentation.</p> <p>To run a step using a configuration file, you need to create a configuration file in YAML format.</p> <pre><code>config/\n\u251c\u2500 step/\n\u2502  \u2514\u2500 my_gwas_catalog_sumstat_preprocess.md\n\u2514\u2500 my_config.yml\n</code></pre> <p>The configuration file should contain the parameters you want to use to run the step. For example, to run the <code>gwas_catalog_sumstat_preprocess</code> step, you need to specify the <code>step.raw_sumstats_path</code> and <code>step.out_sumstats_path</code> parameters. The configuration file should look like this:</p> my_config.yamlstep/my_gwas_catalog_sumstat_preprocess.md <pre><code>defaults:\n    - config\n    - _self_\n</code></pre> <p>This config file will specify that your configuration file will inherit the default configuration (<code>config</code>) and everything provided (<code>_self_</code>) will overwrite the default configuration.</p> <pre><code>defaults:\n    - gwas_catalog_sumstat_preprocess\n\nraw_sumstats_path: /path/to/raw_sumstats\nout_sumstats_path: /path/to/out_sumstats\n</code></pre> <p>This config file will inherit the default configuration for the <code>gwas_catalog_sumstat_preprocess</code> step and overwrite the <code>raw_sumstats_path</code> and <code>out_sumstats_path</code> parameters.</p> <p>Once you have created the configuration file, you can run your own new <code>my_gwas_catalog_sumstat_preprocess</code>:</p> <pre><code>gentropy step=my_gwas_catalog_sumstat_preprocess --config-dir=config --config-name=my_config\n</code></pre>"},{"location":"howto/python_api/_python_api/","title":"Python API","text":"<p>This section explains how to use gentropy in a Python environment providing a foundational understanding on how to perform genetics analyses using the package. This section can be useful for users wishing to use Gentropy in their own projects.</p>"},{"location":"howto/python_api/a_creating_spark_session/","title":"Creating a Spark Session","text":"<p>In this section, we'll guide you through creating a Spark session using Gentropy's Session class. Gentropy uses Apache PySpark as the underlying framework for distributed computing. The Session class provides a convenient way to initialize a Spark session with pre-configured settings.</p>"},{"location":"howto/python_api/a_creating_spark_session/#creating-a-default-session","title":"Creating a Default Session","text":"<p>To begin your journey with Gentropy, start by creating a default Spark session. This is the simplest way to initialize your environment.</p> <pre><code>from gentropy.common.session import Session\n\n# Create a default Spark Session\nsession = Session()\n</code></pre> <p>The above code snippet sets up a default Spark session with pre-configured settings. This is ideal for getting started quickly without needing to tweak any configurations.</p>"},{"location":"howto/python_api/a_creating_spark_session/#customizing-your-spark-session","title":"Customizing Your Spark Session","text":"<p>Gentropy allows you to customize the Spark session to suit your specific needs. You can modify various parameters such as memory allocation, number of executors, and more. This flexibility is particularly useful for optimizing performance in steps that are more computationally intensive.</p>"},{"location":"howto/python_api/a_creating_spark_session/#example-increasing-driver-memory","title":"Example: Increasing Driver Memory","text":"<p>If you require more memory for the Spark driver, you can easily adjust this setting:</p> <pre><code>from gentropy.common.session import Session\n\n# Create a Spark session with increased driver memory\nsession = Session(extended_spark_conf={\"spark.driver.memory\": \"4g\"})\n</code></pre> <p>This code snippet demonstrates how to increase the memory allocated to the Spark driver to 16 gigabytes. You can customize other Spark settings similarly, according to your project's requirements.</p>"},{"location":"howto/python_api/a_creating_spark_session/#whats-next","title":"What's next?","text":"<p>Now that you've created a Spark session, you're ready to start using Gentropy. In the next section, we'll show you how to process a large dataset using Gentropy's powerful SummaryStatistics datatype.</p>"},{"location":"howto/python_api/b_create_dataset/","title":"Create a dataset","text":"<p>Gentropy provides a collection of <code>Dataset</code>s that encapsulate key concepts in the field of genetics. For example, to represent summary statistics, you'll use the <code>SummaryStatistics</code> class. This datatype comes with a set of useful operations to disentangle the genetic architecture of a trait or disease.</p> <p>The full list of <code>Dataset</code>s is available in the Python API documentation.</p> <p>Any instance of Dataset will have 2 common attributes</p> <ul> <li>df: the Spark DataFrame that contains the data</li> <li>schema: the definition of the data structure in Spark format</li> </ul> <p>In this section you'll learn the different ways of how to create a <code>Dataset</code> instances.</p>"},{"location":"howto/python_api/b_create_dataset/#creating-a-dataset-from-parquet","title":"Creating a dataset from parquet","text":"<p>All the <code>Dataset</code>s have a <code>from_parquet</code> method that allows you to create any <code>Dataset</code> instance from a parquet file or directory.</p> <pre><code># Create a SummaryStatistics object by loading data from the specified path\nfrom gentropy.dataset.summary_statistics import SummaryStatistics\n\npath = \"path/to/summary/stats\"\nsummary_stats = SummaryStatistics.from_parquet(session, path)\n</code></pre> <p>Parquet files</p> <p>Parquet is a columnar storage format that is widely used in the Spark ecosystem. It is the recommended format for storing large datasets. For more information about parquet, please visit https://parquet.apache.org/.</p>"},{"location":"howto/python_api/b_create_dataset/#creating-a-dataset-from-a-data-source","title":"Creating a dataset from a data source","text":"<p>Alternatively, <code>Dataset</code>s can be created using a data source harmonisation method. For example, to create a <code>SummaryStatistics</code> object from Finngen's raw summary statistics, you can use the <code>FinnGen</code> data source.</p> <pre><code># Create a SummaryStatistics object by loading raw data from Finngen\nfrom gentropy.datasource.finngen.summary_stats import FinnGenSummaryStats\n\npath = \"path/to/finngen/summary/stats\"\nsummary_stats = FinnGenSummaryStats.from_source(session.spark, path)\n</code></pre>"},{"location":"howto/python_api/b_create_dataset/#creating-a-dataset-from-a-pandas-dataframe","title":"Creating a dataset from a pandas DataFrame","text":"<p>If none of our data sources fit your needs, you can create a <code>Dataset</code> object from your own data. To do so, you need to transform your data to fit the <code>Dataset</code> schema.</p> <p>The schema of a Dataset is defined in Spark format</p> <p>The Dataset schemas can be found in the documentation of each Dataset. For example, the schema of the <code>SummaryStatistics</code> dataset can be found here.</p> <p>You can also create a <code>Dataset</code> from a pandas DataFrame. This is useful when you want to create a <code>Dataset</code> from a small dataset that fits in memory.</p> <pre><code>import pyspark.pandas as ps\n\nfrom gentropy.dataset.summary_statistics import SummaryStatistics\n\n\n# Load your transformed data into a pandas DataFrame\npath = \"path/to/your/data\"\ncustom_summary_stats_pandas_df = pd.read_csv(path)\n\n# Create a SummaryStatistics object specifying the data and schema\ncustom_summary_stats_df = custom_summary_stats_pandas_df.to_spark()\ncustom_summary_stats = SummaryStatistics(\n    _df=custom_summary_stats_df, _schema=SummaryStatistics.get_schema()\n)\n</code></pre>"},{"location":"howto/python_api/b_create_dataset/#whats-next","title":"What's next?","text":"<p>In the next section, we will explore how to apply well-established algorithms that transform and analyse genetic data within the Gentropy framework.</p>"},{"location":"howto/python_api/c_applying_methods/","title":"Applying methods","text":"<p>The available methods implement well established algorithms that transform and analyse data. Methods usually take as input predefined <code>Dataset</code>(s) and produce one or several <code>Dataset</code>(s) as output. This section explains how to apply methods to your data.</p> <p>The full list of available methods can be found in the Python API documentation.</p>"},{"location":"howto/python_api/c_applying_methods/#apply-a-class-method","title":"Apply a class method","text":"<p>Some methods are implemented as class methods. For example, the <code>finemap</code> method is a class method of the <code>PICS</code> class. This method performs fine-mapping using the PICS algorithm. These methods usually take as input one or several <code>Dataset</code>(s) and produce one or several <code>Dataset</code>(s) as output.</p> <pre><code>from gentropy.method.pics import PICS\n\nfinemapped_study_locus = PICS.finemap(\n    study_locus_ld_annotated\n).annotate_credible_sets()\n</code></pre>"},{"location":"howto/python_api/c_applying_methods/#apply-a-dataset-instance-method","title":"Apply a <code>Dataset</code> instance method","text":"<p>Some methods are implemented as instance methods of the <code>Dataset</code> class. For example, the <code>window_based_clumping</code> method is an instance method of the <code>SummaryStatistics</code> class. This method performs window-based clumping on summary statistics.</p> <pre><code># Perform window-based clumping on summary statistics\n# By default, the method uses a 1Mb window and a p-value threshold of 5e-8\nclumped_summary_statistics = summary_stats.window_based_clumping()\n</code></pre> <p>The <code>window_based_clumping</code> method is also available as a class method</p> <p>The <code>window_based_clumping</code> method is also available as a class method of the <code>WindowBasedClumping</code> class. This method performs window-based clumping on summary statistics.</p> <pre><code># Perform window-based clumping on summary statistics\nfrom gentropy.method.window_based_clumping import WindowBasedClumping\n\nclumped_summary_statistics = WindowBasedClumping.clump(\n    summary_stats, distance=250_000\n)\n</code></pre>"},{"location":"howto/python_api/c_applying_methods/#whats-next","title":"What's next?","text":"<p>Up next, we'll show you how to inspect your data to ensure its integrity and the success of your transformations.</p>"},{"location":"howto/python_api/d_inspect_dataset/","title":"Inspect a dataset","text":"<p>We have seen how to create and transform a <code>Dataset</code> instance. This section guides you through inspecting your data to ensure its integrity and the success of your transformations.</p>"},{"location":"howto/python_api/d_inspect_dataset/#inspect-data-in-a-dataset","title":"Inspect data in a <code>Dataset</code>","text":"<p>The <code>df</code> attribute of a Dataset instance is key to interacting with and inspecting the stored data.</p> <p>By accessing the df attribute, you can apply any method that you would typically use on a PySpark DataFrame. See the PySpark documentation for more information.</p>"},{"location":"howto/python_api/d_inspect_dataset/#view-data-samples","title":"View data samples","text":"<pre><code># Inspect the first 10 rows of the data\nsummary_stats.df.show(10)\n</code></pre> <p>This method displays the first 10 rows of your dataset, giving you a snapshot of your data's structure and content.</p>"},{"location":"howto/python_api/d_inspect_dataset/#filter-data","title":"Filter data","text":"<pre><code>import pyspark.sql.functions as f\n\n# Filter summary statistics to only include associations in chromosome 22\nfiltered = summary_stats.filter(condition=f.col(\"chromosome\") == \"22\")\n</code></pre> <p>This method allows you to filter your data based on specific conditions, such as the value of a column. The application of any filter will create a new instance of the <code>Dataset</code> with the filtered data.</p>"},{"location":"howto/python_api/d_inspect_dataset/#understand-the-schema","title":"Understand the schema","text":"<pre><code># Get the Spark schema of any `Dataset` as a `StructType` object\nschema = summary_stats.get_schema()\n\n# Inspect the first 10 rows of the data\nsummary_stats.df.show(10)\n</code></pre>"},{"location":"howto/python_api/d_inspect_dataset/#write-a-dataset-to-disk","title":"Write a <code>Dataset</code> to disk","text":"<pre><code># Write the data to disk in parquet format\nsummary_stats.df.write.parquet(\"path/to/summary/stats\")\n\n# Write the data to disk in csv format\nsummary_stats.df.write.csv(\"path/to/summary/stats\")\n</code></pre> <p>Consider the format's compatibility with your tools, and the partitioning strategy for large datasets to optimize performance.</p>"},{"location":"python_api/_python_api/","title":"Python API","text":"<p>Open Targets Gentropy is a Python package to facilitate the interpretation and analysis of GWAS and functional genomic studies for target identification. The package contains a toolkit for the harmonisation, statistical analysis and prioritisation of genetic signals to assist drug discovery.</p> <p>The overall architecture of the package distinguishes between:</p> <ul> <li>Data Sources: data sources harmonisation tools</li> <li>Datasets: data model</li> <li>Methods: statistical analysis tools</li> <li>Steps: pipeline steps</li> <li>Common: Common classes</li> </ul>"},{"location":"python_api/common/_common/","title":"Common","text":"<p>Common utilities used in gentropy package.</p> <ul> <li>Genomic Region: class to represent genomic regions</li> <li>Types: Literal types used in the gentropy</li> </ul>"},{"location":"python_api/common/genomic_region/","title":"Genomic Region","text":""},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.KnownGenomicRegions","title":"<code>gentropy.common.genomic_region.KnownGenomicRegions</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Known genomic regions in the human genome in string format.</p> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>class KnownGenomicRegions(Enum):\n    \"\"\"Known genomic regions in the human genome in string format.\"\"\"\n\n    MHC = \"chr6:25726063-33400556\"\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.GenomicRegion","title":"<code>gentropy.common.genomic_region.GenomicRegion</code>","text":"<p>Genomic regions of interest.</p> <p>Attributes:</p> Name Type Description <code>chromosome</code> <code>str</code> <p>Chromosome.</p> <code>start</code> <code>int</code> <p>Start position.</p> <code>end</code> <code>int</code> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>class GenomicRegion:\n    \"\"\"Genomic regions of interest.\n\n    Attributes:\n        chromosome (str): Chromosome.\n        start (int): Start position.\n        end (int):\n    \"\"\"\n\n    def __init__(self, chromosome: str, start: int, end: int) -&gt; None:\n        \"\"\"Class constructor.\n\n        Args:\n            chromosome (str): Chromosome.\n            start (int): Start position.\n            end (int): End position.\n        \"\"\"\n        self.chromosome = chromosome\n        self.start = start\n        self.end = end\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of the genomic region.\n\n        Returns:\n            str: Genomic region in chr:start-end format.\n        \"\"\"\n        return f\"{self.chromosome}:{self.start}-{self.end}\"\n\n    @classmethod\n    def from_string(cls: type[\"GenomicRegion\"], region: str) -&gt; \"GenomicRegion\":\n        \"\"\"Parse region string to chr:start-end.\n\n        Args:\n            region (str): Genomic region expected to follow chr##:#,###-#,### format or ##:####-#####.\n\n        Returns:\n            GenomicRegion: Genomic region object.\n\n        Raises:\n            ValueError: If the end and start positions cannot be casted to integer or not all three values value error is raised.\n\n        Examples:\n            &gt;&gt;&gt; print(GenomicRegion.from_string('chr6:28,510,120-33,480,577'))\n            6:28510120-33480577\n            &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-33480577'))\n            6:28510120-33480577\n            &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120'))\n            Traceback (most recent call last):\n                ...\n            ValueError: Genomic region should follow a ##:####-#### format.\n            &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-foo'))\n            Traceback (most recent call last):\n                ...\n            ValueError: Start and the end position of the region has to be integer.\n        \"\"\"\n        region = region.replace(\":\", \"-\").replace(\",\", \"\")\n        try:\n            chromosome, start_position, end_position = region.split(\"-\")\n        except ValueError as err:\n            raise ValueError(\n                \"Genomic region should follow a ##:####-#### format.\"\n            ) from err\n\n        try:\n            return cls(\n                chromosome=chromosome.replace(\"chr\", \"\"),\n                start=int(start_position),\n                end=int(end_position),\n            )\n        except ValueError as err:\n            raise ValueError(\n                \"Start and the end position of the region has to be integer.\"\n            ) from err\n\n    @classmethod\n    def from_known_genomic_region(\n        cls: type[\"GenomicRegion\"], region: KnownGenomicRegions\n    ) -&gt; \"GenomicRegion\":\n        \"\"\"Get known genomic region.\n\n        Args:\n            region (KnownGenomicRegions): Known genomic region.\n\n        Returns:\n            GenomicRegion: Genomic region object.\n\n        Examples:\n            &gt;&gt;&gt; print(GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC))\n            6:25726063-33400556\n        \"\"\"\n        return GenomicRegion.from_string(region.value)\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.GenomicRegion.__init__","title":"<code>__init__(chromosome: str, start: int, end: int) -&gt; None</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>chromosome</code> <code>str</code> <p>Chromosome.</p> required <code>start</code> <code>int</code> <p>Start position.</p> required <code>end</code> <code>int</code> <p>End position.</p> required Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>def __init__(self, chromosome: str, start: int, end: int) -&gt; None:\n    \"\"\"Class constructor.\n\n    Args:\n        chromosome (str): Chromosome.\n        start (int): Start position.\n        end (int): End position.\n    \"\"\"\n    self.chromosome = chromosome\n    self.start = start\n    self.end = end\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.GenomicRegion.from_known_genomic_region","title":"<code>from_known_genomic_region(region: KnownGenomicRegions) -&gt; GenomicRegion</code>  <code>classmethod</code>","text":"<p>Get known genomic region.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>KnownGenomicRegions</code> <p>Known genomic region.</p> required <p>Returns:</p> Name Type Description <code>GenomicRegion</code> <code>GenomicRegion</code> <p>Genomic region object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC))\n6:25726063-33400556\n</code></pre> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>@classmethod\ndef from_known_genomic_region(\n    cls: type[\"GenomicRegion\"], region: KnownGenomicRegions\n) -&gt; \"GenomicRegion\":\n    \"\"\"Get known genomic region.\n\n    Args:\n        region (KnownGenomicRegions): Known genomic region.\n\n    Returns:\n        GenomicRegion: Genomic region object.\n\n    Examples:\n        &gt;&gt;&gt; print(GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC))\n        6:25726063-33400556\n    \"\"\"\n    return GenomicRegion.from_string(region.value)\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.GenomicRegion.from_string","title":"<code>from_string(region: str) -&gt; GenomicRegion</code>  <code>classmethod</code>","text":"<p>Parse region string to chr:start-end.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>str</code> <p>Genomic region expected to follow chr##:#,###-#,### format or ##:####-#####.</p> required <p>Returns:</p> Name Type Description <code>GenomicRegion</code> <code>GenomicRegion</code> <p>Genomic region object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the end and start positions cannot be casted to integer or not all three values value error is raised.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(GenomicRegion.from_string('chr6:28,510,120-33,480,577'))\n6:28510120-33480577\n&gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-33480577'))\n6:28510120-33480577\n&gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120'))\nTraceback (most recent call last):\n    ...\nValueError: Genomic region should follow a ##:####-#### format.\n&gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-foo'))\nTraceback (most recent call last):\n    ...\nValueError: Start and the end position of the region has to be integer.\n</code></pre> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>@classmethod\ndef from_string(cls: type[\"GenomicRegion\"], region: str) -&gt; \"GenomicRegion\":\n    \"\"\"Parse region string to chr:start-end.\n\n    Args:\n        region (str): Genomic region expected to follow chr##:#,###-#,### format or ##:####-#####.\n\n    Returns:\n        GenomicRegion: Genomic region object.\n\n    Raises:\n        ValueError: If the end and start positions cannot be casted to integer or not all three values value error is raised.\n\n    Examples:\n        &gt;&gt;&gt; print(GenomicRegion.from_string('chr6:28,510,120-33,480,577'))\n        6:28510120-33480577\n        &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-33480577'))\n        6:28510120-33480577\n        &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120'))\n        Traceback (most recent call last):\n            ...\n        ValueError: Genomic region should follow a ##:####-#### format.\n        &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-foo'))\n        Traceback (most recent call last):\n            ...\n        ValueError: Start and the end position of the region has to be integer.\n    \"\"\"\n    region = region.replace(\":\", \"-\").replace(\",\", \"\")\n    try:\n        chromosome, start_position, end_position = region.split(\"-\")\n    except ValueError as err:\n        raise ValueError(\n            \"Genomic region should follow a ##:####-#### format.\"\n        ) from err\n\n    try:\n        return cls(\n            chromosome=chromosome.replace(\"chr\", \"\"),\n            start=int(start_position),\n            end=int(end_position),\n        )\n    except ValueError as err:\n        raise ValueError(\n            \"Start and the end position of the region has to be integer.\"\n        ) from err\n</code></pre>"},{"location":"python_api/common/types/","title":"Literal Types","text":""},{"location":"python_api/common/types/#gentropy.common.types","title":"<code>gentropy.common.types</code>","text":"<p>Types and type aliases used in the package.</p>"},{"location":"python_api/common/types/#gentropy.common.types.LD_Population","title":"<code>gentropy.common.types.LD_Population = Literal['afr', 'amr', 'asj', 'eas', 'est', 'fin', 'nfe', 'nwe', 'seu']</code>  <code>module-attribute</code>","text":""},{"location":"python_api/common/types/#gentropy.common.types.VariantPopulation","title":"<code>gentropy.common.types.VariantPopulation = Literal['afr', 'amr', 'ami', 'asj', 'eas', 'fin', 'nfe', 'mid', 'sas', 'remaining']</code>  <code>module-attribute</code>","text":""},{"location":"python_api/common/types/#gentropy.common.types.DataSourceType","title":"<code>gentropy.common.types.DataSourceType = Literal['gnomad', 'finngen', 'gwas_catalog', 'eqtl_catalog', 'ukbiobank', 'open_targets', 'intervals']</code>  <code>module-attribute</code>","text":""},{"location":"python_api/datasets/_datasets/","title":"Datasets","text":"<p>The Dataset classes define the data model behind Open Targets Gentropy. Every class inherits from the <code>Dataset</code> class and contains a dataframe with a predefined schema that can be found in the respective classes.</p>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset","title":"<code>gentropy.dataset.dataset.Dataset</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Open Targets Gentropy Dataset Interface.</p> <p>The <code>Dataset</code> interface is a wrapper around a Spark DataFrame with a predefined schema. Class allows for overwriting the schema with <code>_schema</code> parameter. If the <code>_schema</code> is not provided, the schema is inferred from the Dataset.get_schema specific method which must be implemented by the child classes.</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@dataclass\nclass Dataset(ABC):\n    \"\"\"Open Targets Gentropy Dataset Interface.\n\n    The `Dataset` interface is a wrapper around a Spark DataFrame with a predefined schema.\n    Class allows for overwriting the schema with `_schema` parameter.\n    If the `_schema` is not provided, the schema is inferred from the Dataset.get_schema specific\n    method which must be implemented by the child classes.\n    \"\"\"\n\n    _df: DataFrame\n    _schema: StructType | None = None\n\n    def __post_init__(self: Dataset) -&gt; None:\n        \"\"\"Post init.\n\n        Raises:\n            TypeError: If the type of the _df or _schema is not valid\n        \"\"\"\n        match self._df:\n            case DataFrame():\n                pass\n            case _:\n                raise TypeError(f\"Invalid type for _df: {type(self._df)}\")\n\n        match self._schema:\n            case None | t.StructType():\n                self.validate_schema()\n            case _:\n                raise TypeError(f\"Invalid type for _schema: {type(self._schema)}\")\n\n    @property\n    def df(self: Dataset) -&gt; DataFrame:\n        \"\"\"Dataframe included in the Dataset.\n\n        Returns:\n            DataFrame: Dataframe included in the Dataset\n        \"\"\"\n        return self._df\n\n    @df.setter\n    def df(self: Dataset, new_df: DataFrame) -&gt; None:  # noqa: CCE001\n        \"\"\"Dataframe setter.\n\n        Args:\n            new_df (DataFrame): New dataframe to be included in the Dataset\n        \"\"\"\n        self._df: DataFrame = new_df\n        self.validate_schema()\n\n    @property\n    def schema(self: Dataset) -&gt; StructType:\n        \"\"\"Dataframe expected schema.\n\n        Returns:\n            StructType: Dataframe expected schema\n        \"\"\"\n        return self._schema or self.get_schema()\n\n    @classmethod\n    def _process_class_params(\n        cls, params: dict[str, Any]\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"Separate class initialization parameters from spark session parameters.\n\n        Args:\n            params (dict[str, Any]): Combined parameters dictionary\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: (class_params, spark_params)\n        \"\"\"\n        # Get all field names from the class (including parent classes)\n        class_field_names = {\n            field.name\n            for cls_ in cls.__mro__\n            if hasattr(cls_, \"__dataclass_fields__\")\n            for field in cls_.__dataclass_fields__.values()\n        }\n        # Separate parameters\n        class_params = {k: v for k, v in params.items() if k in class_field_names}\n        spark_params = {k: v for k, v in params.items() if k not in class_field_names}\n        return class_params, spark_params\n\n    @classmethod\n    @abstractmethod\n    def get_schema(cls: type[Self]) -&gt; StructType:\n        \"\"\"Abstract method to get the schema. Must be implemented by child classes.\n\n        Returns:\n            StructType: Schema for the Dataset\n\n        Raises:\n                NotImplementedError: Must be implemented in the child classes\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in the child classes\")\n\n    @classmethod\n    def get_QC_column_name(cls: type[Self]) -&gt; str | None:\n        \"\"\"Abstract method to get the QC column name. Assumes None unless overriden by child classes.\n\n        Returns:\n            str | None: Column name\n        \"\"\"\n        return None\n\n    @classmethod\n    def get_QC_mappings(cls: type[Self]) -&gt; dict[str, str]:\n        \"\"\"Method to get the mapping between QC flag and corresponding QC category value.\n\n        Returns empty dict unless overriden by child classes.\n\n        Returns:\n            dict[str, str]: Mapping between flag name and QC column category value.\n        \"\"\"\n        return {}\n\n    @classmethod\n    def from_parquet(\n        cls: type[Self],\n        session: Session,\n        path: str | list[str],\n        **kwargs: bool | float | int | str | None,\n    ) -&gt; Self:\n        \"\"\"Reads parquet into a Dataset with a given schema.\n\n        Args:\n            session (Session): Spark session\n            path (str | list[str]): Path to the parquet dataset\n            **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.parquet\n\n        Returns:\n            Self: Dataset with the parquet file contents\n\n        Raises:\n            ValueError: Parquet file is empty\n        \"\"\"\n        schema = cls.get_schema()\n\n        # Separate class params from spark params\n        class_params, spark_params = cls._process_class_params(kwargs)\n\n        df = session.load_data(path, format=\"parquet\", schema=schema, **spark_params)\n        if df.isEmpty():\n            raise ValueError(f\"Parquet file is empty: {path}\")\n        return cls(_df=df, _schema=schema, **class_params)\n\n    def filter(self: Self, condition: Column) -&gt; Self:\n        \"\"\"Creates a new instance of a Dataset with the DataFrame filtered by the condition.\n\n        Preserves all attributes from the original instance.\n\n        Args:\n            condition (Column): Condition to filter the DataFrame\n\n        Returns:\n            Self: Filtered Dataset with preserved attributes\n        \"\"\"\n        filtered_df = self._df.filter(condition)\n        attrs = {k: v for k, v in self.__dict__.items() if k != \"_df\"}\n        return self.__class__(_df=filtered_df, **attrs)\n\n    def validate_schema(self: Dataset) -&gt; None:\n        \"\"\"Validate DataFrame schema against expected class schema.\n\n        Raises:\n            SchemaValidationError: If the DataFrame schema does not match the expected schema\n        \"\"\"\n        expected_schema = self.schema\n        observed_schema = self._df.schema\n\n        # Unexpected fields in dataset\n        if discrepancies := compare_struct_schemas(observed_schema, expected_schema):\n            raise SchemaValidationError(\n                f\"Schema validation failed for {type(self).__name__}\", discrepancies\n            )\n\n    def valid_rows(self: Self, invalid_flags: list[str], invalid: bool = False) -&gt; Self:\n        \"\"\"Filters `Dataset` according to a list of quality control flags. Only `Dataset` classes with a QC column can be validated.\n\n        This method checks do following steps:\n        - Check if the Dataset contains a QC column.\n        - Check if the invalid_flags exist in the QC mappings flags.\n        - Filter the Dataset according to the invalid_flags and invalid parameters.\n\n        Args:\n            invalid_flags (list[str]): List of quality control flags to be excluded.\n            invalid (bool): If True returns the invalid rows, instead of the valid. Defaults to False.\n\n        Returns:\n            Self: filtered dataset.\n\n        Raises:\n            ValueError: If the Dataset does not contain a QC column or if the invalid_flags elements do not exist in QC mappings flags.\n        \"\"\"\n        # If the invalid flags are not valid quality checks (enum) for this Dataset we raise an error:\n        invalid_reasons = []\n        for flag in invalid_flags:\n            if flag not in self.get_QC_mappings():\n                raise ValueError(\n                    f\"{flag} is not a valid QC flag for {type(self).__name__} ({self.get_QC_mappings()}).\"\n                )\n            reason = self.get_QC_mappings()[flag]\n            invalid_reasons.append(reason)\n\n        qc_column_name = self.get_QC_column_name()\n        # If Dataset (class) does not contain QC column we raise an error:\n        if not qc_column_name:\n            raise ValueError(\n                f\"{type(self).__name__} objects do not contain a QC column to filter by.\"\n            )\n        else:\n            column: str = qc_column_name\n            # If QC column (nullable) is not available in the dataframe we create an empty array:\n            qc = f.when(f.col(column).isNull(), f.array()).otherwise(f.col(column))\n\n        filterCondition = ~f.arrays_overlap(\n            f.array([f.lit(i) for i in invalid_reasons]), qc\n        )\n        # Returning the filtered dataset:\n        if invalid:\n            return self.filter(~filterCondition)\n        else:\n            return self.filter(filterCondition)\n\n    def drop_infinity_values(self: Self, *cols: str) -&gt; Self:\n        \"\"\"Drop infinity values from Double typed column.\n\n        Infinity type reference - https://spark.apache.org/docs/latest/sql-ref-datatypes.html#floating-point-special-values\n        The implementation comes from https://stackoverflow.com/questions/34432998/how-to-replace-infinity-in-pyspark-dataframe\n\n        Args:\n            *cols (str): names of the columns to check for infinite values, these should be of DoubleType only!\n\n        Returns:\n            Self: Dataset after removing infinite values\n        \"\"\"\n        if len(cols) == 0:\n            return self\n        inf_strings = (\"Inf\", \"+Inf\", \"-Inf\", \"Infinity\", \"+Infinity\", \"-Infinity\")\n        inf_values = [f.lit(v).cast(t.DoubleType()) for v in inf_strings]\n        conditions = [f.col(c).isin(inf_values) for c in cols]\n        # reduce individual filter expressions with or statement\n        # to col(\"beta\").isin([lit(Inf)]) | col(\"beta\").isin([lit(Inf)])...\n        condition = reduce(lambda a, b: a | b, conditions)\n        self.df = self._df.filter(~condition)\n        return self\n\n    def persist(self: Self) -&gt; Self:\n        \"\"\"Persist in memory the DataFrame included in the Dataset.\n\n        Returns:\n            Self: Persisted Dataset\n        \"\"\"\n        self.df = self._df.persist()\n        return self\n\n    def unpersist(self: Self) -&gt; Self:\n        \"\"\"Remove the persisted DataFrame from memory.\n\n        Returns:\n            Self: Unpersisted Dataset\n        \"\"\"\n        self.df = self._df.unpersist()\n        return self\n\n    def coalesce(self: Self, num_partitions: int, **kwargs: Any) -&gt; Self:\n        \"\"\"Coalesce the DataFrame included in the Dataset.\n\n        Coalescing is efficient for decreasing the number of partitions because it avoids a full shuffle of the data.\n\n        Args:\n            num_partitions (int): Number of partitions to coalesce to\n            **kwargs (Any): Arguments to pass to the coalesce method\n\n        Returns:\n            Self: Coalesced Dataset\n        \"\"\"\n        self.df = self._df.coalesce(num_partitions, **kwargs)\n        return self\n\n    def repartition(self: Self, num_partitions: int, **kwargs: Any) -&gt; Self:\n        \"\"\"Repartition the DataFrame included in the Dataset.\n\n        Repartitioning creates new partitions with data that is distributed evenly.\n\n        Args:\n            num_partitions (int): Number of partitions to repartition to\n            **kwargs (Any): Arguments to pass to the repartition method\n\n        Returns:\n            Self: Repartitioned Dataset\n        \"\"\"\n        self.df = self._df.repartition(num_partitions, **kwargs)\n        return self\n\n    @staticmethod\n    def update_quality_flag(\n        qc: Column, flag_condition: Column, flag_text: Enum\n    ) -&gt; Column:\n        \"\"\"Update the provided quality control list with a new flag if condition is met.\n\n        Args:\n            qc (Column): Array column with the current list of qc flags.\n            flag_condition (Column): This is a column of booleans, signing which row should be flagged\n            flag_text (Enum): Text for the new quality control flag\n\n        Returns:\n            Column: Array column with the updated list of qc flags.\n        \"\"\"\n        qc = f.when(qc.isNull(), f.array()).otherwise(qc)\n        return f.when(\n            flag_condition,\n            f.array_union(qc, f.array(f.lit(flag_text.value))),\n        ).otherwise(qc)\n\n    @staticmethod\n    def flag_duplicates(test_column: Column) -&gt; Column:\n        \"\"\"Return True for rows, where the value was already seen in column.\n\n        This implementation allows keeping the first occurrence of the value.\n\n        Args:\n            test_column (Column): Column to check for duplicates\n\n        Returns:\n            Column: Column with a boolean flag for duplicates\n        \"\"\"\n        return (\n            f.row_number().over(Window.partitionBy(test_column).orderBy(f.rand())) &gt; 1\n        )\n\n    @staticmethod\n    def generate_identifier(uniqueness_defining_columns: list[str]) -&gt; Column:\n        \"\"\"Hashes the provided columns to generate a unique identifier.\n\n        Args:\n            uniqueness_defining_columns (list[str]): list of columns defining uniqueness\n\n        Returns:\n            Column: column with a unique identifier\n        \"\"\"\n        hashable_columns = [\n            f.when(f.col(column).cast(\"string\").isNull(), f.lit(\"None\")).otherwise(\n                f.col(column).cast(\"string\")\n            )\n            for column in uniqueness_defining_columns\n        ]\n        return f.md5(f.concat(*hashable_columns))\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.df","title":"<code>df: DataFrame</code>  <code>property</code> <code>writable</code>","text":"<p>Dataframe included in the Dataset.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Dataframe included in the Dataset</p>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.schema","title":"<code>schema: StructType</code>  <code>property</code>","text":"<p>Dataframe expected schema.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Dataframe expected schema</p>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.coalesce","title":"<code>coalesce(num_partitions: int, **kwargs: Any) -&gt; Self</code>","text":"<p>Coalesce the DataFrame included in the Dataset.</p> <p>Coalescing is efficient for decreasing the number of partitions because it avoids a full shuffle of the data.</p> <p>Parameters:</p> Name Type Description Default <code>num_partitions</code> <code>int</code> <p>Number of partitions to coalesce to</p> required <code>**kwargs</code> <code>Any</code> <p>Arguments to pass to the coalesce method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Coalesced Dataset</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def coalesce(self: Self, num_partitions: int, **kwargs: Any) -&gt; Self:\n    \"\"\"Coalesce the DataFrame included in the Dataset.\n\n    Coalescing is efficient for decreasing the number of partitions because it avoids a full shuffle of the data.\n\n    Args:\n        num_partitions (int): Number of partitions to coalesce to\n        **kwargs (Any): Arguments to pass to the coalesce method\n\n    Returns:\n        Self: Coalesced Dataset\n    \"\"\"\n    self.df = self._df.coalesce(num_partitions, **kwargs)\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.drop_infinity_values","title":"<code>drop_infinity_values(*cols: str) -&gt; Self</code>","text":"<p>Drop infinity values from Double typed column.</p> <p>Infinity type reference - https://spark.apache.org/docs/latest/sql-ref-datatypes.html#floating-point-special-values The implementation comes from https://stackoverflow.com/questions/34432998/how-to-replace-infinity-in-pyspark-dataframe</p> <p>Parameters:</p> Name Type Description Default <code>*cols</code> <code>str</code> <p>names of the columns to check for infinite values, these should be of DoubleType only!</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Dataset after removing infinite values</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def drop_infinity_values(self: Self, *cols: str) -&gt; Self:\n    \"\"\"Drop infinity values from Double typed column.\n\n    Infinity type reference - https://spark.apache.org/docs/latest/sql-ref-datatypes.html#floating-point-special-values\n    The implementation comes from https://stackoverflow.com/questions/34432998/how-to-replace-infinity-in-pyspark-dataframe\n\n    Args:\n        *cols (str): names of the columns to check for infinite values, these should be of DoubleType only!\n\n    Returns:\n        Self: Dataset after removing infinite values\n    \"\"\"\n    if len(cols) == 0:\n        return self\n    inf_strings = (\"Inf\", \"+Inf\", \"-Inf\", \"Infinity\", \"+Infinity\", \"-Infinity\")\n    inf_values = [f.lit(v).cast(t.DoubleType()) for v in inf_strings]\n    conditions = [f.col(c).isin(inf_values) for c in cols]\n    # reduce individual filter expressions with or statement\n    # to col(\"beta\").isin([lit(Inf)]) | col(\"beta\").isin([lit(Inf)])...\n    condition = reduce(lambda a, b: a | b, conditions)\n    self.df = self._df.filter(~condition)\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.filter","title":"<code>filter(condition: Column) -&gt; Self</code>","text":"<p>Creates a new instance of a Dataset with the DataFrame filtered by the condition.</p> <p>Preserves all attributes from the original instance.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Column</code> <p>Condition to filter the DataFrame</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Filtered Dataset with preserved attributes</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def filter(self: Self, condition: Column) -&gt; Self:\n    \"\"\"Creates a new instance of a Dataset with the DataFrame filtered by the condition.\n\n    Preserves all attributes from the original instance.\n\n    Args:\n        condition (Column): Condition to filter the DataFrame\n\n    Returns:\n        Self: Filtered Dataset with preserved attributes\n    \"\"\"\n    filtered_df = self._df.filter(condition)\n    attrs = {k: v for k, v in self.__dict__.items() if k != \"_df\"}\n    return self.__class__(_df=filtered_df, **attrs)\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.flag_duplicates","title":"<code>flag_duplicates(test_column: Column) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Return True for rows, where the value was already seen in column.</p> <p>This implementation allows keeping the first occurrence of the value.</p> <p>Parameters:</p> Name Type Description Default <code>test_column</code> <code>Column</code> <p>Column to check for duplicates</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Column with a boolean flag for duplicates</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@staticmethod\ndef flag_duplicates(test_column: Column) -&gt; Column:\n    \"\"\"Return True for rows, where the value was already seen in column.\n\n    This implementation allows keeping the first occurrence of the value.\n\n    Args:\n        test_column (Column): Column to check for duplicates\n\n    Returns:\n        Column: Column with a boolean flag for duplicates\n    \"\"\"\n    return (\n        f.row_number().over(Window.partitionBy(test_column).orderBy(f.rand())) &gt; 1\n    )\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.from_parquet","title":"<code>from_parquet(session: Session, path: str | list[str], **kwargs: bool | float | int | str | None) -&gt; Self</code>  <code>classmethod</code>","text":"<p>Reads parquet into a Dataset with a given schema.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session</p> required <code>path</code> <code>str | list[str]</code> <p>Path to the parquet dataset</p> required <code>**kwargs</code> <code>bool | float | int | str | None</code> <p>Additional arguments to pass to spark.read.parquet</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Dataset with the parquet file contents</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Parquet file is empty</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@classmethod\ndef from_parquet(\n    cls: type[Self],\n    session: Session,\n    path: str | list[str],\n    **kwargs: bool | float | int | str | None,\n) -&gt; Self:\n    \"\"\"Reads parquet into a Dataset with a given schema.\n\n    Args:\n        session (Session): Spark session\n        path (str | list[str]): Path to the parquet dataset\n        **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.parquet\n\n    Returns:\n        Self: Dataset with the parquet file contents\n\n    Raises:\n        ValueError: Parquet file is empty\n    \"\"\"\n    schema = cls.get_schema()\n\n    # Separate class params from spark params\n    class_params, spark_params = cls._process_class_params(kwargs)\n\n    df = session.load_data(path, format=\"parquet\", schema=schema, **spark_params)\n    if df.isEmpty():\n        raise ValueError(f\"Parquet file is empty: {path}\")\n    return cls(_df=df, _schema=schema, **class_params)\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.generate_identifier","title":"<code>generate_identifier(uniqueness_defining_columns: list[str]) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Hashes the provided columns to generate a unique identifier.</p> <p>Parameters:</p> Name Type Description Default <code>uniqueness_defining_columns</code> <code>list[str]</code> <p>list of columns defining uniqueness</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>column with a unique identifier</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@staticmethod\ndef generate_identifier(uniqueness_defining_columns: list[str]) -&gt; Column:\n    \"\"\"Hashes the provided columns to generate a unique identifier.\n\n    Args:\n        uniqueness_defining_columns (list[str]): list of columns defining uniqueness\n\n    Returns:\n        Column: column with a unique identifier\n    \"\"\"\n    hashable_columns = [\n        f.when(f.col(column).cast(\"string\").isNull(), f.lit(\"None\")).otherwise(\n            f.col(column).cast(\"string\")\n        )\n        for column in uniqueness_defining_columns\n    ]\n    return f.md5(f.concat(*hashable_columns))\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.get_QC_column_name","title":"<code>get_QC_column_name() -&gt; str | None</code>  <code>classmethod</code>","text":"<p>Abstract method to get the QC column name. Assumes None unless overriden by child classes.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Column name</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@classmethod\ndef get_QC_column_name(cls: type[Self]) -&gt; str | None:\n    \"\"\"Abstract method to get the QC column name. Assumes None unless overriden by child classes.\n\n    Returns:\n        str | None: Column name\n    \"\"\"\n    return None\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.get_QC_mappings","title":"<code>get_QC_mappings() -&gt; dict[str, str]</code>  <code>classmethod</code>","text":"<p>Method to get the mapping between QC flag and corresponding QC category value.</p> <p>Returns empty dict unless overriden by child classes.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: Mapping between flag name and QC column category value.</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@classmethod\ndef get_QC_mappings(cls: type[Self]) -&gt; dict[str, str]:\n    \"\"\"Method to get the mapping between QC flag and corresponding QC category value.\n\n    Returns empty dict unless overriden by child classes.\n\n    Returns:\n        dict[str, str]: Mapping between flag name and QC column category value.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Abstract method to get the schema. Must be implemented by child classes.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the Dataset</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented in the child classes</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@classmethod\n@abstractmethod\ndef get_schema(cls: type[Self]) -&gt; StructType:\n    \"\"\"Abstract method to get the schema. Must be implemented by child classes.\n\n    Returns:\n        StructType: Schema for the Dataset\n\n    Raises:\n            NotImplementedError: Must be implemented in the child classes\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in the child classes\")\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.persist","title":"<code>persist() -&gt; Self</code>","text":"<p>Persist in memory the DataFrame included in the Dataset.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Persisted Dataset</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def persist(self: Self) -&gt; Self:\n    \"\"\"Persist in memory the DataFrame included in the Dataset.\n\n    Returns:\n        Self: Persisted Dataset\n    \"\"\"\n    self.df = self._df.persist()\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.repartition","title":"<code>repartition(num_partitions: int, **kwargs: Any) -&gt; Self</code>","text":"<p>Repartition the DataFrame included in the Dataset.</p> <p>Repartitioning creates new partitions with data that is distributed evenly.</p> <p>Parameters:</p> Name Type Description Default <code>num_partitions</code> <code>int</code> <p>Number of partitions to repartition to</p> required <code>**kwargs</code> <code>Any</code> <p>Arguments to pass to the repartition method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Repartitioned Dataset</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def repartition(self: Self, num_partitions: int, **kwargs: Any) -&gt; Self:\n    \"\"\"Repartition the DataFrame included in the Dataset.\n\n    Repartitioning creates new partitions with data that is distributed evenly.\n\n    Args:\n        num_partitions (int): Number of partitions to repartition to\n        **kwargs (Any): Arguments to pass to the repartition method\n\n    Returns:\n        Self: Repartitioned Dataset\n    \"\"\"\n    self.df = self._df.repartition(num_partitions, **kwargs)\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.unpersist","title":"<code>unpersist() -&gt; Self</code>","text":"<p>Remove the persisted DataFrame from memory.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Unpersisted Dataset</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def unpersist(self: Self) -&gt; Self:\n    \"\"\"Remove the persisted DataFrame from memory.\n\n    Returns:\n        Self: Unpersisted Dataset\n    \"\"\"\n    self.df = self._df.unpersist()\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.update_quality_flag","title":"<code>update_quality_flag(qc: Column, flag_condition: Column, flag_text: Enum) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Update the provided quality control list with a new flag if condition is met.</p> <p>Parameters:</p> Name Type Description Default <code>qc</code> <code>Column</code> <p>Array column with the current list of qc flags.</p> required <code>flag_condition</code> <code>Column</code> <p>This is a column of booleans, signing which row should be flagged</p> required <code>flag_text</code> <code>Enum</code> <p>Text for the new quality control flag</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Array column with the updated list of qc flags.</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@staticmethod\ndef update_quality_flag(\n    qc: Column, flag_condition: Column, flag_text: Enum\n) -&gt; Column:\n    \"\"\"Update the provided quality control list with a new flag if condition is met.\n\n    Args:\n        qc (Column): Array column with the current list of qc flags.\n        flag_condition (Column): This is a column of booleans, signing which row should be flagged\n        flag_text (Enum): Text for the new quality control flag\n\n    Returns:\n        Column: Array column with the updated list of qc flags.\n    \"\"\"\n    qc = f.when(qc.isNull(), f.array()).otherwise(qc)\n    return f.when(\n        flag_condition,\n        f.array_union(qc, f.array(f.lit(flag_text.value))),\n    ).otherwise(qc)\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.valid_rows","title":"<code>valid_rows(invalid_flags: list[str], invalid: bool = False) -&gt; Self</code>","text":"<p>Filters <code>Dataset</code> according to a list of quality control flags. Only <code>Dataset</code> classes with a QC column can be validated.</p> <p>This method checks do following steps: - Check if the Dataset contains a QC column. - Check if the invalid_flags exist in the QC mappings flags. - Filter the Dataset according to the invalid_flags and invalid parameters.</p> <p>Parameters:</p> Name Type Description Default <code>invalid_flags</code> <code>list[str]</code> <p>List of quality control flags to be excluded.</p> required <code>invalid</code> <code>bool</code> <p>If True returns the invalid rows, instead of the valid. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>filtered dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Dataset does not contain a QC column or if the invalid_flags elements do not exist in QC mappings flags.</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def valid_rows(self: Self, invalid_flags: list[str], invalid: bool = False) -&gt; Self:\n    \"\"\"Filters `Dataset` according to a list of quality control flags. Only `Dataset` classes with a QC column can be validated.\n\n    This method checks do following steps:\n    - Check if the Dataset contains a QC column.\n    - Check if the invalid_flags exist in the QC mappings flags.\n    - Filter the Dataset according to the invalid_flags and invalid parameters.\n\n    Args:\n        invalid_flags (list[str]): List of quality control flags to be excluded.\n        invalid (bool): If True returns the invalid rows, instead of the valid. Defaults to False.\n\n    Returns:\n        Self: filtered dataset.\n\n    Raises:\n        ValueError: If the Dataset does not contain a QC column or if the invalid_flags elements do not exist in QC mappings flags.\n    \"\"\"\n    # If the invalid flags are not valid quality checks (enum) for this Dataset we raise an error:\n    invalid_reasons = []\n    for flag in invalid_flags:\n        if flag not in self.get_QC_mappings():\n            raise ValueError(\n                f\"{flag} is not a valid QC flag for {type(self).__name__} ({self.get_QC_mappings()}).\"\n            )\n        reason = self.get_QC_mappings()[flag]\n        invalid_reasons.append(reason)\n\n    qc_column_name = self.get_QC_column_name()\n    # If Dataset (class) does not contain QC column we raise an error:\n    if not qc_column_name:\n        raise ValueError(\n            f\"{type(self).__name__} objects do not contain a QC column to filter by.\"\n        )\n    else:\n        column: str = qc_column_name\n        # If QC column (nullable) is not available in the dataframe we create an empty array:\n        qc = f.when(f.col(column).isNull(), f.array()).otherwise(f.col(column))\n\n    filterCondition = ~f.arrays_overlap(\n        f.array([f.lit(i) for i in invalid_reasons]), qc\n    )\n    # Returning the filtered dataset:\n    if invalid:\n        return self.filter(~filterCondition)\n    else:\n        return self.filter(filterCondition)\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.validate_schema","title":"<code>validate_schema() -&gt; None</code>","text":"<p>Validate DataFrame schema against expected class schema.</p> <p>Raises:</p> Type Description <code>SchemaValidationError</code> <p>If the DataFrame schema does not match the expected schema</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def validate_schema(self: Dataset) -&gt; None:\n    \"\"\"Validate DataFrame schema against expected class schema.\n\n    Raises:\n        SchemaValidationError: If the DataFrame schema does not match the expected schema\n    \"\"\"\n    expected_schema = self.schema\n    observed_schema = self._df.schema\n\n    # Unexpected fields in dataset\n    if discrepancies := compare_struct_schemas(observed_schema, expected_schema):\n        raise SchemaValidationError(\n            f\"Schema validation failed for {type(self).__name__}\", discrepancies\n        )\n</code></pre>"},{"location":"python_api/datasets/biosample_index/","title":"Biosample index","text":""},{"location":"python_api/datasets/biosample_index/#gentropy.dataset.biosample_index.BiosampleIndex","title":"<code>gentropy.dataset.biosample_index.BiosampleIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Biosample index dataset.</p> <p>A Biosample index dataset captures the metadata of the biosamples (e.g. tissues, cell types, cell lines, etc) such as alternate names and relationships with other biosamples.</p> Source code in <code>src/gentropy/dataset/biosample_index.py</code> <pre><code>@dataclass\nclass BiosampleIndex(Dataset):\n    \"\"\"Biosample index dataset.\n\n    A Biosample index dataset captures the metadata of the biosamples (e.g. tissues, cell types, cell lines, etc) such as alternate names and relationships with other biosamples.\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[BiosampleIndex]) -&gt; StructType:\n        \"\"\"Provide the schema for the BiosampleIndex dataset.\n\n        Returns:\n            StructType: The schema of the BiosampleIndex dataset.\n        \"\"\"\n        return parse_spark_schema(\"biosample_index.json\")\n\n    def merge_indices(\n        self: BiosampleIndex,\n        biosample_indices : list[BiosampleIndex]\n        ) -&gt; BiosampleIndex:\n        \"\"\"Merge a list of biosample indices into a single biosample index.\n\n        Where there are conflicts, in single values - the first value is taken. In list values, the union of all values is taken.\n\n        Args:\n            biosample_indices (list[BiosampleIndex]): Biosample indices to merge.\n\n        Returns:\n            BiosampleIndex: Merged biosample index.\n        \"\"\"\n        # Extract the DataFrames from the BiosampleIndex objects\n        biosample_dfs = [biosample_index.df for biosample_index in biosample_indices] + [self.df]\n\n        # Merge the DataFrames\n        merged_df = reduce(DataFrame.unionAll, biosample_dfs)\n\n        # Determine aggregation functions for each column\n        # Currently this will take the first value for single values and merge lists for list values\n        agg_funcs = []\n        for field in merged_df.schema.fields:\n            if field.name != \"biosampleId\":  # Skip the grouping column\n                if field.dataType == ArrayType(StringType()):\n                    agg_funcs.append(f.array_distinct(f.flatten(f.collect_list(field.name))).alias(field.name))\n                else:\n                    agg_funcs.append(f.first(f.col(field.name), ignorenulls=True).alias(field.name))\n\n        # Perform aggregation\n        aggregated_df = merged_df.groupBy(\"biosampleId\").agg(*agg_funcs)\n\n        return BiosampleIndex(\n            _df=aggregated_df,\n            _schema=BiosampleIndex.get_schema()\n            )\n\n    def retain_rows_with_ancestor_id(\n        self: BiosampleIndex,\n        ancestor_ids : list[str]\n        ) -&gt; BiosampleIndex:\n        \"\"\"Filter the biosample index to retain only rows with the given ancestor IDs.\n\n        Args:\n            ancestor_ids (list[str]): Ancestor IDs to filter on.\n\n        Returns:\n            BiosampleIndex: Filtered biosample index.\n        \"\"\"\n        # Create a Spark array of ancestor IDs prior to filtering\n        ancestor_ids_array = f.array(*[f.lit(id) for id in ancestor_ids])\n\n        return BiosampleIndex(\n            _df=self.df.filter(\n                f.size(f.array_intersect(f.col(\"ancestors\"), ancestor_ids_array)) &gt; 0\n            ),\n            _schema=BiosampleIndex.get_schema()\n            )\n</code></pre>"},{"location":"python_api/datasets/biosample_index/#gentropy.dataset.biosample_index.BiosampleIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provide the schema for the BiosampleIndex dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>The schema of the BiosampleIndex dataset.</p> Source code in <code>src/gentropy/dataset/biosample_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[BiosampleIndex]) -&gt; StructType:\n    \"\"\"Provide the schema for the BiosampleIndex dataset.\n\n    Returns:\n        StructType: The schema of the BiosampleIndex dataset.\n    \"\"\"\n    return parse_spark_schema(\"biosample_index.json\")\n</code></pre>"},{"location":"python_api/datasets/biosample_index/#gentropy.dataset.biosample_index.BiosampleIndex.merge_indices","title":"<code>merge_indices(biosample_indices: list[BiosampleIndex]) -&gt; BiosampleIndex</code>","text":"<p>Merge a list of biosample indices into a single biosample index.</p> <p>Where there are conflicts, in single values - the first value is taken. In list values, the union of all values is taken.</p> <p>Parameters:</p> Name Type Description Default <code>biosample_indices</code> <code>list[BiosampleIndex]</code> <p>Biosample indices to merge.</p> required <p>Returns:</p> Name Type Description <code>BiosampleIndex</code> <code>BiosampleIndex</code> <p>Merged biosample index.</p> Source code in <code>src/gentropy/dataset/biosample_index.py</code> <pre><code>def merge_indices(\n    self: BiosampleIndex,\n    biosample_indices : list[BiosampleIndex]\n    ) -&gt; BiosampleIndex:\n    \"\"\"Merge a list of biosample indices into a single biosample index.\n\n    Where there are conflicts, in single values - the first value is taken. In list values, the union of all values is taken.\n\n    Args:\n        biosample_indices (list[BiosampleIndex]): Biosample indices to merge.\n\n    Returns:\n        BiosampleIndex: Merged biosample index.\n    \"\"\"\n    # Extract the DataFrames from the BiosampleIndex objects\n    biosample_dfs = [biosample_index.df for biosample_index in biosample_indices] + [self.df]\n\n    # Merge the DataFrames\n    merged_df = reduce(DataFrame.unionAll, biosample_dfs)\n\n    # Determine aggregation functions for each column\n    # Currently this will take the first value for single values and merge lists for list values\n    agg_funcs = []\n    for field in merged_df.schema.fields:\n        if field.name != \"biosampleId\":  # Skip the grouping column\n            if field.dataType == ArrayType(StringType()):\n                agg_funcs.append(f.array_distinct(f.flatten(f.collect_list(field.name))).alias(field.name))\n            else:\n                agg_funcs.append(f.first(f.col(field.name), ignorenulls=True).alias(field.name))\n\n    # Perform aggregation\n    aggregated_df = merged_df.groupBy(\"biosampleId\").agg(*agg_funcs)\n\n    return BiosampleIndex(\n        _df=aggregated_df,\n        _schema=BiosampleIndex.get_schema()\n        )\n</code></pre>"},{"location":"python_api/datasets/biosample_index/#gentropy.dataset.biosample_index.BiosampleIndex.retain_rows_with_ancestor_id","title":"<code>retain_rows_with_ancestor_id(ancestor_ids: list[str]) -&gt; BiosampleIndex</code>","text":"<p>Filter the biosample index to retain only rows with the given ancestor IDs.</p> <p>Parameters:</p> Name Type Description Default <code>ancestor_ids</code> <code>list[str]</code> <p>Ancestor IDs to filter on.</p> required <p>Returns:</p> Name Type Description <code>BiosampleIndex</code> <code>BiosampleIndex</code> <p>Filtered biosample index.</p> Source code in <code>src/gentropy/dataset/biosample_index.py</code> <pre><code>def retain_rows_with_ancestor_id(\n    self: BiosampleIndex,\n    ancestor_ids : list[str]\n    ) -&gt; BiosampleIndex:\n    \"\"\"Filter the biosample index to retain only rows with the given ancestor IDs.\n\n    Args:\n        ancestor_ids (list[str]): Ancestor IDs to filter on.\n\n    Returns:\n        BiosampleIndex: Filtered biosample index.\n    \"\"\"\n    # Create a Spark array of ancestor IDs prior to filtering\n    ancestor_ids_array = f.array(*[f.lit(id) for id in ancestor_ids])\n\n    return BiosampleIndex(\n        _df=self.df.filter(\n            f.size(f.array_intersect(f.col(\"ancestors\"), ancestor_ids_array)) &gt; 0\n        ),\n        _schema=BiosampleIndex.get_schema()\n        )\n</code></pre>"},{"location":"python_api/datasets/biosample_index/#schema","title":"Schema","text":"<pre><code>root\n |-- biosampleId: string (nullable = false)\n |-- biosampleName: string (nullable = false)\n |-- description: string (nullable = true)\n |-- xrefs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- synonyms: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- parents: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- ancestors: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- descendants: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- children: array (nullable = true)\n |    |-- element: string (containsNull = true)\n</code></pre>"},{"location":"python_api/datasets/colocalisation/","title":"Colocalisation","text":""},{"location":"python_api/datasets/colocalisation/#gentropy.dataset.colocalisation.Colocalisation","title":"<code>gentropy.dataset.colocalisation.Colocalisation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Colocalisation results for pairs of overlapping study-locus.</p> Source code in <code>src/gentropy/dataset/colocalisation.py</code> <pre><code>@dataclass\nclass Colocalisation(Dataset):\n    \"\"\"Colocalisation results for pairs of overlapping study-locus.\"\"\"\n\n    @classmethod\n    def get_schema(cls: type[Colocalisation]) -&gt; StructType:\n        \"\"\"Provides the schema for the Colocalisation dataset.\n\n        Returns:\n            StructType: Schema for the Colocalisation dataset\n        \"\"\"\n        return parse_spark_schema(\"colocalisation.json\")\n\n    def extract_maximum_coloc_probability_per_region_and_gene(\n        self: Colocalisation,\n        study_locus: StudyLocus,\n        study_index: StudyIndex,\n        *,\n        filter_by_colocalisation_method: str,\n        filter_by_qtls: str | list[str] | None = None,\n    ) -&gt; DataFrame:\n        \"\"\"Get maximum colocalisation probability for a (studyLocus, gene) window.\n\n        Args:\n            study_locus (StudyLocus): Dataset containing study loci to filter the colocalisation dataset on and the geneId linked to the region\n            study_index (StudyIndex): Study index to use to get study metadata\n            filter_by_colocalisation_method (str): optional filter to apply on the colocalisation dataset\n            filter_by_qtls (str | list[str] | None): optional filter to apply on the colocalisation dataset\n\n        Returns:\n            DataFrame: table with the maximum colocalisation scores for the provided study loci\n\n        Raises:\n            ValueError: if filter_by_qtl is not in the list of valid QTL types or is not in the list of valid colocalisation methods\n        \"\"\"\n        from gentropy.colocalisation import ColocalisationStep\n\n        valid_qtls = list(\n            set(EqtlCatalogueStudyIndex.method_to_qtl_type_mapping.values())\n        ) + [\n            f\"sc{qtl}\"\n            for qtl in set(\n                EqtlCatalogueStudyIndex.method_to_qtl_type_mapping.values()\n            )\n        ]\n\n        if filter_by_qtls:\n            filter_by_qtls = (\n                list(map(str.lower, [filter_by_qtls]))\n                if isinstance(filter_by_qtls, str)\n                else list(map(str.lower, filter_by_qtls))\n            )\n            if any(qtl not in valid_qtls for qtl in filter_by_qtls):\n                raise ValueError(f\"There are no studies with QTL type {filter_by_qtls}\")\n\n        if filter_by_colocalisation_method not in [\n            \"ECaviar\",\n            \"Coloc\",\n        ]:  # TODO: Write helper class to retrieve coloc method names\n            raise ValueError(\n                f\"Colocalisation method {filter_by_colocalisation_method} is not supported.\"\n            )\n\n        method_colocalisation_metric = ColocalisationStep._get_colocalisation_class(\n            filter_by_colocalisation_method\n        ).METHOD_METRIC\n\n        coloc_filtering_expr = [\n            f.col(\"rightGeneId\").isNotNull(),\n            f.lower(\"colocalisationMethod\") == filter_by_colocalisation_method.lower(),\n        ]\n        if filter_by_qtls:\n            coloc_filtering_expr.append(f.lower(\"rightStudyType\").isin(filter_by_qtls))\n\n        filtered_colocalisation = (\n            # Bring rightStudyType and rightGeneId and filter by rows where the gene is null,\n            # which is equivalent to filtering studyloci from gwas on the right side\n            self.append_study_metadata(\n                study_locus,\n                study_index,\n                metadata_cols=[\"geneId\", \"studyType\"],\n                colocalisation_side=\"right\",\n            )\n            # it also filters based on method and qtl type\n            .filter(reduce(lambda a, b: a &amp; b, coloc_filtering_expr))\n            # and filters colocalisation results to only include the subset of studylocus that contains gwas studylocusid\n            .join(\n                study_locus.df.selectExpr(\"studyLocusId as leftStudyLocusId\"),\n                \"leftStudyLocusId\",\n            )\n        )\n\n        return get_record_with_maximum_value(\n            filtered_colocalisation.withColumnRenamed(\n                \"leftStudyLocusId\", \"studyLocusId\"\n            ).withColumnRenamed(\"rightGeneId\", \"geneId\"),\n            [\"studyLocusId\", \"geneId\"],\n            method_colocalisation_metric,\n        )\n\n    def append_study_metadata(\n        self: Colocalisation,\n        study_locus: StudyLocus,\n        study_index: StudyIndex,\n        *,\n        metadata_cols: list[str],\n        colocalisation_side: str = \"right\",\n    ) -&gt; DataFrame:\n        \"\"\"Appends metadata from the study to the requested side of the colocalisation dataset.\n\n        Args:\n            study_locus (StudyLocus): Dataset containing study loci that links the colocalisation dataset and the study index via the studyId\n            study_index (StudyIndex): Dataset containing study index that contains the metadata\n            metadata_cols (list[str]): List of study columns to append\n            colocalisation_side (str): Which side of the colocalisation dataset to append metadata to. Must be either 'right' or 'left'\n\n        Returns:\n            DataFrame: Colocalisation dataset with appended metadata of the study from the requested side\n\n        Raises:\n            ValueError: if colocalisation_side is not 'right' or 'left'\n        \"\"\"\n        metadata_cols = [\"studyId\", *metadata_cols]\n        if colocalisation_side not in [\"right\", \"left\"]:\n            raise ValueError(\n                f\"colocalisation_side must be either 'right' or 'left', got {colocalisation_side}\"\n            )\n\n        study_loci_w_metadata = (\n            study_locus.df.select(\"studyLocusId\", \"studyId\")\n            .join(\n                f.broadcast(study_index.df.select(\"studyId\", *metadata_cols)),\n                \"studyId\",\n            )\n            .distinct()\n        )\n        coloc_df = (\n            # drop `rightStudyType` in case it is requested\n            self.df.drop(\"rightStudyType\")\n            if \"studyType\" in metadata_cols and colocalisation_side == \"right\"\n            else self.df\n        )\n        return (\n            # Append that to the respective side of the colocalisation dataset\n            study_loci_w_metadata.selectExpr(\n                f\"studyLocusId as {colocalisation_side}StudyLocusId\",\n                *[\n                    f\"{col} as {colocalisation_side}{col[0].upper() + col[1:]}\"\n                    for col in metadata_cols\n                ],\n            ).join(coloc_df, f\"{colocalisation_side}StudyLocusId\", \"right\")\n        )\n</code></pre>"},{"location":"python_api/datasets/colocalisation/#gentropy.dataset.colocalisation.Colocalisation.append_study_metadata","title":"<code>append_study_metadata(study_locus: StudyLocus, study_index: StudyIndex, *, metadata_cols: list[str], colocalisation_side: str = 'right') -&gt; DataFrame</code>","text":"<p>Appends metadata from the study to the requested side of the colocalisation dataset.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus</code> <code>StudyLocus</code> <p>Dataset containing study loci that links the colocalisation dataset and the study index via the studyId</p> required <code>study_index</code> <code>StudyIndex</code> <p>Dataset containing study index that contains the metadata</p> required <code>metadata_cols</code> <code>list[str]</code> <p>List of study columns to append</p> required <code>colocalisation_side</code> <code>str</code> <p>Which side of the colocalisation dataset to append metadata to. Must be either 'right' or 'left'</p> <code>'right'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Colocalisation dataset with appended metadata of the study from the requested side</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if colocalisation_side is not 'right' or 'left'</p> Source code in <code>src/gentropy/dataset/colocalisation.py</code> <pre><code>def append_study_metadata(\n    self: Colocalisation,\n    study_locus: StudyLocus,\n    study_index: StudyIndex,\n    *,\n    metadata_cols: list[str],\n    colocalisation_side: str = \"right\",\n) -&gt; DataFrame:\n    \"\"\"Appends metadata from the study to the requested side of the colocalisation dataset.\n\n    Args:\n        study_locus (StudyLocus): Dataset containing study loci that links the colocalisation dataset and the study index via the studyId\n        study_index (StudyIndex): Dataset containing study index that contains the metadata\n        metadata_cols (list[str]): List of study columns to append\n        colocalisation_side (str): Which side of the colocalisation dataset to append metadata to. Must be either 'right' or 'left'\n\n    Returns:\n        DataFrame: Colocalisation dataset with appended metadata of the study from the requested side\n\n    Raises:\n        ValueError: if colocalisation_side is not 'right' or 'left'\n    \"\"\"\n    metadata_cols = [\"studyId\", *metadata_cols]\n    if colocalisation_side not in [\"right\", \"left\"]:\n        raise ValueError(\n            f\"colocalisation_side must be either 'right' or 'left', got {colocalisation_side}\"\n        )\n\n    study_loci_w_metadata = (\n        study_locus.df.select(\"studyLocusId\", \"studyId\")\n        .join(\n            f.broadcast(study_index.df.select(\"studyId\", *metadata_cols)),\n            \"studyId\",\n        )\n        .distinct()\n    )\n    coloc_df = (\n        # drop `rightStudyType` in case it is requested\n        self.df.drop(\"rightStudyType\")\n        if \"studyType\" in metadata_cols and colocalisation_side == \"right\"\n        else self.df\n    )\n    return (\n        # Append that to the respective side of the colocalisation dataset\n        study_loci_w_metadata.selectExpr(\n            f\"studyLocusId as {colocalisation_side}StudyLocusId\",\n            *[\n                f\"{col} as {colocalisation_side}{col[0].upper() + col[1:]}\"\n                for col in metadata_cols\n            ],\n        ).join(coloc_df, f\"{colocalisation_side}StudyLocusId\", \"right\")\n    )\n</code></pre>"},{"location":"python_api/datasets/colocalisation/#gentropy.dataset.colocalisation.Colocalisation.extract_maximum_coloc_probability_per_region_and_gene","title":"<code>extract_maximum_coloc_probability_per_region_and_gene(study_locus: StudyLocus, study_index: StudyIndex, *, filter_by_colocalisation_method: str, filter_by_qtls: str | list[str] | None = None) -&gt; DataFrame</code>","text":"<p>Get maximum colocalisation probability for a (studyLocus, gene) window.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus</code> <code>StudyLocus</code> <p>Dataset containing study loci to filter the colocalisation dataset on and the geneId linked to the region</p> required <code>study_index</code> <code>StudyIndex</code> <p>Study index to use to get study metadata</p> required <code>filter_by_colocalisation_method</code> <code>str</code> <p>optional filter to apply on the colocalisation dataset</p> required <code>filter_by_qtls</code> <code>str | list[str] | None</code> <p>optional filter to apply on the colocalisation dataset</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>table with the maximum colocalisation scores for the provided study loci</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if filter_by_qtl is not in the list of valid QTL types or is not in the list of valid colocalisation methods</p> Source code in <code>src/gentropy/dataset/colocalisation.py</code> <pre><code>def extract_maximum_coloc_probability_per_region_and_gene(\n    self: Colocalisation,\n    study_locus: StudyLocus,\n    study_index: StudyIndex,\n    *,\n    filter_by_colocalisation_method: str,\n    filter_by_qtls: str | list[str] | None = None,\n) -&gt; DataFrame:\n    \"\"\"Get maximum colocalisation probability for a (studyLocus, gene) window.\n\n    Args:\n        study_locus (StudyLocus): Dataset containing study loci to filter the colocalisation dataset on and the geneId linked to the region\n        study_index (StudyIndex): Study index to use to get study metadata\n        filter_by_colocalisation_method (str): optional filter to apply on the colocalisation dataset\n        filter_by_qtls (str | list[str] | None): optional filter to apply on the colocalisation dataset\n\n    Returns:\n        DataFrame: table with the maximum colocalisation scores for the provided study loci\n\n    Raises:\n        ValueError: if filter_by_qtl is not in the list of valid QTL types or is not in the list of valid colocalisation methods\n    \"\"\"\n    from gentropy.colocalisation import ColocalisationStep\n\n    valid_qtls = list(\n        set(EqtlCatalogueStudyIndex.method_to_qtl_type_mapping.values())\n    ) + [\n        f\"sc{qtl}\"\n        for qtl in set(\n            EqtlCatalogueStudyIndex.method_to_qtl_type_mapping.values()\n        )\n    ]\n\n    if filter_by_qtls:\n        filter_by_qtls = (\n            list(map(str.lower, [filter_by_qtls]))\n            if isinstance(filter_by_qtls, str)\n            else list(map(str.lower, filter_by_qtls))\n        )\n        if any(qtl not in valid_qtls for qtl in filter_by_qtls):\n            raise ValueError(f\"There are no studies with QTL type {filter_by_qtls}\")\n\n    if filter_by_colocalisation_method not in [\n        \"ECaviar\",\n        \"Coloc\",\n    ]:  # TODO: Write helper class to retrieve coloc method names\n        raise ValueError(\n            f\"Colocalisation method {filter_by_colocalisation_method} is not supported.\"\n        )\n\n    method_colocalisation_metric = ColocalisationStep._get_colocalisation_class(\n        filter_by_colocalisation_method\n    ).METHOD_METRIC\n\n    coloc_filtering_expr = [\n        f.col(\"rightGeneId\").isNotNull(),\n        f.lower(\"colocalisationMethod\") == filter_by_colocalisation_method.lower(),\n    ]\n    if filter_by_qtls:\n        coloc_filtering_expr.append(f.lower(\"rightStudyType\").isin(filter_by_qtls))\n\n    filtered_colocalisation = (\n        # Bring rightStudyType and rightGeneId and filter by rows where the gene is null,\n        # which is equivalent to filtering studyloci from gwas on the right side\n        self.append_study_metadata(\n            study_locus,\n            study_index,\n            metadata_cols=[\"geneId\", \"studyType\"],\n            colocalisation_side=\"right\",\n        )\n        # it also filters based on method and qtl type\n        .filter(reduce(lambda a, b: a &amp; b, coloc_filtering_expr))\n        # and filters colocalisation results to only include the subset of studylocus that contains gwas studylocusid\n        .join(\n            study_locus.df.selectExpr(\"studyLocusId as leftStudyLocusId\"),\n            \"leftStudyLocusId\",\n        )\n    )\n\n    return get_record_with_maximum_value(\n        filtered_colocalisation.withColumnRenamed(\n            \"leftStudyLocusId\", \"studyLocusId\"\n        ).withColumnRenamed(\"rightGeneId\", \"geneId\"),\n        [\"studyLocusId\", \"geneId\"],\n        method_colocalisation_metric,\n    )\n</code></pre>"},{"location":"python_api/datasets/colocalisation/#gentropy.dataset.colocalisation.Colocalisation.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the Colocalisation dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the Colocalisation dataset</p> Source code in <code>src/gentropy/dataset/colocalisation.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[Colocalisation]) -&gt; StructType:\n    \"\"\"Provides the schema for the Colocalisation dataset.\n\n    Returns:\n        StructType: Schema for the Colocalisation dataset\n    \"\"\"\n    return parse_spark_schema(\"colocalisation.json\")\n</code></pre>"},{"location":"python_api/datasets/colocalisation/#schema","title":"Schema","text":"<pre><code>root\n |-- leftStudyLocusId: string (nullable = false)\n |-- rightStudyLocusId: string (nullable = false)\n |-- rightStudyType: string (nullable = false)\n |-- chromosome: string (nullable = false)\n |-- colocalisationMethod: string (nullable = false)\n |-- numberColocalisingVariants: long (nullable = false)\n |-- h0: double (nullable = true)\n |-- h1: double (nullable = true)\n |-- h2: double (nullable = true)\n |-- h3: double (nullable = true)\n |-- h4: double (nullable = true)\n |-- clpp: double (nullable = true)\n |-- betaRatioSignAverage: double (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/intervals/","title":"Intervals","text":""},{"location":"python_api/datasets/intervals/#gentropy.dataset.intervals.Intervals","title":"<code>gentropy.dataset.intervals.Intervals</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Intervals dataset links genes to genomic regions based on genome interaction studies.</p> Source code in <code>src/gentropy/dataset/intervals.py</code> <pre><code>@dataclass\nclass Intervals(Dataset):\n    \"\"\"Intervals dataset links genes to genomic regions based on genome interaction studies.\"\"\"\n\n    @classmethod\n    def get_schema(cls: type[Intervals]) -&gt; StructType:\n        \"\"\"Provides the schema for the Intervals dataset.\n\n        Returns:\n            StructType: Schema for the Intervals dataset\n        \"\"\"\n        return parse_spark_schema(\"intervals.json\")\n\n    @classmethod\n    def from_source(\n        cls: type[Intervals],\n        spark: SparkSession,\n        source_name: str,\n        source_path: str,\n        target_index: TargetIndex,\n        lift: LiftOverSpark,\n    ) -&gt; Intervals:\n        \"\"\"Collect interval data for a particular source.\n\n        Args:\n            spark (SparkSession): Spark session\n            source_name (str): Name of the interval source\n            source_path (str): Path to the interval source file\n            target_index (TargetIndex): Target index\n            lift (LiftOverSpark): LiftOverSpark instance to convert coordinats from hg37 to hg38\n\n        Returns:\n            Intervals: Intervals dataset\n\n        Raises:\n            ValueError: If the source name is not recognised\n        \"\"\"\n        from gentropy.datasource.intervals.andersson import IntervalsAndersson\n        from gentropy.datasource.intervals.javierre import IntervalsJavierre\n        from gentropy.datasource.intervals.jung import IntervalsJung\n        from gentropy.datasource.intervals.thurman import IntervalsThurman\n\n        source_to_class = {\n            \"andersson\": IntervalsAndersson,\n            \"javierre\": IntervalsJavierre,\n            \"jung\": IntervalsJung,\n            \"thurman\": IntervalsThurman,\n        }\n\n        if source_name not in source_to_class:\n            raise ValueError(f\"Unknown interval source: {source_name}\")\n\n        source_class = source_to_class[source_name]\n        data = source_class.read(spark, source_path)  # type: ignore\n        return source_class.parse(data, target_index, lift)  # type: ignore\n</code></pre>"},{"location":"python_api/datasets/intervals/#gentropy.dataset.intervals.Intervals.from_source","title":"<code>from_source(spark: SparkSession, source_name: str, source_path: str, target_index: TargetIndex, lift: LiftOverSpark) -&gt; Intervals</code>  <code>classmethod</code>","text":"<p>Collect interval data for a particular source.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>source_name</code> <code>str</code> <p>Name of the interval source</p> required <code>source_path</code> <code>str</code> <p>Path to the interval source file</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index</p> required <code>lift</code> <code>LiftOverSpark</code> <p>LiftOverSpark instance to convert coordinats from hg37 to hg38</p> required <p>Returns:</p> Name Type Description <code>Intervals</code> <code>Intervals</code> <p>Intervals dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source name is not recognised</p> Source code in <code>src/gentropy/dataset/intervals.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[Intervals],\n    spark: SparkSession,\n    source_name: str,\n    source_path: str,\n    target_index: TargetIndex,\n    lift: LiftOverSpark,\n) -&gt; Intervals:\n    \"\"\"Collect interval data for a particular source.\n\n    Args:\n        spark (SparkSession): Spark session\n        source_name (str): Name of the interval source\n        source_path (str): Path to the interval source file\n        target_index (TargetIndex): Target index\n        lift (LiftOverSpark): LiftOverSpark instance to convert coordinats from hg37 to hg38\n\n    Returns:\n        Intervals: Intervals dataset\n\n    Raises:\n        ValueError: If the source name is not recognised\n    \"\"\"\n    from gentropy.datasource.intervals.andersson import IntervalsAndersson\n    from gentropy.datasource.intervals.javierre import IntervalsJavierre\n    from gentropy.datasource.intervals.jung import IntervalsJung\n    from gentropy.datasource.intervals.thurman import IntervalsThurman\n\n    source_to_class = {\n        \"andersson\": IntervalsAndersson,\n        \"javierre\": IntervalsJavierre,\n        \"jung\": IntervalsJung,\n        \"thurman\": IntervalsThurman,\n    }\n\n    if source_name not in source_to_class:\n        raise ValueError(f\"Unknown interval source: {source_name}\")\n\n    source_class = source_to_class[source_name]\n    data = source_class.read(spark, source_path)  # type: ignore\n    return source_class.parse(data, target_index, lift)  # type: ignore\n</code></pre>"},{"location":"python_api/datasets/intervals/#gentropy.dataset.intervals.Intervals.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the Intervals dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the Intervals dataset</p> Source code in <code>src/gentropy/dataset/intervals.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[Intervals]) -&gt; StructType:\n    \"\"\"Provides the schema for the Intervals dataset.\n\n    Returns:\n        StructType: Schema for the Intervals dataset\n    \"\"\"\n    return parse_spark_schema(\"intervals.json\")\n</code></pre>"},{"location":"python_api/datasets/intervals/#schema","title":"Schema","text":"<pre><code>root\n |-- chromosome: string (nullable = false)\n |-- start: string (nullable = false)\n |-- end: string (nullable = false)\n |-- geneId: string (nullable = false)\n |-- resourceScore: double (nullable = true)\n |-- score: double (nullable = true)\n |-- datasourceId: string (nullable = false)\n |-- datatypeId: string (nullable = false)\n |-- pmid: string (nullable = true)\n |-- biofeature: string (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/","title":"L2G Feature Matrix","text":""},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix","title":"<code>gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix</code>","text":"<p>Dataset with features for Locus to Gene prediction.</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>class L2GFeatureMatrix:\n    \"\"\"Dataset with features for Locus to Gene prediction.\"\"\"\n\n    def __init__(\n        self,\n        _df: DataFrame,\n        features_list: list[str] | None = None,\n        with_gold_standard: bool = False,\n    ) -&gt; None:\n        \"\"\"Post-initialisation to set the features list. If not provided, all columns except the fixed ones are used.\n\n        Args:\n            _df (DataFrame): Feature matrix dataset\n            features_list (list[str] | None): List of features to use. If None, all possible features are used.\n            with_gold_standard (bool): Whether to include the gold standard set in the feature matrix.\n        \"\"\"\n        self.with_gold_standard = with_gold_standard\n        self.fixed_cols = [\"studyLocusId\", \"geneId\"]\n        if self.with_gold_standard:\n            self.fixed_cols.append(\"goldStandardSet\")\n        if \"traitFromSourceMappedId\" in _df.columns:\n            self.fixed_cols.append(\"traitFromSourceMappedId\")\n\n        self.features_list = features_list or [\n            col for col in _df.columns if col not in self.fixed_cols\n        ]\n        self._df = _df.selectExpr(\n            self.fixed_cols\n            + [\n                f\"CAST({feature} AS FLOAT) AS {feature}\"\n                for feature in self.features_list\n            ]\n        )\n\n    @classmethod\n    def from_features_list(\n        cls: type[L2GFeatureMatrix],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        features_list: list[str],\n        features_input_loader: L2GFeatureInputLoader,\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Generate features from the gentropy datasets by calling the feature factory that will instantiate the corresponding features.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): Study locus pairs to annotate\n            features_list (list[str]): List of feature names to be computed.\n            features_input_loader (L2GFeatureInputLoader): Object that contais features input.\n\n        Returns:\n            L2GFeatureMatrix: L2G feature matrix dataset\n        \"\"\"\n        features_long_df = reduce(\n            lambda x, y: x.unionByName(y, allowMissingColumns=True),\n            [\n                # Compute all features and merge them into a single dataframe\n                feature.df\n                for feature in FeatureFactory(\n                    study_loci_to_annotate, features_list\n                ).generate_features(features_input_loader)\n            ],\n        )\n        if isinstance(study_loci_to_annotate, L2GGoldStandard):\n            return cls(\n                _df=convert_from_long_to_wide(\n                    # Add gold standard set to the feature matrix\n                    features_long_df.join(\n                        study_loci_to_annotate.df.select(\n                            \"studyLocusId\", \"geneId\", \"goldStandardSet\"\n                        ),\n                        [\"studyLocusId\", \"geneId\"],\n                    ),\n                    [\"studyLocusId\", \"geneId\", \"goldStandardSet\"],\n                    \"featureName\",\n                    \"featureValue\",\n                ),\n                with_gold_standard=True,\n            )\n        return cls(\n            _df=convert_from_long_to_wide(\n                features_long_df,\n                [\"studyLocusId\", \"geneId\"],\n                \"featureName\",\n                \"featureValue\",\n            ),\n            with_gold_standard=False,\n        )\n\n    def calculate_feature_missingness_rate(\n        self: L2GFeatureMatrix,\n    ) -&gt; dict[str, float]:\n        \"\"\"Calculate the proportion of missing values in each feature.\n\n        Returns:\n            dict[str, float]: Dictionary of feature names and their missingness rate.\n\n        Raises:\n            ValueError: If no features are found.\n        \"\"\"\n        total_count = self._df.count()\n        if not self.features_list:\n            raise ValueError(\"No features found\")\n\n        return {\n            feature: (\n                self._df.filter(\n                    (self._df[feature].isNull()) | (self._df[feature] == 0)\n                ).count()\n                / total_count\n            )\n            for feature in self.features_list\n        }\n\n    def fill_na(\n        self: L2GFeatureMatrix, na_value: float = 0.0, subset: list[str] | None = None\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Fill missing values in a column with a given value.\n\n        For features that correspond to gene attributes, missing values are imputed using the mean of the column.\n\n        Args:\n            na_value (float): Value to replace missing values with. Defaults to 0.0.\n            subset (list[str] | None): Subset of columns to consider. Defaults to None.\n\n        Returns:\n            L2GFeatureMatrix: L2G feature matrix dataset\n        \"\"\"\n        cols_to_impute = [\n            \"proteinGeneCount500kb\",\n            \"geneCount500kb\",\n        ]\n        for col in cols_to_impute:\n            if col not in self._df.columns:\n                continue\n            else:\n                self._df = self._df.withColumn(\n                    col,\n                    f.when(\n                        f.col(col).isNull(),\n                        f.mean(f.col(col)).over(Window.partitionBy(\"studyLocusId\")),\n                    ).otherwise(f.col(col)),\n                )\n        self._df = self._df.fillna(na_value, subset=subset)\n        return self\n\n    def select_features(\n        self: L2GFeatureMatrix,\n        features_list: list[str] | None,\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Returns a new object with a subset of features from the original feature matrix.\n\n        Args:\n            features_list (list[str] | None): List of features to select\n\n        Returns:\n            L2GFeatureMatrix: L2G feature matrix dataset\n\n        Raises:\n            ValueError: If no features have been selected.\n        \"\"\"\n        if features_list := features_list or self.features_list:\n            # cast to float every feature in the features_list\n            return L2GFeatureMatrix(\n                _df=self._df.selectExpr(\n                    self.fixed_cols\n                    + [\n                        f\"CAST({feature} AS FLOAT) AS {feature}\"\n                        for feature in features_list\n                    ]\n                ),\n                features_list=features_list,\n                with_gold_standard=self.with_gold_standard,\n            )\n        raise ValueError(\"features_list cannot be None\")\n\n    def persist(self: Self) -&gt; Self:\n        \"\"\"Persist the feature matrix in memory.\n\n        Returns:\n            Self: Persisted Dataset\n        \"\"\"\n        self._df = self._df.persist()\n        return self\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.__init__","title":"<code>__init__(_df: DataFrame, features_list: list[str] | None = None, with_gold_standard: bool = False) -&gt; None</code>","text":"<p>Post-initialisation to set the features list. If not provided, all columns except the fixed ones are used.</p> <p>Parameters:</p> Name Type Description Default <code>_df</code> <code>DataFrame</code> <p>Feature matrix dataset</p> required <code>features_list</code> <code>list[str] | None</code> <p>List of features to use. If None, all possible features are used.</p> <code>None</code> <code>with_gold_standard</code> <code>bool</code> <p>Whether to include the gold standard set in the feature matrix.</p> <code>False</code> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def __init__(\n    self,\n    _df: DataFrame,\n    features_list: list[str] | None = None,\n    with_gold_standard: bool = False,\n) -&gt; None:\n    \"\"\"Post-initialisation to set the features list. If not provided, all columns except the fixed ones are used.\n\n    Args:\n        _df (DataFrame): Feature matrix dataset\n        features_list (list[str] | None): List of features to use. If None, all possible features are used.\n        with_gold_standard (bool): Whether to include the gold standard set in the feature matrix.\n    \"\"\"\n    self.with_gold_standard = with_gold_standard\n    self.fixed_cols = [\"studyLocusId\", \"geneId\"]\n    if self.with_gold_standard:\n        self.fixed_cols.append(\"goldStandardSet\")\n    if \"traitFromSourceMappedId\" in _df.columns:\n        self.fixed_cols.append(\"traitFromSourceMappedId\")\n\n    self.features_list = features_list or [\n        col for col in _df.columns if col not in self.fixed_cols\n    ]\n    self._df = _df.selectExpr(\n        self.fixed_cols\n        + [\n            f\"CAST({feature} AS FLOAT) AS {feature}\"\n            for feature in self.features_list\n        ]\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.calculate_feature_missingness_rate","title":"<code>calculate_feature_missingness_rate() -&gt; dict[str, float]</code>","text":"<p>Calculate the proportion of missing values in each feature.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Dictionary of feature names and their missingness rate.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no features are found.</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def calculate_feature_missingness_rate(\n    self: L2GFeatureMatrix,\n) -&gt; dict[str, float]:\n    \"\"\"Calculate the proportion of missing values in each feature.\n\n    Returns:\n        dict[str, float]: Dictionary of feature names and their missingness rate.\n\n    Raises:\n        ValueError: If no features are found.\n    \"\"\"\n    total_count = self._df.count()\n    if not self.features_list:\n        raise ValueError(\"No features found\")\n\n    return {\n        feature: (\n            self._df.filter(\n                (self._df[feature].isNull()) | (self._df[feature] == 0)\n            ).count()\n            / total_count\n        )\n        for feature in self.features_list\n    }\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.fill_na","title":"<code>fill_na(na_value: float = 0.0, subset: list[str] | None = None) -&gt; L2GFeatureMatrix</code>","text":"<p>Fill missing values in a column with a given value.</p> <p>For features that correspond to gene attributes, missing values are imputed using the mean of the column.</p> <p>Parameters:</p> Name Type Description Default <code>na_value</code> <code>float</code> <p>Value to replace missing values with. Defaults to 0.0.</p> <code>0.0</code> <code>subset</code> <code>list[str] | None</code> <p>Subset of columns to consider. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>L2G feature matrix dataset</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def fill_na(\n    self: L2GFeatureMatrix, na_value: float = 0.0, subset: list[str] | None = None\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Fill missing values in a column with a given value.\n\n    For features that correspond to gene attributes, missing values are imputed using the mean of the column.\n\n    Args:\n        na_value (float): Value to replace missing values with. Defaults to 0.0.\n        subset (list[str] | None): Subset of columns to consider. Defaults to None.\n\n    Returns:\n        L2GFeatureMatrix: L2G feature matrix dataset\n    \"\"\"\n    cols_to_impute = [\n        \"proteinGeneCount500kb\",\n        \"geneCount500kb\",\n    ]\n    for col in cols_to_impute:\n        if col not in self._df.columns:\n            continue\n        else:\n            self._df = self._df.withColumn(\n                col,\n                f.when(\n                    f.col(col).isNull(),\n                    f.mean(f.col(col)).over(Window.partitionBy(\"studyLocusId\")),\n                ).otherwise(f.col(col)),\n            )\n    self._df = self._df.fillna(na_value, subset=subset)\n    return self\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.from_features_list","title":"<code>from_features_list(study_loci_to_annotate: StudyLocus | L2GGoldStandard, features_list: list[str], features_input_loader: L2GFeatureInputLoader) -&gt; L2GFeatureMatrix</code>  <code>classmethod</code>","text":"<p>Generate features from the gentropy datasets by calling the feature factory that will instantiate the corresponding features.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>Study locus pairs to annotate</p> required <code>features_list</code> <code>list[str]</code> <p>List of feature names to be computed.</p> required <code>features_input_loader</code> <code>L2GFeatureInputLoader</code> <p>Object that contais features input.</p> required <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>L2G feature matrix dataset</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>@classmethod\ndef from_features_list(\n    cls: type[L2GFeatureMatrix],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    features_list: list[str],\n    features_input_loader: L2GFeatureInputLoader,\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Generate features from the gentropy datasets by calling the feature factory that will instantiate the corresponding features.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): Study locus pairs to annotate\n        features_list (list[str]): List of feature names to be computed.\n        features_input_loader (L2GFeatureInputLoader): Object that contais features input.\n\n    Returns:\n        L2GFeatureMatrix: L2G feature matrix dataset\n    \"\"\"\n    features_long_df = reduce(\n        lambda x, y: x.unionByName(y, allowMissingColumns=True),\n        [\n            # Compute all features and merge them into a single dataframe\n            feature.df\n            for feature in FeatureFactory(\n                study_loci_to_annotate, features_list\n            ).generate_features(features_input_loader)\n        ],\n    )\n    if isinstance(study_loci_to_annotate, L2GGoldStandard):\n        return cls(\n            _df=convert_from_long_to_wide(\n                # Add gold standard set to the feature matrix\n                features_long_df.join(\n                    study_loci_to_annotate.df.select(\n                        \"studyLocusId\", \"geneId\", \"goldStandardSet\"\n                    ),\n                    [\"studyLocusId\", \"geneId\"],\n                ),\n                [\"studyLocusId\", \"geneId\", \"goldStandardSet\"],\n                \"featureName\",\n                \"featureValue\",\n            ),\n            with_gold_standard=True,\n        )\n    return cls(\n        _df=convert_from_long_to_wide(\n            features_long_df,\n            [\"studyLocusId\", \"geneId\"],\n            \"featureName\",\n            \"featureValue\",\n        ),\n        with_gold_standard=False,\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.persist","title":"<code>persist() -&gt; Self</code>","text":"<p>Persist the feature matrix in memory.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Persisted Dataset</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def persist(self: Self) -&gt; Self:\n    \"\"\"Persist the feature matrix in memory.\n\n    Returns:\n        Self: Persisted Dataset\n    \"\"\"\n    self._df = self._df.persist()\n    return self\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.select_features","title":"<code>select_features(features_list: list[str] | None) -&gt; L2GFeatureMatrix</code>","text":"<p>Returns a new object with a subset of features from the original feature matrix.</p> <p>Parameters:</p> Name Type Description Default <code>features_list</code> <code>list[str] | None</code> <p>List of features to select</p> required <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>L2G feature matrix dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no features have been selected.</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def select_features(\n    self: L2GFeatureMatrix,\n    features_list: list[str] | None,\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Returns a new object with a subset of features from the original feature matrix.\n\n    Args:\n        features_list (list[str] | None): List of features to select\n\n    Returns:\n        L2GFeatureMatrix: L2G feature matrix dataset\n\n    Raises:\n        ValueError: If no features have been selected.\n    \"\"\"\n    if features_list := features_list or self.features_list:\n        # cast to float every feature in the features_list\n        return L2GFeatureMatrix(\n            _df=self._df.selectExpr(\n                self.fixed_cols\n                + [\n                    f\"CAST({feature} AS FLOAT) AS {feature}\"\n                    for feature in features_list\n                ]\n            ),\n            features_list=features_list,\n            with_gold_standard=self.with_gold_standard,\n        )\n    raise ValueError(\"features_list cannot be None\")\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#schema","title":"Schema","text":""},{"location":"python_api/datasets/l2g_gold_standard/","title":"L2G Gold Standard","text":""},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard","title":"<code>gentropy.dataset.l2g_gold_standard.L2GGoldStandard</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>L2G gold standard dataset.</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>@dataclass\nclass L2GGoldStandard(Dataset):\n    \"\"\"L2G gold standard dataset.\"\"\"\n\n    INTERACTION_THRESHOLD = 0.7\n    GS_POSITIVE_LABEL = \"positive\"\n    GS_NEGATIVE_LABEL = \"negative\"\n\n    @classmethod\n    def from_otg_curation(\n        cls: type[L2GGoldStandard],\n        gold_standard_curation: DataFrame,\n        study_locus_overlap: StudyLocusOverlap,\n        variant_index: VariantIndex,\n        interactions: DataFrame,\n    ) -&gt; L2GGoldStandard:\n        \"\"\"Initialise L2GGoldStandard from source dataset.\n\n        Args:\n            gold_standard_curation (DataFrame): Gold standard curation dataframe, extracted from\n            study_locus_overlap (StudyLocusOverlap): Study locus overlap dataset to remove duplicated loci\n            variant_index (VariantIndex): Dataset to bring distance between a variant and a gene's footprint\n            interactions (DataFrame): Gene-gene interactions dataset to remove negative cases where the gene interacts with a positive gene\n\n        Returns:\n            L2GGoldStandard: L2G Gold Standard dataset\n        \"\"\"\n        from gentropy.datasource.open_targets.l2g_gold_standard import (\n            OpenTargetsL2GGoldStandard,\n        )\n\n        interactions_df = cls.process_gene_interactions(interactions)\n\n        return (\n            OpenTargetsL2GGoldStandard.as_l2g_gold_standard(\n                gold_standard_curation, variant_index\n            )\n            .filter_unique_associations(study_locus_overlap)\n            .remove_false_negatives(interactions_df)\n        )\n\n    @classmethod\n    def get_schema(cls: type[L2GGoldStandard]) -&gt; StructType:\n        \"\"\"Provides the schema for the L2GGoldStandard dataset.\n\n        Returns:\n            StructType: Spark schema for the L2GGoldStandard dataset\n        \"\"\"\n        return parse_spark_schema(\"l2g_gold_standard.json\")\n\n    @classmethod\n    def process_gene_interactions(\n        cls: type[L2GGoldStandard], interactions: DataFrame\n    ) -&gt; DataFrame:\n        \"\"\"Extract top scoring gene-gene interaction from the interactions dataset of the Platform.\n\n        Args:\n            interactions (DataFrame): Gene-gene interactions dataset from the Open Targets Platform\n\n        Returns:\n            DataFrame: Top scoring gene-gene interaction per pair of genes\n\n        Examples:\n            &gt;&gt;&gt; interactions = spark.createDataFrame([(\"gene1\", \"gene2\", 0.8), (\"gene1\", \"gene2\", 0.5), (\"gene2\", \"gene3\", 0.7)], [\"targetA\", \"targetB\", \"scoring\"])\n            &gt;&gt;&gt; L2GGoldStandard.process_gene_interactions(interactions).show()\n            +-------+-------+-----+\n            |geneIdA|geneIdB|score|\n            +-------+-------+-----+\n            |  gene1|  gene2|  0.8|\n            |  gene2|  gene3|  0.7|\n            +-------+-------+-----+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return get_record_with_maximum_value(\n            interactions,\n            [\"targetA\", \"targetB\"],\n            \"scoring\",\n        ).selectExpr(\n            \"targetA as geneIdA\",\n            \"targetB as geneIdB\",\n            \"scoring as score\",\n        )\n\n    def build_feature_matrix(\n        self: L2GGoldStandard,\n        full_feature_matrix: L2GFeatureMatrix,\n        credible_set: StudyLocus,\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Return a feature matrix for study loci in the gold standard.\n\n        Args:\n            full_feature_matrix (L2GFeatureMatrix): Feature matrix for all study loci to join on\n            credible_set (StudyLocus): Full credible sets to annotate the feature matrix with variant and study IDs and perform the join\n\n        Returns:\n            L2GFeatureMatrix: Feature matrix for study loci in the gold standard\n        \"\"\"\n        from gentropy.dataset.l2g_feature_matrix import L2GFeatureMatrix\n\n        return L2GFeatureMatrix(\n            _df=full_feature_matrix._df.join(\n                credible_set.df.select(\"studyLocusId\", \"variantId\", \"studyId\"),\n                \"studyLocusId\",\n                \"left\",\n            )\n            .join(\n                f.broadcast(self.df.drop(\"studyLocusId\", \"sources\")),\n                on=[\"studyId\", \"variantId\", \"geneId\"],\n                how=\"inner\",\n            )\n            .filter(f.col(\"isProteinCoding\") == 1)\n            .drop(\"studyId\", \"variantId\")\n            .distinct(),\n            with_gold_standard=True,\n        ).fill_na()\n\n    def filter_unique_associations(\n        self: L2GGoldStandard,\n        study_locus_overlap: StudyLocusOverlap,\n    ) -&gt; L2GGoldStandard:\n        \"\"\"Refines the gold standard to filter out loci that are not independent.\n\n        Rules:\n        - If two loci point to the same gene, one positive and one negative, and have overlapping variants, we keep the positive one.\n        - If two loci point to the same gene, both positive or negative, and have overlapping variants, we drop one.\n        - If two loci point to different genes, and have overlapping variants, we keep both.\n\n        Args:\n            study_locus_overlap (StudyLocusOverlap): A dataset detailing variants that overlap between StudyLocus.\n\n        Returns:\n            L2GGoldStandard: L2GGoldStandard updated to exclude false negatives and redundant positives.\n        \"\"\"\n        squared_overlaps = study_locus_overlap._convert_to_square_matrix()\n        unique_associations = (\n            self.df.alias(\"left\")\n            # identify all the study loci that point to the same gene\n            .withColumn(\n                \"sl_same_gene\",\n                f.collect_set(\"studyLocusId\").over(Window.partitionBy(\"geneId\")),\n            )\n            # identify all the study loci that have an overlapping variant\n            .join(\n                squared_overlaps.df.alias(\"right\"),\n                (f.col(\"left.studyLocusId\") == f.col(\"right.leftStudyLocusId\"))\n                &amp; (f.col(\"left.variantId\") == f.col(\"right.tagVariantId\")),\n                \"left\",\n            )\n            .withColumn(\n                \"overlaps\",\n                f.when(f.col(\"right.tagVariantId\").isNotNull(), f.lit(True)).otherwise(\n                    f.lit(False)\n                ),\n            )\n            # drop redundant rows: where the variantid overlaps and the gene is \"explained\" by more than one study locus\n            .filter(~((f.size(\"sl_same_gene\") &gt; 1) &amp; (f.col(\"overlaps\") == 1)))\n            .select(*self.df.columns)\n        )\n        return L2GGoldStandard(_df=unique_associations, _schema=self.get_schema())\n\n    def remove_false_negatives(\n        self: L2GGoldStandard,\n        interactions_df: DataFrame,\n    ) -&gt; L2GGoldStandard:\n        \"\"\"Refines the gold standard to remove negative gold standard instances where the gene interacts with a positive gene.\n\n        Args:\n            interactions_df (DataFrame): Top scoring gene-gene interaction per pair of genes\n\n        Returns:\n            L2GGoldStandard: A refined set of locus-to-gene associations with increased reliability, having excluded loci that were likely false negatives due to gene-gene interaction confounding.\n        \"\"\"\n        squared_interactions = interactions_df.unionByName(\n            interactions_df.selectExpr(\n                \"geneIdB as geneIdA\", \"geneIdA as geneIdB\", \"score\"\n            )\n        ).filter(f.col(\"score\") &gt; self.INTERACTION_THRESHOLD)\n        df = (\n            self.df.alias(\"left\")\n            .join(\n                # bring gene partners\n                squared_interactions.alias(\"right\"),\n                f.col(\"left.geneId\") == f.col(\"right.geneIdA\"),\n                \"left\",\n            )\n            .withColumnRenamed(\"geneIdB\", \"interactorGeneId\")\n            .join(\n                # bring gold standard status for gene partners\n                self.df.selectExpr(\n                    \"geneId as interactorGeneId\",\n                    \"goldStandardSet as interactorGeneIdGoldStandardSet\",\n                ),\n                \"interactorGeneId\",\n                \"left\",\n            )\n            # remove self-interactions\n            .filter(\n                (f.col(\"geneId\") != f.col(\"interactorGeneId\"))\n                | (f.col(\"interactorGeneId\").isNull())\n            )\n            # remove false negatives\n            .filter(\n                # drop rows where the GS gene is negative but the interactor is a GS positive\n                ~(f.col(\"goldStandardSet\") == \"negative\")\n                &amp; (f.col(\"interactorGeneIdGoldStandardSet\") == \"positive\")\n                |\n                # keep rows where the gene does not interact\n                (f.col(\"interactorGeneId\").isNull())\n            )\n            .select(*self.df.columns)\n            .distinct()\n        )\n        return L2GGoldStandard(_df=df, _schema=self.get_schema())\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.build_feature_matrix","title":"<code>build_feature_matrix(full_feature_matrix: L2GFeatureMatrix, credible_set: StudyLocus) -&gt; L2GFeatureMatrix</code>","text":"<p>Return a feature matrix for study loci in the gold standard.</p> <p>Parameters:</p> Name Type Description Default <code>full_feature_matrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix for all study loci to join on</p> required <code>credible_set</code> <code>StudyLocus</code> <p>Full credible sets to annotate the feature matrix with variant and study IDs and perform the join</p> required <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix for study loci in the gold standard</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>def build_feature_matrix(\n    self: L2GGoldStandard,\n    full_feature_matrix: L2GFeatureMatrix,\n    credible_set: StudyLocus,\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Return a feature matrix for study loci in the gold standard.\n\n    Args:\n        full_feature_matrix (L2GFeatureMatrix): Feature matrix for all study loci to join on\n        credible_set (StudyLocus): Full credible sets to annotate the feature matrix with variant and study IDs and perform the join\n\n    Returns:\n        L2GFeatureMatrix: Feature matrix for study loci in the gold standard\n    \"\"\"\n    from gentropy.dataset.l2g_feature_matrix import L2GFeatureMatrix\n\n    return L2GFeatureMatrix(\n        _df=full_feature_matrix._df.join(\n            credible_set.df.select(\"studyLocusId\", \"variantId\", \"studyId\"),\n            \"studyLocusId\",\n            \"left\",\n        )\n        .join(\n            f.broadcast(self.df.drop(\"studyLocusId\", \"sources\")),\n            on=[\"studyId\", \"variantId\", \"geneId\"],\n            how=\"inner\",\n        )\n        .filter(f.col(\"isProteinCoding\") == 1)\n        .drop(\"studyId\", \"variantId\")\n        .distinct(),\n        with_gold_standard=True,\n    ).fill_na()\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.filter_unique_associations","title":"<code>filter_unique_associations(study_locus_overlap: StudyLocusOverlap) -&gt; L2GGoldStandard</code>","text":"<p>Refines the gold standard to filter out loci that are not independent.</p> <p>Rules: - If two loci point to the same gene, one positive and one negative, and have overlapping variants, we keep the positive one. - If two loci point to the same gene, both positive or negative, and have overlapping variants, we drop one. - If two loci point to different genes, and have overlapping variants, we keep both.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus_overlap</code> <code>StudyLocusOverlap</code> <p>A dataset detailing variants that overlap between StudyLocus.</p> required <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>L2GGoldStandard updated to exclude false negatives and redundant positives.</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>def filter_unique_associations(\n    self: L2GGoldStandard,\n    study_locus_overlap: StudyLocusOverlap,\n) -&gt; L2GGoldStandard:\n    \"\"\"Refines the gold standard to filter out loci that are not independent.\n\n    Rules:\n    - If two loci point to the same gene, one positive and one negative, and have overlapping variants, we keep the positive one.\n    - If two loci point to the same gene, both positive or negative, and have overlapping variants, we drop one.\n    - If two loci point to different genes, and have overlapping variants, we keep both.\n\n    Args:\n        study_locus_overlap (StudyLocusOverlap): A dataset detailing variants that overlap between StudyLocus.\n\n    Returns:\n        L2GGoldStandard: L2GGoldStandard updated to exclude false negatives and redundant positives.\n    \"\"\"\n    squared_overlaps = study_locus_overlap._convert_to_square_matrix()\n    unique_associations = (\n        self.df.alias(\"left\")\n        # identify all the study loci that point to the same gene\n        .withColumn(\n            \"sl_same_gene\",\n            f.collect_set(\"studyLocusId\").over(Window.partitionBy(\"geneId\")),\n        )\n        # identify all the study loci that have an overlapping variant\n        .join(\n            squared_overlaps.df.alias(\"right\"),\n            (f.col(\"left.studyLocusId\") == f.col(\"right.leftStudyLocusId\"))\n            &amp; (f.col(\"left.variantId\") == f.col(\"right.tagVariantId\")),\n            \"left\",\n        )\n        .withColumn(\n            \"overlaps\",\n            f.when(f.col(\"right.tagVariantId\").isNotNull(), f.lit(True)).otherwise(\n                f.lit(False)\n            ),\n        )\n        # drop redundant rows: where the variantid overlaps and the gene is \"explained\" by more than one study locus\n        .filter(~((f.size(\"sl_same_gene\") &gt; 1) &amp; (f.col(\"overlaps\") == 1)))\n        .select(*self.df.columns)\n    )\n    return L2GGoldStandard(_df=unique_associations, _schema=self.get_schema())\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.from_otg_curation","title":"<code>from_otg_curation(gold_standard_curation: DataFrame, study_locus_overlap: StudyLocusOverlap, variant_index: VariantIndex, interactions: DataFrame) -&gt; L2GGoldStandard</code>  <code>classmethod</code>","text":"<p>Initialise L2GGoldStandard from source dataset.</p> <p>Parameters:</p> Name Type Description Default <code>gold_standard_curation</code> <code>DataFrame</code> <p>Gold standard curation dataframe, extracted from</p> required <code>study_locus_overlap</code> <code>StudyLocusOverlap</code> <p>Study locus overlap dataset to remove duplicated loci</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Dataset to bring distance between a variant and a gene's footprint</p> required <code>interactions</code> <code>DataFrame</code> <p>Gene-gene interactions dataset to remove negative cases where the gene interacts with a positive gene</p> required <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>L2G Gold Standard dataset</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef from_otg_curation(\n    cls: type[L2GGoldStandard],\n    gold_standard_curation: DataFrame,\n    study_locus_overlap: StudyLocusOverlap,\n    variant_index: VariantIndex,\n    interactions: DataFrame,\n) -&gt; L2GGoldStandard:\n    \"\"\"Initialise L2GGoldStandard from source dataset.\n\n    Args:\n        gold_standard_curation (DataFrame): Gold standard curation dataframe, extracted from\n        study_locus_overlap (StudyLocusOverlap): Study locus overlap dataset to remove duplicated loci\n        variant_index (VariantIndex): Dataset to bring distance between a variant and a gene's footprint\n        interactions (DataFrame): Gene-gene interactions dataset to remove negative cases where the gene interacts with a positive gene\n\n    Returns:\n        L2GGoldStandard: L2G Gold Standard dataset\n    \"\"\"\n    from gentropy.datasource.open_targets.l2g_gold_standard import (\n        OpenTargetsL2GGoldStandard,\n    )\n\n    interactions_df = cls.process_gene_interactions(interactions)\n\n    return (\n        OpenTargetsL2GGoldStandard.as_l2g_gold_standard(\n            gold_standard_curation, variant_index\n        )\n        .filter_unique_associations(study_locus_overlap)\n        .remove_false_negatives(interactions_df)\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the L2GGoldStandard dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Spark schema for the L2GGoldStandard dataset</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[L2GGoldStandard]) -&gt; StructType:\n    \"\"\"Provides the schema for the L2GGoldStandard dataset.\n\n    Returns:\n        StructType: Spark schema for the L2GGoldStandard dataset\n    \"\"\"\n    return parse_spark_schema(\"l2g_gold_standard.json\")\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.process_gene_interactions","title":"<code>process_gene_interactions(interactions: DataFrame) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Extract top scoring gene-gene interaction from the interactions dataset of the Platform.</p> <p>Parameters:</p> Name Type Description Default <code>interactions</code> <code>DataFrame</code> <p>Gene-gene interactions dataset from the Open Targets Platform</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Top scoring gene-gene interaction per pair of genes</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; interactions = spark.createDataFrame([(\"gene1\", \"gene2\", 0.8), (\"gene1\", \"gene2\", 0.5), (\"gene2\", \"gene3\", 0.7)], [\"targetA\", \"targetB\", \"scoring\"])\n&gt;&gt;&gt; L2GGoldStandard.process_gene_interactions(interactions).show()\n+-------+-------+-----+\n|geneIdA|geneIdB|score|\n+-------+-------+-----+\n|  gene1|  gene2|  0.8|\n|  gene2|  gene3|  0.7|\n+-------+-------+-----+\n</code></pre> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef process_gene_interactions(\n    cls: type[L2GGoldStandard], interactions: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Extract top scoring gene-gene interaction from the interactions dataset of the Platform.\n\n    Args:\n        interactions (DataFrame): Gene-gene interactions dataset from the Open Targets Platform\n\n    Returns:\n        DataFrame: Top scoring gene-gene interaction per pair of genes\n\n    Examples:\n        &gt;&gt;&gt; interactions = spark.createDataFrame([(\"gene1\", \"gene2\", 0.8), (\"gene1\", \"gene2\", 0.5), (\"gene2\", \"gene3\", 0.7)], [\"targetA\", \"targetB\", \"scoring\"])\n        &gt;&gt;&gt; L2GGoldStandard.process_gene_interactions(interactions).show()\n        +-------+-------+-----+\n        |geneIdA|geneIdB|score|\n        +-------+-------+-----+\n        |  gene1|  gene2|  0.8|\n        |  gene2|  gene3|  0.7|\n        +-------+-------+-----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return get_record_with_maximum_value(\n        interactions,\n        [\"targetA\", \"targetB\"],\n        \"scoring\",\n    ).selectExpr(\n        \"targetA as geneIdA\",\n        \"targetB as geneIdB\",\n        \"scoring as score\",\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.remove_false_negatives","title":"<code>remove_false_negatives(interactions_df: DataFrame) -&gt; L2GGoldStandard</code>","text":"<p>Refines the gold standard to remove negative gold standard instances where the gene interacts with a positive gene.</p> <p>Parameters:</p> Name Type Description Default <code>interactions_df</code> <code>DataFrame</code> <p>Top scoring gene-gene interaction per pair of genes</p> required <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>A refined set of locus-to-gene associations with increased reliability, having excluded loci that were likely false negatives due to gene-gene interaction confounding.</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>def remove_false_negatives(\n    self: L2GGoldStandard,\n    interactions_df: DataFrame,\n) -&gt; L2GGoldStandard:\n    \"\"\"Refines the gold standard to remove negative gold standard instances where the gene interacts with a positive gene.\n\n    Args:\n        interactions_df (DataFrame): Top scoring gene-gene interaction per pair of genes\n\n    Returns:\n        L2GGoldStandard: A refined set of locus-to-gene associations with increased reliability, having excluded loci that were likely false negatives due to gene-gene interaction confounding.\n    \"\"\"\n    squared_interactions = interactions_df.unionByName(\n        interactions_df.selectExpr(\n            \"geneIdB as geneIdA\", \"geneIdA as geneIdB\", \"score\"\n        )\n    ).filter(f.col(\"score\") &gt; self.INTERACTION_THRESHOLD)\n    df = (\n        self.df.alias(\"left\")\n        .join(\n            # bring gene partners\n            squared_interactions.alias(\"right\"),\n            f.col(\"left.geneId\") == f.col(\"right.geneIdA\"),\n            \"left\",\n        )\n        .withColumnRenamed(\"geneIdB\", \"interactorGeneId\")\n        .join(\n            # bring gold standard status for gene partners\n            self.df.selectExpr(\n                \"geneId as interactorGeneId\",\n                \"goldStandardSet as interactorGeneIdGoldStandardSet\",\n            ),\n            \"interactorGeneId\",\n            \"left\",\n        )\n        # remove self-interactions\n        .filter(\n            (f.col(\"geneId\") != f.col(\"interactorGeneId\"))\n            | (f.col(\"interactorGeneId\").isNull())\n        )\n        # remove false negatives\n        .filter(\n            # drop rows where the GS gene is negative but the interactor is a GS positive\n            ~(f.col(\"goldStandardSet\") == \"negative\")\n            &amp; (f.col(\"interactorGeneIdGoldStandardSet\") == \"positive\")\n            |\n            # keep rows where the gene does not interact\n            (f.col(\"interactorGeneId\").isNull())\n        )\n        .select(*self.df.columns)\n        .distinct()\n    )\n    return L2GGoldStandard(_df=df, _schema=self.get_schema())\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#schema","title":"Schema","text":"<pre><code>root\n |-- studyLocusId: string (nullable = false)\n |-- variantId: string (nullable = false)\n |-- studyId: string (nullable = false)\n |-- geneId: string (nullable = false)\n |-- traitFromSourceMappedId: string (nullable = true)\n |-- goldStandardSet: string (nullable = false)\n |-- sources: array (nullable = true)\n |    |-- element: string (containsNull = true)\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/","title":"L2G Prediction","text":""},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction","title":"<code>gentropy.dataset.l2g_prediction.L2GPrediction</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset that contains the Locus to Gene predictions.</p> <p>It is the result of applying the L2G model on a feature matrix, which contains all the study/locus pairs and their functional annotations. The score column informs the confidence of the prediction that a gene is causal to an association.</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>@dataclass\nclass L2GPrediction(Dataset):\n    \"\"\"Dataset that contains the Locus to Gene predictions.\n\n    It is the result of applying the L2G model on a feature matrix, which contains all\n    the study/locus pairs and their functional annotations. The score column informs the\n    confidence of the prediction that a gene is causal to an association.\n    \"\"\"\n\n    model: LocusToGeneModel | None = field(default=None, repr=False)\n\n    @classmethod\n    def get_schema(cls: type[L2GPrediction]) -&gt; StructType:\n        \"\"\"Provides the schema for the L2GPrediction dataset.\n\n        Returns:\n            StructType: Schema for the L2GPrediction dataset\n        \"\"\"\n        return parse_spark_schema(\"l2g_predictions.json\")\n\n    @classmethod\n    def from_credible_set(\n        cls: type[L2GPrediction],\n        session: Session,\n        credible_set: StudyLocus,\n        feature_matrix: L2GFeatureMatrix,\n        model_path: str | None,\n        features_list: list[str] | None = None,\n        hf_token: str | None = None,\n        hf_model_version: str | None = None,\n        download_from_hub: bool = True,\n    ) -&gt; L2GPrediction:\n        \"\"\"Extract L2G predictions for a set of credible sets derived from GWAS.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            credible_set (StudyLocus): Dataset containing credible sets from GWAS only\n            feature_matrix (L2GFeatureMatrix): Dataset containing all credible sets and their annotations\n            model_path (str | None): Path to the model file. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).\n            features_list (list[str] | None): Default list of features the model uses. Only used if the model is not downloaded from the Hub. CAUTION: This default list can differ from the actual list the model was trained on.\n            hf_token (str | None): Hugging Face token to download the model from the Hub. Only required if the model is private.\n            hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n            download_from_hub (bool): Whether to download the model from the Hugging Face Hub. Defaults to True.\n\n        Returns:\n            L2GPrediction: L2G scores for a set of credible sets.\n\n        Raises:\n            AttributeError: If `features_list` is not provided and the model is not downloaded from the Hub.\n        \"\"\"\n        # Load the model\n        if download_from_hub:\n            # Model ID defaults to \"opentargets/locus_to_gene\" and it assumes the name of the classifier is \"classifier.skops\".\n            model_id = model_path or \"opentargets/locus_to_gene\"\n            l2g_model = LocusToGeneModel.load_from_hub(\n                session, model_id, hf_model_version, hf_token\n            )\n        elif model_path:\n            if not features_list:\n                raise AttributeError(\n                    \"features_list is required if the model is not downloaded from the Hub\"\n                )\n            l2g_model = LocusToGeneModel.load_from_disk(\n                session, path=model_path, features_list=features_list\n            )\n\n        # Prepare data\n        fm = (\n            L2GFeatureMatrix(\n                _df=(\n                    credible_set.df.filter(f.col(\"studyType\") == \"gwas\")\n                    .select(\"studyLocusId\")\n                    .join(feature_matrix._df, \"studyLocusId\")\n                    .filter(f.col(\"isProteinCoding\") == 1)\n                ),\n            )\n            .fill_na()\n            .select_features(l2g_model.features_list)\n        )\n        return l2g_model.predict(fm, session)\n\n    def to_disease_target_evidence(\n        self: L2GPrediction,\n        study_locus: StudyLocus,\n        study_index: StudyIndex,\n        l2g_threshold: float = 0.05,\n    ) -&gt; DataFrame:\n        \"\"\"Convert locus to gene predictions to disease target evidence.\n\n        Args:\n            study_locus (StudyLocus): Study locus dataset\n            study_index (StudyIndex): Study index dataset\n            l2g_threshold (float): Threshold to consider a gene as a target. Defaults to 0.05.\n\n        Returns:\n            DataFrame: Disease target evidence\n        \"\"\"\n        datasource_id = \"gwas_credible_sets\"\n        datatype_id = \"genetic_association\"\n\n        return (\n            self.df.filter(f.col(\"score\") &gt;= l2g_threshold)\n            .join(\n                study_locus.df.select(\"studyLocusId\", \"studyId\"),\n                on=\"studyLocusId\",\n                how=\"inner\",\n            )\n            .join(\n                study_index.df.select(\"studyId\", \"diseaseIds\"),\n                on=\"studyId\",\n                how=\"inner\",\n            )\n            .select(\n                f.lit(datatype_id).alias(\"datatypeId\"),\n                f.lit(datasource_id).alias(\"datasourceId\"),\n                f.col(\"geneId\").alias(\"targetFromSourceId\"),\n                f.explode(f.col(\"diseaseIds\")).alias(\"diseaseFromSourceMappedId\"),\n                f.col(\"score\").alias(\"resourceScore\"),\n                \"studyLocusId\",\n            )\n        )\n\n    def explain(\n        self: L2GPrediction, feature_matrix: L2GFeatureMatrix | None = None\n    ) -&gt; L2GPrediction:\n        \"\"\"Extract Shapley values for the L2G predictions and add them as a map in an additional column.\n\n        Args:\n            feature_matrix (L2GFeatureMatrix | None): Feature matrix in case the predictions are missing the feature annotation. If None, the features are fetched from the dataset.\n\n        Returns:\n            L2GPrediction: L2GPrediction object with additional column containing feature name to Shapley value mappings\n\n        Raises:\n            ValueError: If the model is not set or If feature matrix is not provided and the predictions do not have features\n        \"\"\"\n        # Fetch features if they are not present:\n        if \"features\" not in self.df.columns:\n            if feature_matrix is None:\n                raise ValueError(\n                    \"Feature matrix is required to explain the L2G predictions\"\n                )\n            self.add_features(feature_matrix)\n\n        if self.model is None:\n            raise ValueError(\"Model not set, explainer cannot be created\")\n\n        # Format and pivot the dataframe to pass them before calculating shapley values\n        pdf = pivot_df(\n            df=self.df.withColumn(\"feature\", f.explode(\"features\")).select(\n                \"studyLocusId\",\n                \"geneId\",\n                \"score\",\n                f.col(\"feature.name\").alias(\"feature_name\"),\n                f.col(\"feature.value\").alias(\"feature_value\"),\n            ),\n            pivot_col=\"feature_name\",\n            value_col=\"feature_value\",\n            grouping_cols=[f.col(\"studyLocusId\"), f.col(\"geneId\"), f.col(\"score\")],\n        ).toPandas()\n        pdf = pdf.rename(\n            # trim the suffix that is added after pivoting the df\n            columns={\n                col: col.replace(\"_feature_value\", \"\")\n                for col in pdf.columns\n                if col.endswith(\"_feature_value\")\n            }\n        )\n\n        features_list = self.model.features_list  # The matrix needs to present the features in the same order that the model was trained on)\n        base_value, shap_values = L2GPrediction._explain(\n            model=self.model,\n            pdf=pdf.filter(items=features_list),\n        )\n        for i, feature in enumerate(features_list):\n            pdf[f\"shap_{feature}\"] = [row[i] for row in shap_values]\n\n        spark_session = self.df.sparkSession\n        return L2GPrediction(\n            _df=(\n                spark_session.createDataFrame(pdf.to_dict(orient=\"records\"))\n                .withColumn(\n                    \"features\",\n                    f.array(\n                        *(\n                            f.struct(\n                                f.lit(feature).alias(\"name\"),\n                                f.col(feature).cast(\"float\").alias(\"value\"),\n                                f.col(f\"shap_{feature}\")\n                                .cast(\"float\")\n                                .alias(\"shapValue\"),\n                            )\n                            for feature in features_list\n                        )\n                    ),\n                )\n                .withColumn(\"shapBaseValue\", f.lit(base_value).cast(\"float\"))\n                .select(*L2GPrediction.get_schema().names)\n            ),\n            _schema=self.get_schema(),\n            model=self.model,\n        )\n\n    @staticmethod\n    def _explain(\n        model: LocusToGeneModel, pdf: pd_dataframe\n    ) -&gt; tuple[float, list[list[float]]]:\n        \"\"\"Calculate SHAP values. Output is in probability form (approximated from the log odds ratios).\n\n        Args:\n            model (LocusToGeneModel): L2G model\n            pdf (pd_dataframe): Pandas dataframe containing the feature matrix in the same order that the model was trained on\n\n        Returns:\n            tuple[float, list[list[float]]]: A tuple containing:\n                - base_value (float): Base value of the model\n                - shap_values (list[list[float]]): SHAP values for prediction\n\n        Raises:\n            AttributeError: If model.training_data is not set, seed dataset to get shapley values cannot be created.\n        \"\"\"\n        if not model.training_data:\n            raise AttributeError(\n                \"`model.training_data` is missing, seed dataset to get shapley values cannot be created.\"\n            )\n        background_data = (\n            model.training_data._df.select(*model.features_list)\n            .toPandas()\n            .sample(n=1_000)\n        )\n        explainer = shap.TreeExplainer(\n            model.model,\n            data=background_data,\n            model_output=\"probability\",\n        )\n        if pdf.shape[0] &gt;= 10_000:\n            logging.warning(\n                \"Calculating SHAP values for more than 10,000 rows. This may take a while...\"\n            )\n        shap_values = explainer.shap_values(\n            pdf.to_numpy(),\n            check_additivity=False,\n        )\n        base_value = explainer.expected_value\n        return (base_value, shap_values)\n\n    def add_features(\n        self: L2GPrediction,\n        feature_matrix: L2GFeatureMatrix,\n    ) -&gt; L2GPrediction:\n        \"\"\"Add features used to extract the L2G predictions.\n\n        Args:\n            feature_matrix (L2GFeatureMatrix): Feature matrix dataset\n\n        Returns:\n            L2GPrediction: L2G predictions with additional column `features`\n\n        Raises:\n            ValueError: If model is not set, feature list won't be available\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not set, feature annotation cannot be created.\")\n        # Testing if `features` column already exists:\n        if \"features\" in self.df.columns:\n            self.df = self.df.drop(\"features\")\n\n        features_list = self.model.features_list\n        feature_expressions = [\n            f.struct(f.lit(col).alias(\"name\"), f.col(col).alias(\"value\"))\n            for col in features_list\n        ]\n        self.df = self.df.join(\n            feature_matrix._df.select(*features_list, \"studyLocusId\", \"geneId\"),\n            on=[\"studyLocusId\", \"geneId\"],\n            how=\"left\",\n        ).select(\n            \"studyLocusId\",\n            \"geneId\",\n            \"score\",\n            f.array(*feature_expressions).alias(\"features\"),\n        )\n        return self\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.add_features","title":"<code>add_features(feature_matrix: L2GFeatureMatrix) -&gt; L2GPrediction</code>","text":"<p>Add features used to extract the L2G predictions.</p> <p>Parameters:</p> Name Type Description Default <code>feature_matrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix dataset</p> required <p>Returns:</p> Name Type Description <code>L2GPrediction</code> <code>L2GPrediction</code> <p>L2G predictions with additional column <code>features</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model is not set, feature list won't be available</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>def add_features(\n    self: L2GPrediction,\n    feature_matrix: L2GFeatureMatrix,\n) -&gt; L2GPrediction:\n    \"\"\"Add features used to extract the L2G predictions.\n\n    Args:\n        feature_matrix (L2GFeatureMatrix): Feature matrix dataset\n\n    Returns:\n        L2GPrediction: L2G predictions with additional column `features`\n\n    Raises:\n        ValueError: If model is not set, feature list won't be available\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model not set, feature annotation cannot be created.\")\n    # Testing if `features` column already exists:\n    if \"features\" in self.df.columns:\n        self.df = self.df.drop(\"features\")\n\n    features_list = self.model.features_list\n    feature_expressions = [\n        f.struct(f.lit(col).alias(\"name\"), f.col(col).alias(\"value\"))\n        for col in features_list\n    ]\n    self.df = self.df.join(\n        feature_matrix._df.select(*features_list, \"studyLocusId\", \"geneId\"),\n        on=[\"studyLocusId\", \"geneId\"],\n        how=\"left\",\n    ).select(\n        \"studyLocusId\",\n        \"geneId\",\n        \"score\",\n        f.array(*feature_expressions).alias(\"features\"),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.explain","title":"<code>explain(feature_matrix: L2GFeatureMatrix | None = None) -&gt; L2GPrediction</code>","text":"<p>Extract Shapley values for the L2G predictions and add them as a map in an additional column.</p> <p>Parameters:</p> Name Type Description Default <code>feature_matrix</code> <code>L2GFeatureMatrix | None</code> <p>Feature matrix in case the predictions are missing the feature annotation. If None, the features are fetched from the dataset.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>L2GPrediction</code> <code>L2GPrediction</code> <p>L2GPrediction object with additional column containing feature name to Shapley value mappings</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not set or If feature matrix is not provided and the predictions do not have features</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>def explain(\n    self: L2GPrediction, feature_matrix: L2GFeatureMatrix | None = None\n) -&gt; L2GPrediction:\n    \"\"\"Extract Shapley values for the L2G predictions and add them as a map in an additional column.\n\n    Args:\n        feature_matrix (L2GFeatureMatrix | None): Feature matrix in case the predictions are missing the feature annotation. If None, the features are fetched from the dataset.\n\n    Returns:\n        L2GPrediction: L2GPrediction object with additional column containing feature name to Shapley value mappings\n\n    Raises:\n        ValueError: If the model is not set or If feature matrix is not provided and the predictions do not have features\n    \"\"\"\n    # Fetch features if they are not present:\n    if \"features\" not in self.df.columns:\n        if feature_matrix is None:\n            raise ValueError(\n                \"Feature matrix is required to explain the L2G predictions\"\n            )\n        self.add_features(feature_matrix)\n\n    if self.model is None:\n        raise ValueError(\"Model not set, explainer cannot be created\")\n\n    # Format and pivot the dataframe to pass them before calculating shapley values\n    pdf = pivot_df(\n        df=self.df.withColumn(\"feature\", f.explode(\"features\")).select(\n            \"studyLocusId\",\n            \"geneId\",\n            \"score\",\n            f.col(\"feature.name\").alias(\"feature_name\"),\n            f.col(\"feature.value\").alias(\"feature_value\"),\n        ),\n        pivot_col=\"feature_name\",\n        value_col=\"feature_value\",\n        grouping_cols=[f.col(\"studyLocusId\"), f.col(\"geneId\"), f.col(\"score\")],\n    ).toPandas()\n    pdf = pdf.rename(\n        # trim the suffix that is added after pivoting the df\n        columns={\n            col: col.replace(\"_feature_value\", \"\")\n            for col in pdf.columns\n            if col.endswith(\"_feature_value\")\n        }\n    )\n\n    features_list = self.model.features_list  # The matrix needs to present the features in the same order that the model was trained on)\n    base_value, shap_values = L2GPrediction._explain(\n        model=self.model,\n        pdf=pdf.filter(items=features_list),\n    )\n    for i, feature in enumerate(features_list):\n        pdf[f\"shap_{feature}\"] = [row[i] for row in shap_values]\n\n    spark_session = self.df.sparkSession\n    return L2GPrediction(\n        _df=(\n            spark_session.createDataFrame(pdf.to_dict(orient=\"records\"))\n            .withColumn(\n                \"features\",\n                f.array(\n                    *(\n                        f.struct(\n                            f.lit(feature).alias(\"name\"),\n                            f.col(feature).cast(\"float\").alias(\"value\"),\n                            f.col(f\"shap_{feature}\")\n                            .cast(\"float\")\n                            .alias(\"shapValue\"),\n                        )\n                        for feature in features_list\n                    )\n                ),\n            )\n            .withColumn(\"shapBaseValue\", f.lit(base_value).cast(\"float\"))\n            .select(*L2GPrediction.get_schema().names)\n        ),\n        _schema=self.get_schema(),\n        model=self.model,\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.from_credible_set","title":"<code>from_credible_set(session: Session, credible_set: StudyLocus, feature_matrix: L2GFeatureMatrix, model_path: str | None, features_list: list[str] | None = None, hf_token: str | None = None, hf_model_version: str | None = None, download_from_hub: bool = True) -&gt; L2GPrediction</code>  <code>classmethod</code>","text":"<p>Extract L2G predictions for a set of credible sets derived from GWAS.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>credible_set</code> <code>StudyLocus</code> <p>Dataset containing credible sets from GWAS only</p> required <code>feature_matrix</code> <code>L2GFeatureMatrix</code> <p>Dataset containing all credible sets and their annotations</p> required <code>model_path</code> <code>str | None</code> <p>Path to the model file. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).</p> required <code>features_list</code> <code>list[str] | None</code> <p>Default list of features the model uses. Only used if the model is not downloaded from the Hub. CAUTION: This default list can differ from the actual list the model was trained on.</p> <code>None</code> <code>hf_token</code> <code>str | None</code> <p>Hugging Face token to download the model from the Hub. Only required if the model is private.</p> <code>None</code> <code>hf_model_version</code> <code>str | None</code> <p>Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.</p> <code>None</code> <code>download_from_hub</code> <code>bool</code> <p>Whether to download the model from the Hugging Face Hub. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>L2GPrediction</code> <code>L2GPrediction</code> <p>L2G scores for a set of credible sets.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If <code>features_list</code> is not provided and the model is not downloaded from the Hub.</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>@classmethod\ndef from_credible_set(\n    cls: type[L2GPrediction],\n    session: Session,\n    credible_set: StudyLocus,\n    feature_matrix: L2GFeatureMatrix,\n    model_path: str | None,\n    features_list: list[str] | None = None,\n    hf_token: str | None = None,\n    hf_model_version: str | None = None,\n    download_from_hub: bool = True,\n) -&gt; L2GPrediction:\n    \"\"\"Extract L2G predictions for a set of credible sets derived from GWAS.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        credible_set (StudyLocus): Dataset containing credible sets from GWAS only\n        feature_matrix (L2GFeatureMatrix): Dataset containing all credible sets and their annotations\n        model_path (str | None): Path to the model file. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).\n        features_list (list[str] | None): Default list of features the model uses. Only used if the model is not downloaded from the Hub. CAUTION: This default list can differ from the actual list the model was trained on.\n        hf_token (str | None): Hugging Face token to download the model from the Hub. Only required if the model is private.\n        hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n        download_from_hub (bool): Whether to download the model from the Hugging Face Hub. Defaults to True.\n\n    Returns:\n        L2GPrediction: L2G scores for a set of credible sets.\n\n    Raises:\n        AttributeError: If `features_list` is not provided and the model is not downloaded from the Hub.\n    \"\"\"\n    # Load the model\n    if download_from_hub:\n        # Model ID defaults to \"opentargets/locus_to_gene\" and it assumes the name of the classifier is \"classifier.skops\".\n        model_id = model_path or \"opentargets/locus_to_gene\"\n        l2g_model = LocusToGeneModel.load_from_hub(\n            session, model_id, hf_model_version, hf_token\n        )\n    elif model_path:\n        if not features_list:\n            raise AttributeError(\n                \"features_list is required if the model is not downloaded from the Hub\"\n            )\n        l2g_model = LocusToGeneModel.load_from_disk(\n            session, path=model_path, features_list=features_list\n        )\n\n    # Prepare data\n    fm = (\n        L2GFeatureMatrix(\n            _df=(\n                credible_set.df.filter(f.col(\"studyType\") == \"gwas\")\n                .select(\"studyLocusId\")\n                .join(feature_matrix._df, \"studyLocusId\")\n                .filter(f.col(\"isProteinCoding\") == 1)\n            ),\n        )\n        .fill_na()\n        .select_features(l2g_model.features_list)\n    )\n    return l2g_model.predict(fm, session)\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the L2GPrediction dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the L2GPrediction dataset</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[L2GPrediction]) -&gt; StructType:\n    \"\"\"Provides the schema for the L2GPrediction dataset.\n\n    Returns:\n        StructType: Schema for the L2GPrediction dataset\n    \"\"\"\n    return parse_spark_schema(\"l2g_predictions.json\")\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.to_disease_target_evidence","title":"<code>to_disease_target_evidence(study_locus: StudyLocus, study_index: StudyIndex, l2g_threshold: float = 0.05) -&gt; DataFrame</code>","text":"<p>Convert locus to gene predictions to disease target evidence.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus</code> <code>StudyLocus</code> <p>Study locus dataset</p> required <code>study_index</code> <code>StudyIndex</code> <p>Study index dataset</p> required <code>l2g_threshold</code> <code>float</code> <p>Threshold to consider a gene as a target. Defaults to 0.05.</p> <code>0.05</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Disease target evidence</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>def to_disease_target_evidence(\n    self: L2GPrediction,\n    study_locus: StudyLocus,\n    study_index: StudyIndex,\n    l2g_threshold: float = 0.05,\n) -&gt; DataFrame:\n    \"\"\"Convert locus to gene predictions to disease target evidence.\n\n    Args:\n        study_locus (StudyLocus): Study locus dataset\n        study_index (StudyIndex): Study index dataset\n        l2g_threshold (float): Threshold to consider a gene as a target. Defaults to 0.05.\n\n    Returns:\n        DataFrame: Disease target evidence\n    \"\"\"\n    datasource_id = \"gwas_credible_sets\"\n    datatype_id = \"genetic_association\"\n\n    return (\n        self.df.filter(f.col(\"score\") &gt;= l2g_threshold)\n        .join(\n            study_locus.df.select(\"studyLocusId\", \"studyId\"),\n            on=\"studyLocusId\",\n            how=\"inner\",\n        )\n        .join(\n            study_index.df.select(\"studyId\", \"diseaseIds\"),\n            on=\"studyId\",\n            how=\"inner\",\n        )\n        .select(\n            f.lit(datatype_id).alias(\"datatypeId\"),\n            f.lit(datasource_id).alias(\"datasourceId\"),\n            f.col(\"geneId\").alias(\"targetFromSourceId\"),\n            f.explode(f.col(\"diseaseIds\")).alias(\"diseaseFromSourceMappedId\"),\n            f.col(\"score\").alias(\"resourceScore\"),\n            \"studyLocusId\",\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#schema","title":"Schema","text":"<pre><code>root\n |-- studyLocusId: string (nullable = false)\n |-- geneId: string (nullable = false)\n |-- score: double (nullable = false)\n |-- features: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- name: string (nullable = false)\n |    |    |-- value: float (nullable = false)\n |    |    |-- shapValue: float (nullable = true)\n |-- shapBaseValue: float (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/ld_index/","title":"LD Index","text":""},{"location":"python_api/datasets/ld_index/#gentropy.dataset.ld_index.LDIndex","title":"<code>gentropy.dataset.ld_index.LDIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset containing linkage desequilibrium information between variants.</p> Source code in <code>src/gentropy/dataset/ld_index.py</code> <pre><code>@dataclass\nclass LDIndex(Dataset):\n    \"\"\"Dataset containing linkage desequilibrium information between variants.\"\"\"\n\n    @classmethod\n    def get_schema(cls: type[LDIndex]) -&gt; StructType:\n        \"\"\"Provides the schema for the LDIndex dataset.\n\n        Returns:\n            StructType: Schema for the LDIndex dataset\n        \"\"\"\n        return parse_spark_schema(\"ld_index.json\")\n</code></pre>"},{"location":"python_api/datasets/ld_index/#gentropy.dataset.ld_index.LDIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the LDIndex dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the LDIndex dataset</p> Source code in <code>src/gentropy/dataset/ld_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[LDIndex]) -&gt; StructType:\n    \"\"\"Provides the schema for the LDIndex dataset.\n\n    Returns:\n        StructType: Schema for the LDIndex dataset\n    \"\"\"\n    return parse_spark_schema(\"ld_index.json\")\n</code></pre>"},{"location":"python_api/datasets/ld_index/#schema","title":"Schema","text":"<pre><code>root\n |-- variantId: string (nullable = false)\n |-- chromosome: string (nullable = false)\n |-- ldSet: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- tagVariantId: string (nullable = false)\n |    |    |-- rValues: array (nullable = false)\n |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |-- population: string (nullable = false)\n |    |    |    |    |-- r: double (nullable = false)\n</code></pre>"},{"location":"python_api/datasets/study_index/","title":"Study Index","text":""},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex","title":"<code>gentropy.dataset.study_index.StudyIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Study index dataset.</p> <p>A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@dataclass\nclass StudyIndex(Dataset):\n    \"\"\"Study index dataset.\n\n    A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.\n    \"\"\"\n\n    VALID_TYPES = [\n        \"gwas\",\n        \"eqtl\",\n        \"pqtl\",\n        \"sqtl\",\n        \"tuqtl\",\n        \"sceqtl\",\n        \"scpqtl\",\n        \"scsqtl\",\n        \"sctuqtl\",\n    ]\n\n    @staticmethod\n    def _aggregate_samples_by_ancestry(merged: Column, ancestry: Column) -&gt; Column:\n        \"\"\"Aggregate sample counts by ancestry in a list of struct colmns.\n\n        Args:\n            merged (Column): A column representing merged data (list of structs).\n            ancestry (Column): The `ancestry` parameter is a column that represents the ancestry of each\n                sample. (a struct)\n\n        Returns:\n            Column: the modified \"merged\" column after aggregating the samples by ancestry.\n        \"\"\"\n        # Iterating over the list of ancestries and adding the sample size if label matches:\n        return f.transform(\n            merged,\n            lambda a: f.when(\n                a.ancestry == ancestry.ancestry,\n                f.struct(\n                    a.ancestry.alias(\"ancestry\"),\n                    (a.sampleSize + ancestry.sampleSize).alias(\"sampleSize\"),\n                ),\n            ).otherwise(a),\n        )\n\n    @staticmethod\n    def _map_ancestries_to_ld_population(gwas_ancestry_label: Column) -&gt; Column:\n        \"\"\"Normalise ancestry column from GWAS studies into reference LD panel based on a pre-defined map.\n\n        This function assumes all possible ancestry categories have a corresponding\n        LD panel in the LD index. It is very important to have the ancestry labels\n        moved to the LD panel map.\n\n        Args:\n            gwas_ancestry_label (Column): A struct column with ancestry label like Finnish,\n                European, African etc. and the corresponding sample size.\n\n        Returns:\n            Column: Struct column with the mapped LD population label and the sample size.\n        \"\"\"\n        # Loading ancestry label to LD population label:\n        json_dict = json.loads(\n            pkg_resources.read_text(\n                data, \"gwas_population_2_LD_panel_map.json\", encoding=\"utf-8\"\n            )\n        )\n        map_expr = f.create_map(*[f.lit(x) for x in chain(*json_dict.items())])\n\n        return f.struct(\n            map_expr[gwas_ancestry_label.ancestry].alias(\"ancestry\"),\n            gwas_ancestry_label.sampleSize.alias(\"sampleSize\"),\n        )\n\n    @classmethod\n    def get_schema(cls: type[StudyIndex]) -&gt; StructType:\n        \"\"\"Provide the schema for the StudyIndex dataset.\n\n        Returns:\n            StructType: The schema of the StudyIndex dataset.\n        \"\"\"\n        return parse_spark_schema(\"study_index.json\")\n\n    @classmethod\n    def get_QC_column_name(cls: type[StudyIndex]) -&gt; str:\n        \"\"\"Return the name of the quality control column.\n\n        Returns:\n            str: The name of the quality control column.\n        \"\"\"\n        return \"qualityControls\"\n\n    @classmethod\n    def get_QC_mappings(cls: type[StudyIndex]) -&gt; dict[str, str]:\n        \"\"\"Quality control flag to QC column category mappings.\n\n        Returns:\n            dict[str, str]: Mapping between flag name and QC column category value.\n        \"\"\"\n        return {member.name: member.value for member in StudyQualityCheck}\n\n    @classmethod\n    def aggregate_and_map_ancestries(\n        cls: type[StudyIndex], discovery_samples: Column\n    ) -&gt; Column:\n        \"\"\"Map ancestries to populations in the LD reference and calculate relative sample size.\n\n        Args:\n            discovery_samples (Column): A list of struct column. Has an `ancestry` column and a `sampleSize` columns\n\n        Returns:\n            Column: A list of struct with mapped LD population and their relative sample size.\n        \"\"\"\n        # Map ancestry categories to population labels of the LD index:\n        mapped_ancestries = f.transform(\n            discovery_samples, cls._map_ancestries_to_ld_population\n        )\n\n        # Aggregate sample sizes belonging to the same LD population:\n        aggregated_counts = f.aggregate(\n            mapped_ancestries,\n            f.array_distinct(\n                f.transform(\n                    mapped_ancestries,\n                    lambda x: f.struct(\n                        x.ancestry.alias(\"ancestry\"), f.lit(0.0).alias(\"sampleSize\")\n                    ),\n                )\n            ),\n            cls._aggregate_samples_by_ancestry,\n        )\n        # Getting total sample count:\n        total_sample_count = f.aggregate(\n            aggregated_counts, f.lit(0.0), lambda total, pop: total + pop.sampleSize\n        ).alias(\"sampleSize\")\n\n        # Calculating relative sample size for each LD population:\n        return f.transform(\n            aggregated_counts,\n            lambda ld_population: f.struct(\n                ld_population.ancestry.alias(\"ldPopulation\"),\n                (ld_population.sampleSize / total_sample_count).alias(\n                    \"relativeSampleSize\"\n                ),\n            ),\n        )\n\n    def study_type_lut(self: StudyIndex) -&gt; DataFrame:\n        \"\"\"Return a lookup table of study type.\n\n        Returns:\n            DataFrame: A dataframe containing `studyId` and `studyType` columns.\n        \"\"\"\n        return self.df.select(\"studyId\", \"studyType\")\n\n    def is_qtl(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column with true values for QTL studies.\n\n        Returns:\n            Column: True if the study is a QTL study.\n        \"\"\"\n        return self.df.studyType.endswith(\"qtl\")\n\n    def is_gwas(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column with true values for GWAS studies.\n\n        Returns:\n            Column: True if the study is a GWAS study.\n        \"\"\"\n        return self.df.studyType == \"gwas\"\n\n    def has_mapped_trait(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column indicating if a study has mapped disease.\n\n        Returns:\n            Column: True if the study has mapped disease.\n        \"\"\"\n        return f.size(self.df.traitFromSourceMappedIds) &gt; 0\n\n    def is_quality_flagged(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column indicating if a study is flagged due to quality issues.\n\n        Returns:\n            Column: True if the study is flagged.\n        \"\"\"\n        # Testing for the presence of the qualityControls column:\n        if \"qualityControls\" not in self.df.columns:\n            return f.lit(False)\n        else:\n            return f.size(self.df[\"qualityControls\"]) != 0\n\n    def has_summarystats(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column indicating if a study has harmonized summary statistics.\n\n        Returns:\n            Column: True if the study has harmonized summary statistics.\n        \"\"\"\n        return self.df.hasSumstats\n\n    def validate_unique_study_id(self: StudyIndex) -&gt; StudyIndex:\n        \"\"\"Validating the uniqueness of study identifiers and flagging duplicated studies.\n\n        Returns:\n            StudyIndex: with flagged duplicated studies.\n        \"\"\"\n        return StudyIndex(\n            _df=self.df.withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    self.flag_duplicates(f.col(\"studyId\")),\n                    StudyQualityCheck.DUPLICATED_STUDY,\n                ),\n            ),\n            _schema=StudyIndex.get_schema(),\n        )\n\n    def _normalise_disease(\n        self: StudyIndex,\n        source_disease_column_name: str,\n        disease_column_name: str,\n        disease_map: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"Normalising diseases in the study index.\n\n        Given a reference disease map (containing all potential EFO ids with the corresponding reference disease ids),\n        this function maps all EFO ids in the study index to the reference disease ids.\n\n        Args:\n            source_disease_column_name (str): The column name of the disease column to validate.\n            disease_column_name (str): The resulting disease column name that contains the validated ids.\n            disease_map (DataFrame): Reference dataframe with diseases\n\n        Returns:\n            DataFrame: where the newly added diseaseIds column will contain the validated EFO identifiers.\n        \"\"\"\n        return (\n            self.df\n            # Only validating studies with diseases:\n            .filter(f.size(f.col(source_disease_column_name)) &gt; 0)\n            # Explode disease column:\n            .select(\n                \"studyId\",\n                \"studyType\",\n                f.explode_outer(source_disease_column_name).alias(\"efo\"),\n            )\n            # Join disease map:\n            .join(disease_map, on=\"efo\", how=\"left\")\n            .groupBy(\"studyId\")\n            .agg(\n                f.collect_set(f.col(\"diseaseId\")).alias(disease_column_name),\n            )\n        )\n\n    def validate_disease(self: StudyIndex, disease_map: DataFrame) -&gt; StudyIndex:\n        \"\"\"Validate diseases in the study index dataset.\n\n        Args:\n            disease_map (DataFrame): a dataframe with two columns (efo, diseaseId).\n\n        Returns:\n            StudyIndex: where gwas studies are flagged where no valid disease id could be found.\n        \"\"\"\n        # Because the disease ids are not mandatory fields of the schema, we skip vaildation if these columns are not present:\n        if (\"traitFromSourceMappedIds\" not in self.df.columns) or (\n            \"backgroundTraitFromSourceMappedIds\" not in self.df.columns\n        ):\n            return self\n\n        # Disease Column names:\n        foreground_disease_column = \"diseaseIds\"\n        background_disease_column = \"backgroundDiseaseIds\"\n\n        # If diseaseId in schema, we need to drop it:\n        drop_columns = [\n            column\n            for column in self.df.columns\n            if column in [foreground_disease_column, background_disease_column]\n        ]\n\n        if len(drop_columns) &gt; 0:\n            self.df = self.df.drop(*drop_columns)\n\n        # Normalise disease:\n        normalised_disease = self._normalise_disease(\n            \"traitFromSourceMappedIds\", foreground_disease_column, disease_map\n        )\n        normalised_background_disease = self._normalise_disease(\n            \"backgroundTraitFromSourceMappedIds\", background_disease_column, disease_map\n        )\n\n        return StudyIndex(\n            _df=(\n                self.df.join(normalised_disease, on=\"studyId\", how=\"left\")\n                .join(normalised_background_disease, on=\"studyId\", how=\"left\")\n                # Updating disease columns:\n                .withColumn(\n                    foreground_disease_column,\n                    f.when(\n                        f.col(foreground_disease_column).isNull(), f.array()\n                    ).otherwise(f.col(foreground_disease_column)),\n                )\n                .withColumn(\n                    background_disease_column,\n                    f.when(\n                        f.col(background_disease_column).isNull(), f.array()\n                    ).otherwise(f.col(background_disease_column)),\n                )\n                # Flagging gwas studies where no valid disease is avilable:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyIndex.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        # Flagging all gwas studies with no normalised disease:\n                        (f.size(f.col(foreground_disease_column)) == 0)\n                        &amp; (f.col(\"studyType\") == \"gwas\"),\n                        StudyQualityCheck.UNRESOLVED_DISEASE,\n                    ),\n                )\n            ),\n            _schema=StudyIndex.get_schema(),\n        )\n\n    def validate_study_type(self: StudyIndex) -&gt; StudyIndex:\n        \"\"\"Validating study type and flag unsupported types.\n\n        Returns:\n            StudyIndex: with flagged studies with unsupported type.\n        \"\"\"\n        validated_df = (\n            self.df\n            # Flagging unsupported study types:\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.when(\n                        (f.col(\"studyType\") == \"gwas\")\n                        | f.col(\"studyType\").endswith(\"qtl\"),\n                        False,\n                    ).otherwise(True),\n                    StudyQualityCheck.UNKNOWN_STUDY_TYPE,\n                ),\n            )\n        )\n        return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n\n    def validate_target(self: StudyIndex, target_index: TargetIndex) -&gt; StudyIndex:\n        \"\"\"Validating gene identifiers in the study index against the provided target index.\n\n        Args:\n            target_index (TargetIndex): target index containing the reference gene identifiers (Ensembl gene identifiers).\n\n        Returns:\n            StudyIndex: with flagged studies if geneId could not be validated.\n        \"\"\"\n        gene_set = target_index.df.select(f.col(\"id\").alias(\"geneId\"), f.lit(True).alias(\"isIdFound\"))\n\n        # As the geneId is not a mandatory field of study index, we return if the column is not there:\n        if \"geneId\" not in self.df.columns:\n            return self\n\n        validated_df = (\n            self.df.join(gene_set, on=\"geneId\", how=\"left\")\n            .withColumn(\n                \"isIdFound\",\n                f.when(\n                    (f.col(\"studyType\") != \"gwas\") &amp; f.col(\"isIdFound\").isNull(),\n                    f.lit(False),\n                ).otherwise(f.lit(True)),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~f.col(\"isIdFound\"),\n                    StudyQualityCheck.UNRESOLVED_TARGET,\n                ),\n            )\n            .drop(\"isIdFound\")\n        )\n\n        return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n\n    def validate_biosample(\n        self: StudyIndex, biosample_index: BiosampleIndex\n    ) -&gt; StudyIndex:\n        \"\"\"Validating biosample identifiers in the study index against the provided biosample index.\n\n        Args:\n            biosample_index (BiosampleIndex): Biosample index containing a reference of biosample identifiers e.g. cell types, tissues, cell lines, etc.\n\n        Returns:\n            StudyIndex: where non-gwas studies are flagged if biosampleIndex could not be validated.\n        \"\"\"\n        biosample_set = biosample_index.df.select(\n            \"biosampleId\", f.lit(True).alias(\"isIdFound\")\n        )\n\n        # If biosampleId in df, we need to drop it:\n        if \"biosampleId\" in self.df.columns:\n            self.df = self.df.drop(\"biosampleId\")\n\n        # As the biosampleFromSourceId is not a mandatory field of study index, we return if the column is not there:\n        if \"biosampleFromSourceId\" not in self.df.columns:\n            return self\n\n        validated_df = (\n            self.df.join(\n                biosample_set,\n                self.df.biosampleFromSourceId == biosample_set.biosampleId,\n                how=\"left\",\n            )\n            .withColumn(\n                \"isIdFound\",\n                f.when(\n                    (f.col(\"studyType\") != \"gwas\") &amp; (f.col(\"isIdFound\").isNull()),\n                    f.lit(False),\n                ).otherwise(f.lit(True)),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~f.col(\"isIdFound\"),\n                    StudyQualityCheck.UNKNOWN_BIOSAMPLE,\n                ),\n            )\n            .drop(\"isIdFound\")\n        )\n\n        return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n\n    def annotate_sumstats_qc(\n        self: StudyIndex,\n        sumstats_qc: DataFrame,\n        threshold_mean_beta: float = 0.05,\n        threshold_mean_diff_pz: float = 0.05,\n        threshold_se_diff_pz: float = 0.05,\n        threshold_min_gc_lambda: float = 0.7,\n        threshold_max_gc_lambda: float = 2.5,\n        threshold_min_n_variants: int = 2_000_000,\n    ) -&gt; StudyIndex:\n        \"\"\"Annotate summary stats QC information.\n\n        Args:\n            sumstats_qc (DataFrame): containing summary statistics-based quality controls.\n            threshold_mean_beta (float): Threshold for mean beta check. Defaults to 0.05.\n            threshold_mean_diff_pz (float): Threshold for mean diff PZ check. Defaults to 0.05.\n            threshold_se_diff_pz (float): Threshold for SE diff PZ check. Defaults to 0.05.\n            threshold_min_gc_lambda (float): Minimum threshold for GC lambda check. Defaults to 0.7.\n            threshold_max_gc_lambda (float): Maximum threshold for GC lambda check. Defaults to 2.5.\n            threshold_min_n_variants (int): Minimum number of variants for SuSiE check. Defaults to 2_000_000.\n\n        Returns:\n            StudyIndex: Updated study index with QC information\n        \"\"\"\n        # convert all columns in sumstats_qc dataframe in array of structs grouped by studyId\n        cols = [c for c in sumstats_qc.columns if c != \"studyId\"]\n\n        studies = self.df\n\n        melted_df = convert_from_wide_to_long(\n            sumstats_qc,\n            id_vars=[\"studyId\"],\n            value_vars=cols,\n            var_name=\"QCCheckName\",\n            value_name=\"QCCheckValue\",\n        )\n\n        qc_df = (\n            melted_df.groupBy(\"studyId\")\n            .agg(\n                f.map_from_entries(\n                    f.collect_list(\n                        f.struct(f.col(\"QCCheckName\"), f.col(\"QCCheckValue\"))\n                    )\n                ).alias(\"sumStatQCValues\")\n            )\n            .select(\"studyId\", \"sumstatQCValues\")\n        )\n\n        df = (\n            studies.drop(\"sumStatQCValues\", \"hasSumstats\")\n            .join(\n                qc_df.withColumn(\"hasSumstats\", f.lit(True)), how=\"left\", on=\"studyId\"\n            )\n            .withColumn(\"hasSumstats\", f.coalesce(f.col(\"hasSumstats\"), f.lit(False)))\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~f.col(\"hasSumstats\"),\n                    StudyQualityCheck.SUMSTATS_NOT_AVAILABLE,\n                ),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~(f.abs(f.col(\"sumstatQCValues.mean_beta\")) &lt;= threshold_mean_beta),\n                    StudyQualityCheck.FAILED_MEAN_BETA_CHECK,\n                ),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~(\n                        (\n                            f.abs(f.col(\"sumstatQCValues.mean_diff_pz\"))\n                            &lt;= threshold_mean_diff_pz\n                        )\n                        &amp; (f.col(\"sumstatQCValues.se_diff_pz\") &lt;= threshold_se_diff_pz)\n                    ),\n                    StudyQualityCheck.FAILED_PZ_CHECK,\n                ),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~(\n                        (f.col(\"sumstatQCValues.gc_lambda\") &lt;= threshold_max_gc_lambda)\n                        &amp; (\n                            f.col(\"sumstatQCValues.gc_lambda\")\n                            &gt;= threshold_min_gc_lambda\n                        )\n                    ),\n                    StudyQualityCheck.FAILED_GC_LAMBDA_CHECK,\n                ),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    (f.col(\"sumstatQCValues.n_variants\") &lt; threshold_min_n_variants),\n                    StudyQualityCheck.SMALL_NUMBER_OF_SNPS,\n                ),\n            )\n        )\n\n        # Annotate study index with QC information:\n        return StudyIndex(\n            _df=df,\n            _schema=StudyIndex.get_schema(),\n        )\n\n    def deconvolute_studies(self: StudyIndex) -&gt; StudyIndex:\n        \"\"\"Deconvolute the study index dataset.\n\n        When ingesting the study index dataset, the same studyId might be ingested from more than one source.\n        In such cases, the data needs to be merged and the quality control flags need to be combined.\n\n        Returns:\n            StudyIndex: Deconvoluted study index dataset.\n        \"\"\"\n        # Windowing by study ID assume random order, but this is OK, because we are not selecting rows by a specific order.\n        study_id_window = Window.partitionBy(\"studyId\").orderBy(f.rand())\n\n        # For certain aggregation, the full window is needed to be considered:\n        full_study_id_window = study_id_window.orderBy(\"studyId\").rangeBetween(\n            Window.unboundedPreceding, Window.unboundedFollowing\n        )\n\n        # Temporary columns to drop at the end:\n        columns_to_drop = [\"keepTopHit\", \"mostGranular\", \"rank\"]\n\n        return StudyIndex(\n            _df=(\n                self.df\n                # Initialising quality controls column, if not present:\n                .withColumn(\n                    \"qualityControls\",\n                    f.when(\n                        f.col(\"qualityControls\").isNull(),\n                        f.array().cast(ArrayType(StringType())),\n                    ).otherwise(f.col(\"qualityControls\")),\n                )\n                # Keeping top hit studies unless the same study is available from a summmary statistics source:\n                # This value will be set for all rows for the same `studyId`:\n                .withColumn(\n                    \"keepTopHit\",\n                    f.when(\n                        f.array_contains(\n                            f.collect_set(f.col(\"hasSumstats\")).over(\n                                full_study_id_window\n                            ),\n                            True,\n                        ),\n                        f.lit(False),\n                    ).otherwise(True),\n                )\n                # For studies without summary statistics, we remove the \"Not curated by Open Targets\" flag:\n                .withColumn(\n                    \"qualityControls\",\n                    f.when(\n                        ~f.col(\"hasSumstats\"),\n                        f.array_remove(\n                            f.col(\"qualityControls\"),\n                            StudyQualityCheck.NO_OT_CURATION.value,\n                        ),\n                    ).otherwise(f.col(\"qualityControls\")),\n                )\n                # If top hits are not kept, we remove the \"sumstats not available\" flag from all QC lists:\n                .withColumn(\n                    \"qualityControls\",\n                    f.when(\n                        ~f.col(\"keepTopHit\"),\n                        f.array_remove(\n                            f.col(\"qualityControls\"),\n                            StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value,\n                        ),\n                    ).otherwise(f.col(\"qualityControls\")),\n                )\n                # Then propagate quality checks for all sources of the same study:\n                .withColumn(\n                    \"qualityControls\",\n                    f.array_distinct(\n                        f.flatten(\n                            f.collect_set(\"qualityControls\").over(full_study_id_window)\n                        )\n                    ),\n                )\n                # Propagating sumstatQCValues -&gt; map, cannot be flatten:\n                .withColumn(\n                    \"sumstatQCValues\",\n                    f.first(\"sumstatQCValues\", ignorenulls=True).over(\n                        full_study_id_window\n                    ),\n                )\n                # Propagating analysisFlags:\n                .withColumn(\n                    \"analysisFlags\",\n                    f.flatten(\n                        f.collect_list(\"analysisFlags\").over(full_study_id_window)\n                    ),\n                )\n                # Propagating hasSumstatsFlag - if no flag, leave null:\n                .withColumn(\n                    \"hasSumstats\",\n                    f.when(\n                        # There's a true:\n                        f.array_contains(\n                            f.collect_set(\"hasSumstats\").over(full_study_id_window),\n                            True,\n                        ),\n                        f.lit(True),\n                    ).when(\n                        # There's a false:\n                        f.array_contains(\n                            f.collect_set(\"hasSumstats\").over(full_study_id_window),\n                            False,\n                        ),\n                        f.lit(False),\n                    ),\n                )\n                # Propagating disease: when different sets of diseases available for the same study,\n                # we pick the shortest list, becasuse we assume, that is the most accurate disease assignment:\n                .withColumn(\n                    \"mostGranular\",\n                    f.size(f.col(\"traitFromSourceMappedIds\"))\n                    == f.min(f.size(f.col(\"traitFromSourceMappedIds\"))).over(\n                        full_study_id_window\n                    ),\n                )\n                # Remove less granular disease mappings:\n                .withColumn(\n                    \"traitFromSourceMappedIds\",\n                    f.when(f.col(\"mostGranular\"), f.col(\"traitFromSourceMappedIds\")),\n                )\n                # Propagate mapped disease:\n                .withColumn(\n                    \"traitFromSourceMappedIds\",\n                    f.last(f.col(\"traitFromSourceMappedIds\"), True).over(\n                        full_study_id_window\n                    ),\n                )\n                # Repeating these steps for the `traitFromSource` column:\n                .withColumn(\n                    \"traitFromSource\",\n                    f.when(f.col(\"mostGranular\"), f.col(\"traitFromSource\")),\n                )\n                # Propagate disease:\n                .withColumn(\n                    \"traitFromSource\",\n                    f.last(f.col(\"traitFromSource\"), True).over(full_study_id_window),\n                )\n                # Distinct study types are joined together into a string. So, if there's ambiguite, the study will be flagged when the study type is validated:\n                .withColumn(\n                    \"studyType\",\n                    f.concat_ws(\n                        \",\", f.collect_set(\"studyType\").over(full_study_id_window)\n                    ),\n                )\n                # At this point, all studies in one window is expected to be identical. Let's just pick one:\n                .withColumn(\"rank\", f.row_number().over(study_id_window))\n                .filter(f.col(\"rank\") == 1)\n                .drop(*columns_to_drop)\n            ),\n            _schema=StudyIndex.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.aggregate_and_map_ancestries","title":"<code>aggregate_and_map_ancestries(discovery_samples: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Map ancestries to populations in the LD reference and calculate relative sample size.</p> <p>Parameters:</p> Name Type Description Default <code>discovery_samples</code> <code>Column</code> <p>A list of struct column. Has an <code>ancestry</code> column and a <code>sampleSize</code> columns</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A list of struct with mapped LD population and their relative sample size.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@classmethod\ndef aggregate_and_map_ancestries(\n    cls: type[StudyIndex], discovery_samples: Column\n) -&gt; Column:\n    \"\"\"Map ancestries to populations in the LD reference and calculate relative sample size.\n\n    Args:\n        discovery_samples (Column): A list of struct column. Has an `ancestry` column and a `sampleSize` columns\n\n    Returns:\n        Column: A list of struct with mapped LD population and their relative sample size.\n    \"\"\"\n    # Map ancestry categories to population labels of the LD index:\n    mapped_ancestries = f.transform(\n        discovery_samples, cls._map_ancestries_to_ld_population\n    )\n\n    # Aggregate sample sizes belonging to the same LD population:\n    aggregated_counts = f.aggregate(\n        mapped_ancestries,\n        f.array_distinct(\n            f.transform(\n                mapped_ancestries,\n                lambda x: f.struct(\n                    x.ancestry.alias(\"ancestry\"), f.lit(0.0).alias(\"sampleSize\")\n                ),\n            )\n        ),\n        cls._aggregate_samples_by_ancestry,\n    )\n    # Getting total sample count:\n    total_sample_count = f.aggregate(\n        aggregated_counts, f.lit(0.0), lambda total, pop: total + pop.sampleSize\n    ).alias(\"sampleSize\")\n\n    # Calculating relative sample size for each LD population:\n    return f.transform(\n        aggregated_counts,\n        lambda ld_population: f.struct(\n            ld_population.ancestry.alias(\"ldPopulation\"),\n            (ld_population.sampleSize / total_sample_count).alias(\n                \"relativeSampleSize\"\n            ),\n        ),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.annotate_sumstats_qc","title":"<code>annotate_sumstats_qc(sumstats_qc: DataFrame, threshold_mean_beta: float = 0.05, threshold_mean_diff_pz: float = 0.05, threshold_se_diff_pz: float = 0.05, threshold_min_gc_lambda: float = 0.7, threshold_max_gc_lambda: float = 2.5, threshold_min_n_variants: int = 2000000) -&gt; StudyIndex</code>","text":"<p>Annotate summary stats QC information.</p> <p>Parameters:</p> Name Type Description Default <code>sumstats_qc</code> <code>DataFrame</code> <p>containing summary statistics-based quality controls.</p> required <code>threshold_mean_beta</code> <code>float</code> <p>Threshold for mean beta check. Defaults to 0.05.</p> <code>0.05</code> <code>threshold_mean_diff_pz</code> <code>float</code> <p>Threshold for mean diff PZ check. Defaults to 0.05.</p> <code>0.05</code> <code>threshold_se_diff_pz</code> <code>float</code> <p>Threshold for SE diff PZ check. Defaults to 0.05.</p> <code>0.05</code> <code>threshold_min_gc_lambda</code> <code>float</code> <p>Minimum threshold for GC lambda check. Defaults to 0.7.</p> <code>0.7</code> <code>threshold_max_gc_lambda</code> <code>float</code> <p>Maximum threshold for GC lambda check. Defaults to 2.5.</p> <code>2.5</code> <code>threshold_min_n_variants</code> <code>int</code> <p>Minimum number of variants for SuSiE check. Defaults to 2_000_000.</p> <code>2000000</code> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Updated study index with QC information</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def annotate_sumstats_qc(\n    self: StudyIndex,\n    sumstats_qc: DataFrame,\n    threshold_mean_beta: float = 0.05,\n    threshold_mean_diff_pz: float = 0.05,\n    threshold_se_diff_pz: float = 0.05,\n    threshold_min_gc_lambda: float = 0.7,\n    threshold_max_gc_lambda: float = 2.5,\n    threshold_min_n_variants: int = 2_000_000,\n) -&gt; StudyIndex:\n    \"\"\"Annotate summary stats QC information.\n\n    Args:\n        sumstats_qc (DataFrame): containing summary statistics-based quality controls.\n        threshold_mean_beta (float): Threshold for mean beta check. Defaults to 0.05.\n        threshold_mean_diff_pz (float): Threshold for mean diff PZ check. Defaults to 0.05.\n        threshold_se_diff_pz (float): Threshold for SE diff PZ check. Defaults to 0.05.\n        threshold_min_gc_lambda (float): Minimum threshold for GC lambda check. Defaults to 0.7.\n        threshold_max_gc_lambda (float): Maximum threshold for GC lambda check. Defaults to 2.5.\n        threshold_min_n_variants (int): Minimum number of variants for SuSiE check. Defaults to 2_000_000.\n\n    Returns:\n        StudyIndex: Updated study index with QC information\n    \"\"\"\n    # convert all columns in sumstats_qc dataframe in array of structs grouped by studyId\n    cols = [c for c in sumstats_qc.columns if c != \"studyId\"]\n\n    studies = self.df\n\n    melted_df = convert_from_wide_to_long(\n        sumstats_qc,\n        id_vars=[\"studyId\"],\n        value_vars=cols,\n        var_name=\"QCCheckName\",\n        value_name=\"QCCheckValue\",\n    )\n\n    qc_df = (\n        melted_df.groupBy(\"studyId\")\n        .agg(\n            f.map_from_entries(\n                f.collect_list(\n                    f.struct(f.col(\"QCCheckName\"), f.col(\"QCCheckValue\"))\n                )\n            ).alias(\"sumStatQCValues\")\n        )\n        .select(\"studyId\", \"sumstatQCValues\")\n    )\n\n    df = (\n        studies.drop(\"sumStatQCValues\", \"hasSumstats\")\n        .join(\n            qc_df.withColumn(\"hasSumstats\", f.lit(True)), how=\"left\", on=\"studyId\"\n        )\n        .withColumn(\"hasSumstats\", f.coalesce(f.col(\"hasSumstats\"), f.lit(False)))\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~f.col(\"hasSumstats\"),\n                StudyQualityCheck.SUMSTATS_NOT_AVAILABLE,\n            ),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~(f.abs(f.col(\"sumstatQCValues.mean_beta\")) &lt;= threshold_mean_beta),\n                StudyQualityCheck.FAILED_MEAN_BETA_CHECK,\n            ),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~(\n                    (\n                        f.abs(f.col(\"sumstatQCValues.mean_diff_pz\"))\n                        &lt;= threshold_mean_diff_pz\n                    )\n                    &amp; (f.col(\"sumstatQCValues.se_diff_pz\") &lt;= threshold_se_diff_pz)\n                ),\n                StudyQualityCheck.FAILED_PZ_CHECK,\n            ),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~(\n                    (f.col(\"sumstatQCValues.gc_lambda\") &lt;= threshold_max_gc_lambda)\n                    &amp; (\n                        f.col(\"sumstatQCValues.gc_lambda\")\n                        &gt;= threshold_min_gc_lambda\n                    )\n                ),\n                StudyQualityCheck.FAILED_GC_LAMBDA_CHECK,\n            ),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                (f.col(\"sumstatQCValues.n_variants\") &lt; threshold_min_n_variants),\n                StudyQualityCheck.SMALL_NUMBER_OF_SNPS,\n            ),\n        )\n    )\n\n    # Annotate study index with QC information:\n    return StudyIndex(\n        _df=df,\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.deconvolute_studies","title":"<code>deconvolute_studies() -&gt; StudyIndex</code>","text":"<p>Deconvolute the study index dataset.</p> <p>When ingesting the study index dataset, the same studyId might be ingested from more than one source. In such cases, the data needs to be merged and the quality control flags need to be combined.</p> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Deconvoluted study index dataset.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def deconvolute_studies(self: StudyIndex) -&gt; StudyIndex:\n    \"\"\"Deconvolute the study index dataset.\n\n    When ingesting the study index dataset, the same studyId might be ingested from more than one source.\n    In such cases, the data needs to be merged and the quality control flags need to be combined.\n\n    Returns:\n        StudyIndex: Deconvoluted study index dataset.\n    \"\"\"\n    # Windowing by study ID assume random order, but this is OK, because we are not selecting rows by a specific order.\n    study_id_window = Window.partitionBy(\"studyId\").orderBy(f.rand())\n\n    # For certain aggregation, the full window is needed to be considered:\n    full_study_id_window = study_id_window.orderBy(\"studyId\").rangeBetween(\n        Window.unboundedPreceding, Window.unboundedFollowing\n    )\n\n    # Temporary columns to drop at the end:\n    columns_to_drop = [\"keepTopHit\", \"mostGranular\", \"rank\"]\n\n    return StudyIndex(\n        _df=(\n            self.df\n            # Initialising quality controls column, if not present:\n            .withColumn(\n                \"qualityControls\",\n                f.when(\n                    f.col(\"qualityControls\").isNull(),\n                    f.array().cast(ArrayType(StringType())),\n                ).otherwise(f.col(\"qualityControls\")),\n            )\n            # Keeping top hit studies unless the same study is available from a summmary statistics source:\n            # This value will be set for all rows for the same `studyId`:\n            .withColumn(\n                \"keepTopHit\",\n                f.when(\n                    f.array_contains(\n                        f.collect_set(f.col(\"hasSumstats\")).over(\n                            full_study_id_window\n                        ),\n                        True,\n                    ),\n                    f.lit(False),\n                ).otherwise(True),\n            )\n            # For studies without summary statistics, we remove the \"Not curated by Open Targets\" flag:\n            .withColumn(\n                \"qualityControls\",\n                f.when(\n                    ~f.col(\"hasSumstats\"),\n                    f.array_remove(\n                        f.col(\"qualityControls\"),\n                        StudyQualityCheck.NO_OT_CURATION.value,\n                    ),\n                ).otherwise(f.col(\"qualityControls\")),\n            )\n            # If top hits are not kept, we remove the \"sumstats not available\" flag from all QC lists:\n            .withColumn(\n                \"qualityControls\",\n                f.when(\n                    ~f.col(\"keepTopHit\"),\n                    f.array_remove(\n                        f.col(\"qualityControls\"),\n                        StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value,\n                    ),\n                ).otherwise(f.col(\"qualityControls\")),\n            )\n            # Then propagate quality checks for all sources of the same study:\n            .withColumn(\n                \"qualityControls\",\n                f.array_distinct(\n                    f.flatten(\n                        f.collect_set(\"qualityControls\").over(full_study_id_window)\n                    )\n                ),\n            )\n            # Propagating sumstatQCValues -&gt; map, cannot be flatten:\n            .withColumn(\n                \"sumstatQCValues\",\n                f.first(\"sumstatQCValues\", ignorenulls=True).over(\n                    full_study_id_window\n                ),\n            )\n            # Propagating analysisFlags:\n            .withColumn(\n                \"analysisFlags\",\n                f.flatten(\n                    f.collect_list(\"analysisFlags\").over(full_study_id_window)\n                ),\n            )\n            # Propagating hasSumstatsFlag - if no flag, leave null:\n            .withColumn(\n                \"hasSumstats\",\n                f.when(\n                    # There's a true:\n                    f.array_contains(\n                        f.collect_set(\"hasSumstats\").over(full_study_id_window),\n                        True,\n                    ),\n                    f.lit(True),\n                ).when(\n                    # There's a false:\n                    f.array_contains(\n                        f.collect_set(\"hasSumstats\").over(full_study_id_window),\n                        False,\n                    ),\n                    f.lit(False),\n                ),\n            )\n            # Propagating disease: when different sets of diseases available for the same study,\n            # we pick the shortest list, becasuse we assume, that is the most accurate disease assignment:\n            .withColumn(\n                \"mostGranular\",\n                f.size(f.col(\"traitFromSourceMappedIds\"))\n                == f.min(f.size(f.col(\"traitFromSourceMappedIds\"))).over(\n                    full_study_id_window\n                ),\n            )\n            # Remove less granular disease mappings:\n            .withColumn(\n                \"traitFromSourceMappedIds\",\n                f.when(f.col(\"mostGranular\"), f.col(\"traitFromSourceMappedIds\")),\n            )\n            # Propagate mapped disease:\n            .withColumn(\n                \"traitFromSourceMappedIds\",\n                f.last(f.col(\"traitFromSourceMappedIds\"), True).over(\n                    full_study_id_window\n                ),\n            )\n            # Repeating these steps for the `traitFromSource` column:\n            .withColumn(\n                \"traitFromSource\",\n                f.when(f.col(\"mostGranular\"), f.col(\"traitFromSource\")),\n            )\n            # Propagate disease:\n            .withColumn(\n                \"traitFromSource\",\n                f.last(f.col(\"traitFromSource\"), True).over(full_study_id_window),\n            )\n            # Distinct study types are joined together into a string. So, if there's ambiguite, the study will be flagged when the study type is validated:\n            .withColumn(\n                \"studyType\",\n                f.concat_ws(\n                    \",\", f.collect_set(\"studyType\").over(full_study_id_window)\n                ),\n            )\n            # At this point, all studies in one window is expected to be identical. Let's just pick one:\n            .withColumn(\"rank\", f.row_number().over(study_id_window))\n            .filter(f.col(\"rank\") == 1)\n            .drop(*columns_to_drop)\n        ),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.get_QC_column_name","title":"<code>get_QC_column_name() -&gt; str</code>  <code>classmethod</code>","text":"<p>Return the name of the quality control column.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the quality control column.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@classmethod\ndef get_QC_column_name(cls: type[StudyIndex]) -&gt; str:\n    \"\"\"Return the name of the quality control column.\n\n    Returns:\n        str: The name of the quality control column.\n    \"\"\"\n    return \"qualityControls\"\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.get_QC_mappings","title":"<code>get_QC_mappings() -&gt; dict[str, str]</code>  <code>classmethod</code>","text":"<p>Quality control flag to QC column category mappings.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: Mapping between flag name and QC column category value.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@classmethod\ndef get_QC_mappings(cls: type[StudyIndex]) -&gt; dict[str, str]:\n    \"\"\"Quality control flag to QC column category mappings.\n\n    Returns:\n        dict[str, str]: Mapping between flag name and QC column category value.\n    \"\"\"\n    return {member.name: member.value for member in StudyQualityCheck}\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provide the schema for the StudyIndex dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>The schema of the StudyIndex dataset.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[StudyIndex]) -&gt; StructType:\n    \"\"\"Provide the schema for the StudyIndex dataset.\n\n    Returns:\n        StructType: The schema of the StudyIndex dataset.\n    \"\"\"\n    return parse_spark_schema(\"study_index.json\")\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.has_mapped_trait","title":"<code>has_mapped_trait() -&gt; Column</code>","text":"<p>Return a boolean column indicating if a study has mapped disease.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study has mapped disease.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def has_mapped_trait(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column indicating if a study has mapped disease.\n\n    Returns:\n        Column: True if the study has mapped disease.\n    \"\"\"\n    return f.size(self.df.traitFromSourceMappedIds) &gt; 0\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.has_summarystats","title":"<code>has_summarystats() -&gt; Column</code>","text":"<p>Return a boolean column indicating if a study has harmonized summary statistics.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study has harmonized summary statistics.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def has_summarystats(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column indicating if a study has harmonized summary statistics.\n\n    Returns:\n        Column: True if the study has harmonized summary statistics.\n    \"\"\"\n    return self.df.hasSumstats\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.is_gwas","title":"<code>is_gwas() -&gt; Column</code>","text":"<p>Return a boolean column with true values for GWAS studies.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study is a GWAS study.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def is_gwas(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column with true values for GWAS studies.\n\n    Returns:\n        Column: True if the study is a GWAS study.\n    \"\"\"\n    return self.df.studyType == \"gwas\"\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.is_qtl","title":"<code>is_qtl() -&gt; Column</code>","text":"<p>Return a boolean column with true values for QTL studies.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study is a QTL study.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def is_qtl(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column with true values for QTL studies.\n\n    Returns:\n        Column: True if the study is a QTL study.\n    \"\"\"\n    return self.df.studyType.endswith(\"qtl\")\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.is_quality_flagged","title":"<code>is_quality_flagged() -&gt; Column</code>","text":"<p>Return a boolean column indicating if a study is flagged due to quality issues.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study is flagged.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def is_quality_flagged(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column indicating if a study is flagged due to quality issues.\n\n    Returns:\n        Column: True if the study is flagged.\n    \"\"\"\n    # Testing for the presence of the qualityControls column:\n    if \"qualityControls\" not in self.df.columns:\n        return f.lit(False)\n    else:\n        return f.size(self.df[\"qualityControls\"]) != 0\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.study_type_lut","title":"<code>study_type_lut() -&gt; DataFrame</code>","text":"<p>Return a lookup table of study type.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing <code>studyId</code> and <code>studyType</code> columns.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def study_type_lut(self: StudyIndex) -&gt; DataFrame:\n    \"\"\"Return a lookup table of study type.\n\n    Returns:\n        DataFrame: A dataframe containing `studyId` and `studyType` columns.\n    \"\"\"\n    return self.df.select(\"studyId\", \"studyType\")\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_biosample","title":"<code>validate_biosample(biosample_index: BiosampleIndex) -&gt; StudyIndex</code>","text":"<p>Validating biosample identifiers in the study index against the provided biosample index.</p> <p>Parameters:</p> Name Type Description Default <code>biosample_index</code> <code>BiosampleIndex</code> <p>Biosample index containing a reference of biosample identifiers e.g. cell types, tissues, cell lines, etc.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>where non-gwas studies are flagged if biosampleIndex could not be validated.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_biosample(\n    self: StudyIndex, biosample_index: BiosampleIndex\n) -&gt; StudyIndex:\n    \"\"\"Validating biosample identifiers in the study index against the provided biosample index.\n\n    Args:\n        biosample_index (BiosampleIndex): Biosample index containing a reference of biosample identifiers e.g. cell types, tissues, cell lines, etc.\n\n    Returns:\n        StudyIndex: where non-gwas studies are flagged if biosampleIndex could not be validated.\n    \"\"\"\n    biosample_set = biosample_index.df.select(\n        \"biosampleId\", f.lit(True).alias(\"isIdFound\")\n    )\n\n    # If biosampleId in df, we need to drop it:\n    if \"biosampleId\" in self.df.columns:\n        self.df = self.df.drop(\"biosampleId\")\n\n    # As the biosampleFromSourceId is not a mandatory field of study index, we return if the column is not there:\n    if \"biosampleFromSourceId\" not in self.df.columns:\n        return self\n\n    validated_df = (\n        self.df.join(\n            biosample_set,\n            self.df.biosampleFromSourceId == biosample_set.biosampleId,\n            how=\"left\",\n        )\n        .withColumn(\n            \"isIdFound\",\n            f.when(\n                (f.col(\"studyType\") != \"gwas\") &amp; (f.col(\"isIdFound\").isNull()),\n                f.lit(False),\n            ).otherwise(f.lit(True)),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~f.col(\"isIdFound\"),\n                StudyQualityCheck.UNKNOWN_BIOSAMPLE,\n            ),\n        )\n        .drop(\"isIdFound\")\n    )\n\n    return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_disease","title":"<code>validate_disease(disease_map: DataFrame) -&gt; StudyIndex</code>","text":"<p>Validate diseases in the study index dataset.</p> <p>Parameters:</p> Name Type Description Default <code>disease_map</code> <code>DataFrame</code> <p>a dataframe with two columns (efo, diseaseId).</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>where gwas studies are flagged where no valid disease id could be found.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_disease(self: StudyIndex, disease_map: DataFrame) -&gt; StudyIndex:\n    \"\"\"Validate diseases in the study index dataset.\n\n    Args:\n        disease_map (DataFrame): a dataframe with two columns (efo, diseaseId).\n\n    Returns:\n        StudyIndex: where gwas studies are flagged where no valid disease id could be found.\n    \"\"\"\n    # Because the disease ids are not mandatory fields of the schema, we skip vaildation if these columns are not present:\n    if (\"traitFromSourceMappedIds\" not in self.df.columns) or (\n        \"backgroundTraitFromSourceMappedIds\" not in self.df.columns\n    ):\n        return self\n\n    # Disease Column names:\n    foreground_disease_column = \"diseaseIds\"\n    background_disease_column = \"backgroundDiseaseIds\"\n\n    # If diseaseId in schema, we need to drop it:\n    drop_columns = [\n        column\n        for column in self.df.columns\n        if column in [foreground_disease_column, background_disease_column]\n    ]\n\n    if len(drop_columns) &gt; 0:\n        self.df = self.df.drop(*drop_columns)\n\n    # Normalise disease:\n    normalised_disease = self._normalise_disease(\n        \"traitFromSourceMappedIds\", foreground_disease_column, disease_map\n    )\n    normalised_background_disease = self._normalise_disease(\n        \"backgroundTraitFromSourceMappedIds\", background_disease_column, disease_map\n    )\n\n    return StudyIndex(\n        _df=(\n            self.df.join(normalised_disease, on=\"studyId\", how=\"left\")\n            .join(normalised_background_disease, on=\"studyId\", how=\"left\")\n            # Updating disease columns:\n            .withColumn(\n                foreground_disease_column,\n                f.when(\n                    f.col(foreground_disease_column).isNull(), f.array()\n                ).otherwise(f.col(foreground_disease_column)),\n            )\n            .withColumn(\n                background_disease_column,\n                f.when(\n                    f.col(background_disease_column).isNull(), f.array()\n                ).otherwise(f.col(background_disease_column)),\n            )\n            # Flagging gwas studies where no valid disease is avilable:\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    # Flagging all gwas studies with no normalised disease:\n                    (f.size(f.col(foreground_disease_column)) == 0)\n                    &amp; (f.col(\"studyType\") == \"gwas\"),\n                    StudyQualityCheck.UNRESOLVED_DISEASE,\n                ),\n            )\n        ),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_study_type","title":"<code>validate_study_type() -&gt; StudyIndex</code>","text":"<p>Validating study type and flag unsupported types.</p> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>with flagged studies with unsupported type.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_study_type(self: StudyIndex) -&gt; StudyIndex:\n    \"\"\"Validating study type and flag unsupported types.\n\n    Returns:\n        StudyIndex: with flagged studies with unsupported type.\n    \"\"\"\n    validated_df = (\n        self.df\n        # Flagging unsupported study types:\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.when(\n                    (f.col(\"studyType\") == \"gwas\")\n                    | f.col(\"studyType\").endswith(\"qtl\"),\n                    False,\n                ).otherwise(True),\n                StudyQualityCheck.UNKNOWN_STUDY_TYPE,\n            ),\n        )\n    )\n    return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_target","title":"<code>validate_target(target_index: TargetIndex) -&gt; StudyIndex</code>","text":"<p>Validating gene identifiers in the study index against the provided target index.</p> <p>Parameters:</p> Name Type Description Default <code>target_index</code> <code>TargetIndex</code> <p>target index containing the reference gene identifiers (Ensembl gene identifiers).</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>with flagged studies if geneId could not be validated.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_target(self: StudyIndex, target_index: TargetIndex) -&gt; StudyIndex:\n    \"\"\"Validating gene identifiers in the study index against the provided target index.\n\n    Args:\n        target_index (TargetIndex): target index containing the reference gene identifiers (Ensembl gene identifiers).\n\n    Returns:\n        StudyIndex: with flagged studies if geneId could not be validated.\n    \"\"\"\n    gene_set = target_index.df.select(f.col(\"id\").alias(\"geneId\"), f.lit(True).alias(\"isIdFound\"))\n\n    # As the geneId is not a mandatory field of study index, we return if the column is not there:\n    if \"geneId\" not in self.df.columns:\n        return self\n\n    validated_df = (\n        self.df.join(gene_set, on=\"geneId\", how=\"left\")\n        .withColumn(\n            \"isIdFound\",\n            f.when(\n                (f.col(\"studyType\") != \"gwas\") &amp; f.col(\"isIdFound\").isNull(),\n                f.lit(False),\n            ).otherwise(f.lit(True)),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~f.col(\"isIdFound\"),\n                StudyQualityCheck.UNRESOLVED_TARGET,\n            ),\n        )\n        .drop(\"isIdFound\")\n    )\n\n    return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_unique_study_id","title":"<code>validate_unique_study_id() -&gt; StudyIndex</code>","text":"<p>Validating the uniqueness of study identifiers and flagging duplicated studies.</p> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>with flagged duplicated studies.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_unique_study_id(self: StudyIndex) -&gt; StudyIndex:\n    \"\"\"Validating the uniqueness of study identifiers and flagging duplicated studies.\n\n    Returns:\n        StudyIndex: with flagged duplicated studies.\n    \"\"\"\n    return StudyIndex(\n        _df=self.df.withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                self.flag_duplicates(f.col(\"studyId\")),\n                StudyQualityCheck.DUPLICATED_STUDY,\n            ),\n        ),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#schema","title":"Schema","text":"<pre><code>root\n |-- studyId: string (nullable = false)\n |-- projectId: string (nullable = false)\n |-- studyType: string (nullable = false)\n |-- traitFromSource: string (nullable = true)\n |-- traitFromSourceMappedIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- diseaseIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- geneId: string (nullable = true)\n |-- biosampleFromSourceId: string (nullable = true)\n |-- biosampleId: string (nullable = true)\n |-- pubmedId: string (nullable = true)\n |-- publicationTitle: string (nullable = true)\n |-- publicationFirstAuthor: string (nullable = true)\n |-- publicationDate: string (nullable = true)\n |-- publicationJournal: string (nullable = true)\n |-- backgroundTraitFromSourceMappedIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- backgroundDiseaseIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- initialSampleSize: string (nullable = true)\n |-- nCases: integer (nullable = true)\n |-- nControls: integer (nullable = true)\n |-- nSamples: integer (nullable = true)\n |-- cohorts: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- ldPopulationStructure: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- ldPopulation: string (nullable = true)\n |    |    |-- relativeSampleSize: double (nullable = true)\n |-- discoverySamples: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- sampleSize: integer (nullable = true)\n |    |    |-- ancestry: string (nullable = true)\n |-- replicationSamples: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- sampleSize: integer (nullable = true)\n |    |    |-- ancestry: string (nullable = true)\n |-- qualityControls: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- analysisFlags: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- summarystatsLocation: string (nullable = true)\n |-- hasSumstats: boolean (nullable = true)\n |-- condition: string (nullable = true)\n |-- sumstatQCValues: map (nullable = true)\n |    |-- key: string\n |    |-- value: float (valueContainsNull = true)\n</code></pre>"},{"location":"python_api/datasets/study_locus/","title":"Study Locus","text":""},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus","title":"<code>gentropy.dataset.study_locus.StudyLocus</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Study-Locus dataset.</p> <p>This dataset captures associations between study/traits and a genetic loci as provided by finemapping methods.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@dataclass\nclass StudyLocus(Dataset):\n    \"\"\"Study-Locus dataset.\n\n    This dataset captures associations between study/traits and a genetic loci as provided by finemapping methods.\n    \"\"\"\n\n    def validate_study(self: StudyLocus, study_index: StudyIndex) -&gt; StudyLocus:\n        \"\"\"Flagging study loci if the corresponding study has issues.\n\n        There are two different potential flags:\n        - flagged study: flagging locus if the study has quality control flags.\n        - study with summary statistics for top hit: flagging locus if the study has available summary statistics.\n        - missing study: flagging locus if the study was not found in the reference study index.\n\n        Args:\n            study_index (StudyIndex): Study index to resolve study types.\n\n        Returns:\n            StudyLocus: Updated study locus with quality control flags.\n        \"\"\"\n        # Quality controls is not a mandatory field in the study index schema, so we have to be ready to handle it:\n        qc_select_expression = (\n            f.col(\"qualityControls\")\n            if \"qualityControls\" in study_index.df.columns\n            else f.lit(None).cast(StringType())\n        )\n\n        # The study Id of the study index needs to be kept, because we would not know which study was in the index after the left join:\n        study_flags = study_index.df.select(\n            f.col(\"studyId\").alias(\"study_studyId\"),\n            qc_select_expression.alias(\"study_qualityControls\"),\n        )\n\n        return StudyLocus(\n            _df=(\n                self.df.join(\n                    study_flags, f.col(\"studyId\") == f.col(\"study_studyId\"), \"left\"\n                )\n                # Flagging loci with flagged studies - without propagating the actual flags:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyLocus.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        f.size(f.col(\"study_qualityControls\")) &gt; 0,\n                        StudyLocusQualityCheck.FLAGGED_STUDY,\n                    ),\n                )\n                # Flagging top-hits, where the study has available summary statistics:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyLocus.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        # Condition is true, if the study has summary statistics available and the locus is a top hit:\n                        f.array_contains(\n                            f.col(\"qualityControls\"),\n                            StudyLocusQualityCheck.TOP_HIT.value,\n                        )\n                        &amp; ~f.array_contains(\n                            f.col(\"study_qualityControls\"),\n                            StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value,\n                        ),\n                        StudyLocusQualityCheck.TOP_HIT_AND_SUMMARY_STATS,\n                    ),\n                )\n                # Flagging loci where no studies were found:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyLocus.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        f.col(\"study_studyId\").isNull(),\n                        StudyLocusQualityCheck.MISSING_STUDY,\n                    ),\n                )\n                .drop(\"study_studyId\", \"study_qualityControls\")\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def annotate_study_type(self: StudyLocus, study_index: StudyIndex) -&gt; StudyLocus:\n        \"\"\"Gets study type from study index and adds it to study locus.\n\n        Args:\n            study_index (StudyIndex): Study index to get study type.\n\n        Returns:\n            StudyLocus: Updated study locus with study type.\n        \"\"\"\n        return StudyLocus(\n            _df=(\n                self.df.drop(\"studyType\").join(\n                    study_index.study_type_lut(), on=\"studyId\", how=\"left\"\n                )\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def validate_chromosome_label(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Flagging study loci, where chromosome is coded not as 1:22, X, Y, Xy and MT.\n\n        Returns:\n            StudyLocus: Updated study locus with quality control flags.\n        \"\"\"\n        # QC column might not be present in the variant index schema, so we have to be ready to handle it:\n        qc_select_expression = (\n            f.col(\"qualityControls\")\n            if \"qualityControls\" in self.df.columns\n            else f.lit(None).cast(ArrayType(StringType()))\n        )\n        valid_chromosomes = [str(i) for i in range(1, 23)] + [\"X\", \"Y\", \"XY\", \"MT\"]\n\n        return StudyLocus(\n            _df=(\n                self.df.withColumn(\n                    \"qualityControls\",\n                    self.update_quality_flag(\n                        qc_select_expression,\n                        ~f.col(\"chromosome\").isin(valid_chromosomes),\n                        StudyLocusQualityCheck.INVALID_CHROMOSOME,\n                    ),\n                )\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def validate_variant_identifiers(\n        self: StudyLocus, variant_index: VariantIndex\n    ) -&gt; StudyLocus:\n        \"\"\"Flagging study loci, where tagging variant identifiers are not found in variant index.\n\n        Args:\n            variant_index (VariantIndex): Variant index to resolve variant identifiers.\n\n        Returns:\n            StudyLocus: Updated study locus with quality control flags.\n        \"\"\"\n        # QC column might not be present in the variant index schema, so we have to be ready to handle it:\n        qc_select_expression = (\n            f.col(\"qualityControls\")\n            if \"qualityControls\" in self.df.columns\n            else f.lit(None).cast(ArrayType(StringType()))\n        )\n\n        # Find out which study loci have variants not in the variant index:\n        flag = (\n            self.df\n            # Exploding locus:\n            .select(\"studyLocusId\", f.explode(\"locus\").alias(\"locus\"))\n            .select(\"studyLocusId\", \"locus.variantId\")\n            # Join with variant index variants:\n            .join(\n                variant_index.df.select(\n                    \"variantId\", f.lit(True).alias(\"inVariantIndex\")\n                ),\n                on=\"variantId\",\n                how=\"left\",\n            )\n            # Flagging variants not in the variant index:\n            .withColumn(\"inVariantIndex\", f.col(\"inVariantIndex\").isNotNull())\n            # Flagging study loci with ANY variants not in the variant index:\n            .groupBy(\"studyLocusId\")\n            .agg(f.collect_set(\"inVariantIndex\").alias(\"inVariantIndex\"))\n            .select(\n                \"studyLocusId\",\n                f.array_contains(\"inVariantIndex\", False).alias(\"toFlag\"),\n            )\n        )\n\n        return StudyLocus(\n            _df=(\n                self.df.join(flag, on=\"studyLocusId\", how=\"left\")\n                .withColumn(\n                    \"qualityControls\",\n                    self.update_quality_flag(\n                        qc_select_expression,\n                        f.col(\"toFlag\"),\n                        StudyLocusQualityCheck.INVALID_VARIANT_IDENTIFIER,\n                    ),\n                )\n                .drop(\"toFlag\")\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def validate_lead_pvalue(self: StudyLocus, pvalue_cutoff: float) -&gt; StudyLocus:\n        \"\"\"Flag associations below significant threshold.\n\n        Args:\n            pvalue_cutoff (float): association p-value cut-off\n\n        Returns:\n            StudyLocus: Updated study locus with quality control flags.\n        \"\"\"\n        df = self.df\n        qc_colname = StudyLocus.get_QC_column_name()\n        if qc_colname not in self.df.columns:\n            df = self.df.withColumn(\n                qc_colname,\n                create_empty_column_if_not_exists(\n                    qc_colname,\n                    get_struct_field_schema(StudyLocus.get_schema(), qc_colname),\n                ),\n            )\n        return StudyLocus(\n            _df=(\n                df.withColumn(\n                    qc_colname,\n                    # Because this QC might already run on the dataset, the unique set of flags is generated:\n                    f.array_distinct(\n                        self._qc_subsignificant_associations(\n                            f.col(\"qualityControls\"),\n                            f.col(\"pValueMantissa\"),\n                            f.col(\"pValueExponent\"),\n                            pvalue_cutoff,\n                        )\n                    ),\n                )\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def validate_unique_study_locus_id(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Validating the uniqueness of study-locus identifiers and flagging duplicated studyloci.\n\n        Returns:\n            StudyLocus: with flagged duplicated studies.\n        \"\"\"\n        return StudyLocus(\n            _df=self.df.withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    self.flag_duplicates(f.col(\"studyLocusId\")),\n                    StudyLocusQualityCheck.DUPLICATED_STUDYLOCUS_ID,\n                ),\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    @staticmethod\n    def _qc_subsignificant_associations(\n        quality_controls_column: Column,\n        p_value_mantissa: Column,\n        p_value_exponent: Column,\n        pvalue_cutoff: float,\n    ) -&gt; Column:\n        \"\"\"Flag associations below significant threshold.\n\n        Args:\n            quality_controls_column (Column): QC column\n            p_value_mantissa (Column): P-value mantissa column\n            p_value_exponent (Column): P-value exponent column\n            pvalue_cutoff (float): association p-value cut-off\n\n        Returns:\n            Column: Updated QC column with flag.\n\n        Examples:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [{'qc': None, 'p_value_mantissa': 1, 'p_value_exponent': -7}, {'qc': None, 'p_value_mantissa': 1, 'p_value_exponent': -8}, {'qc': None, 'p_value_mantissa': 5, 'p_value_exponent': -8}, {'qc': None, 'p_value_mantissa': 1, 'p_value_exponent': -9}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, t.StructType([t.StructField('qc', t.ArrayType(t.StringType()), True), t.StructField('p_value_mantissa', t.IntegerType()), t.StructField('p_value_exponent', t.IntegerType())]))\n            &gt;&gt;&gt; df.withColumn('qc', StudyLocus._qc_subsignificant_associations(f.col(\"qc\"), f.col(\"p_value_mantissa\"), f.col(\"p_value_exponent\"), 5e-8)).show(truncate = False)\n            +------------------------+----------------+----------------+\n            |qc                      |p_value_mantissa|p_value_exponent|\n            +------------------------+----------------+----------------+\n            |[Subsignificant p-value]|1               |-7              |\n            |[]                      |1               |-8              |\n            |[]                      |5               |-8              |\n            |[]                      |1               |-9              |\n            +------------------------+----------------+----------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            quality_controls_column,\n            calculate_neglog_pvalue(p_value_mantissa, p_value_exponent)\n            &lt; f.lit(-np.log10(pvalue_cutoff)),\n            StudyLocusQualityCheck.SUBSIGNIFICANT_FLAG,\n        )\n\n    def qc_abnormal_pips(\n        self: StudyLocus,\n        sum_pips_lower_threshold: float = 0.99,\n        # Set slightly above 1 to account for floating point errors\n        sum_pips_upper_threshold: float = 1.0001,\n    ) -&gt; StudyLocus:\n        \"\"\"Filter study-locus by sum of posterior inclusion probabilities to ensure that the sum of PIPs is within a given range.\n\n        Args:\n            sum_pips_lower_threshold (float): Lower threshold for the sum of PIPs.\n            sum_pips_upper_threshold (float): Upper threshold for the sum of PIPs.\n\n        Returns:\n            StudyLocus: Filtered study-locus dataset.\n        \"\"\"\n        # QC column might not be present so we have to be ready to handle it:\n        qc_select_expression = (\n            f.col(\"qualityControls\")\n            if \"qualityControls\" in self.df.columns\n            else f.lit(None).cast(ArrayType(StringType()))\n        )\n\n        flag = self.df.withColumn(\n            \"sumPosteriorProbability\",\n            f.aggregate(\n                f.col(\"locus\"),\n                f.lit(0.0),\n                lambda acc, x: acc + x[\"posteriorProbability\"],\n            ),\n        ).withColumn(\n            \"pipOutOfRange\",\n            f.when(\n                (f.col(\"sumPosteriorProbability\") &lt; sum_pips_lower_threshold)\n                | (f.col(\"sumPosteriorProbability\") &gt; sum_pips_upper_threshold),\n                True,\n            ).otherwise(False),\n        )\n\n        return StudyLocus(\n            _df=(\n                flag\n                # Flagging loci with failed studies:\n                .withColumn(\n                    \"qualityControls\",\n                    self.update_quality_flag(\n                        qc_select_expression,\n                        f.col(\"pipOutOfRange\"),\n                        StudyLocusQualityCheck.ABNORMAL_PIPS,\n                    ),\n                ).drop(\"sumPosteriorProbability\", \"pipOutOfRange\")\n            ),\n            _schema=self.get_schema(),\n        )\n\n    @staticmethod\n    def _overlapping_peaks(\n        credset_to_overlap: DataFrame, intra_study_overlap: bool = False\n    ) -&gt; DataFrame:\n        \"\"\"Calculate overlapping signals (study-locus) between GWAS-GWAS and GWAS-Molecular trait.\n\n        Args:\n            credset_to_overlap (DataFrame): DataFrame containing at least `studyLocusId`, `studyType`, `chromosome` and `tagVariantId` columns.\n            intra_study_overlap (bool): When True, finds intra-study overlaps for credible set deduplication. Default is False.\n\n        Returns:\n            DataFrame: containing `leftStudyLocusId`, `rightStudyLocusId` and `chromosome` columns.\n        \"\"\"\n        # Reduce columns to the minimum to reduce the size of the dataframe\n        credset_to_overlap = credset_to_overlap.select(\n            \"studyLocusId\",\n            \"studyId\",\n            \"studyType\",\n            \"chromosome\",\n            \"region\",\n            \"tagVariantId\",\n        )\n        # Define join condition - if intra_study_overlap is True, finds overlaps within the same study. Otherwise finds gwas vs everything overlaps for coloc.\n        join_condition = (\n            [\n                f.col(\"left.studyId\") == f.col(\"right.studyId\"),\n                f.col(\"left.chromosome\") == f.col(\"right.chromosome\"),\n                f.col(\"left.tagVariantId\") == f.col(\"right.tagVariantId\"),\n                f.col(\"left.studyLocusId\") &gt; f.col(\"right.studyLocusId\"),\n                f.col(\"left.region\") != f.col(\"right.region\"),\n            ]\n            if intra_study_overlap\n            else [\n                f.col(\"left.chromosome\") == f.col(\"right.chromosome\"),\n                f.col(\"left.tagVariantId\") == f.col(\"right.tagVariantId\"),\n                (f.col(\"right.studyType\") != \"gwas\")\n                | (f.col(\"left.studyLocusId\") &gt; f.col(\"right.studyLocusId\")),\n                f.col(\"left.studyType\") == f.lit(\"gwas\"),\n            ]\n        )\n\n        return (\n            credset_to_overlap.alias(\"left\")\n            # Self join with complex condition.\n            .join(\n                credset_to_overlap.alias(\"right\"),\n                on=join_condition,\n                how=\"inner\",\n            )\n            .select(\n                f.col(\"left.studyLocusId\").alias(\"leftStudyLocusId\"),\n                f.col(\"right.studyLocusId\").alias(\"rightStudyLocusId\"),\n                f.col(\"right.studyType\").alias(\"rightStudyType\"),\n                f.col(\"left.chromosome\").alias(\"chromosome\"),\n            )\n            .distinct()\n            .repartition(\"chromosome\")\n            .persist()\n        )\n\n    @staticmethod\n    def _align_overlapping_tags(\n        loci_to_overlap: DataFrame, peak_overlaps: DataFrame\n    ) -&gt; StudyLocusOverlap:\n        \"\"\"Align overlapping tags in pairs of overlapping study-locus, keeping all tags in both loci.\n\n        Args:\n            loci_to_overlap (DataFrame): containing `studyLocusId`, `studyType`, `chromosome`, `tagVariantId`, `logBF` and `posteriorProbability` columns.\n            peak_overlaps (DataFrame): containing `leftStudyLocusId`, `rightStudyLocusId` and `chromosome` columns.\n\n        Returns:\n            StudyLocusOverlap: Pairs of overlapping study-locus with aligned tags.\n        \"\"\"\n        # Complete information about all tags in the left study-locus of the overlap\n        stats_cols = [\n            \"logBF\",\n            \"posteriorProbability\",\n            \"beta\",\n            \"pValueMantissa\",\n            \"pValueExponent\",\n        ]\n        overlapping_left = loci_to_overlap.select(\n            f.col(\"chromosome\"),\n            f.col(\"tagVariantId\"),\n            f.col(\"studyLocusId\").alias(\"leftStudyLocusId\"),\n            *[f.col(col).alias(f\"left_{col}\") for col in stats_cols],\n        ).join(peak_overlaps, on=[\"chromosome\", \"leftStudyLocusId\"], how=\"inner\")\n\n        # Complete information about all tags in the right study-locus of the overlap\n        overlapping_right = loci_to_overlap.select(\n            f.col(\"chromosome\"),\n            f.col(\"tagVariantId\"),\n            f.col(\"studyLocusId\").alias(\"rightStudyLocusId\"),\n            *[f.col(col).alias(f\"right_{col}\") for col in stats_cols],\n        ).join(peak_overlaps, on=[\"chromosome\", \"rightStudyLocusId\"], how=\"inner\")\n\n        # Include information about all tag variants in both study-locus aligned by tag variant id\n        overlaps = overlapping_left.join(\n            overlapping_right,\n            on=[\n                \"chromosome\",\n                \"rightStudyLocusId\",\n                \"leftStudyLocusId\",\n                \"tagVariantId\",\n                \"rightStudyType\",\n            ],\n            how=\"outer\",\n        ).select(\n            \"leftStudyLocusId\",\n            \"rightStudyLocusId\",\n            \"rightStudyType\",\n            \"chromosome\",\n            \"tagVariantId\",\n            f.struct(\n                *[f\"left_{e}\" for e in stats_cols] + [f\"right_{e}\" for e in stats_cols]\n            ).alias(\"statistics\"),\n        )\n        return StudyLocusOverlap(\n            _df=overlaps,\n            _schema=StudyLocusOverlap.get_schema(),\n        )\n\n    @staticmethod\n    def assign_study_locus_id(uniqueness_defining_columns: list[str]) -&gt; Column:\n        \"\"\"Hashes the provided columns to extract a consistent studyLocusId.\n\n        Args:\n            uniqueness_defining_columns (list[str]): list of columns defining uniqueness\n\n        Returns:\n            Column: column with a study locus ID\n\n        Examples:\n            &gt;&gt;&gt; df = spark.createDataFrame([(\"GCST000001\", \"1_1000_A_C\", \"SuSiE-inf\"), (\"GCST000002\", \"1_1000_A_C\", \"pics\")]).toDF(\"studyId\", \"variantId\", \"finemappingMethod\")\n            &gt;&gt;&gt; df.withColumn(\"study_locus_id\", StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\", \"finemappingMethod\"])).show(truncate=False)\n            +----------+----------+-----------------+--------------------------------+\n            |studyId   |variantId |finemappingMethod|study_locus_id                  |\n            +----------+----------+-----------------+--------------------------------+\n            |GCST000001|1_1000_A_C|SuSiE-inf        |109804fe1e20c94231a31bafd71b566e|\n            |GCST000002|1_1000_A_C|pics             |de310be4558e0482c9cc359c97d37773|\n            +----------+----------+-----------------+--------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return Dataset.generate_identifier(uniqueness_defining_columns).alias(\n            \"studyLocusId\"\n        )\n\n    @classmethod\n    def calculate_credible_set_log10bf(cls: type[StudyLocus], logbfs: Column) -&gt; Column:\n        \"\"\"Calculate Bayes factor for the entire credible set. The Bayes factor is calculated as the logsumexp of the logBF values of the variants in the locus.\n\n        Args:\n            logbfs (Column): Array column with the logBF values of the variants in the locus.\n\n        Returns:\n            Column: log10 Bayes factor for the entire credible set.\n\n        Examples:\n            &gt;&gt;&gt; spark.createDataFrame([([0.2, 0.1, 0.05, 0.0],)]).toDF(\"logBF\").select(f.round(StudyLocus.calculate_credible_set_log10bf(f.col(\"logBF\")), 7).alias(\"credibleSetlog10BF\")).show()\n            +------------------+\n            |credibleSetlog10BF|\n            +------------------+\n            |         0.6412604|\n            +------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # log10=log/log(10)=log*0.43429448190325176\n        logsumexp_udf = f.udf(\n            lambda x: (get_logsum(x) * 0.43429448190325176), FloatType()\n        )\n        return logsumexp_udf(logbfs).cast(\"double\").alias(\"credibleSetlog10BF\")\n\n    @classmethod\n    def get_schema(cls: type[StudyLocus]) -&gt; StructType:\n        \"\"\"Provides the schema for the StudyLocus dataset.\n\n        Returns:\n            StructType: schema for the StudyLocus dataset.\n        \"\"\"\n        return parse_spark_schema(\"study_locus.json\")\n\n    @classmethod\n    def get_QC_column_name(cls: type[StudyLocus]) -&gt; str:\n        \"\"\"Quality control column.\n\n        Returns:\n            str: Name of the quality control column.\n        \"\"\"\n        return \"qualityControls\"\n\n    @classmethod\n    def get_QC_mappings(cls: type[StudyLocus]) -&gt; dict[str, str]:\n        \"\"\"Quality control flag to QC column category mappings.\n\n        Returns:\n            dict[str, str]: Mapping between flag name and QC column category value.\n        \"\"\"\n        return {member.name: member.value for member in StudyLocusQualityCheck}\n\n    def flag_trans_qtls(\n        self: StudyLocus,\n        study_index: StudyIndex,\n        target_index: TargetIndex,\n        trans_threshold: int = 5_000_000,\n    ) -&gt; StudyLocus:\n        \"\"\"Flagging transQTL credible sets based on genomic location of the measured gene.\n\n        Process:\n        0. Make sure that the `isTransQtl` column does not exist (remove if exists)\n        1. Enrich study-locus dataset with geneId based on study metadata. (only QTL studies are considered)\n        2. Enrich with transcription start site and chromosome of the studied gegne.\n        3. Flagging any tagging variant of QTL credible sets, if chromosome is different from the gene or distance is above the threshold.\n        4. Propagate flags to credible sets where any tags are considered as trans.\n        5. Return study locus object with annotation stored in 'isTransQtl` boolean column, where gwas credible sets will be `null`\n\n        Args:\n            study_index (StudyIndex): study index to extract identifier of the measured gene\n            target_index (TargetIndex): target index bringing TSS and chromosome of the measured gene\n            trans_threshold (int): Distance above which the QTL is considered trans. Default: 5_000_000bp\n\n        Returns:\n            StudyLocus: new column added indicating if the QTL credibles sets are trans.\n        \"\"\"\n        # As the `geneId` column in the study index is optional, we have to test for that:\n        if \"geneId\" not in study_index.df.columns:\n            return self\n\n        # We have to remove the column `isTransQtl` to ensure the column is not duplicated\n        # The duplication can happen when one reads the StudyLocus from parquet with\n        # predefined schema that already contains the `isTransQtl` column.\n        if \"isTransQtl\" in self.df.columns:\n            self.df = self.df.drop(\"isTransQtl\")\n\n        # Process study index:\n        processed_studies = (\n            study_index.df\n            # Dropping gwas studies. This ensures that only QTLs will have \"isTrans\" annotation:\n            .filter(f.col(\"studyType\") != \"gwas\").select(\n                \"studyId\", \"geneId\", \"projectId\"\n            )\n        )\n\n        # Process study locus:\n        processed_credible_set = (\n            self.df\n            # Exploding locus to test all tag variants:\n            .withColumn(\"locus\", f.explode(\"locus\")).select(\n                \"studyLocusId\",\n                \"studyId\",\n                f.split(\"locus.variantId\", \"_\")[0].alias(\"chromosome\"),\n                f.split(\"locus.variantId\", \"_\")[1].cast(LongType()).alias(\"position\"),\n            )\n        )\n\n        # Process target index:\n        processed_targets = target_index.df.select(\n            f.col(\"id\").alias(\"geneId\"),\n            f.col(\"tss\"),\n            f.col(\"genomicLocation.chromosome\").alias(\"geneChromosome\"),\n        )\n\n        # Pool datasets:\n        joined_data = (\n            processed_credible_set\n            # Join processed studies:\n            .join(processed_studies, on=\"studyId\", how=\"inner\")\n            # Join processed targets:\n            .join(processed_targets, on=\"geneId\", how=\"left\")\n            # Assign True/False for QTL studies:\n            .withColumn(\n                \"isTagTrans\",\n                # The QTL signal is considered trans if the locus is on a different chromosome than the measured gene.\n                # OR the distance from the gene's transcription start site is &gt; threshold.\n                f.when(\n                    (f.col(\"chromosome\") != f.col(\"geneChromosome\"))\n                    | (f.abs(f.col(\"tss\") - f.col(\"position\")) &gt; trans_threshold),\n                    f.lit(True),\n                ).otherwise(f.lit(False)),\n            )\n            .groupby(\"studyLocusId\")\n            .agg(\n                # If all tagging variants of the locus is in trans position, the QTL is considered trans:\n                f.when(\n                    f.array_contains(f.collect_set(\"isTagTrans\"), f.lit(False)), False\n                )\n                .otherwise(f.lit(True))\n                .alias(\"isTransQtl\")\n            )\n        )\n        # Adding new column, where the value is null for gwas loci:\n        return StudyLocus(self.df.join(joined_data, on=\"studyLocusId\", how=\"left\"))\n\n    def filter_credible_set(\n        self: StudyLocus,\n        credible_interval: CredibleInterval,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotate and filter study-locus tag variants based on given credible interval.\n\n        Args:\n            credible_interval (CredibleInterval): Credible interval to filter for.\n\n        Returns:\n            StudyLocus: Filtered study-locus dataset.\n        \"\"\"\n        return StudyLocus(\n            _df=self.annotate_credible_sets().df.withColumn(\n                \"locus\",\n                f.filter(\n                    f.col(\"locus\"),\n                    lambda tag: (tag[credible_interval.value]),\n                ),\n            ),\n            _schema=self._schema,\n        )\n\n    @staticmethod\n    def filter_ld_set(ld_set: Column, r2_threshold: float) -&gt; Column:\n        \"\"\"Filter the LD set by a given R2 threshold.\n\n        Args:\n            ld_set (Column): LD set\n            r2_threshold (float): R2 threshold to filter the LD set on\n\n        Returns:\n            Column: Filtered LD index\n        \"\"\"\n        return f.when(\n            ld_set.isNotNull(),\n            f.filter(\n                ld_set,\n                lambda tag: tag[\"r2Overall\"] &gt;= r2_threshold,\n            ),\n        )\n\n    def find_overlaps(\n        self: StudyLocus, intra_study_overlap: bool = False\n    ) -&gt; StudyLocusOverlap:\n        \"\"\"Calculate overlapping study-locus.\n\n        Find overlapping study-locus that share at least one tagging variant. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always\n        appearing on the right side.\n\n        Args:\n            intra_study_overlap (bool): If True, finds intra-study overlaps for credible set deduplication. Default is False.\n\n        Returns:\n            StudyLocusOverlap: Pairs of overlapping study-locus with aligned tags.\n        \"\"\"\n        loci_to_overlap = (\n            self.df.filter(f.col(\"studyType\").isNotNull())\n            .withColumn(\"locus\", f.explode(\"locus\"))\n            .select(\n                \"studyLocusId\",\n                \"studyId\",\n                \"studyType\",\n                \"chromosome\",\n                \"region\",\n                f.col(\"locus.variantId\").alias(\"tagVariantId\"),\n                f.col(\"locus.logBF\").alias(\"logBF\"),\n                f.col(\"locus.posteriorProbability\").alias(\"posteriorProbability\"),\n                f.col(\"locus.pValueMantissa\").alias(\"pValueMantissa\"),\n                f.col(\"locus.pValueExponent\").alias(\"pValueExponent\"),\n                f.col(\"locus.beta\").alias(\"beta\"),\n            )\n            .persist()\n        )\n\n        # overlapping study-locus\n        peak_overlaps = self._overlapping_peaks(loci_to_overlap, intra_study_overlap)\n\n        # study-locus overlap by aligning overlapping variants\n        return self._align_overlapping_tags(loci_to_overlap, peak_overlaps)\n\n    def unique_variants_in_locus(self: StudyLocus) -&gt; DataFrame:\n        \"\"\"All unique variants collected in a `StudyLocus` dataframe.\n\n        Returns:\n            DataFrame: A dataframe containing `variantId` and `chromosome` columns.\n        \"\"\"\n        return (\n            self.df.withColumn(\n                \"variantId\",\n                # Joint array of variants in that studylocus. Locus can be null\n                f.explode(\n                    f.array_union(\n                        f.array(f.col(\"variantId\")),\n                        f.coalesce(f.col(\"locus.variantId\"), f.array()),\n                    )\n                ),\n            )\n            .select(\n                \"variantId\", f.split(f.col(\"variantId\"), \"_\")[0].alias(\"chromosome\")\n            )\n            .distinct()\n        )\n\n    def neglog_pvalue(self: StudyLocus) -&gt; Column:\n        \"\"\"Returns the negative log p-value.\n\n        Returns:\n            Column: Negative log p-value\n        \"\"\"\n        return calculate_neglog_pvalue(\n            self.df.pValueMantissa,\n            self.df.pValueExponent,\n        )\n\n    def build_feature_matrix(\n        self: StudyLocus,\n        features_list: list[str],\n        features_input_loader: L2GFeatureInputLoader,\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Returns the feature matrix for a StudyLocus.\n\n        Args:\n            features_list (list[str]): List of features to include in the feature matrix.\n            features_input_loader (L2GFeatureInputLoader): Feature input loader to use.\n\n        Returns:\n            L2GFeatureMatrix: Feature matrix for this study-locus.\n        \"\"\"\n        from gentropy.dataset.l2g_feature_matrix import L2GFeatureMatrix\n\n        return L2GFeatureMatrix.from_features_list(\n            self,\n            features_list,\n            features_input_loader,\n        ).fill_na()\n\n    def annotate_credible_sets(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Annotate study-locus dataset with credible set flags.\n\n        Sorts the array in the `locus` column elements by their `posteriorProbability` values in descending order and adds\n        `is95CredibleSet` and `is99CredibleSet` fields to the elements, indicating which are the tagging variants whose cumulative sum\n        of their `posteriorProbability` values is below 0.95 and 0.99, respectively.\n\n        Returns:\n            StudyLocus: including annotation on `is95CredibleSet` and `is99CredibleSet`.\n\n        Raises:\n            ValueError: If `locus` column is not available.\n        \"\"\"\n        if \"locus\" not in self.df.columns:\n            raise ValueError(\"Locus column not available.\")\n\n        self.df = self.df.withColumn(\n            # Sort credible set by posterior probability in descending order\n            \"locus\",\n            f.when(\n                f.col(\"locus\").isNotNull() &amp; (f.size(f.col(\"locus\")) &gt; 0),\n                order_array_of_structs_by_field(\"locus\", \"posteriorProbability\"),\n            ),\n        ).withColumn(\n            # Calculate array of cumulative sums of posterior probabilities to determine which variants are in the 95% and 99% credible sets\n            # and zip the cumulative sums array with the credible set array to add the flags\n            \"locus\",\n            f.when(\n                f.col(\"locus\").isNotNull() &amp; (f.size(f.col(\"locus\")) &gt; 0),\n                f.zip_with(\n                    f.col(\"locus\"),\n                    f.transform(\n                        f.sequence(f.lit(1), f.size(f.col(\"locus\"))),\n                        lambda index: f.aggregate(\n                            f.slice(\n                                # By using `index - 1` we introduce a value of `0.0` in the cumulative sums array. to ensure that the last variant\n                                # that exceeds the 0.95 threshold is included in the cumulative sum, as its probability is necessary to satisfy the threshold.\n                                f.col(\"locus.posteriorProbability\"),\n                                1,\n                                index - 1,\n                            ),\n                            f.lit(0.0),\n                            lambda acc, el: acc + el,\n                        ),\n                    ),\n                    lambda struct_e, acc: struct_e.withField(\n                        CredibleInterval.IS95.value, (acc &lt; 0.95) &amp; acc.isNotNull()\n                    ).withField(\n                        CredibleInterval.IS99.value, (acc &lt; 0.99) &amp; acc.isNotNull()\n                    ),\n                ),\n            ),\n        )\n        return self\n\n    def annotate_locus_statistics(\n        self: StudyLocus,\n        summary_statistics: SummaryStatistics,\n        collect_locus_distance: int,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotates study locus with summary statistics in the specified distance around the position.\n\n        Args:\n            summary_statistics (SummaryStatistics): Summary statistics to be used for annotation.\n            collect_locus_distance (int): distance from variant defining window for inclusion of variants in locus.\n\n        Returns:\n            StudyLocus: Study locus annotated with summary statistics in `locus` column. If no statistics are found, the `locus` column will be empty.\n        \"\"\"\n        # The clumps will be used several times (persisting)\n        self.df.persist()\n        # Renaming columns:\n        sumstats_renamed = summary_statistics.df.selectExpr(\n            *[f\"{col} as tag_{col}\" for col in summary_statistics.df.columns]\n        ).alias(\"sumstat\")\n\n        locus_df = (\n            sumstats_renamed\n            # Joining the two datasets together:\n            .join(\n                f.broadcast(\n                    self.df.alias(\"clumped\").select(\n                        \"position\", \"chromosome\", \"studyId\", \"studyLocusId\"\n                    )\n                ),\n                on=[\n                    (f.col(\"sumstat.tag_studyId\") == f.col(\"clumped.studyId\"))\n                    &amp; (f.col(\"sumstat.tag_chromosome\") == f.col(\"clumped.chromosome\"))\n                    &amp; (\n                        f.col(\"sumstat.tag_position\")\n                        &gt;= (f.col(\"clumped.position\") - collect_locus_distance)\n                    )\n                    &amp; (\n                        f.col(\"sumstat.tag_position\")\n                        &lt;= (f.col(\"clumped.position\") + collect_locus_distance)\n                    )\n                ],\n                how=\"inner\",\n            )\n            .withColumn(\n                \"locus\",\n                f.struct(\n                    f.col(\"tag_variantId\").alias(\"variantId\"),\n                    f.col(\"tag_beta\").alias(\"beta\"),\n                    f.col(\"tag_pValueMantissa\").alias(\"pValueMantissa\"),\n                    f.col(\"tag_pValueExponent\").alias(\"pValueExponent\"),\n                    f.col(\"tag_standardError\").alias(\"standardError\"),\n                ),\n            )\n            .groupBy(\"studyLocusId\")\n            .agg(\n                f.collect_list(f.col(\"locus\")).alias(\"locus\"),\n            )\n        )\n\n        self.df = self.df.drop(\"locus\").join(\n            locus_df,\n            on=\"studyLocusId\",\n            how=\"left\",\n        )\n\n        return self\n\n    def annotate_ld(\n        self: StudyLocus,\n        study_index: StudyIndex,\n        ld_index: LDIndex,\n        r2_threshold: float = 0.0,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotate LD information to study-locus.\n\n        Args:\n            study_index (StudyIndex): Study index to resolve ancestries.\n            ld_index (LDIndex): LD index to resolve LD information.\n            r2_threshold (float): R2 threshold to filter the LD index. Default is 0.0.\n\n        Returns:\n            StudyLocus: Study locus annotated with ld information from LD index.\n        \"\"\"\n        from gentropy.method.ld import LDAnnotator\n\n        return LDAnnotator.ld_annotate(self, study_index, ld_index, r2_threshold)\n\n    def clump(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Perform LD clumping of the studyLocus.\n\n        Evaluates whether a lead variant is linked to a tag (with lowest p-value) in the same studyLocus dataset.\n\n        Returns:\n            StudyLocus: with empty credible sets for linked variants and QC flag.\n        \"\"\"\n        clumped_df = (\n            self.df.withColumn(\n                \"is_lead_linked\",\n                LDclumping._is_lead_linked(\n                    self.df.studyId,\n                    self.df.chromosome,\n                    self.df.variantId,\n                    self.df.pValueExponent,\n                    self.df.pValueMantissa,\n                    self.df.ldSet,\n                ),\n            )\n            .withColumn(\n                \"ldSet\",\n                f.when(f.col(\"is_lead_linked\"), f.array()).otherwise(f.col(\"ldSet\")),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.col(\"is_lead_linked\"),\n                    StudyLocusQualityCheck.LD_CLUMPED,\n                ),\n            )\n            .drop(\"is_lead_linked\")\n        )\n        return StudyLocus(\n            _df=clumped_df,\n            _schema=self.get_schema(),\n        )\n\n    def exclude_region(\n        self: StudyLocus, region: GenomicRegion, exclude_overlap: bool = False\n    ) -&gt; StudyLocus:\n        \"\"\"Exclude a region from the StudyLocus dataset.\n\n        Args:\n            region (GenomicRegion): genomic region object.\n            exclude_overlap (bool): If True, excludes StudyLocus windows with any overlap with the region.\n\n        Returns:\n            StudyLocus: filtered StudyLocus object.\n        \"\"\"\n        if exclude_overlap:\n            filter_condition = ~(\n                (f.col(\"chromosome\") == region.chromosome)\n                &amp; (\n                    (f.col(\"locusStart\") &lt;= region.end)\n                    &amp; (f.col(\"locusEnd\") &gt;= region.start)\n                )\n            )\n        else:\n            filter_condition = ~(\n                (f.col(\"chromosome\") == region.chromosome)\n                &amp; (\n                    (f.col(\"position\") &gt;= region.start)\n                    &amp; (f.col(\"position\") &lt;= region.end)\n                )\n            )\n\n        return StudyLocus(\n            _df=self.df.filter(filter_condition),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    def qc_MHC_region(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Adds qualityControl flag when lead overlaps with MHC region.\n\n        Returns:\n            StudyLocus: including qualityControl flag if in MHC region.\n        \"\"\"\n        region = GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC)\n        self.df = self.df.withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                (\n                    (f.col(\"chromosome\") == region.chromosome)\n                    &amp; (\n                        (f.col(\"position\") &lt;= region.end)\n                        &amp; (f.col(\"position\") &gt;= region.start)\n                    )\n                ),\n                StudyLocusQualityCheck.IN_MHC,\n            ),\n        )\n        return self\n\n    def qc_redundant_top_hits_from_PICS(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Flag associations from top hits when the study contains other PICS associations from summary statistics.\n\n        This flag can be useful to identify top hits that should be explained by other associations in the study derived from the summary statistics.\n\n        Returns:\n            StudyLocus: Updated study locus with redundant top hits flagged.\n        \"\"\"\n        studies_with_pics_sumstats = (\n            self.df.filter(f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n            # Returns True if the study contains any PICS associations from summary statistics\n            .withColumn(\n                \"hasPicsSumstats\",\n                ~f.array_contains(\n                    \"qualityControls\", StudyLocusQualityCheck.TOP_HIT.value\n                ),\n            )\n            .groupBy(\"studyId\")\n            .agg(f.max(f.col(\"hasPicsSumstats\")).alias(\"studiesWithPicsSumstats\"))\n        )\n\n        return StudyLocus(\n            _df=self.df.join(studies_with_pics_sumstats, on=\"studyId\", how=\"left\")\n            .withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.array_contains(\n                        \"qualityControls\", StudyLocusQualityCheck.TOP_HIT.value\n                    )\n                    &amp; f.col(\"studiesWithPicsSumstats\"),\n                    StudyLocusQualityCheck.REDUNDANT_PICS_TOP_HIT,\n                ),\n            )\n            .drop(\"studiesWithPicsSumstats\"),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    def qc_explained_by_SuSiE(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Flag associations that are explained by SuSiE associations.\n\n        Credible sets overlapping in the same region as a SuSiE credible set are flagged as explained by SuSiE.\n\n        Returns:\n            StudyLocus: Updated study locus with SuSiE explained flags.\n        \"\"\"\n        # unique study-regions covered by SuSie credible sets\n        susie_study_regions = (\n            self.filter(\n                f.col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n                )\n            )\n            .df.select(\n                \"studyId\",\n                \"chromosome\",\n                \"locusStart\",\n                \"locusEnd\",\n                f.lit(True).alias(\"inSuSiE\"),\n            )\n            .distinct()\n        )\n\n        # non SuSiE credible sets (studyLocusId) overlapping in any variant with SuSiE locus\n        redundant_study_locus = (\n            self.filter(\n                ~f.col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n                )\n            )\n            .df.withColumn(\"l\", f.explode(\"locus\"))\n            .select(\n                \"studyLocusId\",\n                \"studyId\",\n                \"chromosome\",\n                f.split(f.col(\"l.variantId\"), \"_\")[1].alias(\"tag_position\"),\n            )\n            .alias(\"study_locus\")\n            .join(\n                susie_study_regions.alias(\"regions\"),\n                how=\"inner\",\n                on=[\n                    (f.col(\"study_locus.chromosome\") == f.col(\"regions.chromosome\"))\n                    &amp; (f.col(\"study_locus.studyId\") == f.col(\"regions.studyId\"))\n                    &amp; (f.col(\"study_locus.tag_position\") &gt;= f.col(\"regions.locusStart\"))\n                    &amp; (f.col(\"study_locus.tag_position\") &lt;= f.col(\"regions.locusEnd\"))\n                ],\n            )\n            .select(\"studyLocusId\", \"inSuSiE\")\n            .distinct()\n        )\n\n        return StudyLocus(\n            _df=(\n                self.df.join(redundant_study_locus, on=\"studyLocusId\", how=\"left\")\n                .withColumn(\n                    \"qualityControls\",\n                    self.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        # credible set in SuSiE overlapping region\n                        f.col(\"inSuSiE\")\n                        # credible set not based on SuSiE\n                        &amp; (\n                            ~f.col(\"finemappingMethod\").isin(\n                                FinemappingMethod.SUSIE.value,\n                                FinemappingMethod.SUSIE_INF.value,\n                            )\n                        ),\n                        StudyLocusQualityCheck.EXPLAINED_BY_SUSIE,\n                    ),\n                )\n                .drop(\"inSuSiE\")\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    def _qc_no_population(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Flag associations where the study doesn't have population information to resolve LD.\n\n        Returns:\n            StudyLocus: Updated study locus.\n        \"\"\"\n        # If the tested column is not present, return self unchanged:\n        if \"ldPopulationStructure\" not in self.df.columns:\n            return self\n\n        self.df = self.df.withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.col(\"ldPopulationStructure\").isNull(),\n                StudyLocusQualityCheck.NO_POPULATION,\n            ),\n        )\n        return self\n\n    def annotate_locus_statistics_boundaries(\n        self: StudyLocus,\n        summary_statistics: SummaryStatistics,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotates study locus with summary statistics in the specified boundaries - locusStart and locusEnd.\n\n        Args:\n            summary_statistics (SummaryStatistics): Summary statistics to be used for annotation.\n\n        Returns:\n            StudyLocus: Study locus annotated with summary statistics in `locus` column. If no statistics are found, the `locus` column will be empty.\n        \"\"\"\n        # The clumps will be used several times (persisting)\n        self.df.persist()\n        # Renaming columns:\n        sumstats_renamed = summary_statistics.df.selectExpr(\n            *[f\"{col} as tag_{col}\" for col in summary_statistics.df.columns]\n        ).alias(\"sumstat\")\n\n        locus_df = (\n            sumstats_renamed\n            # Joining the two datasets together:\n            .join(\n                f.broadcast(\n                    self.df.alias(\"clumped\").select(\n                        \"position\",\n                        \"chromosome\",\n                        \"studyId\",\n                        \"studyLocusId\",\n                        \"locusStart\",\n                        \"locusEnd\",\n                    )\n                ),\n                on=[\n                    (f.col(\"sumstat.tag_studyId\") == f.col(\"clumped.studyId\"))\n                    &amp; (f.col(\"sumstat.tag_chromosome\") == f.col(\"clumped.chromosome\"))\n                    &amp; (f.col(\"sumstat.tag_position\") &gt;= (f.col(\"clumped.locusStart\")))\n                    &amp; (f.col(\"sumstat.tag_position\") &lt;= (f.col(\"clumped.locusEnd\")))\n                ],\n                how=\"inner\",\n            )\n            .withColumn(\n                \"locus\",\n                f.struct(\n                    f.col(\"tag_variantId\").alias(\"variantId\"),\n                    f.col(\"tag_beta\").alias(\"beta\"),\n                    f.col(\"tag_pValueMantissa\").alias(\"pValueMantissa\"),\n                    f.col(\"tag_pValueExponent\").alias(\"pValueExponent\"),\n                    f.col(\"tag_standardError\").alias(\"standardError\"),\n                ),\n            )\n            .groupBy(\"studyLocusId\")\n            .agg(\n                f.collect_list(f.col(\"locus\")).alias(\"locus\"),\n            )\n        )\n\n        self.df = self.df.drop(\"locus\").join(\n            locus_df,\n            on=\"studyLocusId\",\n            how=\"left\",\n        )\n\n        return self\n\n    def window_based_clumping(\n        self: StudyLocus,\n        window_size: int = WindowBasedClumpingStepConfig().distance,\n    ) -&gt; StudyLocus:\n        \"\"\"Clump study locus by window size.\n\n        Args:\n            window_size (int): Window size for clumping.\n\n        Returns:\n            StudyLocus: Clumped study locus, where clumped associations are flagged.\n        \"\"\"\n        from gentropy.method.window_based_clumping import WindowBasedClumping\n\n        return WindowBasedClumping.clump(self, window_size)\n\n    def assign_confidence(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Assign confidence to study locus.\n\n        Returns:\n            StudyLocus: Study locus with confidence assigned.\n        \"\"\"\n        # Return self if the required columns are not in the dataframe:\n        if (\n            \"qualityControls\" not in self.df.columns\n            or \"finemappingMethod\" not in self.df.columns\n        ):\n            return self\n\n        # Assign confidence based on the presence of quality controls\n        df = self.df.withColumn(\n            \"confidence\",\n            f.when(\n                (\n                    f.col(\"finemappingMethod\").isin(\n                        FinemappingMethod.SUSIE.value,\n                        FinemappingMethod.SUSIE_INF.value,\n                    )\n                )\n                &amp; (\n                    ~f.array_contains(\n                        f.col(\"qualityControls\"),\n                        StudyLocusQualityCheck.OUT_OF_SAMPLE_LD.value,\n                    )\n                ),\n                CredibleSetConfidenceClasses.FINEMAPPED_IN_SAMPLE_LD.value,\n            )\n            .when(\n                (\n                    f.col(\"finemappingMethod\").isin(\n                        FinemappingMethod.SUSIE.value,\n                        FinemappingMethod.SUSIE_INF.value,\n                    )\n                )\n                &amp; (\n                    f.array_contains(\n                        f.col(\"qualityControls\"),\n                        StudyLocusQualityCheck.OUT_OF_SAMPLE_LD.value,\n                    )\n                ),\n                CredibleSetConfidenceClasses.FINEMAPPED_OUT_OF_SAMPLE_LD.value,\n            )\n            .when(\n                (f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n                &amp; (\n                    ~f.array_contains(\n                        f.col(\"qualityControls\"), StudyLocusQualityCheck.TOP_HIT.value\n                    )\n                ),\n                CredibleSetConfidenceClasses.PICSED_SUMMARY_STATS.value,\n            )\n            .when(\n                (f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n                &amp; (\n                    f.array_contains(\n                        f.col(\"qualityControls\"), StudyLocusQualityCheck.TOP_HIT.value\n                    )\n                ),\n                CredibleSetConfidenceClasses.PICSED_TOP_HIT.value,\n            )\n            .otherwise(CredibleSetConfidenceClasses.UNKNOWN.value),\n        )\n\n        return StudyLocus(\n            _df=df,\n            _schema=self.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_credible_sets","title":"<code>annotate_credible_sets() -&gt; StudyLocus</code>","text":"<p>Annotate study-locus dataset with credible set flags.</p> <p>Sorts the array in the <code>locus</code> column elements by their <code>posteriorProbability</code> values in descending order and adds <code>is95CredibleSet</code> and <code>is99CredibleSet</code> fields to the elements, indicating which are the tagging variants whose cumulative sum of their <code>posteriorProbability</code> values is below 0.95 and 0.99, respectively.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>including annotation on <code>is95CredibleSet</code> and <code>is99CredibleSet</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>locus</code> column is not available.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_credible_sets(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Annotate study-locus dataset with credible set flags.\n\n    Sorts the array in the `locus` column elements by their `posteriorProbability` values in descending order and adds\n    `is95CredibleSet` and `is99CredibleSet` fields to the elements, indicating which are the tagging variants whose cumulative sum\n    of their `posteriorProbability` values is below 0.95 and 0.99, respectively.\n\n    Returns:\n        StudyLocus: including annotation on `is95CredibleSet` and `is99CredibleSet`.\n\n    Raises:\n        ValueError: If `locus` column is not available.\n    \"\"\"\n    if \"locus\" not in self.df.columns:\n        raise ValueError(\"Locus column not available.\")\n\n    self.df = self.df.withColumn(\n        # Sort credible set by posterior probability in descending order\n        \"locus\",\n        f.when(\n            f.col(\"locus\").isNotNull() &amp; (f.size(f.col(\"locus\")) &gt; 0),\n            order_array_of_structs_by_field(\"locus\", \"posteriorProbability\"),\n        ),\n    ).withColumn(\n        # Calculate array of cumulative sums of posterior probabilities to determine which variants are in the 95% and 99% credible sets\n        # and zip the cumulative sums array with the credible set array to add the flags\n        \"locus\",\n        f.when(\n            f.col(\"locus\").isNotNull() &amp; (f.size(f.col(\"locus\")) &gt; 0),\n            f.zip_with(\n                f.col(\"locus\"),\n                f.transform(\n                    f.sequence(f.lit(1), f.size(f.col(\"locus\"))),\n                    lambda index: f.aggregate(\n                        f.slice(\n                            # By using `index - 1` we introduce a value of `0.0` in the cumulative sums array. to ensure that the last variant\n                            # that exceeds the 0.95 threshold is included in the cumulative sum, as its probability is necessary to satisfy the threshold.\n                            f.col(\"locus.posteriorProbability\"),\n                            1,\n                            index - 1,\n                        ),\n                        f.lit(0.0),\n                        lambda acc, el: acc + el,\n                    ),\n                ),\n                lambda struct_e, acc: struct_e.withField(\n                    CredibleInterval.IS95.value, (acc &lt; 0.95) &amp; acc.isNotNull()\n                ).withField(\n                    CredibleInterval.IS99.value, (acc &lt; 0.99) &amp; acc.isNotNull()\n                ),\n            ),\n        ),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_ld","title":"<code>annotate_ld(study_index: StudyIndex, ld_index: LDIndex, r2_threshold: float = 0.0) -&gt; StudyLocus</code>","text":"<p>Annotate LD information to study-locus.</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>Study index to resolve ancestries.</p> required <code>ld_index</code> <code>LDIndex</code> <p>LD index to resolve LD information.</p> required <code>r2_threshold</code> <code>float</code> <p>R2 threshold to filter the LD index. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus annotated with ld information from LD index.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_ld(\n    self: StudyLocus,\n    study_index: StudyIndex,\n    ld_index: LDIndex,\n    r2_threshold: float = 0.0,\n) -&gt; StudyLocus:\n    \"\"\"Annotate LD information to study-locus.\n\n    Args:\n        study_index (StudyIndex): Study index to resolve ancestries.\n        ld_index (LDIndex): LD index to resolve LD information.\n        r2_threshold (float): R2 threshold to filter the LD index. Default is 0.0.\n\n    Returns:\n        StudyLocus: Study locus annotated with ld information from LD index.\n    \"\"\"\n    from gentropy.method.ld import LDAnnotator\n\n    return LDAnnotator.ld_annotate(self, study_index, ld_index, r2_threshold)\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_locus_statistics","title":"<code>annotate_locus_statistics(summary_statistics: SummaryStatistics, collect_locus_distance: int) -&gt; StudyLocus</code>","text":"<p>Annotates study locus with summary statistics in the specified distance around the position.</p> <p>Parameters:</p> Name Type Description Default <code>summary_statistics</code> <code>SummaryStatistics</code> <p>Summary statistics to be used for annotation.</p> required <code>collect_locus_distance</code> <code>int</code> <p>distance from variant defining window for inclusion of variants in locus.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus annotated with summary statistics in <code>locus</code> column. If no statistics are found, the <code>locus</code> column will be empty.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_locus_statistics(\n    self: StudyLocus,\n    summary_statistics: SummaryStatistics,\n    collect_locus_distance: int,\n) -&gt; StudyLocus:\n    \"\"\"Annotates study locus with summary statistics in the specified distance around the position.\n\n    Args:\n        summary_statistics (SummaryStatistics): Summary statistics to be used for annotation.\n        collect_locus_distance (int): distance from variant defining window for inclusion of variants in locus.\n\n    Returns:\n        StudyLocus: Study locus annotated with summary statistics in `locus` column. If no statistics are found, the `locus` column will be empty.\n    \"\"\"\n    # The clumps will be used several times (persisting)\n    self.df.persist()\n    # Renaming columns:\n    sumstats_renamed = summary_statistics.df.selectExpr(\n        *[f\"{col} as tag_{col}\" for col in summary_statistics.df.columns]\n    ).alias(\"sumstat\")\n\n    locus_df = (\n        sumstats_renamed\n        # Joining the two datasets together:\n        .join(\n            f.broadcast(\n                self.df.alias(\"clumped\").select(\n                    \"position\", \"chromosome\", \"studyId\", \"studyLocusId\"\n                )\n            ),\n            on=[\n                (f.col(\"sumstat.tag_studyId\") == f.col(\"clumped.studyId\"))\n                &amp; (f.col(\"sumstat.tag_chromosome\") == f.col(\"clumped.chromosome\"))\n                &amp; (\n                    f.col(\"sumstat.tag_position\")\n                    &gt;= (f.col(\"clumped.position\") - collect_locus_distance)\n                )\n                &amp; (\n                    f.col(\"sumstat.tag_position\")\n                    &lt;= (f.col(\"clumped.position\") + collect_locus_distance)\n                )\n            ],\n            how=\"inner\",\n        )\n        .withColumn(\n            \"locus\",\n            f.struct(\n                f.col(\"tag_variantId\").alias(\"variantId\"),\n                f.col(\"tag_beta\").alias(\"beta\"),\n                f.col(\"tag_pValueMantissa\").alias(\"pValueMantissa\"),\n                f.col(\"tag_pValueExponent\").alias(\"pValueExponent\"),\n                f.col(\"tag_standardError\").alias(\"standardError\"),\n            ),\n        )\n        .groupBy(\"studyLocusId\")\n        .agg(\n            f.collect_list(f.col(\"locus\")).alias(\"locus\"),\n        )\n    )\n\n    self.df = self.df.drop(\"locus\").join(\n        locus_df,\n        on=\"studyLocusId\",\n        how=\"left\",\n    )\n\n    return self\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_locus_statistics_boundaries","title":"<code>annotate_locus_statistics_boundaries(summary_statistics: SummaryStatistics) -&gt; StudyLocus</code>","text":"<p>Annotates study locus with summary statistics in the specified boundaries - locusStart and locusEnd.</p> <p>Parameters:</p> Name Type Description Default <code>summary_statistics</code> <code>SummaryStatistics</code> <p>Summary statistics to be used for annotation.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus annotated with summary statistics in <code>locus</code> column. If no statistics are found, the <code>locus</code> column will be empty.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_locus_statistics_boundaries(\n    self: StudyLocus,\n    summary_statistics: SummaryStatistics,\n) -&gt; StudyLocus:\n    \"\"\"Annotates study locus with summary statistics in the specified boundaries - locusStart and locusEnd.\n\n    Args:\n        summary_statistics (SummaryStatistics): Summary statistics to be used for annotation.\n\n    Returns:\n        StudyLocus: Study locus annotated with summary statistics in `locus` column. If no statistics are found, the `locus` column will be empty.\n    \"\"\"\n    # The clumps will be used several times (persisting)\n    self.df.persist()\n    # Renaming columns:\n    sumstats_renamed = summary_statistics.df.selectExpr(\n        *[f\"{col} as tag_{col}\" for col in summary_statistics.df.columns]\n    ).alias(\"sumstat\")\n\n    locus_df = (\n        sumstats_renamed\n        # Joining the two datasets together:\n        .join(\n            f.broadcast(\n                self.df.alias(\"clumped\").select(\n                    \"position\",\n                    \"chromosome\",\n                    \"studyId\",\n                    \"studyLocusId\",\n                    \"locusStart\",\n                    \"locusEnd\",\n                )\n            ),\n            on=[\n                (f.col(\"sumstat.tag_studyId\") == f.col(\"clumped.studyId\"))\n                &amp; (f.col(\"sumstat.tag_chromosome\") == f.col(\"clumped.chromosome\"))\n                &amp; (f.col(\"sumstat.tag_position\") &gt;= (f.col(\"clumped.locusStart\")))\n                &amp; (f.col(\"sumstat.tag_position\") &lt;= (f.col(\"clumped.locusEnd\")))\n            ],\n            how=\"inner\",\n        )\n        .withColumn(\n            \"locus\",\n            f.struct(\n                f.col(\"tag_variantId\").alias(\"variantId\"),\n                f.col(\"tag_beta\").alias(\"beta\"),\n                f.col(\"tag_pValueMantissa\").alias(\"pValueMantissa\"),\n                f.col(\"tag_pValueExponent\").alias(\"pValueExponent\"),\n                f.col(\"tag_standardError\").alias(\"standardError\"),\n            ),\n        )\n        .groupBy(\"studyLocusId\")\n        .agg(\n            f.collect_list(f.col(\"locus\")).alias(\"locus\"),\n        )\n    )\n\n    self.df = self.df.drop(\"locus\").join(\n        locus_df,\n        on=\"studyLocusId\",\n        how=\"left\",\n    )\n\n    return self\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_study_type","title":"<code>annotate_study_type(study_index: StudyIndex) -&gt; StudyLocus</code>","text":"<p>Gets study type from study index and adds it to study locus.</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>Study index to get study type.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with study type.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_study_type(self: StudyLocus, study_index: StudyIndex) -&gt; StudyLocus:\n    \"\"\"Gets study type from study index and adds it to study locus.\n\n    Args:\n        study_index (StudyIndex): Study index to get study type.\n\n    Returns:\n        StudyLocus: Updated study locus with study type.\n    \"\"\"\n    return StudyLocus(\n        _df=(\n            self.df.drop(\"studyType\").join(\n                study_index.study_type_lut(), on=\"studyId\", how=\"left\"\n            )\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.assign_confidence","title":"<code>assign_confidence() -&gt; StudyLocus</code>","text":"<p>Assign confidence to study locus.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus with confidence assigned.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def assign_confidence(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Assign confidence to study locus.\n\n    Returns:\n        StudyLocus: Study locus with confidence assigned.\n    \"\"\"\n    # Return self if the required columns are not in the dataframe:\n    if (\n        \"qualityControls\" not in self.df.columns\n        or \"finemappingMethod\" not in self.df.columns\n    ):\n        return self\n\n    # Assign confidence based on the presence of quality controls\n    df = self.df.withColumn(\n        \"confidence\",\n        f.when(\n            (\n                f.col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value,\n                    FinemappingMethod.SUSIE_INF.value,\n                )\n            )\n            &amp; (\n                ~f.array_contains(\n                    f.col(\"qualityControls\"),\n                    StudyLocusQualityCheck.OUT_OF_SAMPLE_LD.value,\n                )\n            ),\n            CredibleSetConfidenceClasses.FINEMAPPED_IN_SAMPLE_LD.value,\n        )\n        .when(\n            (\n                f.col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value,\n                    FinemappingMethod.SUSIE_INF.value,\n                )\n            )\n            &amp; (\n                f.array_contains(\n                    f.col(\"qualityControls\"),\n                    StudyLocusQualityCheck.OUT_OF_SAMPLE_LD.value,\n                )\n            ),\n            CredibleSetConfidenceClasses.FINEMAPPED_OUT_OF_SAMPLE_LD.value,\n        )\n        .when(\n            (f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n            &amp; (\n                ~f.array_contains(\n                    f.col(\"qualityControls\"), StudyLocusQualityCheck.TOP_HIT.value\n                )\n            ),\n            CredibleSetConfidenceClasses.PICSED_SUMMARY_STATS.value,\n        )\n        .when(\n            (f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n            &amp; (\n                f.array_contains(\n                    f.col(\"qualityControls\"), StudyLocusQualityCheck.TOP_HIT.value\n                )\n            ),\n            CredibleSetConfidenceClasses.PICSED_TOP_HIT.value,\n        )\n        .otherwise(CredibleSetConfidenceClasses.UNKNOWN.value),\n    )\n\n    return StudyLocus(\n        _df=df,\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.assign_study_locus_id","title":"<code>assign_study_locus_id(uniqueness_defining_columns: list[str]) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Hashes the provided columns to extract a consistent studyLocusId.</p> <p>Parameters:</p> Name Type Description Default <code>uniqueness_defining_columns</code> <code>list[str]</code> <p>list of columns defining uniqueness</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>column with a study locus ID</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"GCST000001\", \"1_1000_A_C\", \"SuSiE-inf\"), (\"GCST000002\", \"1_1000_A_C\", \"pics\")]).toDF(\"studyId\", \"variantId\", \"finemappingMethod\")\n&gt;&gt;&gt; df.withColumn(\"study_locus_id\", StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\", \"finemappingMethod\"])).show(truncate=False)\n+----------+----------+-----------------+--------------------------------+\n|studyId   |variantId |finemappingMethod|study_locus_id                  |\n+----------+----------+-----------------+--------------------------------+\n|GCST000001|1_1000_A_C|SuSiE-inf        |109804fe1e20c94231a31bafd71b566e|\n|GCST000002|1_1000_A_C|pics             |de310be4558e0482c9cc359c97d37773|\n+----------+----------+-----------------+--------------------------------+\n</code></pre> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@staticmethod\ndef assign_study_locus_id(uniqueness_defining_columns: list[str]) -&gt; Column:\n    \"\"\"Hashes the provided columns to extract a consistent studyLocusId.\n\n    Args:\n        uniqueness_defining_columns (list[str]): list of columns defining uniqueness\n\n    Returns:\n        Column: column with a study locus ID\n\n    Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"GCST000001\", \"1_1000_A_C\", \"SuSiE-inf\"), (\"GCST000002\", \"1_1000_A_C\", \"pics\")]).toDF(\"studyId\", \"variantId\", \"finemappingMethod\")\n        &gt;&gt;&gt; df.withColumn(\"study_locus_id\", StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\", \"finemappingMethod\"])).show(truncate=False)\n        +----------+----------+-----------------+--------------------------------+\n        |studyId   |variantId |finemappingMethod|study_locus_id                  |\n        +----------+----------+-----------------+--------------------------------+\n        |GCST000001|1_1000_A_C|SuSiE-inf        |109804fe1e20c94231a31bafd71b566e|\n        |GCST000002|1_1000_A_C|pics             |de310be4558e0482c9cc359c97d37773|\n        +----------+----------+-----------------+--------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return Dataset.generate_identifier(uniqueness_defining_columns).alias(\n        \"studyLocusId\"\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.build_feature_matrix","title":"<code>build_feature_matrix(features_list: list[str], features_input_loader: L2GFeatureInputLoader) -&gt; L2GFeatureMatrix</code>","text":"<p>Returns the feature matrix for a StudyLocus.</p> <p>Parameters:</p> Name Type Description Default <code>features_list</code> <code>list[str]</code> <p>List of features to include in the feature matrix.</p> required <code>features_input_loader</code> <code>L2GFeatureInputLoader</code> <p>Feature input loader to use.</p> required <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix for this study-locus.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def build_feature_matrix(\n    self: StudyLocus,\n    features_list: list[str],\n    features_input_loader: L2GFeatureInputLoader,\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Returns the feature matrix for a StudyLocus.\n\n    Args:\n        features_list (list[str]): List of features to include in the feature matrix.\n        features_input_loader (L2GFeatureInputLoader): Feature input loader to use.\n\n    Returns:\n        L2GFeatureMatrix: Feature matrix for this study-locus.\n    \"\"\"\n    from gentropy.dataset.l2g_feature_matrix import L2GFeatureMatrix\n\n    return L2GFeatureMatrix.from_features_list(\n        self,\n        features_list,\n        features_input_loader,\n    ).fill_na()\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.calculate_credible_set_log10bf","title":"<code>calculate_credible_set_log10bf(logbfs: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Calculate Bayes factor for the entire credible set. The Bayes factor is calculated as the logsumexp of the logBF values of the variants in the locus.</p> <p>Parameters:</p> Name Type Description Default <code>logbfs</code> <code>Column</code> <p>Array column with the logBF values of the variants in the locus.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>log10 Bayes factor for the entire credible set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark.createDataFrame([([0.2, 0.1, 0.05, 0.0],)]).toDF(\"logBF\").select(f.round(StudyLocus.calculate_credible_set_log10bf(f.col(\"logBF\")), 7).alias(\"credibleSetlog10BF\")).show()\n+------------------+\n|credibleSetlog10BF|\n+------------------+\n|         0.6412604|\n+------------------+\n</code></pre> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@classmethod\ndef calculate_credible_set_log10bf(cls: type[StudyLocus], logbfs: Column) -&gt; Column:\n    \"\"\"Calculate Bayes factor for the entire credible set. The Bayes factor is calculated as the logsumexp of the logBF values of the variants in the locus.\n\n    Args:\n        logbfs (Column): Array column with the logBF values of the variants in the locus.\n\n    Returns:\n        Column: log10 Bayes factor for the entire credible set.\n\n    Examples:\n        &gt;&gt;&gt; spark.createDataFrame([([0.2, 0.1, 0.05, 0.0],)]).toDF(\"logBF\").select(f.round(StudyLocus.calculate_credible_set_log10bf(f.col(\"logBF\")), 7).alias(\"credibleSetlog10BF\")).show()\n        +------------------+\n        |credibleSetlog10BF|\n        +------------------+\n        |         0.6412604|\n        +------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    # log10=log/log(10)=log*0.43429448190325176\n    logsumexp_udf = f.udf(\n        lambda x: (get_logsum(x) * 0.43429448190325176), FloatType()\n    )\n    return logsumexp_udf(logbfs).cast(\"double\").alias(\"credibleSetlog10BF\")\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.clump","title":"<code>clump() -&gt; StudyLocus</code>","text":"<p>Perform LD clumping of the studyLocus.</p> <p>Evaluates whether a lead variant is linked to a tag (with lowest p-value) in the same studyLocus dataset.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>with empty credible sets for linked variants and QC flag.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def clump(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Perform LD clumping of the studyLocus.\n\n    Evaluates whether a lead variant is linked to a tag (with lowest p-value) in the same studyLocus dataset.\n\n    Returns:\n        StudyLocus: with empty credible sets for linked variants and QC flag.\n    \"\"\"\n    clumped_df = (\n        self.df.withColumn(\n            \"is_lead_linked\",\n            LDclumping._is_lead_linked(\n                self.df.studyId,\n                self.df.chromosome,\n                self.df.variantId,\n                self.df.pValueExponent,\n                self.df.pValueMantissa,\n                self.df.ldSet,\n            ),\n        )\n        .withColumn(\n            \"ldSet\",\n            f.when(f.col(\"is_lead_linked\"), f.array()).otherwise(f.col(\"ldSet\")),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyLocus.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.col(\"is_lead_linked\"),\n                StudyLocusQualityCheck.LD_CLUMPED,\n            ),\n        )\n        .drop(\"is_lead_linked\")\n    )\n    return StudyLocus(\n        _df=clumped_df,\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.exclude_region","title":"<code>exclude_region(region: GenomicRegion, exclude_overlap: bool = False) -&gt; StudyLocus</code>","text":"<p>Exclude a region from the StudyLocus dataset.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>GenomicRegion</code> <p>genomic region object.</p> required <code>exclude_overlap</code> <code>bool</code> <p>If True, excludes StudyLocus windows with any overlap with the region.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>filtered StudyLocus object.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def exclude_region(\n    self: StudyLocus, region: GenomicRegion, exclude_overlap: bool = False\n) -&gt; StudyLocus:\n    \"\"\"Exclude a region from the StudyLocus dataset.\n\n    Args:\n        region (GenomicRegion): genomic region object.\n        exclude_overlap (bool): If True, excludes StudyLocus windows with any overlap with the region.\n\n    Returns:\n        StudyLocus: filtered StudyLocus object.\n    \"\"\"\n    if exclude_overlap:\n        filter_condition = ~(\n            (f.col(\"chromosome\") == region.chromosome)\n            &amp; (\n                (f.col(\"locusStart\") &lt;= region.end)\n                &amp; (f.col(\"locusEnd\") &gt;= region.start)\n            )\n        )\n    else:\n        filter_condition = ~(\n            (f.col(\"chromosome\") == region.chromosome)\n            &amp; (\n                (f.col(\"position\") &gt;= region.start)\n                &amp; (f.col(\"position\") &lt;= region.end)\n            )\n        )\n\n    return StudyLocus(\n        _df=self.df.filter(filter_condition),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.filter_credible_set","title":"<code>filter_credible_set(credible_interval: CredibleInterval) -&gt; StudyLocus</code>","text":"<p>Annotate and filter study-locus tag variants based on given credible interval.</p> <p>Parameters:</p> Name Type Description Default <code>credible_interval</code> <code>CredibleInterval</code> <p>Credible interval to filter for.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Filtered study-locus dataset.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def filter_credible_set(\n    self: StudyLocus,\n    credible_interval: CredibleInterval,\n) -&gt; StudyLocus:\n    \"\"\"Annotate and filter study-locus tag variants based on given credible interval.\n\n    Args:\n        credible_interval (CredibleInterval): Credible interval to filter for.\n\n    Returns:\n        StudyLocus: Filtered study-locus dataset.\n    \"\"\"\n    return StudyLocus(\n        _df=self.annotate_credible_sets().df.withColumn(\n            \"locus\",\n            f.filter(\n                f.col(\"locus\"),\n                lambda tag: (tag[credible_interval.value]),\n            ),\n        ),\n        _schema=self._schema,\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.filter_ld_set","title":"<code>filter_ld_set(ld_set: Column, r2_threshold: float) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Filter the LD set by a given R2 threshold.</p> <p>Parameters:</p> Name Type Description Default <code>ld_set</code> <code>Column</code> <p>LD set</p> required <code>r2_threshold</code> <code>float</code> <p>R2 threshold to filter the LD set on</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Filtered LD index</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@staticmethod\ndef filter_ld_set(ld_set: Column, r2_threshold: float) -&gt; Column:\n    \"\"\"Filter the LD set by a given R2 threshold.\n\n    Args:\n        ld_set (Column): LD set\n        r2_threshold (float): R2 threshold to filter the LD set on\n\n    Returns:\n        Column: Filtered LD index\n    \"\"\"\n    return f.when(\n        ld_set.isNotNull(),\n        f.filter(\n            ld_set,\n            lambda tag: tag[\"r2Overall\"] &gt;= r2_threshold,\n        ),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.find_overlaps","title":"<code>find_overlaps(intra_study_overlap: bool = False) -&gt; StudyLocusOverlap</code>","text":"<p>Calculate overlapping study-locus.</p> <p>Find overlapping study-locus that share at least one tagging variant. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side.</p> <p>Parameters:</p> Name Type Description Default <code>intra_study_overlap</code> <code>bool</code> <p>If True, finds intra-study overlaps for credible set deduplication. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>StudyLocusOverlap</code> <code>StudyLocusOverlap</code> <p>Pairs of overlapping study-locus with aligned tags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def find_overlaps(\n    self: StudyLocus, intra_study_overlap: bool = False\n) -&gt; StudyLocusOverlap:\n    \"\"\"Calculate overlapping study-locus.\n\n    Find overlapping study-locus that share at least one tagging variant. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always\n    appearing on the right side.\n\n    Args:\n        intra_study_overlap (bool): If True, finds intra-study overlaps for credible set deduplication. Default is False.\n\n    Returns:\n        StudyLocusOverlap: Pairs of overlapping study-locus with aligned tags.\n    \"\"\"\n    loci_to_overlap = (\n        self.df.filter(f.col(\"studyType\").isNotNull())\n        .withColumn(\"locus\", f.explode(\"locus\"))\n        .select(\n            \"studyLocusId\",\n            \"studyId\",\n            \"studyType\",\n            \"chromosome\",\n            \"region\",\n            f.col(\"locus.variantId\").alias(\"tagVariantId\"),\n            f.col(\"locus.logBF\").alias(\"logBF\"),\n            f.col(\"locus.posteriorProbability\").alias(\"posteriorProbability\"),\n            f.col(\"locus.pValueMantissa\").alias(\"pValueMantissa\"),\n            f.col(\"locus.pValueExponent\").alias(\"pValueExponent\"),\n            f.col(\"locus.beta\").alias(\"beta\"),\n        )\n        .persist()\n    )\n\n    # overlapping study-locus\n    peak_overlaps = self._overlapping_peaks(loci_to_overlap, intra_study_overlap)\n\n    # study-locus overlap by aligning overlapping variants\n    return self._align_overlapping_tags(loci_to_overlap, peak_overlaps)\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.flag_trans_qtls","title":"<code>flag_trans_qtls(study_index: StudyIndex, target_index: TargetIndex, trans_threshold: int = 5000000) -&gt; StudyLocus</code>","text":"<p>Flagging transQTL credible sets based on genomic location of the measured gene.</p> <p>Process: 0. Make sure that the <code>isTransQtl</code> column does not exist (remove if exists) 1. Enrich study-locus dataset with geneId based on study metadata. (only QTL studies are considered) 2. Enrich with transcription start site and chromosome of the studied gegne. 3. Flagging any tagging variant of QTL credible sets, if chromosome is different from the gene or distance is above the threshold. 4. Propagate flags to credible sets where any tags are considered as trans. 5. Return study locus object with annotation stored in 'isTransQtl<code>boolean column, where gwas credible sets will be</code>null`</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>study index to extract identifier of the measured gene</p> required <code>target_index</code> <code>TargetIndex</code> <p>target index bringing TSS and chromosome of the measured gene</p> required <code>trans_threshold</code> <code>int</code> <p>Distance above which the QTL is considered trans. Default: 5_000_000bp</p> <code>5000000</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>new column added indicating if the QTL credibles sets are trans.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def flag_trans_qtls(\n    self: StudyLocus,\n    study_index: StudyIndex,\n    target_index: TargetIndex,\n    trans_threshold: int = 5_000_000,\n) -&gt; StudyLocus:\n    \"\"\"Flagging transQTL credible sets based on genomic location of the measured gene.\n\n    Process:\n    0. Make sure that the `isTransQtl` column does not exist (remove if exists)\n    1. Enrich study-locus dataset with geneId based on study metadata. (only QTL studies are considered)\n    2. Enrich with transcription start site and chromosome of the studied gegne.\n    3. Flagging any tagging variant of QTL credible sets, if chromosome is different from the gene or distance is above the threshold.\n    4. Propagate flags to credible sets where any tags are considered as trans.\n    5. Return study locus object with annotation stored in 'isTransQtl` boolean column, where gwas credible sets will be `null`\n\n    Args:\n        study_index (StudyIndex): study index to extract identifier of the measured gene\n        target_index (TargetIndex): target index bringing TSS and chromosome of the measured gene\n        trans_threshold (int): Distance above which the QTL is considered trans. Default: 5_000_000bp\n\n    Returns:\n        StudyLocus: new column added indicating if the QTL credibles sets are trans.\n    \"\"\"\n    # As the `geneId` column in the study index is optional, we have to test for that:\n    if \"geneId\" not in study_index.df.columns:\n        return self\n\n    # We have to remove the column `isTransQtl` to ensure the column is not duplicated\n    # The duplication can happen when one reads the StudyLocus from parquet with\n    # predefined schema that already contains the `isTransQtl` column.\n    if \"isTransQtl\" in self.df.columns:\n        self.df = self.df.drop(\"isTransQtl\")\n\n    # Process study index:\n    processed_studies = (\n        study_index.df\n        # Dropping gwas studies. This ensures that only QTLs will have \"isTrans\" annotation:\n        .filter(f.col(\"studyType\") != \"gwas\").select(\n            \"studyId\", \"geneId\", \"projectId\"\n        )\n    )\n\n    # Process study locus:\n    processed_credible_set = (\n        self.df\n        # Exploding locus to test all tag variants:\n        .withColumn(\"locus\", f.explode(\"locus\")).select(\n            \"studyLocusId\",\n            \"studyId\",\n            f.split(\"locus.variantId\", \"_\")[0].alias(\"chromosome\"),\n            f.split(\"locus.variantId\", \"_\")[1].cast(LongType()).alias(\"position\"),\n        )\n    )\n\n    # Process target index:\n    processed_targets = target_index.df.select(\n        f.col(\"id\").alias(\"geneId\"),\n        f.col(\"tss\"),\n        f.col(\"genomicLocation.chromosome\").alias(\"geneChromosome\"),\n    )\n\n    # Pool datasets:\n    joined_data = (\n        processed_credible_set\n        # Join processed studies:\n        .join(processed_studies, on=\"studyId\", how=\"inner\")\n        # Join processed targets:\n        .join(processed_targets, on=\"geneId\", how=\"left\")\n        # Assign True/False for QTL studies:\n        .withColumn(\n            \"isTagTrans\",\n            # The QTL signal is considered trans if the locus is on a different chromosome than the measured gene.\n            # OR the distance from the gene's transcription start site is &gt; threshold.\n            f.when(\n                (f.col(\"chromosome\") != f.col(\"geneChromosome\"))\n                | (f.abs(f.col(\"tss\") - f.col(\"position\")) &gt; trans_threshold),\n                f.lit(True),\n            ).otherwise(f.lit(False)),\n        )\n        .groupby(\"studyLocusId\")\n        .agg(\n            # If all tagging variants of the locus is in trans position, the QTL is considered trans:\n            f.when(\n                f.array_contains(f.collect_set(\"isTagTrans\"), f.lit(False)), False\n            )\n            .otherwise(f.lit(True))\n            .alias(\"isTransQtl\")\n        )\n    )\n    # Adding new column, where the value is null for gwas loci:\n    return StudyLocus(self.df.join(joined_data, on=\"studyLocusId\", how=\"left\"))\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.get_QC_column_name","title":"<code>get_QC_column_name() -&gt; str</code>  <code>classmethod</code>","text":"<p>Quality control column.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Name of the quality control column.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@classmethod\ndef get_QC_column_name(cls: type[StudyLocus]) -&gt; str:\n    \"\"\"Quality control column.\n\n    Returns:\n        str: Name of the quality control column.\n    \"\"\"\n    return \"qualityControls\"\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.get_QC_mappings","title":"<code>get_QC_mappings() -&gt; dict[str, str]</code>  <code>classmethod</code>","text":"<p>Quality control flag to QC column category mappings.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: Mapping between flag name and QC column category value.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@classmethod\ndef get_QC_mappings(cls: type[StudyLocus]) -&gt; dict[str, str]:\n    \"\"\"Quality control flag to QC column category mappings.\n\n    Returns:\n        dict[str, str]: Mapping between flag name and QC column category value.\n    \"\"\"\n    return {member.name: member.value for member in StudyLocusQualityCheck}\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the StudyLocus dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>schema for the StudyLocus dataset.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[StudyLocus]) -&gt; StructType:\n    \"\"\"Provides the schema for the StudyLocus dataset.\n\n    Returns:\n        StructType: schema for the StudyLocus dataset.\n    \"\"\"\n    return parse_spark_schema(\"study_locus.json\")\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.neglog_pvalue","title":"<code>neglog_pvalue() -&gt; Column</code>","text":"<p>Returns the negative log p-value.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Negative log p-value</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def neglog_pvalue(self: StudyLocus) -&gt; Column:\n    \"\"\"Returns the negative log p-value.\n\n    Returns:\n        Column: Negative log p-value\n    \"\"\"\n    return calculate_neglog_pvalue(\n        self.df.pValueMantissa,\n        self.df.pValueExponent,\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.qc_MHC_region","title":"<code>qc_MHC_region() -&gt; StudyLocus</code>","text":"<p>Adds qualityControl flag when lead overlaps with MHC region.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>including qualityControl flag if in MHC region.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def qc_MHC_region(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Adds qualityControl flag when lead overlaps with MHC region.\n\n    Returns:\n        StudyLocus: including qualityControl flag if in MHC region.\n    \"\"\"\n    region = GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC)\n    self.df = self.df.withColumn(\n        \"qualityControls\",\n        self.update_quality_flag(\n            f.col(\"qualityControls\"),\n            (\n                (f.col(\"chromosome\") == region.chromosome)\n                &amp; (\n                    (f.col(\"position\") &lt;= region.end)\n                    &amp; (f.col(\"position\") &gt;= region.start)\n                )\n            ),\n            StudyLocusQualityCheck.IN_MHC,\n        ),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.qc_abnormal_pips","title":"<code>qc_abnormal_pips(sum_pips_lower_threshold: float = 0.99, sum_pips_upper_threshold: float = 1.0001) -&gt; StudyLocus</code>","text":"<p>Filter study-locus by sum of posterior inclusion probabilities to ensure that the sum of PIPs is within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>sum_pips_lower_threshold</code> <code>float</code> <p>Lower threshold for the sum of PIPs.</p> <code>0.99</code> <code>sum_pips_upper_threshold</code> <code>float</code> <p>Upper threshold for the sum of PIPs.</p> <code>1.0001</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Filtered study-locus dataset.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def qc_abnormal_pips(\n    self: StudyLocus,\n    sum_pips_lower_threshold: float = 0.99,\n    # Set slightly above 1 to account for floating point errors\n    sum_pips_upper_threshold: float = 1.0001,\n) -&gt; StudyLocus:\n    \"\"\"Filter study-locus by sum of posterior inclusion probabilities to ensure that the sum of PIPs is within a given range.\n\n    Args:\n        sum_pips_lower_threshold (float): Lower threshold for the sum of PIPs.\n        sum_pips_upper_threshold (float): Upper threshold for the sum of PIPs.\n\n    Returns:\n        StudyLocus: Filtered study-locus dataset.\n    \"\"\"\n    # QC column might not be present so we have to be ready to handle it:\n    qc_select_expression = (\n        f.col(\"qualityControls\")\n        if \"qualityControls\" in self.df.columns\n        else f.lit(None).cast(ArrayType(StringType()))\n    )\n\n    flag = self.df.withColumn(\n        \"sumPosteriorProbability\",\n        f.aggregate(\n            f.col(\"locus\"),\n            f.lit(0.0),\n            lambda acc, x: acc + x[\"posteriorProbability\"],\n        ),\n    ).withColumn(\n        \"pipOutOfRange\",\n        f.when(\n            (f.col(\"sumPosteriorProbability\") &lt; sum_pips_lower_threshold)\n            | (f.col(\"sumPosteriorProbability\") &gt; sum_pips_upper_threshold),\n            True,\n        ).otherwise(False),\n    )\n\n    return StudyLocus(\n        _df=(\n            flag\n            # Flagging loci with failed studies:\n            .withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    qc_select_expression,\n                    f.col(\"pipOutOfRange\"),\n                    StudyLocusQualityCheck.ABNORMAL_PIPS,\n                ),\n            ).drop(\"sumPosteriorProbability\", \"pipOutOfRange\")\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.qc_explained_by_SuSiE","title":"<code>qc_explained_by_SuSiE() -&gt; StudyLocus</code>","text":"<p>Flag associations that are explained by SuSiE associations.</p> <p>Credible sets overlapping in the same region as a SuSiE credible set are flagged as explained by SuSiE.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with SuSiE explained flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def qc_explained_by_SuSiE(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Flag associations that are explained by SuSiE associations.\n\n    Credible sets overlapping in the same region as a SuSiE credible set are flagged as explained by SuSiE.\n\n    Returns:\n        StudyLocus: Updated study locus with SuSiE explained flags.\n    \"\"\"\n    # unique study-regions covered by SuSie credible sets\n    susie_study_regions = (\n        self.filter(\n            f.col(\"finemappingMethod\").isin(\n                FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n            )\n        )\n        .df.select(\n            \"studyId\",\n            \"chromosome\",\n            \"locusStart\",\n            \"locusEnd\",\n            f.lit(True).alias(\"inSuSiE\"),\n        )\n        .distinct()\n    )\n\n    # non SuSiE credible sets (studyLocusId) overlapping in any variant with SuSiE locus\n    redundant_study_locus = (\n        self.filter(\n            ~f.col(\"finemappingMethod\").isin(\n                FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n            )\n        )\n        .df.withColumn(\"l\", f.explode(\"locus\"))\n        .select(\n            \"studyLocusId\",\n            \"studyId\",\n            \"chromosome\",\n            f.split(f.col(\"l.variantId\"), \"_\")[1].alias(\"tag_position\"),\n        )\n        .alias(\"study_locus\")\n        .join(\n            susie_study_regions.alias(\"regions\"),\n            how=\"inner\",\n            on=[\n                (f.col(\"study_locus.chromosome\") == f.col(\"regions.chromosome\"))\n                &amp; (f.col(\"study_locus.studyId\") == f.col(\"regions.studyId\"))\n                &amp; (f.col(\"study_locus.tag_position\") &gt;= f.col(\"regions.locusStart\"))\n                &amp; (f.col(\"study_locus.tag_position\") &lt;= f.col(\"regions.locusEnd\"))\n            ],\n        )\n        .select(\"studyLocusId\", \"inSuSiE\")\n        .distinct()\n    )\n\n    return StudyLocus(\n        _df=(\n            self.df.join(redundant_study_locus, on=\"studyLocusId\", how=\"left\")\n            .withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    # credible set in SuSiE overlapping region\n                    f.col(\"inSuSiE\")\n                    # credible set not based on SuSiE\n                    &amp; (\n                        ~f.col(\"finemappingMethod\").isin(\n                            FinemappingMethod.SUSIE.value,\n                            FinemappingMethod.SUSIE_INF.value,\n                        )\n                    ),\n                    StudyLocusQualityCheck.EXPLAINED_BY_SUSIE,\n                ),\n            )\n            .drop(\"inSuSiE\")\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.qc_redundant_top_hits_from_PICS","title":"<code>qc_redundant_top_hits_from_PICS() -&gt; StudyLocus</code>","text":"<p>Flag associations from top hits when the study contains other PICS associations from summary statistics.</p> <p>This flag can be useful to identify top hits that should be explained by other associations in the study derived from the summary statistics.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with redundant top hits flagged.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def qc_redundant_top_hits_from_PICS(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Flag associations from top hits when the study contains other PICS associations from summary statistics.\n\n    This flag can be useful to identify top hits that should be explained by other associations in the study derived from the summary statistics.\n\n    Returns:\n        StudyLocus: Updated study locus with redundant top hits flagged.\n    \"\"\"\n    studies_with_pics_sumstats = (\n        self.df.filter(f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n        # Returns True if the study contains any PICS associations from summary statistics\n        .withColumn(\n            \"hasPicsSumstats\",\n            ~f.array_contains(\n                \"qualityControls\", StudyLocusQualityCheck.TOP_HIT.value\n            ),\n        )\n        .groupBy(\"studyId\")\n        .agg(f.max(f.col(\"hasPicsSumstats\")).alias(\"studiesWithPicsSumstats\"))\n    )\n\n    return StudyLocus(\n        _df=self.df.join(studies_with_pics_sumstats, on=\"studyId\", how=\"left\")\n        .withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.array_contains(\n                    \"qualityControls\", StudyLocusQualityCheck.TOP_HIT.value\n                )\n                &amp; f.col(\"studiesWithPicsSumstats\"),\n                StudyLocusQualityCheck.REDUNDANT_PICS_TOP_HIT,\n            ),\n        )\n        .drop(\"studiesWithPicsSumstats\"),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.unique_variants_in_locus","title":"<code>unique_variants_in_locus() -&gt; DataFrame</code>","text":"<p>All unique variants collected in a <code>StudyLocus</code> dataframe.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing <code>variantId</code> and <code>chromosome</code> columns.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def unique_variants_in_locus(self: StudyLocus) -&gt; DataFrame:\n    \"\"\"All unique variants collected in a `StudyLocus` dataframe.\n\n    Returns:\n        DataFrame: A dataframe containing `variantId` and `chromosome` columns.\n    \"\"\"\n    return (\n        self.df.withColumn(\n            \"variantId\",\n            # Joint array of variants in that studylocus. Locus can be null\n            f.explode(\n                f.array_union(\n                    f.array(f.col(\"variantId\")),\n                    f.coalesce(f.col(\"locus.variantId\"), f.array()),\n                )\n            ),\n        )\n        .select(\n            \"variantId\", f.split(f.col(\"variantId\"), \"_\")[0].alias(\"chromosome\")\n        )\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_chromosome_label","title":"<code>validate_chromosome_label() -&gt; StudyLocus</code>","text":"<p>Flagging study loci, where chromosome is coded not as 1:22, X, Y, Xy and MT.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with quality control flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_chromosome_label(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Flagging study loci, where chromosome is coded not as 1:22, X, Y, Xy and MT.\n\n    Returns:\n        StudyLocus: Updated study locus with quality control flags.\n    \"\"\"\n    # QC column might not be present in the variant index schema, so we have to be ready to handle it:\n    qc_select_expression = (\n        f.col(\"qualityControls\")\n        if \"qualityControls\" in self.df.columns\n        else f.lit(None).cast(ArrayType(StringType()))\n    )\n    valid_chromosomes = [str(i) for i in range(1, 23)] + [\"X\", \"Y\", \"XY\", \"MT\"]\n\n    return StudyLocus(\n        _df=(\n            self.df.withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    qc_select_expression,\n                    ~f.col(\"chromosome\").isin(valid_chromosomes),\n                    StudyLocusQualityCheck.INVALID_CHROMOSOME,\n                ),\n            )\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_lead_pvalue","title":"<code>validate_lead_pvalue(pvalue_cutoff: float) -&gt; StudyLocus</code>","text":"<p>Flag associations below significant threshold.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue_cutoff</code> <code>float</code> <p>association p-value cut-off</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with quality control flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_lead_pvalue(self: StudyLocus, pvalue_cutoff: float) -&gt; StudyLocus:\n    \"\"\"Flag associations below significant threshold.\n\n    Args:\n        pvalue_cutoff (float): association p-value cut-off\n\n    Returns:\n        StudyLocus: Updated study locus with quality control flags.\n    \"\"\"\n    df = self.df\n    qc_colname = StudyLocus.get_QC_column_name()\n    if qc_colname not in self.df.columns:\n        df = self.df.withColumn(\n            qc_colname,\n            create_empty_column_if_not_exists(\n                qc_colname,\n                get_struct_field_schema(StudyLocus.get_schema(), qc_colname),\n            ),\n        )\n    return StudyLocus(\n        _df=(\n            df.withColumn(\n                qc_colname,\n                # Because this QC might already run on the dataset, the unique set of flags is generated:\n                f.array_distinct(\n                    self._qc_subsignificant_associations(\n                        f.col(\"qualityControls\"),\n                        f.col(\"pValueMantissa\"),\n                        f.col(\"pValueExponent\"),\n                        pvalue_cutoff,\n                    )\n                ),\n            )\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_study","title":"<code>validate_study(study_index: StudyIndex) -&gt; StudyLocus</code>","text":"<p>Flagging study loci if the corresponding study has issues.</p> <p>There are two different potential flags: - flagged study: flagging locus if the study has quality control flags. - study with summary statistics for top hit: flagging locus if the study has available summary statistics. - missing study: flagging locus if the study was not found in the reference study index.</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>Study index to resolve study types.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with quality control flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_study(self: StudyLocus, study_index: StudyIndex) -&gt; StudyLocus:\n    \"\"\"Flagging study loci if the corresponding study has issues.\n\n    There are two different potential flags:\n    - flagged study: flagging locus if the study has quality control flags.\n    - study with summary statistics for top hit: flagging locus if the study has available summary statistics.\n    - missing study: flagging locus if the study was not found in the reference study index.\n\n    Args:\n        study_index (StudyIndex): Study index to resolve study types.\n\n    Returns:\n        StudyLocus: Updated study locus with quality control flags.\n    \"\"\"\n    # Quality controls is not a mandatory field in the study index schema, so we have to be ready to handle it:\n    qc_select_expression = (\n        f.col(\"qualityControls\")\n        if \"qualityControls\" in study_index.df.columns\n        else f.lit(None).cast(StringType())\n    )\n\n    # The study Id of the study index needs to be kept, because we would not know which study was in the index after the left join:\n    study_flags = study_index.df.select(\n        f.col(\"studyId\").alias(\"study_studyId\"),\n        qc_select_expression.alias(\"study_qualityControls\"),\n    )\n\n    return StudyLocus(\n        _df=(\n            self.df.join(\n                study_flags, f.col(\"studyId\") == f.col(\"study_studyId\"), \"left\"\n            )\n            # Flagging loci with flagged studies - without propagating the actual flags:\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.size(f.col(\"study_qualityControls\")) &gt; 0,\n                    StudyLocusQualityCheck.FLAGGED_STUDY,\n                ),\n            )\n            # Flagging top-hits, where the study has available summary statistics:\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    # Condition is true, if the study has summary statistics available and the locus is a top hit:\n                    f.array_contains(\n                        f.col(\"qualityControls\"),\n                        StudyLocusQualityCheck.TOP_HIT.value,\n                    )\n                    &amp; ~f.array_contains(\n                        f.col(\"study_qualityControls\"),\n                        StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value,\n                    ),\n                    StudyLocusQualityCheck.TOP_HIT_AND_SUMMARY_STATS,\n                ),\n            )\n            # Flagging loci where no studies were found:\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.col(\"study_studyId\").isNull(),\n                    StudyLocusQualityCheck.MISSING_STUDY,\n                ),\n            )\n            .drop(\"study_studyId\", \"study_qualityControls\")\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_unique_study_locus_id","title":"<code>validate_unique_study_locus_id() -&gt; StudyLocus</code>","text":"<p>Validating the uniqueness of study-locus identifiers and flagging duplicated studyloci.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>with flagged duplicated studies.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_unique_study_locus_id(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Validating the uniqueness of study-locus identifiers and flagging duplicated studyloci.\n\n    Returns:\n        StudyLocus: with flagged duplicated studies.\n    \"\"\"\n    return StudyLocus(\n        _df=self.df.withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                self.flag_duplicates(f.col(\"studyLocusId\")),\n                StudyLocusQualityCheck.DUPLICATED_STUDYLOCUS_ID,\n            ),\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_variant_identifiers","title":"<code>validate_variant_identifiers(variant_index: VariantIndex) -&gt; StudyLocus</code>","text":"<p>Flagging study loci, where tagging variant identifiers are not found in variant index.</p> <p>Parameters:</p> Name Type Description Default <code>variant_index</code> <code>VariantIndex</code> <p>Variant index to resolve variant identifiers.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with quality control flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_variant_identifiers(\n    self: StudyLocus, variant_index: VariantIndex\n) -&gt; StudyLocus:\n    \"\"\"Flagging study loci, where tagging variant identifiers are not found in variant index.\n\n    Args:\n        variant_index (VariantIndex): Variant index to resolve variant identifiers.\n\n    Returns:\n        StudyLocus: Updated study locus with quality control flags.\n    \"\"\"\n    # QC column might not be present in the variant index schema, so we have to be ready to handle it:\n    qc_select_expression = (\n        f.col(\"qualityControls\")\n        if \"qualityControls\" in self.df.columns\n        else f.lit(None).cast(ArrayType(StringType()))\n    )\n\n    # Find out which study loci have variants not in the variant index:\n    flag = (\n        self.df\n        # Exploding locus:\n        .select(\"studyLocusId\", f.explode(\"locus\").alias(\"locus\"))\n        .select(\"studyLocusId\", \"locus.variantId\")\n        # Join with variant index variants:\n        .join(\n            variant_index.df.select(\n                \"variantId\", f.lit(True).alias(\"inVariantIndex\")\n            ),\n            on=\"variantId\",\n            how=\"left\",\n        )\n        # Flagging variants not in the variant index:\n        .withColumn(\"inVariantIndex\", f.col(\"inVariantIndex\").isNotNull())\n        # Flagging study loci with ANY variants not in the variant index:\n        .groupBy(\"studyLocusId\")\n        .agg(f.collect_set(\"inVariantIndex\").alias(\"inVariantIndex\"))\n        .select(\n            \"studyLocusId\",\n            f.array_contains(\"inVariantIndex\", False).alias(\"toFlag\"),\n        )\n    )\n\n    return StudyLocus(\n        _df=(\n            self.df.join(flag, on=\"studyLocusId\", how=\"left\")\n            .withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    qc_select_expression,\n                    f.col(\"toFlag\"),\n                    StudyLocusQualityCheck.INVALID_VARIANT_IDENTIFIER,\n                ),\n            )\n            .drop(\"toFlag\")\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.window_based_clumping","title":"<code>window_based_clumping(window_size: int = WindowBasedClumpingStepConfig().distance) -&gt; StudyLocus</code>","text":"<p>Clump study locus by window size.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Window size for clumping.</p> <code>distance</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Clumped study locus, where clumped associations are flagged.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def window_based_clumping(\n    self: StudyLocus,\n    window_size: int = WindowBasedClumpingStepConfig().distance,\n) -&gt; StudyLocus:\n    \"\"\"Clump study locus by window size.\n\n    Args:\n        window_size (int): Window size for clumping.\n\n    Returns:\n        StudyLocus: Clumped study locus, where clumped associations are flagged.\n    \"\"\"\n    from gentropy.method.window_based_clumping import WindowBasedClumping\n\n    return WindowBasedClumping.clump(self, window_size)\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.FinemappingMethod","title":"<code>gentropy.dataset.study_locus.FinemappingMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Finemapping method enum.</p> <p>Attributes:</p> Name Type Description <code>PICS</code> <code>str</code> <p>PICS</p> <code>SUSIE</code> <code>str</code> <p>SuSiE method</p> <code>SUSIE_INF</code> <code>str</code> <p>SuSiE-inf method implemented in <code>gentropy</code></p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>class FinemappingMethod(Enum):\n    \"\"\"Finemapping method enum.\n\n    Attributes:\n        PICS (str): PICS\n        SUSIE (str): SuSiE method\n        SUSIE_INF (str): SuSiE-inf method implemented in `gentropy`\n    \"\"\"\n\n    PICS = \"PICS\"\n    SUSIE = \"SuSie\"\n    SUSIE_INF = \"SuSiE-inf\"\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocusQualityCheck","title":"<code>gentropy.dataset.study_locus.StudyLocusQualityCheck</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Study-Locus quality control options listing concerns on the quality of the association.</p> <p>Attributes:</p> Name Type Description <code>SUBSIGNIFICANT_FLAG</code> <code>str</code> <p>p-value below significance threshold</p> <code>NO_GENOMIC_LOCATION_FLAG</code> <code>str</code> <p>Incomplete genomic mapping</p> <code>COMPOSITE_FLAG</code> <code>str</code> <p>Composite association due to variant x variant interactions</p> <code>INCONSISTENCY_FLAG</code> <code>str</code> <p>Inconsistencies in the reported variants</p> <code>NON_MAPPED_VARIANT_FLAG</code> <code>str</code> <p>Variant not mapped to GnomAd</p> <code>PALINDROMIC_ALLELE_FLAG</code> <code>str</code> <p>Alleles are palindromic - cannot harmonize</p> <code>AMBIGUOUS_STUDY</code> <code>str</code> <p>Association with ambiguous study</p> <code>UNRESOLVED_LD</code> <code>str</code> <p>Variant not found in LD reference</p> <code>LD_CLUMPED</code> <code>str</code> <p>Explained by a more significant variant in high LD</p> <code>WINDOW_CLUMPED</code> <code>str</code> <p>Explained by a more significant variant in the same window</p> <code>NO_POPULATION</code> <code>str</code> <p>Study does not have population annotation to resolve LD</p> <code>FLAGGED_STUDY</code> <code>str</code> <p>Study has quality control flag(s)</p> <code>MISSING_STUDY</code> <code>str</code> <p>Flagging study loci if the study is not found in the study index as a reference</p> <code>DUPLICATED_STUDYLOCUS_ID</code> <code>str</code> <p>Study-locus identifier is not unique</p> <code>INVALID_VARIANT_IDENTIFIER</code> <code>str</code> <p>Flagging study loci where identifier of any tagging variant was not found in the variant index</p> <code>TOP_HIT</code> <code>str</code> <p>Study locus from curated top hit</p> <code>IN_MHC</code> <code>str</code> <p>Flagging study loci in the MHC region</p> <code>REDUNDANT_PICS_TOP_HIT</code> <code>str</code> <p>Flagging study loci in studies with PICS results from summary statistics</p> <code>EXPLAINED_BY_SUSIE</code> <code>str</code> <p>Study locus in region explained by a SuSiE credible set</p> <code>ABNORMAL_PIPS</code> <code>str</code> <p>Flagging study loci with a sum of PIPs that are not in [0.99,1]</p> <code>OUT_OF_SAMPLE_LD</code> <code>str</code> <p>Study locus finemapped without in-sample LD reference</p> <code>INVALID_CHROMOSOME</code> <code>str</code> <p>Chromosome not in 1:22, X, Y, XY or MT</p> <code>TOP_HIT_AND_SUMMARY_STATS</code> <code>str</code> <p>Curated top hit is flagged because summary statistics are available for study</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>class StudyLocusQualityCheck(Enum):\n    \"\"\"Study-Locus quality control options listing concerns on the quality of the association.\n\n    Attributes:\n        SUBSIGNIFICANT_FLAG (str): p-value below significance threshold\n        NO_GENOMIC_LOCATION_FLAG (str): Incomplete genomic mapping\n        COMPOSITE_FLAG (str): Composite association due to variant x variant interactions\n        INCONSISTENCY_FLAG (str): Inconsistencies in the reported variants\n        NON_MAPPED_VARIANT_FLAG (str): Variant not mapped to GnomAd\n        PALINDROMIC_ALLELE_FLAG (str): Alleles are palindromic - cannot harmonize\n        AMBIGUOUS_STUDY (str): Association with ambiguous study\n        UNRESOLVED_LD (str): Variant not found in LD reference\n        LD_CLUMPED (str): Explained by a more significant variant in high LD\n        WINDOW_CLUMPED (str): Explained by a more significant variant in the same window\n        NO_POPULATION (str): Study does not have population annotation to resolve LD\n        FLAGGED_STUDY (str): Study has quality control flag(s)\n        MISSING_STUDY (str): Flagging study loci if the study is not found in the study index as a reference\n        DUPLICATED_STUDYLOCUS_ID (str): Study-locus identifier is not unique\n        INVALID_VARIANT_IDENTIFIER (str): Flagging study loci where identifier of any tagging variant was not found in the variant index\n        TOP_HIT (str): Study locus from curated top hit\n        IN_MHC (str): Flagging study loci in the MHC region\n        REDUNDANT_PICS_TOP_HIT (str): Flagging study loci in studies with PICS results from summary statistics\n        EXPLAINED_BY_SUSIE (str): Study locus in region explained by a SuSiE credible set\n        ABNORMAL_PIPS (str): Flagging study loci with a sum of PIPs that are not in [0.99,1]\n        OUT_OF_SAMPLE_LD (str): Study locus finemapped without in-sample LD reference\n        INVALID_CHROMOSOME (str): Chromosome not in 1:22, X, Y, XY or MT\n        TOP_HIT_AND_SUMMARY_STATS (str): Curated top hit is flagged because summary statistics are available for study\n    \"\"\"\n\n    SUBSIGNIFICANT_FLAG = \"Subsignificant p-value\"\n    NO_GENOMIC_LOCATION_FLAG = \"Incomplete genomic mapping\"\n    COMPOSITE_FLAG = \"Composite association\"\n    INCONSISTENCY_FLAG = \"Variant inconsistency\"\n    NON_MAPPED_VARIANT_FLAG = \"No mapping in GnomAd\"\n    PALINDROMIC_ALLELE_FLAG = \"Palindrome alleles - cannot harmonize\"\n    AMBIGUOUS_STUDY = \"Association with ambiguous study\"\n    UNRESOLVED_LD = \"Variant not found in LD reference\"\n    LD_CLUMPED = \"Explained by a more significant variant in high LD\"\n    WINDOW_CLUMPED = \"Explained by a more significant variant in the same window\"\n    NO_POPULATION = \"Study does not have population annotation to resolve LD\"\n    FLAGGED_STUDY = \"Study has quality control flag(s)\"\n    MISSING_STUDY = \"Study not found in the study index\"\n    DUPLICATED_STUDYLOCUS_ID = \"Non-unique study locus identifier\"\n    INVALID_VARIANT_IDENTIFIER = (\n        \"Some variant identifiers of this locus were not found in variant index\"\n    )\n    IN_MHC = \"MHC region\"\n    REDUNDANT_PICS_TOP_HIT = (\n        \"PICS results from summary statistics available for this same study\"\n    )\n    TOP_HIT = \"Study locus from curated top hit\"\n    EXPLAINED_BY_SUSIE = \"Study locus in region explained by a SuSiE credible set\"\n    OUT_OF_SAMPLE_LD = \"Study locus finemapped without in-sample LD reference\"\n    ABNORMAL_PIPS = (\n        \"Study locus with a sum of PIPs that not in the expected range [0.95,1]\"\n    )\n    INVALID_CHROMOSOME = \"Chromosome not in 1:22, X, Y, XY or MT\"\n    TOP_HIT_AND_SUMMARY_STATS = (\n        \"Curated top hit is flagged because summary statistics are available for study\"\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.CredibleInterval","title":"<code>gentropy.dataset.study_locus.CredibleInterval</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Credible interval enum.</p> <p>Interval within which an unobserved parameter value falls with a particular probability.</p> <p>Attributes:</p> Name Type Description <code>IS95</code> <code>str</code> <p>95% credible interval</p> <code>IS99</code> <code>str</code> <p>99% credible interval</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>class CredibleInterval(Enum):\n    \"\"\"Credible interval enum.\n\n    Interval within which an unobserved parameter value falls with a particular probability.\n\n    Attributes:\n        IS95 (str): 95% credible interval\n        IS99 (str): 99% credible interval\n    \"\"\"\n\n    IS95 = \"is95CredibleSet\"\n    IS99 = \"is99CredibleSet\"\n</code></pre>"},{"location":"python_api/datasets/study_locus/#schema","title":"Schema","text":"<pre><code>root\n |-- studyLocusId: string (nullable = false)\n |-- studyType: string (nullable = true)\n |-- variantId: string (nullable = false)\n |-- chromosome: string (nullable = true)\n |-- position: integer (nullable = true)\n |-- region: string (nullable = true)\n |-- studyId: string (nullable = false)\n |-- beta: double (nullable = true)\n |-- zScore: double (nullable = true)\n |-- pValueMantissa: float (nullable = true)\n |-- pValueExponent: integer (nullable = true)\n |-- effectAlleleFrequencyFromSource: float (nullable = true)\n |-- standardError: double (nullable = true)\n |-- subStudyDescription: string (nullable = true)\n |-- qualityControls: array (nullable = true)\n |    |-- element: string (containsNull = false)\n |-- finemappingMethod: string (nullable = true)\n |-- credibleSetIndex: integer (nullable = true)\n |-- credibleSetlog10BF: double (nullable = true)\n |-- purityMeanR2: double (nullable = true)\n |-- purityMinR2: double (nullable = true)\n |-- locusStart: integer (nullable = true)\n |-- locusEnd: integer (nullable = true)\n |-- sampleSize: integer (nullable = true)\n |-- ldSet: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- tagVariantId: string (nullable = true)\n |    |    |-- r2Overall: double (nullable = true)\n |-- locus: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- is95CredibleSet: boolean (nullable = true)\n |    |    |-- is99CredibleSet: boolean (nullable = true)\n |    |    |-- logBF: double (nullable = true)\n |    |    |-- posteriorProbability: double (nullable = true)\n |    |    |-- variantId: string (nullable = true)\n |    |    |-- pValueMantissa: float (nullable = true)\n |    |    |-- pValueExponent: integer (nullable = true)\n |    |    |-- beta: double (nullable = true)\n |    |    |-- standardError: double (nullable = true)\n |    |    |-- r2Overall: double (nullable = true)\n |-- confidence: string (nullable = true)\n |-- isTransQtl: boolean (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/","title":"Study Locus Overlap","text":""},{"location":"python_api/datasets/study_locus_overlap/#gentropy.dataset.study_locus_overlap.StudyLocusOverlap","title":"<code>gentropy.dataset.study_locus_overlap.StudyLocusOverlap</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Study-Locus overlap.</p> <p>This dataset captures pairs of overlapping <code>StudyLocus</code>: that is associations whose credible sets share at least one tagging variant.</p> <p>Note</p> <p>This is a helpful dataset for other downstream analyses, such as colocalisation. This dataset will contain the overlapping signals between studyLocus associations once they have been clumped and fine-mapped.</p> Source code in <code>src/gentropy/dataset/study_locus_overlap.py</code> <pre><code>@dataclass\nclass StudyLocusOverlap(Dataset):\n    \"\"\"Study-Locus overlap.\n\n    This dataset captures pairs of overlapping `StudyLocus`: that is associations whose credible sets share at least one tagging variant.\n\n    !!! note\n\n        This is a helpful dataset for other downstream analyses, such as colocalisation. This dataset will contain the overlapping signals between studyLocus associations once they have been clumped and fine-mapped.\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[StudyLocusOverlap]) -&gt; StructType:\n        \"\"\"Provides the schema for the StudyLocusOverlap dataset.\n\n        Returns:\n            StructType: Schema for the StudyLocusOverlap dataset\n        \"\"\"\n        return parse_spark_schema(\"study_locus_overlap.json\")\n\n    @classmethod\n    def from_associations(\n        cls: type[StudyLocusOverlap], study_locus: StudyLocus\n    ) -&gt; StudyLocusOverlap:\n        \"\"\"Find the overlapping signals in a particular set of associations (StudyLocus dataset).\n\n        Args:\n            study_locus (StudyLocus): Study-locus associations to find the overlapping signals\n\n        Returns:\n            StudyLocusOverlap: Study-locus overlap dataset\n        \"\"\"\n        return study_locus.find_overlaps()\n\n\n    def calculate_beta_ratio(self: StudyLocusOverlap) -&gt; DataFrame:\n        \"\"\"Calculate the beta ratio for the overlapping signals.\n\n        Returns:\n            DataFrame: A dataframe containing left and right loci IDs, chromosome\n            and the average sign of the beta ratio\n        \"\"\"\n        return (\n            # Unpack statistics column:\n            self.df.select(\"*\", \"statistics.*\")\n            .drop(\"statistics\")\n            # Drop any rows where the beta is null or zero\n            .filter(\n                f.col(\"left_beta\").isNotNull() &amp;\n                f.col(\"right_beta\").isNotNull() &amp;\n                (f.col(\"left_beta\") != 0) &amp;\n                (f.col(\"right_beta\") != 0)\n            )\n            # Calculate the beta ratio and get the sign, then calculate the average sign across all variants in the locus\n            .withColumn(\n                \"betaRatioSign\",\n                f.signum(f.col(\"left_beta\") / f.col(\"right_beta\"))\n            )\n            # Aggregate beta signs:\n            .groupBy(\"leftStudyLocusId\",\"rightStudyLocusId\",\"chromosome\")\n            .agg(\n                f.avg(\"betaRatioSign\").alias(\"betaRatioSignAverage\")\n            )\n        )\n\n    def _convert_to_square_matrix(self: StudyLocusOverlap) -&gt; StudyLocusOverlap:\n        \"\"\"Convert the dataset to a square matrix.\n\n        Returns:\n            StudyLocusOverlap: Square matrix of the dataset\n        \"\"\"\n        return StudyLocusOverlap(\n            _df=self.df.unionByName(\n                self.df.selectExpr(\n                    \"leftStudyLocusId as rightStudyLocusId\",\n                    \"rightStudyLocusId as leftStudyLocusId\",\n                    \"rightStudyType\",\n                    \"tagVariantId\",\n                    \"chromosome\",\n                    \"statistics\",\n                )\n            ).distinct(),\n            _schema=self.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/#gentropy.dataset.study_locus_overlap.StudyLocusOverlap.calculate_beta_ratio","title":"<code>calculate_beta_ratio() -&gt; DataFrame</code>","text":"<p>Calculate the beta ratio for the overlapping signals.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing left and right loci IDs, chromosome</p> <code>DataFrame</code> <p>and the average sign of the beta ratio</p> Source code in <code>src/gentropy/dataset/study_locus_overlap.py</code> <pre><code>def calculate_beta_ratio(self: StudyLocusOverlap) -&gt; DataFrame:\n    \"\"\"Calculate the beta ratio for the overlapping signals.\n\n    Returns:\n        DataFrame: A dataframe containing left and right loci IDs, chromosome\n        and the average sign of the beta ratio\n    \"\"\"\n    return (\n        # Unpack statistics column:\n        self.df.select(\"*\", \"statistics.*\")\n        .drop(\"statistics\")\n        # Drop any rows where the beta is null or zero\n        .filter(\n            f.col(\"left_beta\").isNotNull() &amp;\n            f.col(\"right_beta\").isNotNull() &amp;\n            (f.col(\"left_beta\") != 0) &amp;\n            (f.col(\"right_beta\") != 0)\n        )\n        # Calculate the beta ratio and get the sign, then calculate the average sign across all variants in the locus\n        .withColumn(\n            \"betaRatioSign\",\n            f.signum(f.col(\"left_beta\") / f.col(\"right_beta\"))\n        )\n        # Aggregate beta signs:\n        .groupBy(\"leftStudyLocusId\",\"rightStudyLocusId\",\"chromosome\")\n        .agg(\n            f.avg(\"betaRatioSign\").alias(\"betaRatioSignAverage\")\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/#gentropy.dataset.study_locus_overlap.StudyLocusOverlap.from_associations","title":"<code>from_associations(study_locus: StudyLocus) -&gt; StudyLocusOverlap</code>  <code>classmethod</code>","text":"<p>Find the overlapping signals in a particular set of associations (StudyLocus dataset).</p> <p>Parameters:</p> Name Type Description Default <code>study_locus</code> <code>StudyLocus</code> <p>Study-locus associations to find the overlapping signals</p> required <p>Returns:</p> Name Type Description <code>StudyLocusOverlap</code> <code>StudyLocusOverlap</code> <p>Study-locus overlap dataset</p> Source code in <code>src/gentropy/dataset/study_locus_overlap.py</code> <pre><code>@classmethod\ndef from_associations(\n    cls: type[StudyLocusOverlap], study_locus: StudyLocus\n) -&gt; StudyLocusOverlap:\n    \"\"\"Find the overlapping signals in a particular set of associations (StudyLocus dataset).\n\n    Args:\n        study_locus (StudyLocus): Study-locus associations to find the overlapping signals\n\n    Returns:\n        StudyLocusOverlap: Study-locus overlap dataset\n    \"\"\"\n    return study_locus.find_overlaps()\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/#gentropy.dataset.study_locus_overlap.StudyLocusOverlap.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the StudyLocusOverlap dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the StudyLocusOverlap dataset</p> Source code in <code>src/gentropy/dataset/study_locus_overlap.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[StudyLocusOverlap]) -&gt; StructType:\n    \"\"\"Provides the schema for the StudyLocusOverlap dataset.\n\n    Returns:\n        StructType: Schema for the StudyLocusOverlap dataset\n    \"\"\"\n    return parse_spark_schema(\"study_locus_overlap.json\")\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/#schema","title":"Schema","text":"<pre><code>root\n |-- leftStudyLocusId: string (nullable = false)\n |-- rightStudyLocusId: string (nullable = false)\n |-- rightStudyType: string (nullable = false)\n |-- chromosome: string (nullable = true)\n |-- tagVariantId: string (nullable = false)\n |-- statistics: struct (nullable = true)\n |    |-- left_pValueMantissa: float (nullable = true)\n |    |-- left_pValueExponent: integer (nullable = true)\n |    |-- right_pValueMantissa: float (nullable = true)\n |    |-- right_pValueExponent: integer (nullable = true)\n |    |-- left_beta: double (nullable = true)\n |    |-- right_beta: double (nullable = true)\n |    |-- left_logBF: double (nullable = true)\n |    |-- right_logBF: double (nullable = true)\n |    |-- left_posteriorProbability: double (nullable = true)\n |    |-- right_posteriorProbability: double (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/","title":"Summary Statistics","text":""},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics","title":"<code>gentropy.dataset.summary_statistics.SummaryStatistics</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Summary Statistics dataset.</p> <p>A summary statistics dataset contains all single point statistics resulting from a GWAS.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>@dataclass\nclass SummaryStatistics(Dataset):\n    \"\"\"Summary Statistics dataset.\n\n    A summary statistics dataset contains all single point statistics resulting from a GWAS.\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[SummaryStatistics]) -&gt; StructType:\n        \"\"\"Provides the schema for the SummaryStatistics dataset.\n\n        Returns:\n            StructType: Schema for the SummaryStatistics dataset\n        \"\"\"\n        return parse_spark_schema(\"summary_statistics.json\")\n\n    def pvalue_filter(self: SummaryStatistics, pvalue: float) -&gt; SummaryStatistics:\n        \"\"\"Filter summary statistics based on the provided p-value threshold.\n\n        Args:\n            pvalue (float): upper limit of the p-value to be filtered upon.\n\n        Returns:\n            SummaryStatistics: summary statistics object containing single point associations with p-values at least as significant as the provided threshold.\n        \"\"\"\n        # Converting p-value to mantissa and exponent:\n        (mantissa, exponent) = split_pvalue(pvalue)\n\n        # Applying filter:\n        df = self._df.filter(\n            (f.col(\"pValueExponent\") &lt; exponent)\n            | (\n                (f.col(\"pValueExponent\") == exponent)\n                &amp; (f.col(\"pValueMantissa\") &lt;= mantissa)\n            )\n        )\n        return SummaryStatistics(_df=df, _schema=self._schema)\n\n    def window_based_clumping(\n        self: SummaryStatistics,\n        distance: int = WindowBasedClumpingStepConfig().distance,\n        gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance,\n    ) -&gt; StudyLocus:\n        \"\"\"Generate study-locus from summary statistics using window-based clumping.\n\n        For more info, see [`WindowBasedClumping`][gentropy.method.window_based_clumping.WindowBasedClumping]\n\n        Args:\n            distance (int): Distance in base pairs to be used for clumping. Defaults to 500_000.\n            gwas_significance (float, optional): GWAS significance threshold. Defaults to 5e-8.\n\n        Returns:\n            StudyLocus: Clumped study-locus optionally containing variants based on window.\n            Check WindowBasedClumpingStepConfig object for default values.\n        \"\"\"\n        from gentropy.method.window_based_clumping import WindowBasedClumping\n\n        return WindowBasedClumping.clump(\n            # Before clumping, we filter the summary statistics by p-value:\n            self.pvalue_filter(gwas_significance),\n            distance=distance,\n            # After applying the clumping, we filter the clumped loci by the flag:\n        ).valid_rows([\"WINDOW_CLUMPED\"])\n\n    def locus_breaker_clumping(\n        self: SummaryStatistics,\n        baseline_pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_baseline_pvalue,\n        distance_cutoff: int = LocusBreakerClumpingConfig.lbc_distance_cutoff,\n        pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_pvalue_threshold,\n        flanking_distance: int = LocusBreakerClumpingConfig.lbc_flanking_distance,\n    ) -&gt; StudyLocus:\n        \"\"\"Generate study-locus from summary statistics using locus-breaker clumping method with locus boundaries.\n\n        For more info, see [`locus_breaker`][gentropy.method.locus_breaker_clumping.LocusBreakerClumping]\n\n        Args:\n            baseline_pvalue_cutoff (float, optional): Baseline significance we consider for the locus.\n            distance_cutoff (int, optional): Distance in base pairs to be used for clumping.\n            pvalue_cutoff (float, optional): GWAS significance threshold.\n            flanking_distance (int, optional): Flank distance in base pairs to be used for clumping.\n\n        Returns:\n            StudyLocus: Clumped study-locus optionally containing variants based on window.\n            Check LocusBreakerClumpingConfig object for default values.\n        \"\"\"\n        from gentropy.method.locus_breaker_clumping import LocusBreakerClumping\n\n        return LocusBreakerClumping.locus_breaker(\n            self,\n            baseline_pvalue_cutoff,\n            distance_cutoff,\n            pvalue_cutoff,\n            flanking_distance,\n        )\n\n    def exclude_region(\n        self: SummaryStatistics, region: GenomicRegion\n    ) -&gt; SummaryStatistics:\n        \"\"\"Exclude a region from the summary stats dataset.\n\n        Args:\n            region (GenomicRegion): Genomic region to be excluded.\n\n        Returns:\n            SummaryStatistics: filtered summary statistics.\n        \"\"\"\n        return SummaryStatistics(\n            _df=(\n                self.df.filter(\n                    ~(\n                        (f.col(\"chromosome\") == region.chromosome)\n                        &amp; (\n                            (f.col(\"position\") &gt;= region.start)\n                            &amp; (f.col(\"position\") &lt;= region.end)\n                        )\n                    )\n                )\n            ),\n            _schema=SummaryStatistics.get_schema(),\n        )\n\n    def sanity_filter(self: SummaryStatistics) -&gt; SummaryStatistics:\n        \"\"\"The function filters the summary statistics by sanity filters.\n\n        The function filters the summary statistics by the following filters:\n            - The p-value should be less than 1.\n            - The pValueMantissa should be greater than 0.\n            - The beta should not be equal 0.\n            - The p-value, beta and se should not be NaN.\n            - The se should be positive.\n            - The beta and se should not be infinite.\n\n        Returns:\n            SummaryStatistics: The filtered summary statistics.\n        \"\"\"\n        gwas_df = self._df\n        gwas_df = gwas_df.dropna(\n            subset=[\"beta\", \"standardError\", \"pValueMantissa\", \"pValueExponent\"]\n        )\n        gwas_df = gwas_df.filter((f.col(\"beta\") != 0) &amp; (f.col(\"standardError\") &gt; 0))\n        gwas_df = gwas_df.filter(\n            (f.col(\"pValueMantissa\") * 10 ** f.col(\"pValueExponent\") &lt; 1)\n            &amp; (f.col(\"pValueMantissa\") &gt; 0)\n        )\n        cols = [\"beta\", \"standardError\"]\n        summary_stats = SummaryStatistics(\n            _df=gwas_df,\n            _schema=SummaryStatistics.get_schema(),\n        ).drop_infinity_values(*cols)\n\n        return summary_stats\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.exclude_region","title":"<code>exclude_region(region: GenomicRegion) -&gt; SummaryStatistics</code>","text":"<p>Exclude a region from the summary stats dataset.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>GenomicRegion</code> <p>Genomic region to be excluded.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>filtered summary statistics.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def exclude_region(\n    self: SummaryStatistics, region: GenomicRegion\n) -&gt; SummaryStatistics:\n    \"\"\"Exclude a region from the summary stats dataset.\n\n    Args:\n        region (GenomicRegion): Genomic region to be excluded.\n\n    Returns:\n        SummaryStatistics: filtered summary statistics.\n    \"\"\"\n    return SummaryStatistics(\n        _df=(\n            self.df.filter(\n                ~(\n                    (f.col(\"chromosome\") == region.chromosome)\n                    &amp; (\n                        (f.col(\"position\") &gt;= region.start)\n                        &amp; (f.col(\"position\") &lt;= region.end)\n                    )\n                )\n            )\n        ),\n        _schema=SummaryStatistics.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the SummaryStatistics dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the SummaryStatistics dataset</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[SummaryStatistics]) -&gt; StructType:\n    \"\"\"Provides the schema for the SummaryStatistics dataset.\n\n    Returns:\n        StructType: Schema for the SummaryStatistics dataset\n    \"\"\"\n    return parse_spark_schema(\"summary_statistics.json\")\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.locus_breaker_clumping","title":"<code>locus_breaker_clumping(baseline_pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_baseline_pvalue, distance_cutoff: int = LocusBreakerClumpingConfig.lbc_distance_cutoff, pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_pvalue_threshold, flanking_distance: int = LocusBreakerClumpingConfig.lbc_flanking_distance) -&gt; StudyLocus</code>","text":"<p>Generate study-locus from summary statistics using locus-breaker clumping method with locus boundaries.</p> <p>For more info, see <code>locus_breaker</code></p> <p>Parameters:</p> Name Type Description Default <code>baseline_pvalue_cutoff</code> <code>float</code> <p>Baseline significance we consider for the locus.</p> <code>lbc_baseline_pvalue</code> <code>distance_cutoff</code> <code>int</code> <p>Distance in base pairs to be used for clumping.</p> <code>lbc_distance_cutoff</code> <code>pvalue_cutoff</code> <code>float</code> <p>GWAS significance threshold.</p> <code>lbc_pvalue_threshold</code> <code>flanking_distance</code> <code>int</code> <p>Flank distance in base pairs to be used for clumping.</p> <code>lbc_flanking_distance</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Clumped study-locus optionally containing variants based on window.</p> <code>StudyLocus</code> <p>Check LocusBreakerClumpingConfig object for default values.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def locus_breaker_clumping(\n    self: SummaryStatistics,\n    baseline_pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_baseline_pvalue,\n    distance_cutoff: int = LocusBreakerClumpingConfig.lbc_distance_cutoff,\n    pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_pvalue_threshold,\n    flanking_distance: int = LocusBreakerClumpingConfig.lbc_flanking_distance,\n) -&gt; StudyLocus:\n    \"\"\"Generate study-locus from summary statistics using locus-breaker clumping method with locus boundaries.\n\n    For more info, see [`locus_breaker`][gentropy.method.locus_breaker_clumping.LocusBreakerClumping]\n\n    Args:\n        baseline_pvalue_cutoff (float, optional): Baseline significance we consider for the locus.\n        distance_cutoff (int, optional): Distance in base pairs to be used for clumping.\n        pvalue_cutoff (float, optional): GWAS significance threshold.\n        flanking_distance (int, optional): Flank distance in base pairs to be used for clumping.\n\n    Returns:\n        StudyLocus: Clumped study-locus optionally containing variants based on window.\n        Check LocusBreakerClumpingConfig object for default values.\n    \"\"\"\n    from gentropy.method.locus_breaker_clumping import LocusBreakerClumping\n\n    return LocusBreakerClumping.locus_breaker(\n        self,\n        baseline_pvalue_cutoff,\n        distance_cutoff,\n        pvalue_cutoff,\n        flanking_distance,\n    )\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.pvalue_filter","title":"<code>pvalue_filter(pvalue: float) -&gt; SummaryStatistics</code>","text":"<p>Filter summary statistics based on the provided p-value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue</code> <code>float</code> <p>upper limit of the p-value to be filtered upon.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>summary statistics object containing single point associations with p-values at least as significant as the provided threshold.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def pvalue_filter(self: SummaryStatistics, pvalue: float) -&gt; SummaryStatistics:\n    \"\"\"Filter summary statistics based on the provided p-value threshold.\n\n    Args:\n        pvalue (float): upper limit of the p-value to be filtered upon.\n\n    Returns:\n        SummaryStatistics: summary statistics object containing single point associations with p-values at least as significant as the provided threshold.\n    \"\"\"\n    # Converting p-value to mantissa and exponent:\n    (mantissa, exponent) = split_pvalue(pvalue)\n\n    # Applying filter:\n    df = self._df.filter(\n        (f.col(\"pValueExponent\") &lt; exponent)\n        | (\n            (f.col(\"pValueExponent\") == exponent)\n            &amp; (f.col(\"pValueMantissa\") &lt;= mantissa)\n        )\n    )\n    return SummaryStatistics(_df=df, _schema=self._schema)\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.sanity_filter","title":"<code>sanity_filter() -&gt; SummaryStatistics</code>","text":"<p>The function filters the summary statistics by sanity filters.</p> The function filters the summary statistics by the following filters <ul> <li>The p-value should be less than 1.</li> <li>The pValueMantissa should be greater than 0.</li> <li>The beta should not be equal 0.</li> <li>The p-value, beta and se should not be NaN.</li> <li>The se should be positive.</li> <li>The beta and se should not be infinite.</li> </ul> <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>The filtered summary statistics.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def sanity_filter(self: SummaryStatistics) -&gt; SummaryStatistics:\n    \"\"\"The function filters the summary statistics by sanity filters.\n\n    The function filters the summary statistics by the following filters:\n        - The p-value should be less than 1.\n        - The pValueMantissa should be greater than 0.\n        - The beta should not be equal 0.\n        - The p-value, beta and se should not be NaN.\n        - The se should be positive.\n        - The beta and se should not be infinite.\n\n    Returns:\n        SummaryStatistics: The filtered summary statistics.\n    \"\"\"\n    gwas_df = self._df\n    gwas_df = gwas_df.dropna(\n        subset=[\"beta\", \"standardError\", \"pValueMantissa\", \"pValueExponent\"]\n    )\n    gwas_df = gwas_df.filter((f.col(\"beta\") != 0) &amp; (f.col(\"standardError\") &gt; 0))\n    gwas_df = gwas_df.filter(\n        (f.col(\"pValueMantissa\") * 10 ** f.col(\"pValueExponent\") &lt; 1)\n        &amp; (f.col(\"pValueMantissa\") &gt; 0)\n    )\n    cols = [\"beta\", \"standardError\"]\n    summary_stats = SummaryStatistics(\n        _df=gwas_df,\n        _schema=SummaryStatistics.get_schema(),\n    ).drop_infinity_values(*cols)\n\n    return summary_stats\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.window_based_clumping","title":"<code>window_based_clumping(distance: int = WindowBasedClumpingStepConfig().distance, gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance) -&gt; StudyLocus</code>","text":"<p>Generate study-locus from summary statistics using window-based clumping.</p> <p>For more info, see <code>WindowBasedClumping</code></p> <p>Parameters:</p> Name Type Description Default <code>distance</code> <code>int</code> <p>Distance in base pairs to be used for clumping. Defaults to 500_000.</p> <code>distance</code> <code>gwas_significance</code> <code>float</code> <p>GWAS significance threshold. Defaults to 5e-8.</p> <code>gwas_significance</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Clumped study-locus optionally containing variants based on window.</p> <code>StudyLocus</code> <p>Check WindowBasedClumpingStepConfig object for default values.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def window_based_clumping(\n    self: SummaryStatistics,\n    distance: int = WindowBasedClumpingStepConfig().distance,\n    gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance,\n) -&gt; StudyLocus:\n    \"\"\"Generate study-locus from summary statistics using window-based clumping.\n\n    For more info, see [`WindowBasedClumping`][gentropy.method.window_based_clumping.WindowBasedClumping]\n\n    Args:\n        distance (int): Distance in base pairs to be used for clumping. Defaults to 500_000.\n        gwas_significance (float, optional): GWAS significance threshold. Defaults to 5e-8.\n\n    Returns:\n        StudyLocus: Clumped study-locus optionally containing variants based on window.\n        Check WindowBasedClumpingStepConfig object for default values.\n    \"\"\"\n    from gentropy.method.window_based_clumping import WindowBasedClumping\n\n    return WindowBasedClumping.clump(\n        # Before clumping, we filter the summary statistics by p-value:\n        self.pvalue_filter(gwas_significance),\n        distance=distance,\n        # After applying the clumping, we filter the clumped loci by the flag:\n    ).valid_rows([\"WINDOW_CLUMPED\"])\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#schema","title":"Schema","text":"<pre><code>root\n |-- studyId: string (nullable = false)\n |-- variantId: string (nullable = false)\n |-- chromosome: string (nullable = false)\n |-- position: integer (nullable = false)\n |-- beta: double (nullable = false)\n |-- sampleSize: integer (nullable = true)\n |-- pValueMantissa: float (nullable = false)\n |-- pValueExponent: integer (nullable = false)\n |-- effectAlleleFrequencyFromSource: float (nullable = true)\n |-- standardError: double (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/target_index/","title":"Target Index","text":""},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex","title":"<code>gentropy.dataset.target_index.TargetIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Target index dataset.</p> <p>Gene-based annotation.</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>@dataclass\nclass TargetIndex(Dataset):\n    \"\"\"Target index dataset.\n\n    Gene-based annotation.\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[TargetIndex]) -&gt; StructType:\n        \"\"\"Provides the schema for the TargetIndex dataset.\n\n        Returns:\n            StructType: Schema for the TargetIndex dataset\n        \"\"\"\n        return parse_spark_schema(\"target_index.json\")\n\n    def filter_by_biotypes(self: TargetIndex, biotypes: list[str]) -&gt; TargetIndex:\n        \"\"\"Filter by approved biotypes.\n\n        Args:\n            biotypes (list[str]): List of Ensembl biotypes to keep.\n\n        Returns:\n            TargetIndex: Target index dataset filtered by biotypes.\n        \"\"\"\n        self.df = self._df.filter(f.col(\"biotype\").isin(biotypes))\n        return self\n\n    def locations_lut(self: TargetIndex) -&gt; DataFrame:\n        \"\"\"Gene location information.\n\n        Returns:\n            DataFrame: Gene LUT including genomic location information.\n        \"\"\"\n        return self.df.select(\n            f.col(\"id\").alias(\"geneId\"),\n            f.col(\"genomicLocation.chromosome\").alias(\"chromosome\"),\n            f.col(\"genomicLocation.start\").alias(\"start\"),\n            f.col(\"genomicLocation.end\").alias(\"end\"),\n            f.col(\"genomicLocation.strand\").alias(\"strand\"),\n            \"tss\",\n        )\n\n    def symbols_lut(self: TargetIndex) -&gt; DataFrame:\n        \"\"\"Gene symbol lookup table.\n\n        Pre-processess gene/target dataset to create lookup table of gene symbols, including\n        obsoleted gene symbols.\n\n        Returns:\n            DataFrame: Gene LUT for symbol mapping containing `geneId` and `geneSymbol` columns.\n        \"\"\"\n        return self.df.select(\n            f.explode(\n                f.array_union(f.array(\"approvedSymbol\"), f.col(\"obsoleteSymbols.label\"))\n            ).alias(\"geneSymbol\"),\n            f.col(\"id\").alias(\"geneId\"),\n            f.col(\"genomicLocation.chromosome\").alias(\"chromosome\"),\n            \"tss\",\n        )\n</code></pre>"},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex.filter_by_biotypes","title":"<code>filter_by_biotypes(biotypes: list[str]) -&gt; TargetIndex</code>","text":"<p>Filter by approved biotypes.</p> <p>Parameters:</p> Name Type Description Default <code>biotypes</code> <code>list[str]</code> <p>List of Ensembl biotypes to keep.</p> required <p>Returns:</p> Name Type Description <code>TargetIndex</code> <code>TargetIndex</code> <p>Target index dataset filtered by biotypes.</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>def filter_by_biotypes(self: TargetIndex, biotypes: list[str]) -&gt; TargetIndex:\n    \"\"\"Filter by approved biotypes.\n\n    Args:\n        biotypes (list[str]): List of Ensembl biotypes to keep.\n\n    Returns:\n        TargetIndex: Target index dataset filtered by biotypes.\n    \"\"\"\n    self.df = self._df.filter(f.col(\"biotype\").isin(biotypes))\n    return self\n</code></pre>"},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the TargetIndex dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the TargetIndex dataset</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[TargetIndex]) -&gt; StructType:\n    \"\"\"Provides the schema for the TargetIndex dataset.\n\n    Returns:\n        StructType: Schema for the TargetIndex dataset\n    \"\"\"\n    return parse_spark_schema(\"target_index.json\")\n</code></pre>"},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex.locations_lut","title":"<code>locations_lut() -&gt; DataFrame</code>","text":"<p>Gene location information.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Gene LUT including genomic location information.</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>def locations_lut(self: TargetIndex) -&gt; DataFrame:\n    \"\"\"Gene location information.\n\n    Returns:\n        DataFrame: Gene LUT including genomic location information.\n    \"\"\"\n    return self.df.select(\n        f.col(\"id\").alias(\"geneId\"),\n        f.col(\"genomicLocation.chromosome\").alias(\"chromosome\"),\n        f.col(\"genomicLocation.start\").alias(\"start\"),\n        f.col(\"genomicLocation.end\").alias(\"end\"),\n        f.col(\"genomicLocation.strand\").alias(\"strand\"),\n        \"tss\",\n    )\n</code></pre>"},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex.symbols_lut","title":"<code>symbols_lut() -&gt; DataFrame</code>","text":"<p>Gene symbol lookup table.</p> <p>Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Gene LUT for symbol mapping containing <code>geneId</code> and <code>geneSymbol</code> columns.</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>def symbols_lut(self: TargetIndex) -&gt; DataFrame:\n    \"\"\"Gene symbol lookup table.\n\n    Pre-processess gene/target dataset to create lookup table of gene symbols, including\n    obsoleted gene symbols.\n\n    Returns:\n        DataFrame: Gene LUT for symbol mapping containing `geneId` and `geneSymbol` columns.\n    \"\"\"\n    return self.df.select(\n        f.explode(\n            f.array_union(f.array(\"approvedSymbol\"), f.col(\"obsoleteSymbols.label\"))\n        ).alias(\"geneSymbol\"),\n        f.col(\"id\").alias(\"geneId\"),\n        f.col(\"genomicLocation.chromosome\").alias(\"chromosome\"),\n        \"tss\",\n    )\n</code></pre>"},{"location":"python_api/datasets/target_index/#schema","title":"Schema","text":"<pre><code>root\n |-- id: string (nullable = false)\n |-- approvedSymbol: string (nullable = true)\n |-- biotype: string (nullable = true)\n |-- transcriptIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- canonicalTranscript: struct (nullable = true)\n |    |-- id: string (nullable = true)\n |    |-- chromosome: string (nullable = true)\n |    |-- start: long (nullable = true)\n |    |-- end: long (nullable = true)\n |    |-- strand: string (nullable = true)\n |-- canonicalExons: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- genomicLocation: struct (nullable = true)\n |    |-- chromosome: string (nullable = true)\n |    |-- start: long (nullable = true)\n |    |-- end: long (nullable = true)\n |    |-- strand: integer (nullable = true)\n |-- alternativeGenes: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- approvedName: string (nullable = true)\n |-- go: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |    |    |-- evidence: string (nullable = true)\n |    |    |-- aspect: string (nullable = true)\n |    |    |-- geneProduct: string (nullable = true)\n |    |    |-- ecoId: string (nullable = true)\n |-- hallmarks: struct (nullable = true)\n |    |-- attributes: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- pmid: long (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- attribute_name: string (nullable = true)\n |    |-- cancerHallmarks: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- pmid: long (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- impact: string (nullable = true)\n |    |    |    |-- label: string (nullable = true)\n |-- synonyms: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- symbolSynonyms: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- nameSynonyms: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- functionDescriptions: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- subcellularLocations: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- location: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |    |    |-- termSL: string (nullable = true)\n |    |    |-- labelSL: string (nullable = true)\n |-- targetClass: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- level: string (nullable = true)\n |-- obsoleteSymbols: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- obsoleteNames: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- constraint: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- constraintType: string (nullable = true)\n |    |    |-- score: float (nullable = true)\n |    |    |-- exp: float (nullable = true)\n |    |    |-- obs: integer (nullable = true)\n |    |    |-- oe: float (nullable = true)\n |    |    |-- oeLower: float (nullable = true)\n |    |    |-- oeUpper: float (nullable = true)\n |    |    |-- upperRank: integer (nullable = true)\n |    |    |-- upperBin: integer (nullable = true)\n |    |    |-- upperBin6: integer (nullable = true)\n |-- tep: struct (nullable = true)\n |    |-- targetFromSourceId: string (nullable = true)\n |    |-- description: string (nullable = true)\n |    |-- therapeuticArea: string (nullable = true)\n |    |-- url: string (nullable = true)\n |-- proteinIds: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- dbXrefs: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- chemicalProbes: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- control: string (nullable = true)\n |    |    |-- drugId: string (nullable = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- isHighQuality: boolean (nullable = true)\n |    |    |-- mechanismOfAction: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- origin: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- probeMinerScore: long (nullable = true)\n |    |    |-- probesDrugsScore: long (nullable = true)\n |    |    |-- scoreInCells: long (nullable = true)\n |    |    |-- scoreInOrganisms: long (nullable = true)\n |    |    |-- targetFromSourceId: string (nullable = true)\n |    |    |-- urls: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- niceName: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |-- homologues: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- speciesId: string (nullable = true)\n |    |    |-- speciesName: string (nullable = true)\n |    |    |-- homologyType: string (nullable = true)\n |    |    |-- targetGeneId: string (nullable = true)\n |    |    |-- isHighConfidence: string (nullable = true)\n |    |    |-- targetGeneSymbol: string (nullable = true)\n |    |    |-- queryPercentageIdentity: double (nullable = true)\n |    |    |-- targetPercentageIdentity: double (nullable = true)\n |    |    |-- priority: integer (nullable = true)\n |-- tractability: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- modality: string (nullable = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- value: boolean (nullable = true)\n |-- safetyLiabilities: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- event: string (nullable = true)\n |    |    |-- eventId: string (nullable = true)\n |    |    |-- effects: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- direction: string (nullable = true)\n |    |    |    |    |-- dosing: string (nullable = true)\n |    |    |-- biosamples: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- cellFormat: string (nullable = true)\n |    |    |    |    |-- cellLabel: string (nullable = true)\n |    |    |    |    |-- tissueId: string (nullable = true)\n |    |    |    |    |-- tissueLabel: string (nullable = true)\n |    |    |-- datasource: string (nullable = true)\n |    |    |-- literature: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- studies: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |-- pathways: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- pathwayId: string (nullable = true)\n |    |    |-- pathway: string (nullable = true)\n |    |    |-- topLevelTerm: string (nullable = true)\n |-- tss: long (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/variant_index/","title":"Variant index","text":""},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex","title":"<code>gentropy.dataset.variant_index.VariantIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for representing variants and methods applied on them.</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>@dataclass\nclass VariantIndex(Dataset):\n    \"\"\"Dataset for representing variants and methods applied on them.\"\"\"\n\n    id_threshold: int = field(default=300)\n\n    def __post_init__(self: VariantIndex) -&gt; None:\n        \"\"\"Forcing the presence of empty arrays even if the schema allows missing values.\n\n        To bring in annotations from other sources, we use the `array_union()` function. However it assumes\n        both columns have arrays (not just the array schema!). If one of the array is null, the union\n        is nullified. This needs to be avoided.\n        \"\"\"\n        # Calling dataset's post init to validate schema:\n        super().__post_init__()\n\n        # Composing a list of expressions to replace nulls with empty arrays if the schema assumes:\n        array_columns = {\n            column.name: f.when(f.col(column.name).isNull(), f.array()).otherwise(\n                f.col(column.name)\n            )\n            for column in self.df.schema\n            if \"ArrayType\" in column.dataType.__str__()\n        }\n\n        # Not returning, but changing the data:\n        self.df = self.df.withColumns(array_columns).withColumn(\n            # Hashing long variant identifiers:\n            \"variantId\",\n            self.hash_long_variant_ids(\n                f.col(\"variantId\"),\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                self.id_threshold,\n            ),\n        )\n\n    @classmethod\n    def get_schema(cls: type[VariantIndex]) -&gt; StructType:\n        \"\"\"Provides the schema for the variant index dataset.\n\n        Returns:\n            StructType: Schema for the VariantIndex dataset\n        \"\"\"\n        return parse_spark_schema(\"variant_index.json\")\n\n    @staticmethod\n    def hash_long_variant_ids(\n        variant_id: Column, chromosome: Column, position: Column, threshold: int\n    ) -&gt; Column:\n        \"\"\"Hash long variant identifiers.\n\n        Args:\n            variant_id (Column): Column containing variant identifiers.\n            chromosome (Column): Chromosome column.\n            position (Column): position column.\n            threshold (int): Above this limit, a hash will be generated.\n\n        Returns:\n            Column: Hashed variant identifiers for long variants.\n\n        Examples:\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame([('v_short', 'x', 23),('v_looooooong', '23', 23), ('no_chrom', None, None), (None, None, None)], ['variantId', 'chromosome', 'position'])\n            ...    .select('variantId', VariantIndex.hash_long_variant_ids(f.col('variantId'), f.col('chromosome'), f.col('position'), 10).alias('hashedVariantId'))\n            ...    .show(truncate=False)\n            ... )\n            +------------+--------------------------------------------+\n            |variantId   |hashedVariantId                             |\n            +------------+--------------------------------------------+\n            |v_short     |v_short                                     |\n            |v_looooooong|OTVAR_23_23_3749d019d645894770c364992ae70a05|\n            |no_chrom    |OTVAR_41acfcd7d4fd523b33600b504914ef25      |\n            |NULL        |NULL                                        |\n            +------------+--------------------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return (\n            # If either the position or the chromosome is missing, we hash the identifier:\n            f.when(\n                chromosome.isNull() | position.isNull(),\n                f.concat(\n                    f.lit(\"OTVAR_\"),\n                    f.md5(variant_id).cast(\"string\"),\n                ),\n            )\n            # If chromosome and position are given, but alleles are too long, create hash:\n            .when(\n                f.length(variant_id) &gt;= threshold,\n                f.concat_ws(\n                    \"_\",\n                    f.lit(\"OTVAR\"),\n                    chromosome,\n                    position,\n                    f.md5(variant_id).cast(\"string\"),\n                ),\n            )\n            # Missing and regular variant identifiers are left unchanged:\n            .otherwise(variant_id)\n        )\n\n    def add_annotation(\n        self: VariantIndex, annotation_source: VariantIndex\n    ) -&gt; VariantIndex:\n        \"\"\"Import annotation from an other variant index dataset.\n\n        At this point the annotation can be extended with extra cross-references,\n        variant effects, allele frequencies, and variant descriptions.\n\n        Args:\n            annotation_source (VariantIndex): Annotation to add to the dataset\n\n        Returns:\n            VariantIndex: VariantIndex dataset with the annotation added\n        \"\"\"\n        # Prefix for renaming columns:\n        prefix = \"annotation_\"\n\n        # Generate select expressions that to merge and import columns from annotation:\n        select_expressions = []\n\n        # Collect columns by iterating over the variant index schema:\n        for schema_field in VariantIndex.get_schema():\n            column = schema_field.name\n\n            # If an annotation column can be found in both datasets:\n            if (column in self.df.columns) and (column in annotation_source.df.columns):\n                # Arrays are merged:\n                if isinstance(schema_field.dataType, t.ArrayType):\n                    fields_order = None\n                    if isinstance(schema_field.dataType.elementType, t.StructType):\n                        # Extract the schema of the array to get the order of the fields:\n                        array_schema = [\n                            schema_field\n                            for schema_field in VariantIndex.get_schema().fields\n                            if schema_field.name == column\n                        ][0].dataType\n                        fields_order = get_nested_struct_schema(\n                            array_schema\n                        ).fieldNames()\n                    select_expressions.append(\n                        safe_array_union(\n                            f.col(column), f.col(f\"{prefix}{column}\"), fields_order\n                        ).alias(column)\n                    )\n                # variantDescription columns are concatenated:\n                elif column == \"variantDescription\":\n                    select_expressions.append(\n                        f.concat_ws(\" \", f.col(column), f.col(f\"{prefix}{column}\")).alias(column)\n                    )\n                # All other non-array columns are coalesced:\n                else:\n                    select_expressions.append(\n                        f.coalesce(f.col(column), f.col(f\"{prefix}{column}\")).alias(\n                            column\n                        )\n                    )\n            # If the column is only found in the annotation dataset rename it:\n            elif column in annotation_source.df.columns:\n                select_expressions.append(f.col(f\"{prefix}{column}\").alias(column))\n            # If the column is only found in the main dataset:\n            elif column in self.df.columns:\n                select_expressions.append(f.col(column))\n            # VariantIndex columns not found in either dataset are ignored.\n\n        # Join the annotation to the dataset:\n        return VariantIndex(\n            _df=(\n                f.broadcast(self.df)\n                .join(\n                    rename_all_columns(annotation_source.df, prefix),\n                    on=[f.col(\"variantId\") == f.col(f\"{prefix}variantId\")],\n                    how=\"left\",\n                )\n                .select(*select_expressions)\n            ),\n            _schema=self.schema,\n        )\n\n    def max_maf(self: VariantIndex) -&gt; Column:\n        \"\"\"Maximum minor allele frequency accross all populations assuming all variants biallelic.\n\n        Returns:\n            Column: Maximum minor allele frequency accross all populations.\n\n        Raises:\n            ValueError: Allele frequencies are not present in the dataset.\n        \"\"\"\n        if \"alleleFrequencies\" not in self.df.columns:\n            raise ValueError(\"Allele frequencies are not present in the dataset.\")\n\n        return f.array_max(\n            f.transform(\n                self.df.alleleFrequencies,\n                lambda af: f.when(\n                    af.alleleFrequency &gt; 0.5, 1 - af.alleleFrequency\n                ).otherwise(af.alleleFrequency),\n            )\n        )\n\n    def filter_by_variant(self: VariantIndex, df: DataFrame) -&gt; VariantIndex:\n        \"\"\"Filter variant annotation dataset by a variant dataframe.\n\n        Args:\n            df (DataFrame): A dataframe of variants.\n\n        Returns:\n            VariantIndex: A filtered variant annotation dataset.\n\n        Raises:\n            AssertionError: When the variant dataframe does not contain eiter `variantId` or `chromosome` column.\n        \"\"\"\n        join_columns = [\"variantId\", \"chromosome\"]\n\n        assert all(\n            col in df.columns for col in join_columns\n        ), \"The variant dataframe must contain the columns 'variantId' and 'chromosome'.\"\n\n        return VariantIndex(\n            _df=self._df.join(\n                f.broadcast(df.select(*join_columns).distinct()),\n                on=join_columns,\n                how=\"inner\",\n            ),\n            _schema=self.schema,\n        )\n\n    def get_distance_to_gene(\n        self: VariantIndex,\n        *,\n        distance_type: str = \"distanceFromTss\",\n        max_distance: int = 500_000,\n    ) -&gt; DataFrame:\n        \"\"\"Extracts variant to gene assignments for variants falling within a window of a gene's TSS or footprint.\n\n        Args:\n            distance_type (str): The type of distance to use. Can be \"distanceFromTss\" or \"distanceFromFootprint\". Defaults to \"distanceFromTss\".\n            max_distance (int): The maximum distance to consider. Defaults to 500_000, the default window size for VEP.\n\n        Returns:\n            DataFrame: A dataframe with the distance between a variant and a gene's TSS or footprint.\n\n        Raises:\n            ValueError: Invalid distance type.\n        \"\"\"\n        if distance_type not in {\"distanceFromTss\", \"distanceFromFootprint\"}:\n            raise ValueError(\n                f\"Invalid distance_type: {distance_type}. Must be 'distanceFromTss' or 'distanceFromFootprint'.\"\n            )\n        df = self.df.select(\n            \"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\")\n        ).select(\"variantId\", \"tc.targetId\", f\"tc.{distance_type}\")\n        if max_distance == 500_000:\n            return df\n        elif max_distance &lt; 500_000:\n            return df.filter(f\"{distance_type} &lt;= {max_distance}\")\n        else:\n            raise ValueError(\n                f\"max_distance must be less than 500_000. Got {max_distance}.\"\n            )\n\n    def annotate_with_amino_acid_consequences(\n        self: VariantIndex, annotation: AminoAcidVariants\n    ) -&gt; VariantIndex:\n        \"\"\"Enriching variant effect assessments with amino-acid derived predicted consequences.\n\n        Args:\n            annotation (AminoAcidVariants): amio-acid level variant consequences.\n\n        Returns:\n            VariantIndex: where amino-acid causing variants are enriched with extra annotation\n        \"\"\"\n        w = Window.partitionBy(\"variantId\").orderBy(f.size(\"variantEffect\").desc())\n\n        return VariantIndex(\n            _df=self.df\n            # Extracting variant consequence on Uniprot and amino-acid changes from the transcripts:\n            .withColumns(\n                {\n                    \"aminoAcidChange\": f.filter(\n                        \"transcriptConsequences\",\n                        lambda vep: vep.aminoAcidChange.isNotNull(),\n                    )[0].aminoAcidChange,\n                    \"uniprotAccession\": f.explode_outer(\n                        f.filter(\n                            \"transcriptConsequences\",\n                            lambda vep: vep.aminoAcidChange.isNotNull(),\n                        )[0].uniprotAccessions\n                    ),\n                }\n            )\n            # Joining with amino-acid predictions:\n            .join(\n                annotation.df.withColumnRenamed(\"variantEffect\", \"annotations\"),\n                on=[\"uniprotAccession\", \"aminoAcidChange\"],\n                how=\"left\",\n            )\n            # Merge predictors:\n            .withColumn(\n                \"variantEffect\",\n                f.when(\n                    f.col(\"annotations\").isNotNull(),\n                    f.array_union(\"variantEffect\", \"annotations\"),\n                ).otherwise(f.col(\"variantEffect\")),\n            )\n            # Dropping unused columns:\n            .drop(\"uniprotAccession\", \"aminoAcidChange\", \"annotations\")\n            # Dropping potentially exploded variant rows:\n            .distinct()\n            .withColumn(\"rank\", f.row_number().over(w))\n            .filter(f.col(\"rank\") == 1)\n            .drop(\"rank\"),\n            _schema=self.get_schema(),\n        )\n\n    def get_loftee(self: VariantIndex) -&gt; DataFrame:\n        \"\"\"Returns a dataframe with a flag indicating whether a variant is predicted to cause loss of function in a gene. The source of this information is the LOFTEE algorithm (https://github.com/konradjk/loftee).\n\n        !!! note, \"This will return a filtered dataframe with only variants that have been annotated by LOFTEE.\"\n\n        Returns:\n            DataFrame: variant to gene assignments from the LOFTEE algorithm\n        \"\"\"\n        return (\n            self.df.select(\"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\"))\n            .filter(f.col(\"tc.lofteePrediction\").isNotNull())\n            .withColumn(\n                \"isHighQualityPlof\",\n                f.when(f.col(\"tc.lofteePrediction\") == \"HC\", True).when(\n                    f.col(\"tc.lofteePrediction\") == \"LC\", False\n                ),\n            )\n            .select(\n                \"variantId\",\n                f.col(\"tc.targetId\"),\n                f.col(\"tc.lofteePrediction\"),\n                \"isHighQualityPlof\",\n            )\n        )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.add_annotation","title":"<code>add_annotation(annotation_source: VariantIndex) -&gt; VariantIndex</code>","text":"<p>Import annotation from an other variant index dataset.</p> <p>At this point the annotation can be extended with extra cross-references, variant effects, allele frequencies, and variant descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_source</code> <code>VariantIndex</code> <p>Annotation to add to the dataset</p> required <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>VariantIndex dataset with the annotation added</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def add_annotation(\n    self: VariantIndex, annotation_source: VariantIndex\n) -&gt; VariantIndex:\n    \"\"\"Import annotation from an other variant index dataset.\n\n    At this point the annotation can be extended with extra cross-references,\n    variant effects, allele frequencies, and variant descriptions.\n\n    Args:\n        annotation_source (VariantIndex): Annotation to add to the dataset\n\n    Returns:\n        VariantIndex: VariantIndex dataset with the annotation added\n    \"\"\"\n    # Prefix for renaming columns:\n    prefix = \"annotation_\"\n\n    # Generate select expressions that to merge and import columns from annotation:\n    select_expressions = []\n\n    # Collect columns by iterating over the variant index schema:\n    for schema_field in VariantIndex.get_schema():\n        column = schema_field.name\n\n        # If an annotation column can be found in both datasets:\n        if (column in self.df.columns) and (column in annotation_source.df.columns):\n            # Arrays are merged:\n            if isinstance(schema_field.dataType, t.ArrayType):\n                fields_order = None\n                if isinstance(schema_field.dataType.elementType, t.StructType):\n                    # Extract the schema of the array to get the order of the fields:\n                    array_schema = [\n                        schema_field\n                        for schema_field in VariantIndex.get_schema().fields\n                        if schema_field.name == column\n                    ][0].dataType\n                    fields_order = get_nested_struct_schema(\n                        array_schema\n                    ).fieldNames()\n                select_expressions.append(\n                    safe_array_union(\n                        f.col(column), f.col(f\"{prefix}{column}\"), fields_order\n                    ).alias(column)\n                )\n            # variantDescription columns are concatenated:\n            elif column == \"variantDescription\":\n                select_expressions.append(\n                    f.concat_ws(\" \", f.col(column), f.col(f\"{prefix}{column}\")).alias(column)\n                )\n            # All other non-array columns are coalesced:\n            else:\n                select_expressions.append(\n                    f.coalesce(f.col(column), f.col(f\"{prefix}{column}\")).alias(\n                        column\n                    )\n                )\n        # If the column is only found in the annotation dataset rename it:\n        elif column in annotation_source.df.columns:\n            select_expressions.append(f.col(f\"{prefix}{column}\").alias(column))\n        # If the column is only found in the main dataset:\n        elif column in self.df.columns:\n            select_expressions.append(f.col(column))\n        # VariantIndex columns not found in either dataset are ignored.\n\n    # Join the annotation to the dataset:\n    return VariantIndex(\n        _df=(\n            f.broadcast(self.df)\n            .join(\n                rename_all_columns(annotation_source.df, prefix),\n                on=[f.col(\"variantId\") == f.col(f\"{prefix}variantId\")],\n                how=\"left\",\n            )\n            .select(*select_expressions)\n        ),\n        _schema=self.schema,\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.annotate_with_amino_acid_consequences","title":"<code>annotate_with_amino_acid_consequences(annotation: AminoAcidVariants) -&gt; VariantIndex</code>","text":"<p>Enriching variant effect assessments with amino-acid derived predicted consequences.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>AminoAcidVariants</code> <p>amio-acid level variant consequences.</p> required <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>where amino-acid causing variants are enriched with extra annotation</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def annotate_with_amino_acid_consequences(\n    self: VariantIndex, annotation: AminoAcidVariants\n) -&gt; VariantIndex:\n    \"\"\"Enriching variant effect assessments with amino-acid derived predicted consequences.\n\n    Args:\n        annotation (AminoAcidVariants): amio-acid level variant consequences.\n\n    Returns:\n        VariantIndex: where amino-acid causing variants are enriched with extra annotation\n    \"\"\"\n    w = Window.partitionBy(\"variantId\").orderBy(f.size(\"variantEffect\").desc())\n\n    return VariantIndex(\n        _df=self.df\n        # Extracting variant consequence on Uniprot and amino-acid changes from the transcripts:\n        .withColumns(\n            {\n                \"aminoAcidChange\": f.filter(\n                    \"transcriptConsequences\",\n                    lambda vep: vep.aminoAcidChange.isNotNull(),\n                )[0].aminoAcidChange,\n                \"uniprotAccession\": f.explode_outer(\n                    f.filter(\n                        \"transcriptConsequences\",\n                        lambda vep: vep.aminoAcidChange.isNotNull(),\n                    )[0].uniprotAccessions\n                ),\n            }\n        )\n        # Joining with amino-acid predictions:\n        .join(\n            annotation.df.withColumnRenamed(\"variantEffect\", \"annotations\"),\n            on=[\"uniprotAccession\", \"aminoAcidChange\"],\n            how=\"left\",\n        )\n        # Merge predictors:\n        .withColumn(\n            \"variantEffect\",\n            f.when(\n                f.col(\"annotations\").isNotNull(),\n                f.array_union(\"variantEffect\", \"annotations\"),\n            ).otherwise(f.col(\"variantEffect\")),\n        )\n        # Dropping unused columns:\n        .drop(\"uniprotAccession\", \"aminoAcidChange\", \"annotations\")\n        # Dropping potentially exploded variant rows:\n        .distinct()\n        .withColumn(\"rank\", f.row_number().over(w))\n        .filter(f.col(\"rank\") == 1)\n        .drop(\"rank\"),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.filter_by_variant","title":"<code>filter_by_variant(df: DataFrame) -&gt; VariantIndex</code>","text":"<p>Filter variant annotation dataset by a variant dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe of variants.</p> required <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>A filtered variant annotation dataset.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>When the variant dataframe does not contain eiter <code>variantId</code> or <code>chromosome</code> column.</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def filter_by_variant(self: VariantIndex, df: DataFrame) -&gt; VariantIndex:\n    \"\"\"Filter variant annotation dataset by a variant dataframe.\n\n    Args:\n        df (DataFrame): A dataframe of variants.\n\n    Returns:\n        VariantIndex: A filtered variant annotation dataset.\n\n    Raises:\n        AssertionError: When the variant dataframe does not contain eiter `variantId` or `chromosome` column.\n    \"\"\"\n    join_columns = [\"variantId\", \"chromosome\"]\n\n    assert all(\n        col in df.columns for col in join_columns\n    ), \"The variant dataframe must contain the columns 'variantId' and 'chromosome'.\"\n\n    return VariantIndex(\n        _df=self._df.join(\n            f.broadcast(df.select(*join_columns).distinct()),\n            on=join_columns,\n            how=\"inner\",\n        ),\n        _schema=self.schema,\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.get_distance_to_gene","title":"<code>get_distance_to_gene(*, distance_type: str = 'distanceFromTss', max_distance: int = 500000) -&gt; DataFrame</code>","text":"<p>Extracts variant to gene assignments for variants falling within a window of a gene's TSS or footprint.</p> <p>Parameters:</p> Name Type Description Default <code>distance_type</code> <code>str</code> <p>The type of distance to use. Can be \"distanceFromTss\" or \"distanceFromFootprint\". Defaults to \"distanceFromTss\".</p> <code>'distanceFromTss'</code> <code>max_distance</code> <code>int</code> <p>The maximum distance to consider. Defaults to 500_000, the default window size for VEP.</p> <code>500000</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the distance between a variant and a gene's TSS or footprint.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Invalid distance type.</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def get_distance_to_gene(\n    self: VariantIndex,\n    *,\n    distance_type: str = \"distanceFromTss\",\n    max_distance: int = 500_000,\n) -&gt; DataFrame:\n    \"\"\"Extracts variant to gene assignments for variants falling within a window of a gene's TSS or footprint.\n\n    Args:\n        distance_type (str): The type of distance to use. Can be \"distanceFromTss\" or \"distanceFromFootprint\". Defaults to \"distanceFromTss\".\n        max_distance (int): The maximum distance to consider. Defaults to 500_000, the default window size for VEP.\n\n    Returns:\n        DataFrame: A dataframe with the distance between a variant and a gene's TSS or footprint.\n\n    Raises:\n        ValueError: Invalid distance type.\n    \"\"\"\n    if distance_type not in {\"distanceFromTss\", \"distanceFromFootprint\"}:\n        raise ValueError(\n            f\"Invalid distance_type: {distance_type}. Must be 'distanceFromTss' or 'distanceFromFootprint'.\"\n        )\n    df = self.df.select(\n        \"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\")\n    ).select(\"variantId\", \"tc.targetId\", f\"tc.{distance_type}\")\n    if max_distance == 500_000:\n        return df\n    elif max_distance &lt; 500_000:\n        return df.filter(f\"{distance_type} &lt;= {max_distance}\")\n    else:\n        raise ValueError(\n            f\"max_distance must be less than 500_000. Got {max_distance}.\"\n        )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.get_loftee","title":"<code>get_loftee() -&gt; DataFrame</code>","text":"<p>Returns a dataframe with a flag indicating whether a variant is predicted to cause loss of function in a gene. The source of this information is the LOFTEE algorithm (https://github.com/konradjk/loftee).</p> <p>!!! note, \"This will return a filtered dataframe with only variants that have been annotated by LOFTEE.\"</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>variant to gene assignments from the LOFTEE algorithm</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def get_loftee(self: VariantIndex) -&gt; DataFrame:\n    \"\"\"Returns a dataframe with a flag indicating whether a variant is predicted to cause loss of function in a gene. The source of this information is the LOFTEE algorithm (https://github.com/konradjk/loftee).\n\n    !!! note, \"This will return a filtered dataframe with only variants that have been annotated by LOFTEE.\"\n\n    Returns:\n        DataFrame: variant to gene assignments from the LOFTEE algorithm\n    \"\"\"\n    return (\n        self.df.select(\"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\"))\n        .filter(f.col(\"tc.lofteePrediction\").isNotNull())\n        .withColumn(\n            \"isHighQualityPlof\",\n            f.when(f.col(\"tc.lofteePrediction\") == \"HC\", True).when(\n                f.col(\"tc.lofteePrediction\") == \"LC\", False\n            ),\n        )\n        .select(\n            \"variantId\",\n            f.col(\"tc.targetId\"),\n            f.col(\"tc.lofteePrediction\"),\n            \"isHighQualityPlof\",\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the variant index dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the VariantIndex dataset</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[VariantIndex]) -&gt; StructType:\n    \"\"\"Provides the schema for the variant index dataset.\n\n    Returns:\n        StructType: Schema for the VariantIndex dataset\n    \"\"\"\n    return parse_spark_schema(\"variant_index.json\")\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.hash_long_variant_ids","title":"<code>hash_long_variant_ids(variant_id: Column, chromosome: Column, position: Column, threshold: int) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Hash long variant identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>variant_id</code> <code>Column</code> <p>Column containing variant identifiers.</p> required <code>chromosome</code> <code>Column</code> <p>Chromosome column.</p> required <code>position</code> <code>Column</code> <p>position column.</p> required <code>threshold</code> <code>int</code> <p>Above this limit, a hash will be generated.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Hashed variant identifiers for long variants.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; (\n...    spark.createDataFrame([('v_short', 'x', 23),('v_looooooong', '23', 23), ('no_chrom', None, None), (None, None, None)], ['variantId', 'chromosome', 'position'])\n...    .select('variantId', VariantIndex.hash_long_variant_ids(f.col('variantId'), f.col('chromosome'), f.col('position'), 10).alias('hashedVariantId'))\n...    .show(truncate=False)\n... )\n+------------+--------------------------------------------+\n|variantId   |hashedVariantId                             |\n+------------+--------------------------------------------+\n|v_short     |v_short                                     |\n|v_looooooong|OTVAR_23_23_3749d019d645894770c364992ae70a05|\n|no_chrom    |OTVAR_41acfcd7d4fd523b33600b504914ef25      |\n|NULL        |NULL                                        |\n+------------+--------------------------------------------+\n</code></pre> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>@staticmethod\ndef hash_long_variant_ids(\n    variant_id: Column, chromosome: Column, position: Column, threshold: int\n) -&gt; Column:\n    \"\"\"Hash long variant identifiers.\n\n    Args:\n        variant_id (Column): Column containing variant identifiers.\n        chromosome (Column): Chromosome column.\n        position (Column): position column.\n        threshold (int): Above this limit, a hash will be generated.\n\n    Returns:\n        Column: Hashed variant identifiers for long variants.\n\n    Examples:\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame([('v_short', 'x', 23),('v_looooooong', '23', 23), ('no_chrom', None, None), (None, None, None)], ['variantId', 'chromosome', 'position'])\n        ...    .select('variantId', VariantIndex.hash_long_variant_ids(f.col('variantId'), f.col('chromosome'), f.col('position'), 10).alias('hashedVariantId'))\n        ...    .show(truncate=False)\n        ... )\n        +------------+--------------------------------------------+\n        |variantId   |hashedVariantId                             |\n        +------------+--------------------------------------------+\n        |v_short     |v_short                                     |\n        |v_looooooong|OTVAR_23_23_3749d019d645894770c364992ae70a05|\n        |no_chrom    |OTVAR_41acfcd7d4fd523b33600b504914ef25      |\n        |NULL        |NULL                                        |\n        +------------+--------------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return (\n        # If either the position or the chromosome is missing, we hash the identifier:\n        f.when(\n            chromosome.isNull() | position.isNull(),\n            f.concat(\n                f.lit(\"OTVAR_\"),\n                f.md5(variant_id).cast(\"string\"),\n            ),\n        )\n        # If chromosome and position are given, but alleles are too long, create hash:\n        .when(\n            f.length(variant_id) &gt;= threshold,\n            f.concat_ws(\n                \"_\",\n                f.lit(\"OTVAR\"),\n                chromosome,\n                position,\n                f.md5(variant_id).cast(\"string\"),\n            ),\n        )\n        # Missing and regular variant identifiers are left unchanged:\n        .otherwise(variant_id)\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.max_maf","title":"<code>max_maf() -&gt; Column</code>","text":"<p>Maximum minor allele frequency accross all populations assuming all variants biallelic.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Maximum minor allele frequency accross all populations.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Allele frequencies are not present in the dataset.</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def max_maf(self: VariantIndex) -&gt; Column:\n    \"\"\"Maximum minor allele frequency accross all populations assuming all variants biallelic.\n\n    Returns:\n        Column: Maximum minor allele frequency accross all populations.\n\n    Raises:\n        ValueError: Allele frequencies are not present in the dataset.\n    \"\"\"\n    if \"alleleFrequencies\" not in self.df.columns:\n        raise ValueError(\"Allele frequencies are not present in the dataset.\")\n\n    return f.array_max(\n        f.transform(\n            self.df.alleleFrequencies,\n            lambda af: f.when(\n                af.alleleFrequency &gt; 0.5, 1 - af.alleleFrequency\n            ).otherwise(af.alleleFrequency),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#schema","title":"Schema","text":"<pre><code>root\n |-- variantId: string (nullable = false)\n |-- chromosome: string (nullable = false)\n |-- position: integer (nullable = false)\n |-- referenceAllele: string (nullable = false)\n |-- alternateAllele: string (nullable = false)\n |-- variantEffect: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- method: string (nullable = true)\n |    |    |-- assessment: string (nullable = true)\n |    |    |-- score: float (nullable = true)\n |    |    |-- assessmentFlag: string (nullable = true)\n |    |    |-- targetId: string (nullable = true)\n |    |    |-- normalisedScore: double (nullable = true)\n |-- mostSevereConsequenceId: string (nullable = true)\n |-- transcriptConsequences: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- variantFunctionalConsequenceIds: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- aminoAcidChange: string (nullable = true)\n |    |    |-- uniprotAccessions: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- isEnsemblCanonical: boolean (nullable = false)\n |    |    |-- codons: string (nullable = true)\n |    |    |-- distanceFromFootprint: long (nullable = true)\n |    |    |-- distanceFromTss: long (nullable = true)\n |    |    |-- appris: string (nullable = true)\n |    |    |-- maneSelect: string (nullable = true)\n |    |    |-- targetId: string (nullable = true)\n |    |    |-- impact: string (nullable = true)\n |    |    |-- lofteePrediction: string (nullable = true)\n |    |    |-- siftPrediction: float (nullable = true)\n |    |    |-- polyphenPrediction: float (nullable = true)\n |    |    |-- consequenceScore: float (nullable = true)\n |    |    |-- transcriptIndex: integer (nullable = true)\n |    |    |-- approvedSymbol: string (nullable = true)\n |    |    |-- biotype: string (nullable = true)\n |    |    |-- transcriptId: string (nullable = true)\n |-- rsIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- hgvsId: string (nullable = true)\n |-- alleleFrequencies: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- populationName: string (nullable = true)\n |    |    |-- alleleFrequency: double (nullable = true)\n |-- dbXrefs: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- variantDescription: string (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/l2g_features/_l2g_feature/","title":"L2G Feature","text":""},{"location":"python_api/datasets/l2g_features/_l2g_feature/#abstract-class","title":"Abstract Class","text":""},{"location":"python_api/datasets/l2g_features/_l2g_feature/#gentropy.dataset.l2g_features.l2g_feature.L2GFeature","title":"<code>gentropy.dataset.l2g_features.l2g_feature.L2GFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code>, <code>ABC</code></p> <p>Locus-to-gene feature dataset that serves as template to generate each of the features that inform about locus to gene assignments.</p> Source code in <code>src/gentropy/dataset/l2g_features/l2g_feature.py</code> <pre><code>@dataclass\nclass L2GFeature(Dataset, ABC):\n    \"\"\"Locus-to-gene feature dataset that serves as template to generate each of the features that inform about locus to gene assignments.\"\"\"\n\n    def __post_init__(\n        self: L2GFeature,\n        feature_dependency_type: Any = None,\n        credible_set: StudyLocus | None = None,\n    ) -&gt; None:\n        \"\"\"Initializes a L2GFeature dataset. Any child class of L2GFeature must implement the `compute` method.\n\n        Args:\n            feature_dependency_type (Any): The dependency that the L2GFeature dataset depends on. Defaults to None.\n            credible_set (StudyLocus | None): The credible set that the L2GFeature dataset is based on. Defaults to None.\n        \"\"\"\n        super().__post_init__()\n        self.feature_dependency_type = feature_dependency_type\n        self.credible_set = credible_set\n\n    @classmethod\n    def get_schema(cls: type[L2GFeature]) -&gt; StructType:\n        \"\"\"Provides the schema for the L2GFeature dataset.\n\n        Returns:\n            StructType: Schema for the L2GFeature dataset\n        \"\"\"\n        return parse_spark_schema(\"l2g_feature.json\")\n\n    @classmethod\n    @abstractmethod\n    def compute(\n        cls: type[L2GFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: Any,\n    ) -&gt; L2GFeature:\n        \"\"\"Computes the L2GFeature dataset.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (Any): The dependency that the L2GFeature class needs to compute the feature\n        Returns:\n            L2GFeature: a L2GFeature dataset\n\n        Raises:\n                NotImplementedError: This method must be implemented in the child classes\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in the child classes\")\n</code></pre>"},{"location":"python_api/datasets/l2g_features/_l2g_feature/#gentropy.dataset.l2g_features.l2g_feature.L2GFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: Any) -&gt; L2GFeature</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Computes the L2GFeature dataset.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>Any</code> <p>The dependency that the L2GFeature class needs to compute the feature</p> required <p>Returns:     L2GFeature: a L2GFeature dataset</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented in the child classes</p> Source code in <code>src/gentropy/dataset/l2g_features/l2g_feature.py</code> <pre><code>@classmethod\n@abstractmethod\ndef compute(\n    cls: type[L2GFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: Any,\n) -&gt; L2GFeature:\n    \"\"\"Computes the L2GFeature dataset.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (Any): The dependency that the L2GFeature class needs to compute the feature\n    Returns:\n        L2GFeature: a L2GFeature dataset\n\n    Raises:\n            NotImplementedError: This method must be implemented in the child classes\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in the child classes\")\n</code></pre>"},{"location":"python_api/datasets/l2g_features/_l2g_feature/#gentropy.dataset.l2g_features.l2g_feature.L2GFeature.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the L2GFeature dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the L2GFeature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/l2g_feature.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[L2GFeature]) -&gt; StructType:\n    \"\"\"Provides the schema for the L2GFeature dataset.\n\n    Returns:\n        StructType: Schema for the L2GFeature dataset\n    \"\"\"\n    return parse_spark_schema(\"l2g_feature.json\")\n</code></pre>"},{"location":"python_api/datasets/l2g_features/_l2g_feature/#schema","title":"Schema","text":"<pre><code>root\n |-- studyLocusId: string (nullable = false)\n |-- geneId: string (nullable = false)\n |-- featureName: string (nullable = false)\n |-- featureValue: float (nullable = false)\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/","title":"From colocalisation","text":""},{"location":"python_api/datasets/l2g_features/colocalisation/#list-of-features","title":"List of features","text":""},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all eQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class EQtlColocClppMaximumFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all eQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"eQtlColocClppMaximum\"\n\n    @classmethod\n    def compute(\n        cls: type[EQtlColocClppMaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; EQtlColocClppMaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary with the dependencies required. They are passed as keyword arguments.\n\n        Returns:\n            EQtlColocClppMaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_type = [\"eqtl\", \"sceqtl\"]\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; EQtlColocClppMaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary with the dependencies required. They are passed as keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>EQtlColocClppMaximumFeature</code> <code>EQtlColocClppMaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[EQtlColocClppMaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; EQtlColocClppMaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary with the dependencies required. They are passed as keyword arguments.\n\n    Returns:\n        EQtlColocClppMaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_type = [\"eqtl\", \"sceqtl\"]\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all pQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class PQtlColocClppMaximumFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all pQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"pQtlColocClppMaximum\"\n\n    @classmethod\n    def compute(\n        cls: type[PQtlColocClppMaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; PQtlColocClppMaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            PQtlColocClppMaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_type = \"pqtl\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; PQtlColocClppMaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>PQtlColocClppMaximumFeature</code> <code>PQtlColocClppMaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[PQtlColocClppMaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; PQtlColocClppMaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        PQtlColocClppMaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_type = \"pqtl\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all sQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class SQtlColocClppMaximumFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all sQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"sQtlColocClppMaximum\"\n\n    @classmethod\n    def compute(\n        cls: type[SQtlColocClppMaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; SQtlColocClppMaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            SQtlColocClppMaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_types,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; SQtlColocClppMaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>SQtlColocClppMaximumFeature</code> <code>SQtlColocClppMaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[SQtlColocClppMaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; SQtlColocClppMaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        SQtlColocClppMaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_types,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus, gene) aggregating over all eQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class EQtlColocH4MaximumFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus, gene) aggregating over all eQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"eQtlColocH4Maximum\"\n\n    @classmethod\n    def compute(\n        cls: type[EQtlColocH4MaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; EQtlColocH4MaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            EQtlColocH4MaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_type = [\"eqtl\", \"sceqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; EQtlColocH4MaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>EQtlColocH4MaximumFeature</code> <code>EQtlColocH4MaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[EQtlColocH4MaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; EQtlColocH4MaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        EQtlColocH4MaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_type = [\"eqtl\", \"sceqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus, gene) aggregating over all pQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class PQtlColocH4MaximumFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus, gene) aggregating over all pQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"pQtlColocH4Maximum\"\n\n    @classmethod\n    def compute(\n        cls: type[PQtlColocH4MaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; PQtlColocH4MaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            PQtlColocH4MaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_type = \"pqtl\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; PQtlColocH4MaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>PQtlColocH4MaximumFeature</code> <code>PQtlColocH4MaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[PQtlColocH4MaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; PQtlColocH4MaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        PQtlColocH4MaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_type = \"pqtl\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus, gene) aggregating over all sQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class SQtlColocH4MaximumFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus, gene) aggregating over all sQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"sQtlColocH4Maximum\"\n\n    @classmethod\n    def compute(\n        cls: type[SQtlColocH4MaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; SQtlColocH4MaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            SQtlColocH4MaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_types,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; SQtlColocH4MaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>SQtlColocH4MaximumFeature</code> <code>SQtlColocH4MaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[SQtlColocH4MaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; SQtlColocH4MaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        SQtlColocH4MaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_types,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus) aggregating over all eQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class EQtlColocClppMaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus) aggregating over all eQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"eQtlColocClppMaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[EQtlColocClppMaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; EQtlColocClppMaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary with the dependencies required. They are passed as keyword arguments.\n\n        Returns:\n            EQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_type = [\"eqtl\", \"sceqtl\"]\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; EQtlColocClppMaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary with the dependencies required. They are passed as keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>EQtlColocClppMaximumNeighbourhoodFeature</code> <code>EQtlColocClppMaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[EQtlColocClppMaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; EQtlColocClppMaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary with the dependencies required. They are passed as keyword arguments.\n\n    Returns:\n        EQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_type = [\"eqtl\", \"sceqtl\"]\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all pQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class PQtlColocClppMaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all pQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"pQtlColocClppMaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[PQtlColocClppMaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; PQtlColocClppMaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            PQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_type = \"pqtl\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; PQtlColocClppMaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>PQtlColocClppMaximumNeighbourhoodFeature</code> <code>PQtlColocClppMaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[PQtlColocClppMaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; PQtlColocClppMaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        PQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_type = \"pqtl\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all sQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class SQtlColocClppMaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all sQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"sQtlColocClppMaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[SQtlColocClppMaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; SQtlColocClppMaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            SQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_types,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; SQtlColocClppMaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>SQtlColocClppMaximumNeighbourhoodFeature</code> <code>SQtlColocClppMaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[SQtlColocClppMaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; SQtlColocClppMaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        SQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_types,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus) aggregating over all eQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class EQtlColocH4MaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus) aggregating over all eQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"eQtlColocH4MaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[EQtlColocH4MaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; EQtlColocH4MaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            EQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_type = [\"eqtl\", \"sceqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; EQtlColocH4MaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>EQtlColocH4MaximumNeighbourhoodFeature</code> <code>EQtlColocH4MaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[EQtlColocH4MaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; EQtlColocH4MaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        EQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_type = [\"eqtl\", \"sceqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus) aggregating over all pQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class PQtlColocH4MaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus) aggregating over all pQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"pQtlColocH4MaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[PQtlColocH4MaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; PQtlColocH4MaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            PQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_type = \"pqtl\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; PQtlColocH4MaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>PQtlColocH4MaximumNeighbourhoodFeature</code> <code>PQtlColocH4MaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[PQtlColocH4MaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; PQtlColocH4MaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        PQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_type = \"pqtl\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus) aggregating over all sQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class SQtlColocH4MaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus) aggregating over all sQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"sQtlColocH4MaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[SQtlColocH4MaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; SQtlColocH4MaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            SQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_types,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; SQtlColocH4MaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>SQtlColocH4MaximumNeighbourhoodFeature</code> <code>SQtlColocH4MaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[SQtlColocH4MaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; SQtlColocH4MaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        SQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_types,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#common-logic","title":"Common logic","text":""},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.common_colocalisation_feature_logic","title":"<code>gentropy.dataset.l2g_features.colocalisation.common_colocalisation_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, colocalisation_method: str, colocalisation_metric: str, feature_name: str, qtl_types: list[str] | str, *, colocalisation: Colocalisation, study_index: StudyIndex, study_locus: StudyLocus) -&gt; DataFrame</code>","text":"<p>Wrapper to call the logic that creates a type of colocalisation features.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>colocalisation_method</code> <code>str</code> <p>The colocalisation method to filter the data by</p> required <code>colocalisation_metric</code> <code>str</code> <p>The colocalisation metric to use</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature to create</p> required <code>qtl_types</code> <code>list[str] | str</code> <p>The types of QTL to filter the data by</p> required <code>colocalisation</code> <code>Colocalisation</code> <p>Dataset with the colocalisation results</p> required <code>study_index</code> <code>StudyIndex</code> <p>Study index to fetch study type and gene</p> required <code>study_locus</code> <code>StudyLocus</code> <p>Study locus to traverse between colocalisation and study index</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature annotation in long format with the columns: studyLocusId, geneId, featureName, featureValue</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>def common_colocalisation_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    colocalisation_method: str,\n    colocalisation_metric: str,\n    feature_name: str,\n    qtl_types: list[str] | str,\n    *,\n    colocalisation: Colocalisation,\n    study_index: StudyIndex,\n    study_locus: StudyLocus,\n) -&gt; DataFrame:\n    \"\"\"Wrapper to call the logic that creates a type of colocalisation features.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        colocalisation_method (str): The colocalisation method to filter the data by\n        colocalisation_metric (str): The colocalisation metric to use\n        feature_name (str): The name of the feature to create\n        qtl_types (list[str] | str): The types of QTL to filter the data by\n        colocalisation (Colocalisation): Dataset with the colocalisation results\n        study_index (StudyIndex): Study index to fetch study type and gene\n        study_locus (StudyLocus): Study locus to traverse between colocalisation and study index\n\n    Returns:\n        DataFrame: Feature annotation in long format with the columns: studyLocusId, geneId, featureName, featureValue\n    \"\"\"\n    joining_cols = (\n        [\"studyLocusId\", \"geneId\"]\n        if isinstance(study_loci_to_annotate, L2GGoldStandard)\n        else [\"studyLocusId\"]\n    )\n    return (\n        study_loci_to_annotate.df.join(\n            colocalisation.extract_maximum_coloc_probability_per_region_and_gene(\n                study_locus,\n                study_index,\n                filter_by_colocalisation_method=colocalisation_method,\n                filter_by_qtls=qtl_types,\n            ),\n            on=joining_cols,\n        )\n        .selectExpr(\n            \"studyLocusId\",\n            \"geneId\",\n            f\"{colocalisation_metric} as {feature_name}\",\n        )\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.extend_missing_colocalisation_to_neighbourhood_genes","title":"<code>gentropy.dataset.l2g_features.colocalisation.extend_missing_colocalisation_to_neighbourhood_genes(feature_name: str, local_features: DataFrame, variant_index: VariantIndex, target_index: TargetIndex, study_locus: StudyLocus) -&gt; DataFrame</code>","text":"<p>This function creates an artificial dataset of features that represents the missing colocalisation to the neighbourhood genes.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>The name of the feature to extend</p> required <code>local_features</code> <code>DataFrame</code> <p>The dataframe of features to extend</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Variant index containing all variant/gene relationships</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index to fetch the gene information</p> required <code>study_locus</code> <code>StudyLocus</code> <p>Study locus to traverse between colocalisation and variant index</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Dataframe of features that include genes in the neighbourhood not present in the colocalisation results. For these genes, the feature value is set to 0.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>def extend_missing_colocalisation_to_neighbourhood_genes(\n    feature_name: str,\n    local_features: DataFrame,\n    variant_index: VariantIndex,\n    target_index: TargetIndex,\n    study_locus: StudyLocus,\n) -&gt; DataFrame:\n    \"\"\"This function creates an artificial dataset of features that represents the missing colocalisation to the neighbourhood genes.\n\n    Args:\n        feature_name (str): The name of the feature to extend\n        local_features (DataFrame): The dataframe of features to extend\n        variant_index (VariantIndex): Variant index containing all variant/gene relationships\n        target_index (TargetIndex): Target index to fetch the gene information\n        study_locus (StudyLocus): Study locus to traverse between colocalisation and variant index\n\n    Returns:\n        DataFrame: Dataframe of features that include genes in the neighbourhood not present in the colocalisation results. For these genes, the feature value is set to 0.\n    \"\"\"\n    coding_variant_gene_lut = (\n        variant_index.df.select(\n            \"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\")\n        )\n        .select(f.col(\"tc.targetId\").alias(\"geneId\"), \"variantId\")\n        .join(target_index.df.select(f.col(\"id\").alias(\"geneId\"), \"biotype\"), \"geneId\", \"left\")\n        .filter(f.col(\"biotype\") == \"protein_coding\")\n        .drop(\"biotype\")\n        .distinct()\n    )\n    local_features_w_variant = local_features.join(\n        study_locus.df.select(\"studyLocusId\", \"variantId\"), \"studyLocusId\"\n    )\n    return (\n        # Get the genes that are not present in the colocalisation results\n        coding_variant_gene_lut.join(\n            local_features_w_variant, [\"variantId\", \"geneId\"], \"left_anti\"\n        )\n        # We now link the missing variant/gene to the study locus from the original dataframe\n        .join(\n            local_features_w_variant.select(\"studyLocusId\", \"variantId\").distinct(),\n            \"variantId\",\n        )\n        .drop(\"variantId\")\n        # Fill the information for missing genes with 0\n        .withColumn(feature_name, f.lit(0.0))\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.common_neighbourhood_colocalisation_feature_logic","title":"<code>gentropy.dataset.l2g_features.colocalisation.common_neighbourhood_colocalisation_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, colocalisation_method: str, colocalisation_metric: str, feature_name: str, qtl_types: list[str] | str, *, colocalisation: Colocalisation, study_index: StudyIndex, target_index: TargetIndex, study_locus: StudyLocus, variant_index: VariantIndex) -&gt; DataFrame</code>","text":"<p>Wrapper to call the logic that creates a type of colocalisation features.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>colocalisation_method</code> <code>str</code> <p>The colocalisation method to filter the data by</p> required <code>colocalisation_metric</code> <code>str</code> <p>The colocalisation metric to use</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature to create</p> required <code>qtl_types</code> <code>list[str] | str</code> <p>The types of QTL to filter the data by</p> required <code>colocalisation</code> <code>Colocalisation</code> <p>Dataset with the colocalisation results</p> required <code>study_index</code> <code>StudyIndex</code> <p>Study index to fetch study type and gene</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index to add gene type</p> required <code>study_locus</code> <code>StudyLocus</code> <p>Study locus to traverse between colocalisation and study index</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Variant index to annotate all overlapping genes</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature annotation in long format with the columns: studyLocusId, geneId, featureName, featureValue</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>def common_neighbourhood_colocalisation_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    colocalisation_method: str,\n    colocalisation_metric: str,\n    feature_name: str,\n    qtl_types: list[str] | str,\n    *,\n    colocalisation: Colocalisation,\n    study_index: StudyIndex,\n    target_index: TargetIndex,\n    study_locus: StudyLocus,\n    variant_index: VariantIndex,\n) -&gt; DataFrame:\n    \"\"\"Wrapper to call the logic that creates a type of colocalisation features.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        colocalisation_method (str): The colocalisation method to filter the data by\n        colocalisation_metric (str): The colocalisation metric to use\n        feature_name (str): The name of the feature to create\n        qtl_types (list[str] | str): The types of QTL to filter the data by\n        colocalisation (Colocalisation): Dataset with the colocalisation results\n        study_index (StudyIndex): Study index to fetch study type and gene\n        target_index (TargetIndex): Target index to add gene type\n        study_locus (StudyLocus): Study locus to traverse between colocalisation and study index\n        variant_index (VariantIndex): Variant index to annotate all overlapping genes\n\n    Returns:\n        DataFrame: Feature annotation in long format with the columns: studyLocusId, geneId, featureName, featureValue\n    \"\"\"\n    # First maximum colocalisation score for each studylocus, gene\n    local_feature_name = feature_name.replace(\"Neighbourhood\", \"\")\n    local_max = common_colocalisation_feature_logic(\n        study_loci_to_annotate,\n        colocalisation_method,\n        colocalisation_metric,\n        local_feature_name,\n        qtl_types,\n        colocalisation=colocalisation,\n        study_index=study_index,\n        study_locus=study_locus,\n    )\n    extended_local_max = local_max.unionByName(\n        extend_missing_colocalisation_to_neighbourhood_genes(\n            local_feature_name,\n            local_max,\n            variant_index,\n            target_index,\n            study_locus,\n        )\n    )\n    return (\n        extended_local_max.join(\n            # Compute average score in the vicinity (feature will be the same for any gene associated with a studyLocus)\n            # (non protein coding genes in the vicinity are excluded see #3552)\n            target_index.df.filter(f.col(\"biotype\") == \"protein_coding\").select(f.col(\"id\").alias(\"geneId\")),\n            \"geneId\",\n            \"inner\",\n        )\n        .withColumn(\n            \"regional_max\",\n            f.max(local_feature_name).over(Window.partitionBy(\"studyLocusId\")),\n        )\n        .withColumn(\n            feature_name,\n            f.when(\n                (f.col(\"regional_max\").isNotNull()) &amp; (f.col(\"regional_max\") != 0.0),\n                f.col(local_feature_name)\n                / f.coalesce(f.col(\"regional_max\"), f.lit(0.0)),\n            ).otherwise(f.lit(0.0)),\n        )\n        .drop(\"regional_max\", local_feature_name)\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/","title":"From distance","text":""},{"location":"python_api/datasets/l2g_features/distance/#list-of-features","title":"List of features","text":""},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelTssFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceSentinelTssFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance of the sentinel variant to gene TSS. This is not weighted by the causal probability.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceSentinelTssFeature(L2GFeature):\n    \"\"\"Distance of the sentinel variant to gene TSS. This is not weighted by the causal probability.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"distanceSentinelTss\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceSentinelTssFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceSentinelTssFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceSentinelTssFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromTss\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelTssFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceSentinelTssFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceSentinelTssFeature</code> <code>DistanceSentinelTssFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceSentinelTssFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceSentinelTssFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceSentinelTssFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromTss\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelTssNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceSentinelTssNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance between the sentinel variant and a gene TSS as a relation of the distnace with all the genes in the vicinity of a studyLocus. This is not weighted by the causal probability.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceSentinelTssNeighbourhoodFeature(L2GFeature):\n    \"\"\"Distance between the sentinel variant and a gene TSS as a relation of the distnace with all the genes in the vicinity of a studyLocus. This is not weighted by the causal probability.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"distanceSentinelTssNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceSentinelTssNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceSentinelTssNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceSentinelTssNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromTss\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelTssNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceSentinelTssNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceSentinelTssNeighbourhoodFeature</code> <code>DistanceSentinelTssNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceSentinelTssNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceSentinelTssNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceSentinelTssNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromTss\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceTssMeanFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceTssMeanFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Average distance of all tagging variants to gene TSS.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceTssMeanFeature(L2GFeature):\n    \"\"\"Average distance of all tagging variants to gene TSS.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"distanceTssMean\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceTssMeanFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceTssMeanFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceTssMeanFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromTss\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ).withColumn(\n                    cls.feature_name,\n                    f.when(f.col(cls.feature_name) &lt; 0, f.lit(0.0)).otherwise(\n                        f.col(cls.feature_name)\n                    ),\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceTssMeanFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceTssMeanFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceTssMeanFeature</code> <code>DistanceTssMeanFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceTssMeanFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceTssMeanFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceTssMeanFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromTss\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ).withColumn(\n                cls.feature_name,\n                f.when(f.col(cls.feature_name) &lt; 0, f.lit(0.0)).otherwise(\n                    f.col(cls.feature_name)\n                ),\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceTssMeanNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceTssMeanNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Minimum mean distance to TSS for all genes in the vicinity of a studyLocus.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceTssMeanNeighbourhoodFeature(L2GFeature):\n    \"\"\"Minimum mean distance to TSS for all genes in the vicinity of a studyLocus.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"distanceTssMeanNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceTssMeanNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceTssMeanNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceTssMeanNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromTss\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceTssMeanNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceTssMeanNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceTssMeanNeighbourhoodFeature</code> <code>DistanceTssMeanNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceTssMeanNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceTssMeanNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceTssMeanNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromTss\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance between the sentinel variant and the footprint of a gene.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceSentinelFootprintFeature(L2GFeature):\n    \"\"\"Distance between the sentinel variant and the footprint of a gene.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"distanceSentinelFootprint\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceSentinelFootprintFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceSentinelFootprintFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceSentinelFootprintFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromFootprint\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceSentinelFootprintFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceSentinelFootprintFeature</code> <code>DistanceSentinelFootprintFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceSentinelFootprintFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceSentinelFootprintFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceSentinelFootprintFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromFootprint\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance between the sentinel variant and a gene footprint as a relation of the distnace with all the genes in the vicinity of a studyLocus. This is not weighted by the causal probability.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceSentinelFootprintNeighbourhoodFeature(L2GFeature):\n    \"\"\"Distance between the sentinel variant and a gene footprint as a relation of the distnace with all the genes in the vicinity of a studyLocus. This is not weighted by the causal probability.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"distanceSentinelFootprintNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceSentinelFootprintNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceSentinelFootprintNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceSentinelFootprintNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromFootprint\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceSentinelFootprintNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceSentinelFootprintNeighbourhoodFeature</code> <code>DistanceSentinelFootprintNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceSentinelFootprintNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceSentinelFootprintNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceSentinelFootprintNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromFootprint\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceFootprintMeanFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceFootprintMeanFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Average distance of all tagging variants to the footprint of a gene.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceFootprintMeanFeature(L2GFeature):\n    \"\"\"Average distance of all tagging variants to the footprint of a gene.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"distanceFootprintMean\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceFootprintMeanFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceFootprintMeanFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceFootprintMeanFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromFootprint\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ).withColumn(\n                    cls.feature_name,\n                    f.when(f.col(cls.feature_name) &lt; 0, f.lit(0.0)).otherwise(\n                        f.col(cls.feature_name)\n                    ),\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceFootprintMeanFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceFootprintMeanFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceFootprintMeanFeature</code> <code>DistanceFootprintMeanFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceFootprintMeanFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceFootprintMeanFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceFootprintMeanFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromFootprint\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ).withColumn(\n                cls.feature_name,\n                f.when(f.col(cls.feature_name) &lt; 0, f.lit(0.0)).otherwise(\n                    f.col(cls.feature_name)\n                ),\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceFootprintMeanNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceFootprintMeanNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Minimum mean distance to footprint for all genes in the vicinity of a studyLocus.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceFootprintMeanNeighbourhoodFeature(L2GFeature):\n    \"\"\"Minimum mean distance to footprint for all genes in the vicinity of a studyLocus.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"distanceFootprintMeanNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceFootprintMeanNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceFootprintMeanNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceFootprintMeanNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromFootprint\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceFootprintMeanNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceFootprintMeanNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceFootprintMeanNeighbourhoodFeature</code> <code>DistanceFootprintMeanNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceFootprintMeanNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceFootprintMeanNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceFootprintMeanNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromFootprint\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#common-logic","title":"Common logic","text":""},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.common_distance_feature_logic","title":"<code>gentropy.dataset.l2g_features.distance.common_distance_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, variant_index: VariantIndex, feature_name: str, distance_type: str, genomic_window: int = 500000) -&gt; DataFrame</code>","text":"<p>Calculate the distance feature that correlates a variant in a credible set with a gene.</p> <p>The distance is weighted by the posterior probability of the variant to factor in its contribution to the trait when we look at the average distance score for all variants in the credible set.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>The dataset containing distance to gene information</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <code>distance_type</code> <code>str</code> <p>The type of distance to gene</p> required <code>genomic_window</code> <code>int</code> <p>The maximum window size to consider</p> <code>500000</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>def common_distance_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    variant_index: VariantIndex,\n    feature_name: str,\n    distance_type: str,\n    genomic_window: int = 500_000,\n) -&gt; DataFrame:\n    \"\"\"Calculate the distance feature that correlates a variant in a credible set with a gene.\n\n    The distance is weighted by the posterior probability of the variant to factor in its contribution to the trait when we look at the average distance score for all variants in the credible set.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        variant_index (VariantIndex): The dataset containing distance to gene information\n        feature_name (str): The name of the feature\n        distance_type (str): The type of distance to gene\n        genomic_window (int): The maximum window size to consider\n\n    Returns:\n        DataFrame: Feature dataset\n    \"\"\"\n    distances_dataset = variant_index.get_distance_to_gene(distance_type=distance_type)\n    if \"Mean\" in feature_name:\n        # Weighting by the SNP contribution is only applied when we are averaging all distances\n        df = study_loci_to_annotate.df.withColumn(\n            \"variantInLocus\", f.explode_outer(\"locus\")\n        ).select(\n            \"studyLocusId\",\n            f.col(\"variantInLocus.variantId\").alias(\"variantId\"),\n            f.col(\"variantInLocus.posteriorProbability\").alias(\"posteriorProbability\"),\n        )\n        distance_score_expr = (\n            f.lit(genomic_window) - f.col(distance_type) + f.lit(1)\n        ) * f.col(\"posteriorProbability\")\n        agg_expr = f.sum(f.col(\"distance_score\"))\n    elif \"Sentinel\" in feature_name:\n        df = study_loci_to_annotate.df.select(\"studyLocusId\", \"variantId\")\n        # For minimum distances we calculate the unweighted distance between the sentinel (lead) and the gene.\n        distance_score_expr = f.lit(genomic_window) - f.col(distance_type) + f.lit(1)\n        agg_expr = f.first(f.col(\"distance_score\"))\n    return (\n        df.join(\n            distances_dataset.withColumnRenamed(\"targetId\", \"geneId\"),\n            on=\"variantId\",\n            how=\"inner\",\n        )\n        .withColumn(\n            \"distance_score\",\n            distance_score_expr,\n        )\n        .groupBy(\"studyLocusId\", \"geneId\")\n        .agg(agg_expr.alias(\"distance_score_agg\"))\n        .withColumn(\n            feature_name,\n            f.log10(f.col(\"distance_score_agg\")) / f.log10(f.lit(genomic_window + 1)),\n        )\n        .drop(\"distance_score_agg\")\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.common_neighbourhood_distance_feature_logic","title":"<code>gentropy.dataset.l2g_features.distance.common_neighbourhood_distance_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, variant_index: VariantIndex, feature_name: str, distance_type: str, target_index: TargetIndex, genomic_window: int = 500000) -&gt; DataFrame</code>","text":"<p>Calculate the distance feature that correlates any variant in a credible set with any protein coding gene nearby the locus. The distance is weighted by the posterior probability of the variant to factor in its contribution to the trait.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>The dataset containing distance to gene information</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <code>distance_type</code> <code>str</code> <p>The type of distance to gene</p> required <code>target_index</code> <code>TargetIndex</code> <p>The dataset containing gene information</p> required <code>genomic_window</code> <code>int</code> <p>The maximum window size to consider</p> <code>500000</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>def common_neighbourhood_distance_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    variant_index: VariantIndex,\n    feature_name: str,\n    distance_type: str,\n    target_index: TargetIndex,\n    genomic_window: int = 500_000,\n) -&gt; DataFrame:\n    \"\"\"Calculate the distance feature that correlates any variant in a credible set with any protein coding gene nearby the locus. The distance is weighted by the posterior probability of the variant to factor in its contribution to the trait.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        variant_index (VariantIndex): The dataset containing distance to gene information\n        feature_name (str): The name of the feature\n        distance_type (str): The type of distance to gene\n        target_index (TargetIndex): The dataset containing gene information\n        genomic_window (int): The maximum window size to consider\n\n    Returns:\n        DataFrame: Feature dataset\n    \"\"\"\n    local_feature_name = feature_name.replace(\"Neighbourhood\", \"\")\n    # First compute mean distances to a gene\n    local_metric = common_distance_feature_logic(\n        study_loci_to_annotate,\n        feature_name=local_feature_name,\n        distance_type=distance_type,\n        variant_index=variant_index,\n        genomic_window=genomic_window,\n    )\n    return (\n        # Then compute mean distance in the vicinity (feature will be the same for any gene associated with a studyLocus)\n        local_metric.join(\n            target_index.df.filter(f.col(\"biotype\") == \"protein_coding\").select(f.col(\"id\").alias(\"geneId\")),\n            \"geneId\",\n            \"inner\",\n        )\n        .withColumn(\n            \"regional_max\",\n            f.max(local_feature_name).over(Window.partitionBy(\"studyLocusId\")),\n        )\n        .withColumn(\n            feature_name,\n            f.when(\n                (f.col(\"regional_max\").isNotNull()) &amp; (f.col(\"regional_max\") != 0.0),\n                f.col(local_feature_name)\n                / f.coalesce(f.col(\"regional_max\"), f.lit(0.0)),\n            ).otherwise(f.lit(0.0)),\n        )\n        .withColumn(\n            feature_name,\n            f.when(f.col(feature_name) &lt; 0, f.lit(0.0))\n            .when(f.col(feature_name) &gt; 1, f.lit(1.0))\n            .otherwise(f.col(feature_name)),\n        )\n        .drop(\"regional_max\", local_feature_name)\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/","title":"Other features","text":""},{"location":"python_api/datasets/l2g_features/other/#list-of-features","title":"List of features","text":""},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.GeneCountFeature","title":"<code>gentropy.dataset.l2g_features.other.GeneCountFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Counts the number of genes within a specified window size from the study locus.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>class GeneCountFeature(L2GFeature):\n    \"\"\"Counts the number of genes within a specified window size from the study locus.\"\"\"\n\n    feature_dependency_type = TargetIndex\n    feature_name = \"geneCount500kb\"\n\n    @classmethod\n    def compute(\n        cls: type[GeneCountFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; GeneCountFeature:\n        \"\"\"Computes the gene count feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary containing dependencies, with target index and window size\n\n        Returns:\n            GeneCountFeature: Feature dataset\n        \"\"\"\n        genomic_window = 500000\n        gene_count_df = common_genecount_feature_logic(\n            study_loci_to_annotate=study_loci_to_annotate,\n            feature_name=cls.feature_name,\n            genomic_window=genomic_window,\n            **feature_dependency,\n        )\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                gene_count_df,\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.GeneCountFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; GeneCountFeature</code>  <code>classmethod</code>","text":"<p>Computes the gene count feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary containing dependencies, with target index and window size</p> required <p>Returns:</p> Name Type Description <code>GeneCountFeature</code> <code>GeneCountFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[GeneCountFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; GeneCountFeature:\n    \"\"\"Computes the gene count feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary containing dependencies, with target index and window size\n\n    Returns:\n        GeneCountFeature: Feature dataset\n    \"\"\"\n    genomic_window = 500000\n    gene_count_df = common_genecount_feature_logic(\n        study_loci_to_annotate=study_loci_to_annotate,\n        feature_name=cls.feature_name,\n        genomic_window=genomic_window,\n        **feature_dependency,\n    )\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            gene_count_df,\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.ProteinGeneCountFeature","title":"<code>gentropy.dataset.l2g_features.other.ProteinGeneCountFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Counts the number of protein coding genes within a specified window size from the study locus.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>class ProteinGeneCountFeature(L2GFeature):\n    \"\"\"Counts the number of protein coding genes within a specified window size from the study locus.\"\"\"\n\n    feature_dependency_type = TargetIndex\n    feature_name = \"proteinGeneCount500kb\"\n\n    @classmethod\n    def compute(\n        cls: type[ProteinGeneCountFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; ProteinGeneCountFeature:\n        \"\"\"Computes the gene count feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary containing dependencies, with target index and window size\n\n        Returns:\n            ProteinGeneCountFeature: Feature dataset\n        \"\"\"\n        genomic_window = 500000\n        gene_count_df = common_genecount_feature_logic(\n            study_loci_to_annotate=study_loci_to_annotate,\n            feature_name=cls.feature_name,\n            genomic_window=genomic_window,\n            protein_coding_only=True,\n            **feature_dependency,\n        )\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                gene_count_df,\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.ProteinGeneCountFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; ProteinGeneCountFeature</code>  <code>classmethod</code>","text":"<p>Computes the gene count feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary containing dependencies, with target index and window size</p> required <p>Returns:</p> Name Type Description <code>ProteinGeneCountFeature</code> <code>ProteinGeneCountFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[ProteinGeneCountFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; ProteinGeneCountFeature:\n    \"\"\"Computes the gene count feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary containing dependencies, with target index and window size\n\n    Returns:\n        ProteinGeneCountFeature: Feature dataset\n    \"\"\"\n    genomic_window = 500000\n    gene_count_df = common_genecount_feature_logic(\n        study_loci_to_annotate=study_loci_to_annotate,\n        feature_name=cls.feature_name,\n        genomic_window=genomic_window,\n        protein_coding_only=True,\n        **feature_dependency,\n    )\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            gene_count_df,\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.ProteinCodingFeature","title":"<code>gentropy.dataset.l2g_features.other.ProteinCodingFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Indicates whether a gene is protein-coding within a specified window size from the study locus.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>class ProteinCodingFeature(L2GFeature):\n    \"\"\"Indicates whether a gene is protein-coding within a specified window size from the study locus.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"isProteinCoding\"\n\n    @classmethod\n    def compute(\n        cls: type[ProteinCodingFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; ProteinCodingFeature:\n        \"\"\"Computes the protein coding feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary containing dependencies, including variant index\n\n        Returns:\n            ProteinCodingFeature: Feature dataset with 1 if the gene is protein-coding, 0 otherwise\n        \"\"\"\n        genomic_window = 500_000\n        protein_coding_df = is_protein_coding_feature_logic(\n            study_loci_to_annotate=study_loci_to_annotate,\n            feature_name=cls.feature_name,\n            genomic_window=genomic_window,\n            **feature_dependency,\n        )\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                protein_coding_df,\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.ProteinCodingFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; ProteinCodingFeature</code>  <code>classmethod</code>","text":"<p>Computes the protein coding feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary containing dependencies, including variant index</p> required <p>Returns:</p> Name Type Description <code>ProteinCodingFeature</code> <code>ProteinCodingFeature</code> <p>Feature dataset with 1 if the gene is protein-coding, 0 otherwise</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[ProteinCodingFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; ProteinCodingFeature:\n    \"\"\"Computes the protein coding feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary containing dependencies, including variant index\n\n    Returns:\n        ProteinCodingFeature: Feature dataset with 1 if the gene is protein-coding, 0 otherwise\n    \"\"\"\n    genomic_window = 500_000\n    protein_coding_df = is_protein_coding_feature_logic(\n        study_loci_to_annotate=study_loci_to_annotate,\n        feature_name=cls.feature_name,\n        genomic_window=genomic_window,\n        **feature_dependency,\n    )\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            protein_coding_df,\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.CredibleSetConfidenceFeature","title":"<code>gentropy.dataset.l2g_features.other.CredibleSetConfidenceFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance of the sentinel variant to gene TSS. This is not weighted by the causal probability.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>class CredibleSetConfidenceFeature(L2GFeature):\n    \"\"\"Distance of the sentinel variant to gene TSS. This is not weighted by the causal probability.\"\"\"\n\n    feature_dependency_type = [StudyLocus, VariantIndex]\n    feature_name = \"credibleSetConfidence\"\n\n    @classmethod\n    def compute(\n        cls: type[CredibleSetConfidenceFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; CredibleSetConfidenceFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            CredibleSetConfidenceFeature: Feature dataset\n        \"\"\"\n        full_credible_set = feature_dependency[\"study_locus\"].df.select(\n            \"studyLocusId\",\n            \"studyId\",\n            f.explode(\"locus.variantId\").alias(\"variantId\"),\n            cls.score_credible_set_confidence(f.col(\"confidence\")).alias(\n                cls.feature_name\n            ),\n        )\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                (\n                    study_loci_to_annotate.df.drop(\"studyLocusId\")\n                    # Annotate genes\n                    .join(\n                        feature_dependency[\"variant_index\"].df.select(\n                            \"variantId\",\n                            f.explode(\"transcriptConsequences.targetId\").alias(\n                                \"geneId\"\n                            ),\n                        ),\n                        on=\"variantId\",\n                        how=\"inner\",\n                    )\n                    # Annotate credible set confidence\n                    .join(full_credible_set, [\"variantId\", \"studyId\"])\n                    .select(\"studyLocusId\", \"geneId\", cls.feature_name)\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n\n    @classmethod\n    def score_credible_set_confidence(\n        cls: type[CredibleSetConfidenceFeature],\n        confidence_column: Column,\n    ) -&gt; Column:\n        \"\"\"Expression that assigns a score to the credible set confidence.\n\n        Args:\n            confidence_column (Column): Confidence column in the StudyLocus object\n\n        Returns:\n            Column: A confidence score between 0 and 1\n        \"\"\"\n        return (\n            f.when(\n                f.col(\"confidence\")\n                == CredibleSetConfidenceClasses.FINEMAPPED_IN_SAMPLE_LD.value,\n                f.lit(1.0),\n            )\n            .when(\n                f.col(\"confidence\")\n                == CredibleSetConfidenceClasses.FINEMAPPED_OUT_OF_SAMPLE_LD.value,\n                f.lit(0.75),\n            )\n            .when(\n                f.col(\"confidence\")\n                == CredibleSetConfidenceClasses.PICSED_SUMMARY_STATS.value,\n                f.lit(0.5),\n            )\n            .when(\n                f.col(\"confidence\")\n                == CredibleSetConfidenceClasses.PICSED_TOP_HIT.value,\n                f.lit(0.25),\n            )\n            .when(\n                f.col(\"confidence\") == CredibleSetConfidenceClasses.UNKNOWN.value,\n                f.lit(0.0),\n            )\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.CredibleSetConfidenceFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; CredibleSetConfidenceFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>CredibleSetConfidenceFeature</code> <code>CredibleSetConfidenceFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[CredibleSetConfidenceFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; CredibleSetConfidenceFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        CredibleSetConfidenceFeature: Feature dataset\n    \"\"\"\n    full_credible_set = feature_dependency[\"study_locus\"].df.select(\n        \"studyLocusId\",\n        \"studyId\",\n        f.explode(\"locus.variantId\").alias(\"variantId\"),\n        cls.score_credible_set_confidence(f.col(\"confidence\")).alias(\n            cls.feature_name\n        ),\n    )\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            (\n                study_loci_to_annotate.df.drop(\"studyLocusId\")\n                # Annotate genes\n                .join(\n                    feature_dependency[\"variant_index\"].df.select(\n                        \"variantId\",\n                        f.explode(\"transcriptConsequences.targetId\").alias(\n                            \"geneId\"\n                        ),\n                    ),\n                    on=\"variantId\",\n                    how=\"inner\",\n                )\n                # Annotate credible set confidence\n                .join(full_credible_set, [\"variantId\", \"studyId\"])\n                .select(\"studyLocusId\", \"geneId\", cls.feature_name)\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.CredibleSetConfidenceFeature.score_credible_set_confidence","title":"<code>score_credible_set_confidence(confidence_column: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Expression that assigns a score to the credible set confidence.</p> <p>Parameters:</p> Name Type Description Default <code>confidence_column</code> <code>Column</code> <p>Confidence column in the StudyLocus object</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A confidence score between 0 and 1</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef score_credible_set_confidence(\n    cls: type[CredibleSetConfidenceFeature],\n    confidence_column: Column,\n) -&gt; Column:\n    \"\"\"Expression that assigns a score to the credible set confidence.\n\n    Args:\n        confidence_column (Column): Confidence column in the StudyLocus object\n\n    Returns:\n        Column: A confidence score between 0 and 1\n    \"\"\"\n    return (\n        f.when(\n            f.col(\"confidence\")\n            == CredibleSetConfidenceClasses.FINEMAPPED_IN_SAMPLE_LD.value,\n            f.lit(1.0),\n        )\n        .when(\n            f.col(\"confidence\")\n            == CredibleSetConfidenceClasses.FINEMAPPED_OUT_OF_SAMPLE_LD.value,\n            f.lit(0.75),\n        )\n        .when(\n            f.col(\"confidence\")\n            == CredibleSetConfidenceClasses.PICSED_SUMMARY_STATS.value,\n            f.lit(0.5),\n        )\n        .when(\n            f.col(\"confidence\")\n            == CredibleSetConfidenceClasses.PICSED_TOP_HIT.value,\n            f.lit(0.25),\n        )\n        .when(\n            f.col(\"confidence\") == CredibleSetConfidenceClasses.UNKNOWN.value,\n            f.lit(0.0),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#common-logic","title":"Common logic","text":""},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.common_genecount_feature_logic","title":"<code>gentropy.dataset.l2g_features.other.common_genecount_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, target_index: TargetIndex, feature_name: str, genomic_window: int, protein_coding_only: bool = False) -&gt; DataFrame</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>target_index</code> <code>TargetIndex</code> <p>Dataset containing information related to all genes in release.</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <code>genomic_window</code> <code>int</code> <p>The maximum window size to consider</p> required <code>protein_coding_only</code> <code>bool</code> <p>Whether to only consider protein coding genes in calculation.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>def common_genecount_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    target_index: TargetIndex,\n    feature_name: str,\n    genomic_window: int,\n    protein_coding_only: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci\n            that will be used for annotation\n        target_index (TargetIndex): Dataset containing information related to all genes in release.\n        feature_name (str): The name of the feature\n        genomic_window (int): The maximum window size to consider\n        protein_coding_only (bool): Whether to only consider protein coding genes in calculation.\n\n    Returns:\n            DataFrame: Feature dataset\n    \"\"\"\n    study_loci_window = (\n        study_loci_to_annotate.df.withColumn(\n            \"window_start\", f.col(\"position\") - (genomic_window / 2)\n        )\n        .withColumn(\"window_end\", f.col(\"position\") + (genomic_window / 2))\n        .withColumnRenamed(\"chromosome\", \"SL_chromosome\")\n    )\n    target_index_filter = target_index.df\n\n    if protein_coding_only:\n        target_index_filter = target_index_filter.filter(\n            f.col(\"biotype\") == \"protein_coding\"\n        )\n\n    distinct_gene_counts = (\n        study_loci_window.join(\n            target_index_filter.alias(\"genes\"),\n            on=(\n                (f.col(\"SL_chromosome\") == f.col(\"genes.genomicLocation.chromosome\"))\n                &amp; (f.col(\"genes.tss\") &gt;= f.col(\"window_start\"))\n                &amp; (f.col(\"genes.tss\") &lt;= f.col(\"window_end\"))\n            ),\n            how=\"inner\",\n        )\n        .groupBy(\"studyLocusId\")\n        .agg(f.approx_count_distinct(f.col(\"id\").alias(\"geneId\")).alias(feature_name))\n    )\n\n    return (\n        study_loci_window.join(\n            target_index_filter.alias(\"genes\"),\n            on=(\n                (f.col(\"SL_chromosome\") == f.col(\"genes.genomicLocation.chromosome\"))\n                &amp; (f.col(\"genes.tss\") &gt;= f.col(\"window_start\"))\n                &amp; (f.col(\"genes.tss\") &lt;= f.col(\"window_end\"))\n            ),\n            how=\"inner\",\n        )\n        .join(distinct_gene_counts, on=\"studyLocusId\", how=\"inner\")\n        .select(\"studyLocusId\", f.col(\"id\").alias(\"geneId\"), feature_name)\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.is_protein_coding_feature_logic","title":"<code>gentropy.dataset.l2g_features.other.is_protein_coding_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, variant_index: VariantIndex, feature_name: str, genomic_window: int = 500000) -&gt; DataFrame</code>","text":"<p>Computes the feature to indicate if a gene is protein-coding or not.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Dataset containing information related to all overlapping genes within a genomic window.</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <code>genomic_window</code> <code>int</code> <p>The window size around the locus to consider. Defaults to its maximum value: 500kb up and downstream the locus</p> <code>500000</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset, with 1 if the gene is protein-coding, 0 if not.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>when provided <code>genomic_window</code> is more or equal to 500kb.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>def is_protein_coding_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    variant_index: VariantIndex,\n    feature_name: str,\n    genomic_window: int = 500_000,\n) -&gt; DataFrame:\n    \"\"\"Computes the feature to indicate if a gene is protein-coding or not.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci\n            that will be used for annotation\n        variant_index (VariantIndex): Dataset containing information related to all overlapping genes within a genomic window.\n        feature_name (str): The name of the feature\n        genomic_window (int): The window size around the locus to consider. Defaults to its maximum value: 500kb up and downstream the locus\n\n    Returns:\n        DataFrame: Feature dataset, with 1 if the gene is protein-coding, 0 if not.\n\n    Raises:\n        AssertionError: when provided `genomic_window` is more or equal to 500kb.\n    \"\"\"\n    assert genomic_window &lt;= 500_000, \"Genomic window must be less than 500kb.\"\n    genes_in_window = (\n        variant_index.df.withColumn(\n            \"transcriptConsequence\", f.explode(\"transcriptConsequences\")\n        )\n        .select(\n            \"variantId\",\n            f.col(\"transcriptConsequence.targetId\").alias(\"geneId\"),\n            f.col(\"transcriptConsequence.biotype\").alias(\"biotype\"),\n            f.col(\"transcriptConsequence.distanceFromFootprint\").alias(\n                \"distanceFromFootprint\"\n            ),\n        )\n        .filter(f.col(\"distanceFromFootprint\") &lt;= genomic_window)\n    )\n    if isinstance(study_loci_to_annotate, StudyLocus):\n        variants_df = study_loci_to_annotate.df.select(\n            f.explode_outer(\"locus.variantId\").alias(\"variantId\"),\n            \"studyLocusId\",\n        ).filter(f.col(\"variantId\").isNotNull())\n    elif isinstance(study_loci_to_annotate, L2GGoldStandard):\n        variants_df = study_loci_to_annotate.df.select(\"studyLocusId\", \"variantId\")\n    return (\n        # Annotate all genes in the window of a locus\n        variants_df.join(\n            genes_in_window,\n            on=\"variantId\",\n        )\n        # Apply flag across all variants in the locus\n        .withColumn(\n            feature_name,\n            f.when(f.col(\"biotype\") == \"protein_coding\", f.lit(1.0)).otherwise(\n                f.lit(0.0)\n            ),\n        )\n        .select(\"studyLocusId\", \"geneId\", feature_name)\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/","title":"From VEP","text":""},{"location":"python_api/datasets/l2g_features/vep/#list-of-features","title":"List of features","text":""},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMeanFeature","title":"<code>gentropy.dataset.l2g_features.vep.VepMeanFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Average functional consequence score among all variants in a credible set for a studyLocus/gene.</p> <p>The mean severity score is weighted by the posterior probability of each variant.</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>class VepMeanFeature(L2GFeature):\n    \"\"\"Average functional consequence score among all variants in a credible set for a studyLocus/gene.\n\n    The mean severity score is weighted by the posterior probability of each variant.\n    \"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"vepMean\"\n\n    @classmethod\n    def compute(\n        cls: type[VepMeanFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; VepMeanFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n        Returns:\n            VepMeanFeature: Feature dataset\n        \"\"\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_vep_feature_logic(\n                    study_loci_to_annotate=study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMeanFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; VepMeanFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the functional consequence information</p> required <p>Returns:</p> Name Type Description <code>VepMeanFeature</code> <code>VepMeanFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[VepMeanFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; VepMeanFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n    Returns:\n        VepMeanFeature: Feature dataset\n    \"\"\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_vep_feature_logic(\n                study_loci_to_annotate=study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMeanNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.vep.VepMeanNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Mean functional consequence score among all variants in a credible set for a studyLocus/gene relative to the mean VEP score across all protein coding genes in the vicinity.</p> <p>The mean severity score is weighted by the posterior probability of each variant.</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>class VepMeanNeighbourhoodFeature(L2GFeature):\n    \"\"\"Mean functional consequence score among all variants in a credible set for a studyLocus/gene relative to the mean VEP score across all protein coding genes in the vicinity.\n\n    The mean severity score is weighted by the posterior probability of each variant.\n    \"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"vepMeanNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[VepMeanNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; VepMeanNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n        Returns:\n            VepMeanNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_vep_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMeanNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; VepMeanNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the functional consequence information</p> required <p>Returns:</p> Name Type Description <code>VepMeanNeighbourhoodFeature</code> <code>VepMeanNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[VepMeanNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; VepMeanNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n    Returns:\n        VepMeanNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_vep_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMaximumFeature","title":"<code>gentropy.dataset.l2g_features.vep.VepMaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Maximum functional consequence score among all variants in a credible set for a studyLocus/gene.</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>class VepMaximumFeature(L2GFeature):\n    \"\"\"Maximum functional consequence score among all variants in a credible set for a studyLocus/gene.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"vepMaximum\"\n\n    @classmethod\n    def compute(\n        cls: type[VepMaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; VepMaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n        Returns:\n            VepMaximumFeature: Feature dataset\n        \"\"\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_vep_feature_logic(\n                    study_loci_to_annotate=study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; VepMaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the functional consequence information</p> required <p>Returns:</p> Name Type Description <code>VepMaximumFeature</code> <code>VepMaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[VepMaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; VepMaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n    Returns:\n        VepMaximumFeature: Feature dataset\n    \"\"\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_vep_feature_logic(\n                study_loci_to_annotate=study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.vep.VepMaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Maximum functional consequence score among all variants in a credible set for a studyLocus/gene relative to the mean VEP score across all protein coding genes in the vicinity.</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>class VepMaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Maximum functional consequence score among all variants in a credible set for a studyLocus/gene relative to the mean VEP score across all protein coding genes in the vicinity.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"vepMaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[VepMaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; VepMaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n        Returns:\n            VepMaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_vep_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; VepMaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the functional consequence information</p> required <p>Returns:</p> Name Type Description <code>VepMaximumNeighbourhoodFeature</code> <code>VepMaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[VepMaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; VepMaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n    Returns:\n        VepMaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_vep_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#common-logic","title":"Common logic","text":""},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.common_vep_feature_logic","title":"<code>gentropy.dataset.l2g_features.vep.common_vep_feature_logic(study_loci_to_annotate: L2GGoldStandard | StudyLocus, *, variant_index: VariantIndex, feature_name: str) -&gt; DataFrame</code>","text":"<p>Extracts variant severity score computed from VEP.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>L2GGoldStandard | StudyLocus</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>The dataset containing functional consequence information</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>def common_vep_feature_logic(\n    study_loci_to_annotate: L2GGoldStandard | StudyLocus,\n    *,\n    variant_index: VariantIndex,\n    feature_name: str,\n) -&gt; DataFrame:\n    \"\"\"Extracts variant severity score computed from VEP.\n\n    Args:\n        study_loci_to_annotate (L2GGoldStandard | StudyLocus): The dataset containing study loci that will be used for annotation\n        variant_index (VariantIndex): The dataset containing functional consequence information\n        feature_name (str): The name of the feature\n\n    Returns:\n        DataFrame: Feature dataset\n    \"\"\"\n    # Variant/Target/Severity dataframe\n    consequences_dataset = variant_index.df.withColumn(\n        \"transcriptConsequence\", f.explode(\"transcriptConsequences\")\n    ).select(\n        \"variantId\",\n        f.col(\"transcriptConsequence.targetId\").alias(\"geneId\"),\n        f.col(\"transcriptConsequence.consequenceScore\").alias(\"severityScore\"),\n    )\n    if isinstance(study_loci_to_annotate, StudyLocus):\n        variants_df = (\n            study_loci_to_annotate.df.withColumn(\n                \"variantInLocus\", f.explode_outer(\"locus\")\n            )\n            .select(\n                \"studyLocusId\",\n                f.col(\"variantInLocus.variantId\").alias(\"variantId\"),\n                f.col(\"variantInLocus.posteriorProbability\").alias(\n                    \"posteriorProbability\"\n                ),\n            )\n            .join(consequences_dataset, \"variantId\")\n        )\n    elif isinstance(study_loci_to_annotate, L2GGoldStandard):\n        variants_df = study_loci_to_annotate.df.select(\n            \"studyLocusId\", \"variantId\", f.lit(1.0).alias(\"posteriorProbability\")\n        ).join(consequences_dataset, \"variantId\")\n\n    if \"Maximum\" in feature_name:\n        agg_expr = f.max(\"severityScore\")\n    elif \"Mean\" in feature_name:\n        variants_df = variants_df.withColumn(\n            \"weightedScore\", f.col(\"severityScore\") * f.col(\"posteriorProbability\")\n        )\n        agg_expr = f.mean(\"weightedScore\")\n    return variants_df.groupBy(\"studyLocusId\", \"geneId\").agg(\n        agg_expr.alias(feature_name)\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.common_neighbourhood_vep_feature_logic","title":"<code>gentropy.dataset.l2g_features.vep.common_neighbourhood_vep_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, variant_index: VariantIndex, target_index: TargetIndex, feature_name: str) -&gt; DataFrame</code>","text":"<p>Extracts variant severity score computed from VEP for any gene, based on what is the max score for protein coding genes that are nearby the locus.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>The dataset containing functional consequence information</p> required <code>target_index</code> <code>TargetIndex</code> <p>The dataset containing the gene biotype</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>def common_neighbourhood_vep_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    variant_index: VariantIndex,\n    target_index: TargetIndex,\n    feature_name: str,\n) -&gt; DataFrame:\n    \"\"\"Extracts variant severity score computed from VEP for any gene, based on what is the max score for protein coding genes that are nearby the locus.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        variant_index (VariantIndex): The dataset containing functional consequence information\n        target_index (TargetIndex): The dataset containing the gene biotype\n        feature_name (str): The name of the feature\n\n    Returns:\n        DataFrame: Feature dataset\n    \"\"\"\n    local_feature_name = feature_name.replace(\"Neighbourhood\", \"\")\n    local_metric = common_vep_feature_logic(\n        study_loci_to_annotate,\n        feature_name=local_feature_name,\n        variant_index=variant_index,\n    )\n    return (\n        local_metric\n        # Compute average score in the vicinity (feature will be the same for any gene associated with a studyLocus)\n        # (non protein coding genes in the vicinity are excluded see #3552)\n        .join(\n            target_index.df.filter(f.col(\"biotype\") == \"protein_coding\").select(f.col(\"id\").alias(\"geneId\")),\n            \"geneId\",\n            \"inner\",\n        )\n        .withColumn(\n            \"regional_max\",\n            f.max(local_feature_name).over(Window.partitionBy(\"studyLocusId\")),\n        )\n        .withColumn(\n            feature_name,\n            f.when(\n                (f.col(\"regional_max\").isNotNull()) &amp; (f.col(\"regional_max\") != 0.0),\n                f.col(local_feature_name)\n                / f.coalesce(f.col(\"regional_max\"), f.lit(0.0)),\n            ).otherwise(f.lit(0.0)),\n        )\n        .drop(\"regional_max\", local_feature_name)\n    )\n</code></pre>"},{"location":"python_api/datasources/_datasources/","title":"Data Sources","text":"<p>This section contains information about the data source harmonisation tools available in Open Targets Gentropy.</p>"},{"location":"python_api/datasources/_datasources/#gwas-study-sources","title":"GWAS study sources","text":"<ol> <li>GWAS Catalog (with or without full summary statistics)</li> <li>FinnGen</li> <li>UKB PPP (EUR)</li> </ol>"},{"location":"python_api/datasources/_datasources/#molecular-qtls","title":"Molecular QTLs","text":"<ol> <li>GTEx (eQTL catalogue)</li> </ol>"},{"location":"python_api/datasources/_datasources/#interaction-interval-based-experiments","title":"Interaction / Interval-based Experiments","text":"<ol> <li>Intervals-based datasets, informing about the relationships between genetic elements and their functional implications.</li> </ol>"},{"location":"python_api/datasources/_datasources/#variant-annotationvalidation","title":"Variant annotation/validation","text":"<ol> <li>GnomAD v4.0</li> <li>GWAS catalog's harmonisation pipeline</li> <li>Ensembl's Variant Effect Predictor</li> </ol>"},{"location":"python_api/datasources/_datasources/#linkage-disequilibrium","title":"Linkage disequilibrium","text":"<ol> <li>GnomAD v2.1.1 LD matrixes (7 ancestries)</li> </ol>"},{"location":"python_api/datasources/_datasources/#locus-to-gene-gold-standard","title":"Locus-to-gene gold standard","text":"<ol> <li>Open Targets training set</li> </ol>"},{"location":"python_api/datasources/_datasources/#gene-annotation","title":"Gene annotation","text":"<ol> <li>Open Targets Platform Target Dataset (derived from Ensembl)</li> </ol>"},{"location":"python_api/datasources/_datasources/#biological-samples","title":"Biological samples","text":"<ol> <li>Uberon</li> <li>Cell Ontology</li> </ol>"},{"location":"python_api/datasources/biosample_ontologies/_cell_ontology/","title":"Cell Ontology","text":"<p>The Cell Ontology is a structured controlled vocabulary for cell types. It is used to annotate cell types in single-cell RNA-seq data and other omics data.</p>"},{"location":"python_api/datasources/biosample_ontologies/_uberon/","title":"Uberon","text":"<p>The Uberon ontology is a multi-species anatomy ontology that integrates cross-species ontologies into a single ontology.</p>"},{"location":"python_api/datasources/ensembl/_ensembl/","title":"Ensembl annotations","text":"Ensembl <p>Ensembl provides a diverse set of genetic data Gentropy takes advantage of including gene set, and variant annotations.</p>"},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/","title":"Variant effector parser","text":""},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/#gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser","title":"<code>gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser</code>","text":"<p>Collection of methods to parse VEP output in json format.</p> Source code in <code>src/gentropy/datasource/ensembl/vep_parser.py</code> <pre><code>class VariantEffectPredictorParser:\n    \"\"\"Collection of methods to parse VEP output in json format.\"\"\"\n\n    # NOTE: Due to the fact that the comparison of the xrefs is done om the base of rsids\n    # if the field `colocalised_variants` have multiple rsids, this extracting xrefs will result in\n    # an array of xref structs, rather then the struct itself.\n\n    DBXREF_SCHEMA = VariantIndex.get_schema()[\"dbXrefs\"].dataType\n\n    # Schema description of the variant effect object:\n    VARIANT_EFFECT_SCHEMA = get_nested_struct_schema(\n        VariantIndex.get_schema()[\"variantEffect\"]\n    )\n\n    # Schema for the allele frequency column:\n    ALLELE_FREQUENCY_SCHEMA = VariantIndex.get_schema()[\"alleleFrequencies\"].dataType\n\n    # Consequence to sequence ontology map:\n    SEQUENCE_ONTOLOGY_MAP = {\n        item[\"label\"]: item[\"id\"]\n        for item in VariantIndexConfig.consequence_to_pathogenicity_score\n    }\n\n    # Sequence ontology to score map:\n    LABEL_TO_SCORE_MAP = {\n        item[\"label\"]: item[\"score\"]\n        for item in VariantIndexConfig.consequence_to_pathogenicity_score\n    }\n\n    @staticmethod\n    def get_schema() -&gt; t.StructType:\n        \"\"\"Return the schema of the VEP output.\n\n        Returns:\n            t.StructType: VEP output schema.\n\n        Examples:\n            &gt;&gt;&gt; type(VariantEffectPredictorParser.get_schema())\n            &lt;class 'pyspark.sql.types.StructType'&gt;\n        \"\"\"\n        return parse_spark_schema(\"vep_json_output.json\")\n\n    @classmethod\n    def extract_variant_index_from_vep(\n        cls: type[VariantEffectPredictorParser],\n        spark: SparkSession,\n        vep_output_path: str | list[str],\n        hash_threshold: int,\n        **kwargs: bool | float | int | str | None,\n    ) -&gt; VariantIndex:\n        \"\"\"Extract variant index from VEP output.\n\n        Args:\n            spark (SparkSession): Spark session.\n            vep_output_path (str | list[str]): Path to the VEP output.\n            hash_threshold (int): Threshold above which variant identifiers will be hashed.\n            **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.json.\n\n        Returns:\n            VariantIndex: Variant index dataset.\n\n        Raises:\n            ValueError: Failed reading file or if the dataset is empty.\n        \"\"\"\n        # To speed things up and simplify the json structure, read data following an expected schema:\n        vep_schema = cls.get_schema()\n\n        try:\n            vep_data = spark.read.json(vep_output_path, schema=vep_schema, **kwargs)\n        except ValueError as e:\n            raise ValueError(f\"Failed reading file: {vep_output_path}.\") from e\n\n        if vep_data.isEmpty():\n            raise ValueError(f\"The dataset is empty: {vep_output_path}\")\n\n        # Convert to VariantAnnotation dataset:\n        return VariantIndex(\n            _df=VariantEffectPredictorParser.process_vep_output(\n                vep_data, hash_threshold\n            ),\n            _schema=VariantIndex.get_schema(),\n            id_threshold=hash_threshold,\n        )\n\n    @staticmethod\n    def _extract_ensembl_xrefs(colocated_variants: Column) -&gt; Column:\n        \"\"\"Extract rs identifiers and build cross reference to Ensembl's variant page.\n\n        Args:\n            colocated_variants (Column): Colocated variants field from VEP output.\n\n        Returns:\n            Column: List of dbXrefs for rs identifiers.\n        \"\"\"\n        return VariantEffectPredictorParser._generate_dbxrefs(\n            VariantEffectPredictorParser._colocated_variants_to_rsids(\n                colocated_variants\n            ),\n            \"ensembl_variation\",\n        )\n\n    @enforce_schema(DBXREF_SCHEMA)\n    @staticmethod\n    def _generate_dbxrefs(ids: Column, source: str) -&gt; Column:\n        \"\"\"Convert a list of variant identifiers to dbXrefs given the source label.\n\n        Identifiers are cast to strings, then Null values are filtered out of the id list.\n\n        Args:\n            ids (Column): List of variant identifiers.\n            source (str): Source label for the dbXrefs.\n\n        Returns:\n            Column: List of dbXrefs.\n\n        Examples:\n            &gt;&gt;&gt; (\n            ...     spark.createDataFrame([('rs12',),(None,)], ['test_id'])\n            ...     .select(VariantEffectPredictorParser._generate_dbxrefs(f.array(f.col('test_id')), \"ensemblVariation\").alias('col'))\n            ...     .show(truncate=False)\n            ... )\n            +--------------------------+\n            |col                       |\n            +--------------------------+\n            |[{rs12, ensemblVariation}]|\n            |[]                        |\n            +--------------------------+\n            &lt;BLANKLINE&gt;\n            &gt;&gt;&gt; (\n            ...     spark.createDataFrame([('rs12',),(None,)], ['test_id'])\n            ...     .select(VariantEffectPredictorParser._generate_dbxrefs(f.array(f.col('test_id')), \"ensemblVariation\").alias('col'))\n            ...     .first().col[0].asDict()\n            ... )\n            {'id': 'rs12', 'source': 'ensemblVariation'}\n        \"\"\"\n        ids = f.filter(ids, lambda id: id.isNotNull())\n        xref_column = f.transform(\n            ids,\n            lambda id: f.struct(\n                id.cast(t.StringType()).alias(\"id\"), f.lit(source).alias(\"source\")\n            ),\n        )\n\n        return f.when(xref_column.isNull(), f.array()).otherwise(xref_column)\n\n    @staticmethod\n    def _colocated_variants_to_rsids(colocated_variants: Column) -&gt; Column:\n        \"\"\"Extract rs identifiers from the colocated variants VEP field.\n\n        Args:\n            colocated_variants (Column): Colocated variants field from VEP output.\n\n        Returns:\n            Column: List of rs identifiers.\n\n        Examples:\n            &gt;&gt;&gt; data = [('s1', 'rs1'),('s1', 'rs2'),('s1', 'rs3'),('s2', None),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['s','v'])\n            ...    .groupBy('s')\n            ...    .agg(f.collect_list(f.struct(f.col('v').alias('id'))).alias('cv'))\n            ...    .select(VariantEffectPredictorParser._colocated_variants_to_rsids(f.col('cv')).alias('rsIds'),)\n            ...    .show(truncate=False)\n            ... )\n            +---------------+\n            |rsIds          |\n            +---------------+\n            |[rs1, rs2, rs3]|\n            |[NULL]         |\n            +---------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.when(\n            colocated_variants.isNotNull(),\n            f.transform(\n                colocated_variants, lambda variant: variant.getItem(\"id\").alias(\"id\")\n            ),\n        ).alias(\"rsIds\")\n\n    @staticmethod\n    def _extract_omim_xrefs(colocated_variants: Column) -&gt; Column:\n        \"\"\"Build xdbRefs for OMIM identifiers.\n\n        OMIM identifiers are extracted from the colocated variants field, casted to strings and formatted as dbXrefs.\n\n        Args:\n            colocated_variants (Column): Colocated variants field from VEP output.\n\n        Returns:\n            Column: List of dbXrefs for OMIM identifiers.\n\n        Examples:\n            &gt;&gt;&gt; data = [('234234.32', 'rs1', 's1',),(None, 'rs1', 's1',),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['id', 'rsid', 's'])\n            ...    .groupBy('s')\n            ...    .agg(f.collect_list(f.struct(f.struct(f.array(f.col('id')).alias('OMIM')).alias('var_synonyms'),f.col('rsid').alias('id'),),).alias('cv'),).select(VariantEffectPredictorParser._extract_omim_xrefs(f.col('cv')).alias('dbXrefs'))\n            ...    .show(truncate=False)\n            ... )\n            +-------------------+\n            |dbXrefs            |\n            +-------------------+\n            |[{234234#32, omim}]|\n            +-------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        variants_w_omim_ref = f.filter(\n            colocated_variants,\n            lambda variant: variant.getItem(\"var_synonyms\").getItem(\"OMIM\").isNotNull(),\n        )\n\n        omim_var_ids = f.transform(\n            variants_w_omim_ref,\n            lambda var: f.transform(\n                var.getItem(\"var_synonyms\").getItem(\"OMIM\"),\n                lambda var: f.regexp_replace(var.cast(t.StringType()), r\"\\.\", r\"#\"),\n            ),\n        )\n\n        return VariantEffectPredictorParser._generate_dbxrefs(\n            f.flatten(omim_var_ids), \"omim\"\n        )\n\n    @staticmethod\n    def _extract_clinvar_xrefs(colocated_variants: Column) -&gt; Column:\n        \"\"\"Build xdbRefs for VCV ClinVar identifiers.\n\n        ClinVar identifiers are extracted from the colocated variants field.\n        VCV-identifiers are filtered out to generate cross-references.\n\n        Args:\n            colocated_variants (Column): Colocated variants field from VEP output.\n\n        Returns:\n            Column: List of dbXrefs for VCV ClinVar identifiers.\n\n        Examples:\n            &gt;&gt;&gt; data = [('VCV2323,RCV3423', 'rs1', 's1',),(None, 'rs1', 's1',),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['id', 'rsid', 's'])\n            ...    .groupBy('s')\n            ...    .agg(f.collect_list(f.struct(f.struct(f.split(f.col('id'),',').alias('ClinVar')).alias('var_synonyms'),f.col('rsid').alias('id'),),).alias('cv'),).select(VariantEffectPredictorParser._extract_clinvar_xrefs(f.col('cv')).alias('dbXrefs'))\n            ...    .show(truncate=False)\n            ... )\n            +--------------------+\n            |dbXrefs             |\n            +--------------------+\n            |[{VCV2323, clinvar}]|\n            +--------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        variants_w_clinvar_ref = f.filter(\n            colocated_variants,\n            lambda variant: variant.getItem(\"var_synonyms\")\n            .getItem(\"ClinVar\")\n            .isNotNull(),\n        )\n\n        clin_var_ids = f.transform(\n            variants_w_clinvar_ref,\n            lambda var: f.filter(\n                var.getItem(\"var_synonyms\").getItem(\"ClinVar\"),\n                lambda x: x.startswith(\"VCV\"),\n            ),\n        )\n\n        return VariantEffectPredictorParser._generate_dbxrefs(\n            f.flatten(clin_var_ids), \"clinvar\"\n        )\n\n    @staticmethod\n    def _get_most_severe_transcript(\n        transcript_column_name: str, score_field_name: str\n    ) -&gt; Column:\n        \"\"\"Return transcript with the highest in silico predicted score.\n\n        This method assumes the higher the score, the more severe the consequence of the variant is.\n        Hence, by selecting the transcript with the highest score, we are selecting the most severe consequence.\n\n        Args:\n            transcript_column_name (str): Name of the column containing the list of transcripts.\n            score_field_name (str): Name of the field containing the severity score.\n\n        Returns:\n            Column: Most severe transcript.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"v1\", 0.2, 'transcript1'),(\"v1\", None, 'transcript2'),(\"v1\", 0.6, 'transcript3'),(\"v1\", 0.4, 'transcript4'),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['v', 'score', 'transcriptId'])\n            ...    .groupBy('v')\n            ...    .agg(f.collect_list(f.struct(f.col('score'),f.col('transcriptId'))).alias('transcripts'))\n            ...    .select(VariantEffectPredictorParser._get_most_severe_transcript('transcripts', 'score').alias('most_severe_transcript'))\n            ...    .show(truncate=False)\n            ... )\n            +----------------------+\n            |most_severe_transcript|\n            +----------------------+\n            |{0.6, transcript3}    |\n            +----------------------+\n            &lt;BLANKLINE&gt;\n\n        Raises:\n            AssertionError: When `transcript_column_name` is not a string.\n        \"\"\"\n        assert isinstance(\n            transcript_column_name, str\n        ), \"transcript_column_name must be a string and not a column.\"\n        # Order transcripts by severity score:\n        ordered_transcripts = order_array_of_structs_by_field(\n            transcript_column_name, score_field_name\n        )\n\n        # Drop transcripts with no severity score and return the most severe one:\n        return f.filter(\n            ordered_transcripts,\n            lambda transcript: transcript.getItem(score_field_name).isNotNull(),\n        )[0]\n\n    @classmethod\n    @enforce_schema(VARIANT_EFFECT_SCHEMA)\n    def _get_vep_prediction(cls, most_severe_consequence: Column) -&gt; Column:\n        return f.struct(\n            f.lit(\"VEP\").alias(\"method\"),\n            most_severe_consequence.alias(\"assessment\"),\n            map_column_by_dictionary(\n                most_severe_consequence, cls.LABEL_TO_SCORE_MAP\n            ).alias(\"score\"),\n        )\n\n    @staticmethod\n    @enforce_schema(VARIANT_EFFECT_SCHEMA)\n    def _get_max_alpha_missense(transcripts: Column) -&gt; Column:\n        \"\"\"Return the most severe alpha missense prediction from all transcripts.\n\n        This function assumes one variant can fall onto only one gene with alpha-sense prediction available on the canonical transcript.\n\n        Args:\n            transcripts (Column): List of transcripts.\n\n        Returns:\n            Column: Most severe alpha missense prediction.\n\n        Examples:\n        &gt;&gt;&gt; data = [('v1', 0.4, 'assessment 1'), ('v1', None, None), ('v1', None, None),('v2', None, None),]\n        &gt;&gt;&gt; (\n        ...     spark.createDataFrame(data, ['v', 'a', 'b'])\n        ...    .groupBy('v')\n        ...    .agg(f.collect_list(f.struct(f.struct(\n        ...        f.col('a').alias('am_pathogenicity'),\n        ...        f.col('b').alias('am_class')).alias('alphamissense'),\n        ...        f.lit('gene1').alias('gene_id'))).alias('transcripts')\n        ...    )\n        ...    .select(VariantEffectPredictorParser._get_max_alpha_missense(f.col('transcripts')).alias('am'))\n        ...    .show(truncate=False)\n        ... )\n        +-----------------------------------------------------+\n        |am                                                   |\n        +-----------------------------------------------------+\n        |{AlphaMissense, assessment 1, 0.4, NULL, gene1, NULL}|\n        |{AlphaMissense, NULL, NULL, NULL, gene1, NULL}       |\n        +-----------------------------------------------------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        # Extracting transcript with alpha missense values:\n        transcript = f.filter(\n            transcripts,\n            lambda transcript: transcript.getItem(\"alphamissense\").isNotNull(),\n        )[0]\n\n        return f.when(\n            transcript.isNotNull(),\n            f.struct(\n                # Adding method:\n                f.lit(\"AlphaMissense\").alias(\"method\"),\n                # Extracting assessment:\n                transcript.alphamissense.am_class.alias(\"assessment\"),\n                # Extracting score:\n                transcript.alphamissense.am_pathogenicity.cast(t.FloatType()).alias(\n                    \"score\"\n                ),\n                # Adding assessment flag:\n                f.lit(None).cast(t.StringType()).alias(\"assessmentFlag\"),\n                # Extracting target id:\n                transcript.gene_id.alias(\"targetId\"),\n            ),\n        )\n\n    @classmethod\n    @enforce_schema(VARIANT_EFFECT_SCHEMA)\n    def _vep_variant_effect_extractor(\n        cls: type[VariantEffectPredictorParser],\n        transcript_column_name: str,\n        method_name: str,\n        score_column_name: str | None = None,\n        assessment_column_name: str | None = None,\n        assessment_flag_column_name: str | None = None,\n    ) -&gt; Column:\n        \"\"\"Extract variant effect from VEP output.\n\n        Args:\n            transcript_column_name (str): Name of the column containing the list of transcripts.\n            method_name (str): Name of the variant effect.\n            score_column_name (str | None): Name of the column containing the score.\n            assessment_column_name (str | None): Name of the column containing the assessment.\n            assessment_flag_column_name (str | None): Name of the column containing the assessment flag.\n\n        Returns:\n            Column: Variant effect.\n        \"\"\"\n        # Get transcript with the highest score:\n        most_severe_transcript: Column = (\n            # Getting the most severe transcript:\n            VariantEffectPredictorParser._get_most_severe_transcript(\n                transcript_column_name, score_column_name\n            )\n            if score_column_name is not None\n            # If we don't have score, just pick one of the transcript where assessment is available:\n            else f.filter(\n                f.col(transcript_column_name),\n                lambda transcript: transcript.getItem(\n                    assessment_column_name\n                ).isNotNull(),\n            )\n        )\n\n        # Get assessment:\n        assessment = (\n            f.lit(None).cast(t.StringType()).alias(\"assessment\")\n            if assessment_column_name is None\n            else most_severe_transcript.getField(assessment_column_name).alias(\n                \"assessment\"\n            )\n        )\n\n        # Get score:\n        score = (\n            f.lit(None).cast(t.FloatType()).alias(\"score\")\n            if score_column_name is None\n            else most_severe_transcript.getField(score_column_name)\n            .cast(t.FloatType())\n            .alias(\"score\")\n        )\n\n        # Get assessment flag:\n        assessment_flag = (\n            f.lit(None).cast(t.StringType()).alias(\"assessmentFlag\")\n            if assessment_flag_column_name is None\n            else most_severe_transcript.getField(assessment_flag_column_name)\n            .cast(t.StringType())\n            .alias(\"assessmentFlag\")\n        )\n\n        # Extract gene id:\n        gene_id = most_severe_transcript.getItem(\"gene_id\").alias(\"targetId\")\n\n        return f.when(\n            most_severe_transcript.isNotNull(),\n            f.struct(\n                f.lit(method_name).alias(\"method\"),\n                assessment,\n                score,\n                assessment_flag,\n                gene_id,\n            ),\n        )\n\n    @staticmethod\n    def _parser_amino_acid_change(amino_acids: Column, protein_end: Column) -&gt; Column:\n        \"\"\"Convert VEP amino acid change information to one letter code aa substitution code.\n\n        The logic assumes that the amino acid change is given in the format \"from/to\" and the protein end is given also.\n        If any of the information is missing, the amino acid change will be set to None.\n\n        Args:\n            amino_acids (Column): Amino acid change information.\n            protein_end (Column): Protein end information.\n\n        Returns:\n            Column: Amino acid change in one letter code.\n\n        Examples:\n            &gt;&gt;&gt; data = [('A/B', 1),('A/B', None),(None, 1),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['amino_acids', 'protein_end'])\n            ...    .select(VariantEffectPredictorParser._parser_amino_acid_change(f.col('amino_acids'), f.col('protein_end')).alias('amino_acid_change'))\n            ...    .show()\n            ... )\n            +-----------------+\n            |amino_acid_change|\n            +-----------------+\n            |              A1B|\n            |             NULL|\n            |             NULL|\n            +-----------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.when(\n            amino_acids.isNotNull() &amp; protein_end.isNotNull(),\n            f.concat(\n                f.split(amino_acids, r\"\\/\")[0],\n                protein_end,\n                f.split(amino_acids, r\"\\/\")[1],\n            ),\n        ).otherwise(f.lit(None))\n\n    @staticmethod\n    def _collect_uniprot_accessions(trembl: Column, swissprot: Column) -&gt; Column:\n        \"\"\"Flatten arrays containing Uniprot accessions.\n\n        Args:\n            trembl (Column): List of TrEMBL protein accessions.\n            swissprot (Column): List of SwissProt protein accessions.\n\n        Returns:\n            Column: List of unique Uniprot accessions extracted from swissprot and trembl arrays, splitted from version numbers.\n\n        Examples:\n        &gt;&gt;&gt; data = [('v1', [\"sp_1\"], [\"tr_1\"]), ('v1', None, None), ('v1', None, [\"tr_2\"]),]\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame(data, ['v', 'sp', 'tr'])\n        ...    .select(VariantEffectPredictorParser._collect_uniprot_accessions(f.col('sp'), f.col('tr')).alias('proteinIds'))\n        ...    .show()\n        ... )\n        +------------+\n        |  proteinIds|\n        +------------+\n        |[sp_1, tr_1]|\n        |          []|\n        |      [tr_2]|\n        +------------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        # Dropping Null values and flattening the arrays:\n        return f.filter(\n            f.array_distinct(\n                f.transform(\n                    f.flatten(\n                        f.filter(\n                            f.array(trembl, swissprot),\n                            lambda x: x.isNotNull(),\n                        )\n                    ),\n                    lambda x: f.split(x, r\"\\.\")[0],\n                )\n            ),\n            lambda x: x.isNotNull(),\n        )\n\n    @staticmethod\n    def _parse_variant_location_id(vep_input_field: Column) -&gt; list[Column]:\n        r\"\"\"Parse variant identifier, chromosome, position, reference allele and alternate allele from VEP input field.\n\n        Args:\n            vep_input_field (Column): Column containing variant vcf string used as VEP input.\n\n        Returns:\n            list[Column]: List of columns containing chromosome, position, reference allele and alternate allele.\n        \"\"\"\n        variant_fields = f.split(vep_input_field, r\"\\t\")\n        return [\n            f.concat_ws(\n                \"_\",\n                f.array(\n                    variant_fields[0],\n                    variant_fields[1],\n                    variant_fields[3],\n                    variant_fields[4],\n                ),\n            ).alias(\"variantId\"),\n            variant_fields[0].cast(t.StringType()).alias(\"chromosome\"),\n            variant_fields[1].cast(t.IntegerType()).alias(\"position\"),\n            variant_fields[3].cast(t.StringType()).alias(\"referenceAllele\"),\n            variant_fields[4].cast(t.StringType()).alias(\"alternateAllele\"),\n        ]\n\n    @classmethod\n    def process_vep_output(\n        cls, vep_output: DataFrame, hash_threshold: int = 100\n    ) -&gt; DataFrame:\n        \"\"\"Process and format a VEP output in JSON format.\n\n        Args:\n            vep_output (DataFrame): raw VEP output, read as spark DataFrame.\n            hash_threshold (int): threshold above which variant identifiers will be hashed.\n\n        Returns:\n           DataFrame: processed data in the right shape.\n        \"\"\"\n        # Processing VEP output:\n        return (\n            vep_output\n            # Dropping non-canonical transcript consequences:  # TODO: parametrize this.\n            .withColumn(\n                \"transcript_consequences\",\n                f.filter(\n                    f.col(\"transcript_consequences\"),\n                    lambda consequence: consequence.getItem(\"canonical\") == 1,\n                ),\n            )\n            .select(\n                # Parse id and chr:pos:alt:ref:\n                *cls._parse_variant_location_id(f.col(\"input\")),\n                # Extracting corss_references from colocated variants:\n                cls._extract_ensembl_xrefs(f.col(\"colocated_variants\")).alias(\n                    \"ensembl_xrefs\"\n                ),\n                cls._extract_omim_xrefs(f.col(\"colocated_variants\")).alias(\n                    \"omim_xrefs\"\n                ),\n                cls._extract_clinvar_xrefs(f.col(\"colocated_variants\")).alias(\n                    \"clinvar_xrefs\"\n                ),\n                # Extracting variant effect assessments\n                f.when(\n                    # The following variant effect assessments are only available for variants with transcript consequences:\n                    f.col(\"transcript_consequences\").isNotNull(),\n                    f.filter(\n                        f.array(\n                            # Extract CADD scores:\n                            cls._vep_variant_effect_extractor(\n                                transcript_column_name=\"transcript_consequences\",\n                                method_name=\"CADD\",\n                                score_column_name=\"cadd_phred\",\n                            ),\n                            # Extract polyphen scores:\n                            cls._vep_variant_effect_extractor(\n                                transcript_column_name=\"transcript_consequences\",\n                                method_name=\"PolyPhen\",\n                                score_column_name=\"polyphen_score\",\n                                assessment_column_name=\"polyphen_prediction\",\n                            ),\n                            # Extract sift scores:\n                            cls._vep_variant_effect_extractor(\n                                transcript_column_name=\"transcript_consequences\",\n                                method_name=\"SIFT\",\n                                score_column_name=\"sift_score\",\n                                assessment_column_name=\"sift_prediction\",\n                            ),\n                            # Extract loftee scores:\n                            cls._vep_variant_effect_extractor(\n                                method_name=\"LOFTEE\",\n                                transcript_column_name=\"transcript_consequences\",\n                                score_column_name=\"lof\",\n                                assessment_column_name=\"lof\",\n                                assessment_flag_column_name=\"lof_filter\",\n                            ),\n                            # Extract GERP conservation score:\n                            cls._vep_variant_effect_extractor(\n                                method_name=\"GERP\",\n                                transcript_column_name=\"transcript_consequences\",\n                                score_column_name=\"conservation\",\n                            ),\n                            # Extract max alpha missense:\n                            cls._get_max_alpha_missense(\n                                f.col(\"transcript_consequences\")\n                            ),\n                            # Extract VEP prediction:\n                            cls._get_vep_prediction(f.col(\"most_severe_consequence\")),\n                        ),\n                        lambda predictor: predictor.isNotNull(),\n                    ),\n                )\n                .otherwise(\n                    f.array(\n                        # Extract VEP prediction:\n                        cls._get_vep_prediction(f.col(\"most_severe_consequence\")),\n                    )\n                )\n                .alias(\"variantEffect\"),\n                # Convert consequence to SO:\n                map_column_by_dictionary(\n                    f.col(\"most_severe_consequence\"), cls.SEQUENCE_ONTOLOGY_MAP\n                ).alias(\"mostSevereConsequenceId\"),\n                # Propagate most severe consequence:\n                \"most_severe_consequence\",\n                # Extract HGVS identifier:\n                f.when(\n                    f.size(\"transcript_consequences\") &gt; 0,\n                    f.col(\"transcript_consequences\").getItem(0).getItem(\"hgvsg\"),\n                )\n                .when(\n                    f.size(\"intergenic_consequences\") &gt; 0,\n                    f.col(\"intergenic_consequences\").getItem(0).getItem(\"hgvsg\"),\n                )\n                .otherwise(f.lit(None))\n                .alias(\"hgvsId\"),\n                # Collect transcript consequence:\n                f.when(\n                    f.col(\"transcript_consequences\").isNotNull(),\n                    f.transform(\n                        f.col(\"transcript_consequences\"),\n                        lambda transcript: f.struct(\n                            # Convert consequence terms to SO identifier:\n                            f.transform(\n                                transcript.consequence_terms,\n                                lambda y: map_column_by_dictionary(\n                                    y, cls.SEQUENCE_ONTOLOGY_MAP\n                                ),\n                            ).alias(\"variantFunctionalConsequenceIds\"),\n                            # Convert consequence terms to consequence score:\n                            f.array_max(\n                                f.transform(\n                                    transcript.consequence_terms,\n                                    lambda term: map_column_by_dictionary(\n                                        term, cls.LABEL_TO_SCORE_MAP\n                                    ),\n                                )\n                            )\n                            .cast(t.FloatType())\n                            .alias(\"consequenceScore\"),\n                            # Format amino acid change:\n                            cls._parser_amino_acid_change(\n                                transcript.amino_acids, transcript.protein_end\n                            ).alias(\"aminoAcidChange\"),\n                            # Extract and clean uniprot identifiers:\n                            cls._collect_uniprot_accessions(\n                                transcript.swissprot,\n                                transcript.trembl,\n                            ).alias(\"uniprotAccessions\"),\n                            # Add canonical flag:\n                            f.when(transcript.canonical == 1, f.lit(True))\n                            .otherwise(f.lit(False))\n                            .alias(\"isEnsemblCanonical\"),\n                            # Extract footprint distance:\n                            transcript.codons.alias(\"codons\"),\n                            f.when(transcript.distance.isNotNull(), transcript.distance)\n                            .otherwise(f.lit(0))\n                            .cast(t.LongType())\n                            .alias(\"distanceFromFootprint\"),\n                            # Extract distance from the transcription start site:\n                            transcript.tssdistance.cast(t.LongType()).alias(\n                                \"distanceFromTss\"\n                            ),\n                            # Extracting APPRIS isoform annotation for this transcript:\n                            transcript.appris.alias(\"appris\"),\n                            # Extracting MANE select transcript:\n                            transcript.mane_select.alias(\"maneSelect\"),\n                            transcript.gene_id.alias(\"targetId\"),\n                            transcript.impact.alias(\"impact\"),\n                            transcript.lof.cast(t.StringType()).alias(\n                                \"lofteePrediction\"\n                            ),\n                            transcript.lof.cast(t.FloatType()).alias(\"siftPrediction\"),\n                            transcript.lof.cast(t.FloatType()).alias(\n                                \"polyphenPrediction\"\n                            ),\n                            transcript.transcript_id.alias(\"transcriptId\"),\n                            transcript.biotype.alias(\"biotype\"),\n                            transcript.gene_symbol.alias(\"approvedSymbol\"),\n                        ),\n                    ),\n                ).alias(\"transcriptConsequences\"),\n                # Extracting rsids:\n                cls._colocated_variants_to_rsids(f.col(\"colocated_variants\")).alias(\n                    \"rsIds\"\n                ),\n                # Adding empty array for allele frequencies - now this piece of data is not coming form the VEP data:\n                f.array().cast(cls.ALLELE_FREQUENCY_SCHEMA).alias(\"alleleFrequencies\"),\n            )\n            # Dropping transcripts where the consequence score or the distance is null:\n            .withColumn(\n                \"transcriptConsequences\",\n                f.filter(\n                    f.col(\"transcriptConsequences\"),\n                    lambda x: x.getItem(\"consequenceScore\").isNotNull()\n                    &amp; x.getItem(\"distanceFromFootprint\").isNotNull(),\n                ),\n            )\n            # Sort transcript consequences by consequence score and distance from footprint and add index:\n            .withColumn(\n                \"transcriptConsequences\",\n                f.when(\n                    f.col(\"transcriptConsequences\").isNotNull(),\n                    f.transform(\n                        order_array_of_structs_by_two_fields(\n                            \"transcriptConsequences\",\n                            \"consequenceScore\",\n                            \"distanceFromFootprint\",\n                        ),\n                        lambda x, i: x.withField(\"transcriptIndex\", i + f.lit(1)),\n                    ),\n                ),\n            )\n            # Adding protvar xref for missense variants:  # TODO: making and extendable list of consequences\n            .withColumn(\n                \"protvar_xrefs\",\n                f.when(\n                    f.size(\n                        f.filter(\n                            f.col(\"transcriptConsequences\"),\n                            lambda x: f.array_contains(\n                                x.variantFunctionalConsequenceIds, \"SO_0001583\"\n                            ),\n                        )\n                    )\n                    &gt; 0,\n                    cls._generate_dbxrefs(f.array(f.col(\"variantId\")), \"protvar\"),\n                ),\n            )\n            .withColumn(\n                \"dbXrefs\",\n                f.flatten(\n                    f.filter(\n                        f.array(\n                            f.col(\"ensembl_xrefs\"),\n                            f.col(\"omim_xrefs\"),\n                            f.col(\"clinvar_xrefs\"),\n                            f.col(\"protvar_xrefs\"),\n                        ),\n                        lambda x: x.isNotNull(),\n                    )\n                ),\n            )\n            # If the variantId is too long, hash it:\n            .withColumn(\n                \"variantId\",\n                VariantIndex.hash_long_variant_ids(\n                    f.col(\"variantId\"),\n                    f.col(\"chromosome\"),\n                    f.col(\"position\"),\n                    hash_threshold,\n                ),\n            )\n            # Generating a temporary column with only protein coding transcripts:\n            .withColumn(\n                \"proteinCodingTranscripts\",\n                f.filter(\n                    f.col(\"transcriptConsequences\"),\n                    lambda x: x.getItem(\"biotype\") == \"protein_coding\",\n                ),\n            )\n            # Generate variant descrioption:\n            .withColumn(\n                \"variantDescription\",\n                cls._compose_variant_description(\n                    # Passing the most severe consequence:\n                    f.col(\"most_severe_consequence\"),\n                    # The first transcript:\n                    f.filter(\n                        f.col(\"transcriptConsequences\"),\n                        lambda vep: vep.transcriptIndex == 1,\n                    ).getItem(0),\n                    # The first protein coding transcript:\n                    order_array_of_structs_by_field(\n                        \"proteinCodingTranscripts\", \"transcriptIndex\"\n                    )[f.size(\"proteinCodingTranscripts\") - 1],\n                ),\n            )\n            # Normalising variant effect assessments:\n            .withColumn(\n                \"variantEffect\",\n                VariantEffectNormaliser.normalise_variant_effect(\n                    f.col(\"variantEffect\")\n                ),\n            )\n            # Dropping intermediate xref columns:\n            .drop(\n                *[\n                    \"ensembl_xrefs\",\n                    \"omim_xrefs\",\n                    \"clinvar_xrefs\",\n                    \"protvar_xrefs\",\n                    \"most_severe_consequence\",\n                    \"proteinCodingTranscripts\",\n                ]\n            )\n            # Drooping rows with null position:\n            .filter(f.col(\"position\").isNotNull())\n        )\n\n    @classmethod\n    def _compose_variant_description(\n        cls: type[VariantEffectPredictorParser],\n        most_severe_consequence: Column,\n        first_transcript: Column,\n        first_protein_coding: Column,\n    ) -&gt; Column:\n        \"\"\"Compose variant description based on the most severe consequence.\n\n        Args:\n            most_severe_consequence (Column): Most severe consequence\n            first_transcript (Column): First transcript\n            first_protein_coding (Column): First protein coding transcript\n\n        Returns:\n            Column: Variant description\n        \"\"\"\n        return (\n            # When there's no transcript whatsoever:\n            f.when(\n                first_transcript.isNull(),\n                f.lit(\"Intergenic variant no gene in window\"),\n            )\n            # When the biotype of the first gene is protein coding:\n            .when(\n                first_transcript.getItem(\"biotype\") == \"protein_coding\",\n                cls._process_protein_coding_transcript(\n                    first_transcript, most_severe_consequence\n                ),\n            )\n            # When the first gene is not protein coding, we also pass the first protein coding gene:\n            .otherwise(\n                cls._process_non_protein_coding_transcript(\n                    most_severe_consequence, first_transcript, first_protein_coding\n                )\n            )\n        )\n\n    @staticmethod\n    def _process_consequence_term(consequence_term: Column) -&gt; Column:\n        \"\"\"Cleaning up consequence term: capitalizing and replacing underscores.\n\n        Args:\n            consequence_term (Column): Consequence term.\n\n        Returns:\n            Column: Cleaned up consequence term.\n        \"\"\"\n        last = f.when(consequence_term.contains(\"variant\"), f.lit(\"\")).otherwise(\n            \" variant\"\n        )\n        return f.concat(f.regexp_replace(f.initcap(consequence_term), \"_\", \" \"), last)\n\n    @staticmethod\n    def _process_overlap(transcript: Column) -&gt; Column:\n        \"\"\"Process overlap with gene: if the variant overlaps with the gene, return the gene name or distance.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: string column with overlap description.\n        \"\"\"\n        gene_label = f.when(\n            transcript.getField(\"approvedSymbol\").isNotNull(),\n            transcript.getField(\"approvedSymbol\"),\n        ).otherwise(transcript.getField(\"targetId\"))\n\n        return f.when(\n            transcript.getField(\"distanceFromFootprint\") == 0,\n            # \"overlapping with CCDC8\"\n            f.concat(f.lit(\" overlapping with \"), gene_label),\n        ).otherwise(\n            # \" 123 basepair away from CCDC8\"\n            f.concat(\n                f.lit(\" \"),\n                f.format_number(transcript.getField(\"distanceFromFootprint\"), 0),\n                f.lit(\" basepair away from \"),\n                gene_label,\n            )\n        )\n\n    @staticmethod\n    def _process_aa_change(transcript: Column) -&gt; Column:\n        \"\"\"Extract amino acid change information from transcript when available.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: Amino acid change information.\n        \"\"\"\n        return f.when(\n            transcript.getField(\"aminoAcidChange\").isNotNull(),\n            f.concat(\n                f.lit(\", causing amio-acid change: \"),\n                transcript.getField(\"aminoAcidChange\"),\n                f.lit(\" with \"),\n                f.lower(transcript.getField(\"impact\")),\n                f.lit(\" impact.\"),\n            ),\n        ).otherwise(f.lit(\".\"))\n\n    @staticmethod\n    def _process_lof(transcript: Column) -&gt; Column:\n        \"\"\"Process loss of function annotation from LOFTEE prediction.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: Loss of function annotation.\n        \"\"\"\n        return f.when(\n            transcript.getField(\"lofteePrediction\").isNotNull()\n            &amp; (transcript.getField(\"lofteePrediction\") == \"HC\"),\n            f.lit(\" A high-confidence loss-of-function variant by loftee.\"),\n        ).otherwise(f.lit(\"\"))\n\n    @classmethod\n    def _process_protein_coding_transcript(\n        cls: type[VariantEffectPredictorParser],\n        transcript: Column,\n        most_severe_consequence: Column,\n    ) -&gt; Column:\n        \"\"\"Extract information from the first, protein coding transcript.\n\n        Args:\n            transcript (Column): Transcript.\n            most_severe_consequence (Column): Most severe consequence.\n\n        Returns:\n            Column: Variant description.\n        \"\"\"\n        # Process consequence term:\n        consequence_text = cls._process_consequence_term(most_severe_consequence)\n\n        # Does it overlap with the gene:\n        overlap = cls._process_overlap(transcript)\n\n        # Does it cause amino acid change:\n        amino_acid_change = cls._process_aa_change(transcript)\n\n        # Processing lof annotation:\n        lof_assessment = cls._process_lof(transcript)\n\n        # Concat all together:\n        return f.concat(consequence_text, overlap, amino_acid_change, lof_assessment)\n\n    @staticmethod\n    def _adding_biotype(transcript: Column) -&gt; Column:\n        \"\"\"Adding biotype information to the variant description.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: Biotype information.\n        \"\"\"\n        biotype = f.when(\n            transcript.getField(\"biotype\").contains(\"gene\"),\n            f.regexp_replace(transcript.getField(\"biotype\"), \"_\", \" \"),\n        ).otherwise(\n            f.concat(\n                f.regexp_replace(transcript.getField(\"biotype\"), \"_\", \" \"),\n                f.lit(\" gene.\"),\n            )\n        )\n\n        return f.concat(f.lit(\", a \"), biotype)\n\n    @staticmethod\n    def _parse_protein_coding_transcript(transcript: Column) -&gt; Column:\n        \"\"\"Parse the closest, not first protein coding transcript: extract gene symbol and distance.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: Protein coding transcript information.\n        \"\"\"\n        gene_label = f.when(\n            transcript.getField(\"approvedSymbol\").isNotNull(),\n            transcript.getField(\"approvedSymbol\"),\n        ).otherwise(transcript.getField(\"targetId\"))\n\n        return f.when(\n            transcript.isNotNull(),\n            f.concat(\n                f.lit(\" The closest protein-coding gene is \"),\n                gene_label,\n                f.lit(\" (\"),\n                f.format_number(transcript.getField(\"distanceFromFootprint\"), 0),\n                f.lit(\" basepair away).\"),\n            ),\n        ).otherwise(f.lit(\"\"))\n\n    @classmethod\n    def _process_non_protein_coding_transcript(\n        cls: type[VariantEffectPredictorParser],\n        most_severe_consequence: Column,\n        first_transcript: Column,\n        first_protein_coding: Column,\n    ) -&gt; Column:\n        \"\"\"Extract information from the first, non-protein coding transcript.\n\n        Args:\n            most_severe_consequence (Column): Most severe consequence.\n            first_transcript (Column): First transcript.\n            first_protein_coding (Column): First protein coding transcript.\n\n        Returns:\n            Column: Variant description.\n        \"\"\"\n        # Process consequence term:\n        consequence_text = cls._process_consequence_term(most_severe_consequence)\n\n        # Does it overlap with the gene:\n        overlap = cls._process_overlap(first_transcript)\n\n        # Adding biotype:\n        biotype = cls._adding_biotype(first_transcript)\n\n        # Adding protein coding gene:\n        protein_transcript = cls._parse_protein_coding_transcript(first_protein_coding)\n\n        # Concat all together:\n        return f.concat(consequence_text, overlap, biotype, protein_transcript)\n</code></pre>"},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/#gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser.extract_variant_index_from_vep","title":"<code>extract_variant_index_from_vep(spark: SparkSession, vep_output_path: str | list[str], hash_threshold: int, **kwargs: bool | float | int | str | None) -&gt; VariantIndex</code>  <code>classmethod</code>","text":"<p>Extract variant index from VEP output.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>vep_output_path</code> <code>str | list[str]</code> <p>Path to the VEP output.</p> required <code>hash_threshold</code> <code>int</code> <p>Threshold above which variant identifiers will be hashed.</p> required <code>**kwargs</code> <code>bool | float | int | str | None</code> <p>Additional arguments to pass to spark.read.json.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>Variant index dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Failed reading file or if the dataset is empty.</p> Source code in <code>src/gentropy/datasource/ensembl/vep_parser.py</code> <pre><code>@classmethod\ndef extract_variant_index_from_vep(\n    cls: type[VariantEffectPredictorParser],\n    spark: SparkSession,\n    vep_output_path: str | list[str],\n    hash_threshold: int,\n    **kwargs: bool | float | int | str | None,\n) -&gt; VariantIndex:\n    \"\"\"Extract variant index from VEP output.\n\n    Args:\n        spark (SparkSession): Spark session.\n        vep_output_path (str | list[str]): Path to the VEP output.\n        hash_threshold (int): Threshold above which variant identifiers will be hashed.\n        **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.json.\n\n    Returns:\n        VariantIndex: Variant index dataset.\n\n    Raises:\n        ValueError: Failed reading file or if the dataset is empty.\n    \"\"\"\n    # To speed things up and simplify the json structure, read data following an expected schema:\n    vep_schema = cls.get_schema()\n\n    try:\n        vep_data = spark.read.json(vep_output_path, schema=vep_schema, **kwargs)\n    except ValueError as e:\n        raise ValueError(f\"Failed reading file: {vep_output_path}.\") from e\n\n    if vep_data.isEmpty():\n        raise ValueError(f\"The dataset is empty: {vep_output_path}\")\n\n    # Convert to VariantAnnotation dataset:\n    return VariantIndex(\n        _df=VariantEffectPredictorParser.process_vep_output(\n            vep_data, hash_threshold\n        ),\n        _schema=VariantIndex.get_schema(),\n        id_threshold=hash_threshold,\n    )\n</code></pre>"},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/#gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser.get_schema","title":"<code>get_schema() -&gt; t.StructType</code>  <code>staticmethod</code>","text":"<p>Return the schema of the VEP output.</p> <p>Returns:</p> Type Description <code>StructType</code> <p>t.StructType: VEP output schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; type(VariantEffectPredictorParser.get_schema())\n&lt;class 'pyspark.sql.types.StructType'&gt;\n</code></pre> Source code in <code>src/gentropy/datasource/ensembl/vep_parser.py</code> <pre><code>@staticmethod\ndef get_schema() -&gt; t.StructType:\n    \"\"\"Return the schema of the VEP output.\n\n    Returns:\n        t.StructType: VEP output schema.\n\n    Examples:\n        &gt;&gt;&gt; type(VariantEffectPredictorParser.get_schema())\n        &lt;class 'pyspark.sql.types.StructType'&gt;\n    \"\"\"\n    return parse_spark_schema(\"vep_json_output.json\")\n</code></pre>"},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/#gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser.process_vep_output","title":"<code>process_vep_output(vep_output: DataFrame, hash_threshold: int = 100) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Process and format a VEP output in JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>vep_output</code> <code>DataFrame</code> <p>raw VEP output, read as spark DataFrame.</p> required <code>hash_threshold</code> <code>int</code> <p>threshold above which variant identifiers will be hashed.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>processed data in the right shape.</p> Source code in <code>src/gentropy/datasource/ensembl/vep_parser.py</code> <pre><code>@classmethod\ndef process_vep_output(\n    cls, vep_output: DataFrame, hash_threshold: int = 100\n) -&gt; DataFrame:\n    \"\"\"Process and format a VEP output in JSON format.\n\n    Args:\n        vep_output (DataFrame): raw VEP output, read as spark DataFrame.\n        hash_threshold (int): threshold above which variant identifiers will be hashed.\n\n    Returns:\n       DataFrame: processed data in the right shape.\n    \"\"\"\n    # Processing VEP output:\n    return (\n        vep_output\n        # Dropping non-canonical transcript consequences:  # TODO: parametrize this.\n        .withColumn(\n            \"transcript_consequences\",\n            f.filter(\n                f.col(\"transcript_consequences\"),\n                lambda consequence: consequence.getItem(\"canonical\") == 1,\n            ),\n        )\n        .select(\n            # Parse id and chr:pos:alt:ref:\n            *cls._parse_variant_location_id(f.col(\"input\")),\n            # Extracting corss_references from colocated variants:\n            cls._extract_ensembl_xrefs(f.col(\"colocated_variants\")).alias(\n                \"ensembl_xrefs\"\n            ),\n            cls._extract_omim_xrefs(f.col(\"colocated_variants\")).alias(\n                \"omim_xrefs\"\n            ),\n            cls._extract_clinvar_xrefs(f.col(\"colocated_variants\")).alias(\n                \"clinvar_xrefs\"\n            ),\n            # Extracting variant effect assessments\n            f.when(\n                # The following variant effect assessments are only available for variants with transcript consequences:\n                f.col(\"transcript_consequences\").isNotNull(),\n                f.filter(\n                    f.array(\n                        # Extract CADD scores:\n                        cls._vep_variant_effect_extractor(\n                            transcript_column_name=\"transcript_consequences\",\n                            method_name=\"CADD\",\n                            score_column_name=\"cadd_phred\",\n                        ),\n                        # Extract polyphen scores:\n                        cls._vep_variant_effect_extractor(\n                            transcript_column_name=\"transcript_consequences\",\n                            method_name=\"PolyPhen\",\n                            score_column_name=\"polyphen_score\",\n                            assessment_column_name=\"polyphen_prediction\",\n                        ),\n                        # Extract sift scores:\n                        cls._vep_variant_effect_extractor(\n                            transcript_column_name=\"transcript_consequences\",\n                            method_name=\"SIFT\",\n                            score_column_name=\"sift_score\",\n                            assessment_column_name=\"sift_prediction\",\n                        ),\n                        # Extract loftee scores:\n                        cls._vep_variant_effect_extractor(\n                            method_name=\"LOFTEE\",\n                            transcript_column_name=\"transcript_consequences\",\n                            score_column_name=\"lof\",\n                            assessment_column_name=\"lof\",\n                            assessment_flag_column_name=\"lof_filter\",\n                        ),\n                        # Extract GERP conservation score:\n                        cls._vep_variant_effect_extractor(\n                            method_name=\"GERP\",\n                            transcript_column_name=\"transcript_consequences\",\n                            score_column_name=\"conservation\",\n                        ),\n                        # Extract max alpha missense:\n                        cls._get_max_alpha_missense(\n                            f.col(\"transcript_consequences\")\n                        ),\n                        # Extract VEP prediction:\n                        cls._get_vep_prediction(f.col(\"most_severe_consequence\")),\n                    ),\n                    lambda predictor: predictor.isNotNull(),\n                ),\n            )\n            .otherwise(\n                f.array(\n                    # Extract VEP prediction:\n                    cls._get_vep_prediction(f.col(\"most_severe_consequence\")),\n                )\n            )\n            .alias(\"variantEffect\"),\n            # Convert consequence to SO:\n            map_column_by_dictionary(\n                f.col(\"most_severe_consequence\"), cls.SEQUENCE_ONTOLOGY_MAP\n            ).alias(\"mostSevereConsequenceId\"),\n            # Propagate most severe consequence:\n            \"most_severe_consequence\",\n            # Extract HGVS identifier:\n            f.when(\n                f.size(\"transcript_consequences\") &gt; 0,\n                f.col(\"transcript_consequences\").getItem(0).getItem(\"hgvsg\"),\n            )\n            .when(\n                f.size(\"intergenic_consequences\") &gt; 0,\n                f.col(\"intergenic_consequences\").getItem(0).getItem(\"hgvsg\"),\n            )\n            .otherwise(f.lit(None))\n            .alias(\"hgvsId\"),\n            # Collect transcript consequence:\n            f.when(\n                f.col(\"transcript_consequences\").isNotNull(),\n                f.transform(\n                    f.col(\"transcript_consequences\"),\n                    lambda transcript: f.struct(\n                        # Convert consequence terms to SO identifier:\n                        f.transform(\n                            transcript.consequence_terms,\n                            lambda y: map_column_by_dictionary(\n                                y, cls.SEQUENCE_ONTOLOGY_MAP\n                            ),\n                        ).alias(\"variantFunctionalConsequenceIds\"),\n                        # Convert consequence terms to consequence score:\n                        f.array_max(\n                            f.transform(\n                                transcript.consequence_terms,\n                                lambda term: map_column_by_dictionary(\n                                    term, cls.LABEL_TO_SCORE_MAP\n                                ),\n                            )\n                        )\n                        .cast(t.FloatType())\n                        .alias(\"consequenceScore\"),\n                        # Format amino acid change:\n                        cls._parser_amino_acid_change(\n                            transcript.amino_acids, transcript.protein_end\n                        ).alias(\"aminoAcidChange\"),\n                        # Extract and clean uniprot identifiers:\n                        cls._collect_uniprot_accessions(\n                            transcript.swissprot,\n                            transcript.trembl,\n                        ).alias(\"uniprotAccessions\"),\n                        # Add canonical flag:\n                        f.when(transcript.canonical == 1, f.lit(True))\n                        .otherwise(f.lit(False))\n                        .alias(\"isEnsemblCanonical\"),\n                        # Extract footprint distance:\n                        transcript.codons.alias(\"codons\"),\n                        f.when(transcript.distance.isNotNull(), transcript.distance)\n                        .otherwise(f.lit(0))\n                        .cast(t.LongType())\n                        .alias(\"distanceFromFootprint\"),\n                        # Extract distance from the transcription start site:\n                        transcript.tssdistance.cast(t.LongType()).alias(\n                            \"distanceFromTss\"\n                        ),\n                        # Extracting APPRIS isoform annotation for this transcript:\n                        transcript.appris.alias(\"appris\"),\n                        # Extracting MANE select transcript:\n                        transcript.mane_select.alias(\"maneSelect\"),\n                        transcript.gene_id.alias(\"targetId\"),\n                        transcript.impact.alias(\"impact\"),\n                        transcript.lof.cast(t.StringType()).alias(\n                            \"lofteePrediction\"\n                        ),\n                        transcript.lof.cast(t.FloatType()).alias(\"siftPrediction\"),\n                        transcript.lof.cast(t.FloatType()).alias(\n                            \"polyphenPrediction\"\n                        ),\n                        transcript.transcript_id.alias(\"transcriptId\"),\n                        transcript.biotype.alias(\"biotype\"),\n                        transcript.gene_symbol.alias(\"approvedSymbol\"),\n                    ),\n                ),\n            ).alias(\"transcriptConsequences\"),\n            # Extracting rsids:\n            cls._colocated_variants_to_rsids(f.col(\"colocated_variants\")).alias(\n                \"rsIds\"\n            ),\n            # Adding empty array for allele frequencies - now this piece of data is not coming form the VEP data:\n            f.array().cast(cls.ALLELE_FREQUENCY_SCHEMA).alias(\"alleleFrequencies\"),\n        )\n        # Dropping transcripts where the consequence score or the distance is null:\n        .withColumn(\n            \"transcriptConsequences\",\n            f.filter(\n                f.col(\"transcriptConsequences\"),\n                lambda x: x.getItem(\"consequenceScore\").isNotNull()\n                &amp; x.getItem(\"distanceFromFootprint\").isNotNull(),\n            ),\n        )\n        # Sort transcript consequences by consequence score and distance from footprint and add index:\n        .withColumn(\n            \"transcriptConsequences\",\n            f.when(\n                f.col(\"transcriptConsequences\").isNotNull(),\n                f.transform(\n                    order_array_of_structs_by_two_fields(\n                        \"transcriptConsequences\",\n                        \"consequenceScore\",\n                        \"distanceFromFootprint\",\n                    ),\n                    lambda x, i: x.withField(\"transcriptIndex\", i + f.lit(1)),\n                ),\n            ),\n        )\n        # Adding protvar xref for missense variants:  # TODO: making and extendable list of consequences\n        .withColumn(\n            \"protvar_xrefs\",\n            f.when(\n                f.size(\n                    f.filter(\n                        f.col(\"transcriptConsequences\"),\n                        lambda x: f.array_contains(\n                            x.variantFunctionalConsequenceIds, \"SO_0001583\"\n                        ),\n                    )\n                )\n                &gt; 0,\n                cls._generate_dbxrefs(f.array(f.col(\"variantId\")), \"protvar\"),\n            ),\n        )\n        .withColumn(\n            \"dbXrefs\",\n            f.flatten(\n                f.filter(\n                    f.array(\n                        f.col(\"ensembl_xrefs\"),\n                        f.col(\"omim_xrefs\"),\n                        f.col(\"clinvar_xrefs\"),\n                        f.col(\"protvar_xrefs\"),\n                    ),\n                    lambda x: x.isNotNull(),\n                )\n            ),\n        )\n        # If the variantId is too long, hash it:\n        .withColumn(\n            \"variantId\",\n            VariantIndex.hash_long_variant_ids(\n                f.col(\"variantId\"),\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                hash_threshold,\n            ),\n        )\n        # Generating a temporary column with only protein coding transcripts:\n        .withColumn(\n            \"proteinCodingTranscripts\",\n            f.filter(\n                f.col(\"transcriptConsequences\"),\n                lambda x: x.getItem(\"biotype\") == \"protein_coding\",\n            ),\n        )\n        # Generate variant descrioption:\n        .withColumn(\n            \"variantDescription\",\n            cls._compose_variant_description(\n                # Passing the most severe consequence:\n                f.col(\"most_severe_consequence\"),\n                # The first transcript:\n                f.filter(\n                    f.col(\"transcriptConsequences\"),\n                    lambda vep: vep.transcriptIndex == 1,\n                ).getItem(0),\n                # The first protein coding transcript:\n                order_array_of_structs_by_field(\n                    \"proteinCodingTranscripts\", \"transcriptIndex\"\n                )[f.size(\"proteinCodingTranscripts\") - 1],\n            ),\n        )\n        # Normalising variant effect assessments:\n        .withColumn(\n            \"variantEffect\",\n            VariantEffectNormaliser.normalise_variant_effect(\n                f.col(\"variantEffect\")\n            ),\n        )\n        # Dropping intermediate xref columns:\n        .drop(\n            *[\n                \"ensembl_xrefs\",\n                \"omim_xrefs\",\n                \"clinvar_xrefs\",\n                \"protvar_xrefs\",\n                \"most_severe_consequence\",\n                \"proteinCodingTranscripts\",\n            ]\n        )\n        # Drooping rows with null position:\n        .filter(f.col(\"position\").isNotNull())\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/_eqtl_catalogue/","title":"eQTL Catalogue","text":"<p>The eQTL Catalogue aims to provide unified gene, protein expression and splicing Quantitative Trait Loci (QTLs) from all available human public studies.</p> <p>It serves as the ultimate resource of molecular QTLs that we use for colocalization and target prioritization.</p>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/","title":"Fine mapping results","text":""},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping","title":"<code>gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping</code>  <code>dataclass</code>","text":"<p>SuSIE finemapping dataset for eQTL Catalogue.</p> <p>Credible sets from SuSIE are extracted and transformed into StudyLocus objects: - A study ID is defined as a triad between: the publication, the tissue, and the measured trait (e.g. Braineac2_substantia_nigra_ENSG00000248275) - Each row in the <code>credible_sets.tsv.gz</code> files is represented by molecular_trait_id/variant/rsid trios relevant for a given tissue. Each have their own finemapping statistics - log Bayes Factors are available for all variants in the <code>lbf_variable.txt</code> files</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@dataclass\nclass EqtlCatalogueFinemapping:\n    \"\"\"SuSIE finemapping dataset for eQTL Catalogue.\n\n    Credible sets from SuSIE are extracted and transformed into StudyLocus objects:\n    - A study ID is defined as a triad between: the publication, the tissue, and the measured trait (e.g. Braineac2_substantia_nigra_ENSG00000248275)\n    - Each row in the `credible_sets.tsv.gz` files is represented by molecular_trait_id/variant/rsid trios relevant for a given tissue. Each have their own finemapping statistics\n    - log Bayes Factors are available for all variants in the `lbf_variable.txt` files\n    \"\"\"\n\n    raw_credible_set_schema: StructType = StructType(\n        [\n            StructField(\"molecular_trait_id\", StringType(), True),\n            StructField(\"gene_id\", StringType(), True),\n            StructField(\"cs_id\", StringType(), True),\n            StructField(\"variant\", StringType(), True),\n            StructField(\"rsid\", StringType(), True),\n            StructField(\"cs_size\", IntegerType(), True),\n            StructField(\"pip\", DoubleType(), True),\n            StructField(\"pvalue\", DoubleType(), True),\n            StructField(\"beta\", DoubleType(), True),\n            StructField(\"se\", DoubleType(), True),\n            StructField(\"z\", DoubleType(), True),\n            StructField(\"cs_min_r2\", DoubleType(), True),\n            StructField(\"region\", StringType(), True),\n        ]\n    )\n    raw_lbf_schema: StructType = StructType(\n        [\n            StructField(\"molecular_trait_id\", StringType(), True),\n            StructField(\"region\", StringType(), True),\n            StructField(\"variant\", StringType(), True),\n            StructField(\"chromosome\", StringType(), True),\n            StructField(\"position\", IntegerType(), True),\n            StructField(\"lbf_variable1\", DoubleType(), True),\n            StructField(\"lbf_variable2\", DoubleType(), True),\n            StructField(\"lbf_variable3\", DoubleType(), True),\n            StructField(\"lbf_variable4\", DoubleType(), True),\n            StructField(\"lbf_variable5\", DoubleType(), True),\n            StructField(\"lbf_variable6\", DoubleType(), True),\n            StructField(\"lbf_variable7\", DoubleType(), True),\n            StructField(\"lbf_variable8\", DoubleType(), True),\n            StructField(\"lbf_variable9\", DoubleType(), True),\n            StructField(\"lbf_variable10\", DoubleType(), True),\n        ]\n    )\n\n    @classmethod\n    def _extract_credible_set_index(\n        cls: type[EqtlCatalogueFinemapping], cs_id: Column\n    ) -&gt; Column:\n        \"\"\"Extract the credible set index from the cs_id.\n\n        Args:\n            cs_id (Column): column with the credible set id as defined in the eQTL Catalogue.\n\n        Returns:\n            Column: The credible set index.\n\n        Examples:\n            &gt;&gt;&gt; spark.createDataFrame([(\"QTD000046_L1\",)], [\"cs_id\"]).select(EqtlCatalogueFinemapping._extract_credible_set_index(f.col(\"cs_id\"))).show()\n            +----------------+\n            |credibleSetIndex|\n            +----------------+\n            |               1|\n            +----------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.split(cs_id, \"_L\")[1].cast(IntegerType()).alias(\"credibleSetIndex\")\n\n    @classmethod\n    def _extract_dataset_id_from_file_path(\n        cls: type[EqtlCatalogueFinemapping], file_path: Column\n    ) -&gt; Column:\n        \"\"\"Extract the dataset_id from the file_path. The dataset_id follows the pattern QTD{6}.\n\n        Args:\n            file_path (Column): A column containing the file path.\n\n        Returns:\n            Column: The dataset_id.\n\n        Examples:\n            &gt;&gt;&gt; spark.createDataFrame([(\"QTD000046.credible_sets.tsv\",)], [\"filename\"]).select(EqtlCatalogueFinemapping._extract_dataset_id_from_file_path(f.col(\"filename\"))).show()\n            +----------+\n            |dataset_id|\n            +----------+\n            | QTD000046|\n            +----------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.regexp_extract(file_path, r\"QTD\\d{6}\", 0).alias(\"dataset_id\")\n\n    @classmethod\n    def parse_susie_results(\n        cls: type[EqtlCatalogueFinemapping],\n        credible_sets: DataFrame,\n        lbf: DataFrame,\n        studies_metadata: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"Parse the SuSIE results into a DataFrame containing the finemapping statistics and metadata about the studies.\n\n        Args:\n            credible_sets (DataFrame): DataFrame containing raw statistics of all variants in the credible sets.\n            lbf (DataFrame): DataFrame containing the raw log Bayes Factors for all variants.\n            studies_metadata (DataFrame): DataFrame containing the study metadata.\n\n        Returns:\n            DataFrame: Processed SuSIE results to contain metadata about the studies and the finemapping statistics.\n        \"\"\"\n        ss_ftp_path_template = \"https://ftp.ebi.ac.uk/pub/databases/spot/eQTL/sumstats\"\n        return (\n            lbf.join(\n                credible_sets.join(f.broadcast(studies_metadata), on=\"dataset_id\"),\n                on=[\"molecular_trait_id\", \"region\", \"variant\", \"dataset_id\"],\n                how=\"inner\",\n            )\n            .withColumn(\n                \"logBF\",\n                f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"lbf_variable1\"))\n                .when(f.col(\"credibleSetIndex\") == 2, f.col(\"lbf_variable2\"))\n                .when(f.col(\"credibleSetIndex\") == 3, f.col(\"lbf_variable3\"))\n                .when(f.col(\"credibleSetIndex\") == 4, f.col(\"lbf_variable4\"))\n                .when(f.col(\"credibleSetIndex\") == 5, f.col(\"lbf_variable5\"))\n                .when(f.col(\"credibleSetIndex\") == 6, f.col(\"lbf_variable6\"))\n                .when(f.col(\"credibleSetIndex\") == 7, f.col(\"lbf_variable7\"))\n                .when(f.col(\"credibleSetIndex\") == 8, f.col(\"lbf_variable8\"))\n                .when(f.col(\"credibleSetIndex\") == 9, f.col(\"lbf_variable9\"))\n                .when(f.col(\"credibleSetIndex\") == 10, f.col(\"lbf_variable10\")),\n            )\n            .select(\n                f.regexp_replace(f.col(\"variant\"), r\"chr\", \"\").alias(\"variantId\"),\n                f.col(\"region\"),\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                f.col(\"pip\").alias(\"posteriorProbability\"),\n                *parse_pvalue(f.col(\"pvalue\")),\n                f.col(\"sample_size\").alias(\"nSamples\"),\n                f.col(\"beta\"),\n                f.col(\"se\").alias(\"standardError\"),\n                f.col(\"credibleSetIndex\"),\n                f.col(\"logBF\"),\n                f.lit(FinemappingMethod.SUSIE.value).alias(\"finemappingMethod\"),\n                # Study metadata\n                f.col(\"molecular_trait_id\").alias(\"traitFromSource\"),\n                f.col(\"gene_id\").alias(\"geneId\"),\n                f.col(\"dataset_id\"),\n                # Upon creation, the studyId cleaned from symbols:\n                clean_strings_from_symbols(\n                    f.concat_ws(\n                        \"_\",\n                        f.col(\"study_label\"),\n                        f.col(\"quant_method\"),\n                        f.col(\"sample_group\"),\n                        f.col(\"molecular_trait_id\"),\n                    )\n                ).alias(\"studyId\"),\n                f.col(\"tissue_id\").alias(\"biosampleFromSourceId\"),\n                EqtlCatalogueStudyIndex._identify_study_type().alias(\"studyType\"),\n                f.col(\"study_label\").alias(\"projectId\"),\n                f.concat_ws(\n                    \"/\",\n                    f.lit(ss_ftp_path_template),\n                    f.col(\"study_id\"),\n                    f.col(\"dataset_id\"),\n                ).alias(\"summarystatsLocation\"),\n                f.lit(True).alias(\"hasSumstats\"),\n                f.col(\"molecular_trait_id\"),\n                f.col(\"pmid\").alias(\"pubmedId\"),\n                f.col(\"condition_label\").alias(\"condition\"),\n            )\n        )\n\n    @classmethod\n    def from_susie_results(\n        cls: type[EqtlCatalogueFinemapping], processed_finemapping_df: DataFrame\n    ) -&gt; StudyLocus:\n        \"\"\"Create a StudyLocus object from the processed SuSIE results.\n\n        Args:\n            processed_finemapping_df (DataFrame): DataFrame containing the processed SuSIE results.\n\n        Returns:\n            StudyLocus: eQTL Catalogue credible sets.\n        \"\"\"\n        lead_w = Window.partitionBy(\n            \"dataset_id\", \"molecular_trait_id\", \"region\", \"credibleSetIndex\"\n        )\n        study_locus_cols = [\n            field.name\n            for field in StudyLocus.get_schema().fields\n            if field.name in processed_finemapping_df.columns\n        ] + [\"locus\"]\n        return StudyLocus(\n            _df=(\n                processed_finemapping_df.withColumn(\n                    \"isLead\",\n                    f.row_number().over(lead_w.orderBy(f.desc(\"posteriorProbability\")))\n                    == f.lit(1),\n                )\n                .withColumn(\n                    # Collecting all variants that constitute the credible set brings as many variants as the credible set size\n                    \"locus\",\n                    f.when(\n                        f.col(\"isLead\"),\n                        f.collect_list(\n                            f.struct(\n                                \"variantId\",\n                                \"posteriorProbability\",\n                                \"pValueMantissa\",\n                                \"pValueExponent\",\n                                \"logBF\",\n                                \"beta\",\n                                \"standardError\",\n                            )\n                        ).over(lead_w),\n                    ),\n                )\n                .filter(f.col(\"isLead\"))\n                .drop(\"isLead\")\n                .select(\n                    *study_locus_cols,\n                    StudyLocus.assign_study_locus_id(\n                        [\"studyId\", \"variantId\", \"finemappingMethod\"]\n                    ),\n                    StudyLocus.calculate_credible_set_log10bf(\n                        f.col(\"locus.logBF\")\n                    ).alias(\"credibleSetlog10BF\"),\n                )\n            ),\n            _schema=StudyLocus.get_schema(),\n        ).annotate_credible_sets()\n\n    @classmethod\n    def read_credible_set_from_source(\n        cls: type[EqtlCatalogueFinemapping],\n        session: Session,\n        credible_set_path: str | list[str],\n    ) -&gt; DataFrame:\n        \"\"\"Load raw credible sets from eQTL Catalogue.\n\n        Args:\n            session (Session): Spark session.\n            credible_set_path (str | list[str]): Path to raw table(s) containing finemapping results for any variant belonging to a credible set.\n\n        Returns:\n            DataFrame: Credible sets DataFrame.\n        \"\"\"\n        return (\n            session.spark.read.csv(\n                credible_set_path,\n                sep=\"\\t\",\n                header=True,\n                schema=cls.raw_credible_set_schema,\n            )\n            .withColumns(\n                {\n                    # Adding dataset id based on the input file name:\n                    \"dataset_id\": cls._extract_dataset_id_from_file_path(\n                        f.input_file_name()\n                    ),\n                    # Parsing credible set index from the cs_id:\n                    \"credibleSetIndex\": cls._extract_credible_set_index(f.col(\"cs_id\")),\n                }\n            )\n            # Remove duplicates caused by explosion of single variants to multiple rsid-s:\n            .drop(\"rsid\")\n            .distinct()\n        )\n\n    @classmethod\n    def read_lbf_from_source(\n        cls: type[EqtlCatalogueFinemapping],\n        session: Session,\n        lbf_path: str | list[str],\n    ) -&gt; DataFrame:\n        \"\"\"Load raw log Bayes Factors from eQTL Catalogue.\n\n        Args:\n            session (Session): Spark session.\n            lbf_path (str | list[str]): Path to raw table(s) containing Log Bayes Factors for each variant.\n\n        Returns:\n            DataFrame: Log Bayes Factors DataFrame.\n        \"\"\"\n        return (\n            session.spark.read.csv(\n                lbf_path,\n                sep=\"\\t\",\n                header=True,\n                schema=cls.raw_lbf_schema,\n            )\n            .withColumn(\n                \"dataset_id\",\n                cls._extract_dataset_id_from_file_path(f.input_file_name()),\n            )\n            .distinct()\n        )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping.from_susie_results","title":"<code>from_susie_results(processed_finemapping_df: DataFrame) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Create a StudyLocus object from the processed SuSIE results.</p> <p>Parameters:</p> Name Type Description Default <code>processed_finemapping_df</code> <code>DataFrame</code> <p>DataFrame containing the processed SuSIE results.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>eQTL Catalogue credible sets.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@classmethod\ndef from_susie_results(\n    cls: type[EqtlCatalogueFinemapping], processed_finemapping_df: DataFrame\n) -&gt; StudyLocus:\n    \"\"\"Create a StudyLocus object from the processed SuSIE results.\n\n    Args:\n        processed_finemapping_df (DataFrame): DataFrame containing the processed SuSIE results.\n\n    Returns:\n        StudyLocus: eQTL Catalogue credible sets.\n    \"\"\"\n    lead_w = Window.partitionBy(\n        \"dataset_id\", \"molecular_trait_id\", \"region\", \"credibleSetIndex\"\n    )\n    study_locus_cols = [\n        field.name\n        for field in StudyLocus.get_schema().fields\n        if field.name in processed_finemapping_df.columns\n    ] + [\"locus\"]\n    return StudyLocus(\n        _df=(\n            processed_finemapping_df.withColumn(\n                \"isLead\",\n                f.row_number().over(lead_w.orderBy(f.desc(\"posteriorProbability\")))\n                == f.lit(1),\n            )\n            .withColumn(\n                # Collecting all variants that constitute the credible set brings as many variants as the credible set size\n                \"locus\",\n                f.when(\n                    f.col(\"isLead\"),\n                    f.collect_list(\n                        f.struct(\n                            \"variantId\",\n                            \"posteriorProbability\",\n                            \"pValueMantissa\",\n                            \"pValueExponent\",\n                            \"logBF\",\n                            \"beta\",\n                            \"standardError\",\n                        )\n                    ).over(lead_w),\n                ),\n            )\n            .filter(f.col(\"isLead\"))\n            .drop(\"isLead\")\n            .select(\n                *study_locus_cols,\n                StudyLocus.assign_study_locus_id(\n                    [\"studyId\", \"variantId\", \"finemappingMethod\"]\n                ),\n                StudyLocus.calculate_credible_set_log10bf(\n                    f.col(\"locus.logBF\")\n                ).alias(\"credibleSetlog10BF\"),\n            )\n        ),\n        _schema=StudyLocus.get_schema(),\n    ).annotate_credible_sets()\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping.parse_susie_results","title":"<code>parse_susie_results(credible_sets: DataFrame, lbf: DataFrame, studies_metadata: DataFrame) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Parse the SuSIE results into a DataFrame containing the finemapping statistics and metadata about the studies.</p> <p>Parameters:</p> Name Type Description Default <code>credible_sets</code> <code>DataFrame</code> <p>DataFrame containing raw statistics of all variants in the credible sets.</p> required <code>lbf</code> <code>DataFrame</code> <p>DataFrame containing the raw log Bayes Factors for all variants.</p> required <code>studies_metadata</code> <code>DataFrame</code> <p>DataFrame containing the study metadata.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Processed SuSIE results to contain metadata about the studies and the finemapping statistics.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@classmethod\ndef parse_susie_results(\n    cls: type[EqtlCatalogueFinemapping],\n    credible_sets: DataFrame,\n    lbf: DataFrame,\n    studies_metadata: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"Parse the SuSIE results into a DataFrame containing the finemapping statistics and metadata about the studies.\n\n    Args:\n        credible_sets (DataFrame): DataFrame containing raw statistics of all variants in the credible sets.\n        lbf (DataFrame): DataFrame containing the raw log Bayes Factors for all variants.\n        studies_metadata (DataFrame): DataFrame containing the study metadata.\n\n    Returns:\n        DataFrame: Processed SuSIE results to contain metadata about the studies and the finemapping statistics.\n    \"\"\"\n    ss_ftp_path_template = \"https://ftp.ebi.ac.uk/pub/databases/spot/eQTL/sumstats\"\n    return (\n        lbf.join(\n            credible_sets.join(f.broadcast(studies_metadata), on=\"dataset_id\"),\n            on=[\"molecular_trait_id\", \"region\", \"variant\", \"dataset_id\"],\n            how=\"inner\",\n        )\n        .withColumn(\n            \"logBF\",\n            f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"lbf_variable1\"))\n            .when(f.col(\"credibleSetIndex\") == 2, f.col(\"lbf_variable2\"))\n            .when(f.col(\"credibleSetIndex\") == 3, f.col(\"lbf_variable3\"))\n            .when(f.col(\"credibleSetIndex\") == 4, f.col(\"lbf_variable4\"))\n            .when(f.col(\"credibleSetIndex\") == 5, f.col(\"lbf_variable5\"))\n            .when(f.col(\"credibleSetIndex\") == 6, f.col(\"lbf_variable6\"))\n            .when(f.col(\"credibleSetIndex\") == 7, f.col(\"lbf_variable7\"))\n            .when(f.col(\"credibleSetIndex\") == 8, f.col(\"lbf_variable8\"))\n            .when(f.col(\"credibleSetIndex\") == 9, f.col(\"lbf_variable9\"))\n            .when(f.col(\"credibleSetIndex\") == 10, f.col(\"lbf_variable10\")),\n        )\n        .select(\n            f.regexp_replace(f.col(\"variant\"), r\"chr\", \"\").alias(\"variantId\"),\n            f.col(\"region\"),\n            f.col(\"chromosome\"),\n            f.col(\"position\"),\n            f.col(\"pip\").alias(\"posteriorProbability\"),\n            *parse_pvalue(f.col(\"pvalue\")),\n            f.col(\"sample_size\").alias(\"nSamples\"),\n            f.col(\"beta\"),\n            f.col(\"se\").alias(\"standardError\"),\n            f.col(\"credibleSetIndex\"),\n            f.col(\"logBF\"),\n            f.lit(FinemappingMethod.SUSIE.value).alias(\"finemappingMethod\"),\n            # Study metadata\n            f.col(\"molecular_trait_id\").alias(\"traitFromSource\"),\n            f.col(\"gene_id\").alias(\"geneId\"),\n            f.col(\"dataset_id\"),\n            # Upon creation, the studyId cleaned from symbols:\n            clean_strings_from_symbols(\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"study_label\"),\n                    f.col(\"quant_method\"),\n                    f.col(\"sample_group\"),\n                    f.col(\"molecular_trait_id\"),\n                )\n            ).alias(\"studyId\"),\n            f.col(\"tissue_id\").alias(\"biosampleFromSourceId\"),\n            EqtlCatalogueStudyIndex._identify_study_type().alias(\"studyType\"),\n            f.col(\"study_label\").alias(\"projectId\"),\n            f.concat_ws(\n                \"/\",\n                f.lit(ss_ftp_path_template),\n                f.col(\"study_id\"),\n                f.col(\"dataset_id\"),\n            ).alias(\"summarystatsLocation\"),\n            f.lit(True).alias(\"hasSumstats\"),\n            f.col(\"molecular_trait_id\"),\n            f.col(\"pmid\").alias(\"pubmedId\"),\n            f.col(\"condition_label\").alias(\"condition\"),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping.read_credible_set_from_source","title":"<code>read_credible_set_from_source(session: Session, credible_set_path: str | list[str]) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Load raw credible sets from eQTL Catalogue.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session.</p> required <code>credible_set_path</code> <code>str | list[str]</code> <p>Path to raw table(s) containing finemapping results for any variant belonging to a credible set.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Credible sets DataFrame.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@classmethod\ndef read_credible_set_from_source(\n    cls: type[EqtlCatalogueFinemapping],\n    session: Session,\n    credible_set_path: str | list[str],\n) -&gt; DataFrame:\n    \"\"\"Load raw credible sets from eQTL Catalogue.\n\n    Args:\n        session (Session): Spark session.\n        credible_set_path (str | list[str]): Path to raw table(s) containing finemapping results for any variant belonging to a credible set.\n\n    Returns:\n        DataFrame: Credible sets DataFrame.\n    \"\"\"\n    return (\n        session.spark.read.csv(\n            credible_set_path,\n            sep=\"\\t\",\n            header=True,\n            schema=cls.raw_credible_set_schema,\n        )\n        .withColumns(\n            {\n                # Adding dataset id based on the input file name:\n                \"dataset_id\": cls._extract_dataset_id_from_file_path(\n                    f.input_file_name()\n                ),\n                # Parsing credible set index from the cs_id:\n                \"credibleSetIndex\": cls._extract_credible_set_index(f.col(\"cs_id\")),\n            }\n        )\n        # Remove duplicates caused by explosion of single variants to multiple rsid-s:\n        .drop(\"rsid\")\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping.read_lbf_from_source","title":"<code>read_lbf_from_source(session: Session, lbf_path: str | list[str]) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Load raw log Bayes Factors from eQTL Catalogue.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session.</p> required <code>lbf_path</code> <code>str | list[str]</code> <p>Path to raw table(s) containing Log Bayes Factors for each variant.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Log Bayes Factors DataFrame.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@classmethod\ndef read_lbf_from_source(\n    cls: type[EqtlCatalogueFinemapping],\n    session: Session,\n    lbf_path: str | list[str],\n) -&gt; DataFrame:\n    \"\"\"Load raw log Bayes Factors from eQTL Catalogue.\n\n    Args:\n        session (Session): Spark session.\n        lbf_path (str | list[str]): Path to raw table(s) containing Log Bayes Factors for each variant.\n\n    Returns:\n        DataFrame: Log Bayes Factors DataFrame.\n    \"\"\"\n    return (\n        session.spark.read.csv(\n            lbf_path,\n            sep=\"\\t\",\n            header=True,\n            schema=cls.raw_lbf_schema,\n        )\n        .withColumn(\n            \"dataset_id\",\n            cls._extract_dataset_id_from_file_path(f.input_file_name()),\n        )\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/study_index/","title":"Study Index","text":""},{"location":"python_api/datasources/eqtl_catalogue/study_index/#gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex","title":"<code>gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex</code>","text":"<p>Study index dataset from eQTL Catalogue.</p> <p>We extract study level metadata from eQTL Catalogue's fine mapping results. All available studies can be found here.</p> <p>One study from the eQTL Catalogue clusters together all the molecular QTLs (mQTLs) that were found:</p> <pre><code>- in the same publication (e.g. Alasoo_2018)\n- in the same cell type or tissue (e.g. monocytes)\n- and for the same measured molecular trait (e.g. ENSG00000141510)\n</code></pre> Source code in <code>src/gentropy/datasource/eqtl_catalogue/study_index.py</code> <pre><code>class EqtlCatalogueStudyIndex:\n    \"\"\"Study index dataset from eQTL Catalogue.\n\n    We extract study level metadata from eQTL Catalogue's fine mapping results. All available studies can be found [here](https://www.ebi.ac.uk/eqtl/Studies/).\n\n    One study from the eQTL Catalogue clusters together all the molecular QTLs (mQTLs) that were found:\n\n        - in the same publication (e.g. Alasoo_2018)\n        - in the same cell type or tissue (e.g. monocytes)\n        - and for the same measured molecular trait (e.g. ENSG00000141510)\n\n    \"\"\"\n\n    raw_studies_metadata_schema: StructType = StructType(\n        [\n            StructField(\"study_id\", StringType(), True),\n            StructField(\"dataset_id\", StringType(), True),\n            StructField(\"study_label\", StringType(), True),\n            StructField(\"sample_group\", StringType(), True),\n            StructField(\"tissue_id\", StringType(), True),\n            StructField(\"tissue_label\", StringType(), True),\n            StructField(\"condition_label\", StringType(), True),\n            StructField(\"sample_size\", IntegerType(), True),\n            StructField(\"quant_method\", StringType(), True),\n            StructField(\"pmid\", StringType(), True),\n            StructField(\"study_type\", StringType(), True),\n        ]\n    )\n    raw_studies_metadata_path = \"https://raw.githubusercontent.com/eQTL-Catalogue/eQTL-Catalogue-resources/fe3c4b4ed911b3a184271a6aadcd8c8769a66aba/data_tables/dataset_metadata.tsv\"\n    method_to_qtl_type_mapping = {\n        \"ge\": \"eqtl\",\n        \"exon\": \"eqtl\",\n        \"tx\": \"eqtl\",\n        \"microarray\": \"eqtl\",\n        \"leafcutter\": \"sqtl\",\n        \"aptamer\": \"pqtl\",\n        \"txrev\": \"tuqtl\",\n    }\n\n    @classmethod\n    def _identify_study_type(\n        cls: type[EqtlCatalogueStudyIndex],\n    ) -&gt; Column:\n        \"\"\"Identify the qtl type based on the quantification method and eqtl catalogue study type.\n\n        Returns:\n            Column: The study type.\n\n        Examples:\n            &gt;&gt;&gt; df = spark.createDataFrame([(\"ge\", \"bulk\"), (\"leafcutter\", \"bulk\"), (\"tx\", \"single-cell\")], [\"quant_method\", \"study_type\"])\n            &gt;&gt;&gt; df.withColumn(\"studyType\", EqtlCatalogueStudyIndex._identify_study_type()).show()\n            +------------+-----------+---------+\n            |quant_method| study_type|studyType|\n            +------------+-----------+---------+\n            |          ge|       bulk|     eqtl|\n            |  leafcutter|       bulk|     sqtl|\n            |          tx|single-cell|   sceqtl|\n            +------------+-----------+---------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        qtl_type_mapping = f.create_map(\n            *[f.lit(x) for x in chain(*cls.method_to_qtl_type_mapping.items())]\n        )[f.col(\"quant_method\")]\n        return f.when(\n            f.col(\"study_type\") == \"single-cell\",\n            f.concat(f.lit(\"sc\"), qtl_type_mapping),\n        ).otherwise(qtl_type_mapping)\n\n    @classmethod\n    def get_studies_of_interest(\n        cls: type[EqtlCatalogueStudyIndex],\n        studies_metadata: DataFrame,\n    ) -&gt; list[str]:\n        \"\"\"Filter studies of interest from the raw studies metadata.\n\n        Args:\n            studies_metadata (DataFrame): raw studies metadata filtered with studies of interest.\n\n        Returns:\n            list[str]: QTD IDs defining the studies of interest for ingestion.\n        \"\"\"\n        return (\n            studies_metadata.select(\"dataset_id\")\n            .distinct()\n            .toPandas()[\"dataset_id\"]\n            .tolist()\n        )\n\n    @classmethod\n    def from_susie_results(\n        cls: type[EqtlCatalogueStudyIndex],\n        processed_finemapping_df: DataFrame,\n    ) -&gt; StudyIndex:\n        \"\"\"Ingest study level metadata from eQTL Catalogue.\n\n        Args:\n            processed_finemapping_df (DataFrame): processed fine mapping results with study metadata.\n\n        Returns:\n            StudyIndex: eQTL Catalogue study index dataset derived from the selected SuSIE results.\n        \"\"\"\n        study_index_cols = [\n            field.name\n            for field in StudyIndex.get_schema().fields\n            if field.name in processed_finemapping_df.columns\n        ]\n        return StudyIndex(\n            _df=processed_finemapping_df.select(study_index_cols).distinct(),\n            _schema=StudyIndex.get_schema(),\n        )\n\n    @classmethod\n    def read_studies_from_source(\n        cls: type[EqtlCatalogueStudyIndex],\n        session: Session,\n        mqtl_quantification_methods_blacklist: list[str],\n    ) -&gt; DataFrame:\n        \"\"\"Read raw studies metadata from eQTL Catalogue.\n\n        Args:\n            session (Session): Spark session.\n            mqtl_quantification_methods_blacklist (list[str]): Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv\n\n        Returns:\n            DataFrame: raw studies metadata.\n        \"\"\"\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        return session.spark.createDataFrame(\n            pd.read_csv(cls.raw_studies_metadata_path, sep=\"\\t\"),\n            schema=cls.raw_studies_metadata_schema,\n        ).filter(~(f.col(\"quant_method\").isin(mqtl_quantification_methods_blacklist)))\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/study_index/#gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex.from_susie_results","title":"<code>from_susie_results(processed_finemapping_df: DataFrame) -&gt; StudyIndex</code>  <code>classmethod</code>","text":"<p>Ingest study level metadata from eQTL Catalogue.</p> <p>Parameters:</p> Name Type Description Default <code>processed_finemapping_df</code> <code>DataFrame</code> <p>processed fine mapping results with study metadata.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>eQTL Catalogue study index dataset derived from the selected SuSIE results.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/study_index.py</code> <pre><code>@classmethod\ndef from_susie_results(\n    cls: type[EqtlCatalogueStudyIndex],\n    processed_finemapping_df: DataFrame,\n) -&gt; StudyIndex:\n    \"\"\"Ingest study level metadata from eQTL Catalogue.\n\n    Args:\n        processed_finemapping_df (DataFrame): processed fine mapping results with study metadata.\n\n    Returns:\n        StudyIndex: eQTL Catalogue study index dataset derived from the selected SuSIE results.\n    \"\"\"\n    study_index_cols = [\n        field.name\n        for field in StudyIndex.get_schema().fields\n        if field.name in processed_finemapping_df.columns\n    ]\n    return StudyIndex(\n        _df=processed_finemapping_df.select(study_index_cols).distinct(),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/study_index/#gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex.get_studies_of_interest","title":"<code>get_studies_of_interest(studies_metadata: DataFrame) -&gt; list[str]</code>  <code>classmethod</code>","text":"<p>Filter studies of interest from the raw studies metadata.</p> <p>Parameters:</p> Name Type Description Default <code>studies_metadata</code> <code>DataFrame</code> <p>raw studies metadata filtered with studies of interest.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: QTD IDs defining the studies of interest for ingestion.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/study_index.py</code> <pre><code>@classmethod\ndef get_studies_of_interest(\n    cls: type[EqtlCatalogueStudyIndex],\n    studies_metadata: DataFrame,\n) -&gt; list[str]:\n    \"\"\"Filter studies of interest from the raw studies metadata.\n\n    Args:\n        studies_metadata (DataFrame): raw studies metadata filtered with studies of interest.\n\n    Returns:\n        list[str]: QTD IDs defining the studies of interest for ingestion.\n    \"\"\"\n    return (\n        studies_metadata.select(\"dataset_id\")\n        .distinct()\n        .toPandas()[\"dataset_id\"]\n        .tolist()\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/study_index/#gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex.read_studies_from_source","title":"<code>read_studies_from_source(session: Session, mqtl_quantification_methods_blacklist: list[str]) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Read raw studies metadata from eQTL Catalogue.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session.</p> required <code>mqtl_quantification_methods_blacklist</code> <code>list[str]</code> <p>Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>raw studies metadata.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/study_index.py</code> <pre><code>@classmethod\ndef read_studies_from_source(\n    cls: type[EqtlCatalogueStudyIndex],\n    session: Session,\n    mqtl_quantification_methods_blacklist: list[str],\n) -&gt; DataFrame:\n    \"\"\"Read raw studies metadata from eQTL Catalogue.\n\n    Args:\n        session (Session): Spark session.\n        mqtl_quantification_methods_blacklist (list[str]): Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv\n\n    Returns:\n        DataFrame: raw studies metadata.\n    \"\"\"\n    pd.DataFrame.iteritems = pd.DataFrame.items\n    return session.spark.createDataFrame(\n        pd.read_csv(cls.raw_studies_metadata_path, sep=\"\\t\"),\n        schema=cls.raw_studies_metadata_schema,\n    ).filter(~(f.col(\"quant_method\").isin(mqtl_quantification_methods_blacklist)))\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/summary_stats/","title":"Summary Stats","text":""},{"location":"python_api/datasources/eqtl_catalogue/summary_stats/#gentropy.datasource.eqtl_catalogue.summary_stats.EqtlCatalogueSummaryStats","title":"<code>gentropy.datasource.eqtl_catalogue.summary_stats.EqtlCatalogueSummaryStats</code>  <code>dataclass</code>","text":"<p>Summary statistics dataset for eQTL Catalogue.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/summary_stats.py</code> <pre><code>@dataclass\nclass EqtlCatalogueSummaryStats:\n    \"\"\"Summary statistics dataset for eQTL Catalogue.\"\"\"\n\n    @staticmethod\n    def _full_study_id_regexp() -&gt; Column:\n        \"\"\"Constructs a full study ID from the URI.\n\n        Returns:\n            Column: expression to extract a full study ID from the URI.\n        \"\"\"\n        # Example of a URI which is used for parsing:\n        # \"gs://genetics_etl_python_playground/input/preprocess/eqtl_catalogue/imported/GTEx_V8/ge/Adipose_Subcutaneous.tsv.gz\".\n\n        # Regular expession to extract project ID from URI.  Example: \"GTEx_V8\".\n        _project_id = f.regexp_extract(\n            f.input_file_name(),\n            r\"imported/([^/]+)/.*\",\n            1,\n        )\n        # Regular expression to extract QTL group from URI.  Example: \"Adipose_Subcutaneous\".\n        _qtl_group = f.regexp_extract(f.input_file_name(), r\"([^/]+)\\.tsv\\.gz\", 1)\n        # Extracting gene ID from the column.  Example: \"ENSG00000225630\".\n        _gene_id = f.col(\"gene_id\")\n\n        # We can now construct the full study ID based on all fields.\n        # Example: \"GTEx_V8_Adipose_Subcutaneous_ENSG00000225630\".\n        return f.concat(_project_id, f.lit(\"_\"), _qtl_group, f.lit(\"_\"), _gene_id)\n\n    @classmethod\n    def from_source(\n        cls: type[EqtlCatalogueSummaryStats],\n        summary_stats_df: DataFrame,\n    ) -&gt; SummaryStatistics:\n        \"\"\"Ingests all summary stats for all eQTL Catalogue studies.\n\n        Args:\n            summary_stats_df (DataFrame): an ingested but unprocessed summary statistics dataframe from eQTL Catalogue.\n\n        Returns:\n            SummaryStatistics: a processed summary statistics dataframe for eQTL Catalogue.\n        \"\"\"\n        processed_summary_stats_df = (\n            summary_stats_df.select(\n                # Construct study ID from the appropriate columns.\n                cls._full_study_id_regexp().alias(\"studyId\"),\n                # Add variant information.\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"chromosome\"),\n                    f.col(\"position\"),\n                    f.col(\"ref\"),\n                    f.col(\"alt\"),\n                ).alias(\"variantId\"),\n                f.col(\"chromosome\"),\n                f.col(\"position\").cast(t.IntegerType()),\n                # Parse p-value into mantissa and exponent.\n                *parse_pvalue(f.col(\"pvalue\")),\n                # Add beta, standard error, and allele frequency information.\n                f.col(\"beta\").cast(\"double\"),\n                f.col(\"se\").cast(\"double\").alias(\"standardError\"),\n                f.col(\"maf\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n            )\n            # Drop rows which don't have proper position or beta value.\n            .filter(\n                f.col(\"position\").cast(t.IntegerType()).isNotNull()\n                &amp; (f.col(\"beta\") != 0)\n            )\n        )\n\n        # Initialise a summary statistics object.\n        return SummaryStatistics(\n            _df=processed_summary_stats_df,\n            _schema=SummaryStatistics.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/summary_stats/#gentropy.datasource.eqtl_catalogue.summary_stats.EqtlCatalogueSummaryStats.from_source","title":"<code>from_source(summary_stats_df: DataFrame) -&gt; SummaryStatistics</code>  <code>classmethod</code>","text":"<p>Ingests all summary stats for all eQTL Catalogue studies.</p> <p>Parameters:</p> Name Type Description Default <code>summary_stats_df</code> <code>DataFrame</code> <p>an ingested but unprocessed summary statistics dataframe from eQTL Catalogue.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>a processed summary statistics dataframe for eQTL Catalogue.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/summary_stats.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[EqtlCatalogueSummaryStats],\n    summary_stats_df: DataFrame,\n) -&gt; SummaryStatistics:\n    \"\"\"Ingests all summary stats for all eQTL Catalogue studies.\n\n    Args:\n        summary_stats_df (DataFrame): an ingested but unprocessed summary statistics dataframe from eQTL Catalogue.\n\n    Returns:\n        SummaryStatistics: a processed summary statistics dataframe for eQTL Catalogue.\n    \"\"\"\n    processed_summary_stats_df = (\n        summary_stats_df.select(\n            # Construct study ID from the appropriate columns.\n            cls._full_study_id_regexp().alias(\"studyId\"),\n            # Add variant information.\n            f.concat_ws(\n                \"_\",\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                f.col(\"ref\"),\n                f.col(\"alt\"),\n            ).alias(\"variantId\"),\n            f.col(\"chromosome\"),\n            f.col(\"position\").cast(t.IntegerType()),\n            # Parse p-value into mantissa and exponent.\n            *parse_pvalue(f.col(\"pvalue\")),\n            # Add beta, standard error, and allele frequency information.\n            f.col(\"beta\").cast(\"double\"),\n            f.col(\"se\").cast(\"double\").alias(\"standardError\"),\n            f.col(\"maf\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n        )\n        # Drop rows which don't have proper position or beta value.\n        .filter(\n            f.col(\"position\").cast(t.IntegerType()).isNotNull()\n            &amp; (f.col(\"beta\") != 0)\n        )\n    )\n\n    # Initialise a summary statistics object.\n    return SummaryStatistics(\n        _df=processed_summary_stats_df,\n        _schema=SummaryStatistics.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen/_finngen/","title":"FinnGen","text":"<p>FinnGen is a research project in genomics and personalized medicine, representing a large public-private partnership. The project has collected and analyzed genome and health data from 500,000 Finnish biobank donors to understand the genetic basis of diseases. FinnGen is now expanding its focus to comprehend the progression and biological mechanisms of diseases. This initiative provides a world-class resource for further breakthroughs in disease prevention, diagnosis, and treatment, offering insights into our genetic makeup.</p> <p>For a comprehensive understanding of the dataset and methods, refer to Kurki et al., 2023.</p> <p>We ingested full GWAS summary statistics and SuSiE-based fine-mapping results.</p>"},{"location":"python_api/datasources/finngen/finemapping/","title":"Finemapping","text":""},{"location":"python_api/datasources/finngen/finemapping/#gentropy.datasource.finngen.finemapping.FinnGenFinemapping","title":"<code>gentropy.datasource.finngen.finemapping.FinnGenFinemapping</code>  <code>dataclass</code>","text":"<p>SuSIE finemapping dataset for FinnGen.</p> <p>Credible sets from SuSIE are extracted and transformed into StudyLocus objects:</p> <ul> <li>Study ID in the special format (e.g. FINNGEN_R11*)</li> <li>Credible set specific finemapping statistics (e.g. LogBayesFactors, Alphas/Posterior)</li> <li>Additional credible set level BayesFactor filtering is applied (LBF &gt; 2)</li> <li>StudyLocusId is annotated for each credible set.</li> </ul> <p>Finemapping method is populated as a constant (\"SuSIE\").</p> Source code in <code>src/gentropy/datasource/finngen/finemapping.py</code> <pre><code>@dataclass\nclass FinnGenFinemapping:\n    \"\"\"SuSIE finemapping dataset for FinnGen.\n\n    Credible sets from SuSIE are extracted and transformed into StudyLocus objects:\n\n    - Study ID in the special format (e.g. FINNGEN_R11*)\n    - Credible set specific finemapping statistics (e.g. LogBayesFactors, Alphas/Posterior)\n    - Additional credible set level BayesFactor filtering is applied (LBF &gt; 2)\n    - StudyLocusId is annotated for each credible set.\n\n    Finemapping method is populated as a constant (\"SuSIE\").\n    \"\"\"\n\n    raw_schema: t.StructType = StructType(\n        [\n            StructField(\"trait\", StringType(), True),\n            StructField(\"region\", StringType(), True),\n            StructField(\"v\", StringType(), True),\n            StructField(\"rsid\", StringType(), True),\n            StructField(\"chromosome\", StringType(), True),\n            StructField(\"position\", StringType(), True),\n            StructField(\"allele1\", StringType(), True),\n            StructField(\"allele2\", StringType(), True),\n            StructField(\"maf\", StringType(), True),\n            StructField(\"beta\", StringType(), True),\n            StructField(\"se\", StringType(), True),\n            StructField(\"p\", StringType(), True),\n            StructField(\"mean\", StringType(), True),\n            StructField(\"sd\", StringType(), True),\n            StructField(\"prob\", StringType(), True),\n            StructField(\"cs\", StringType(), True),\n            StructField(\"cs_specific_prob\", DoubleType(), True),\n            StructField(\"low_purity\", StringType(), True),\n            StructField(\"lead_r2\", StringType(), True),\n            StructField(\"mean_99\", StringType(), True),\n            StructField(\"sd_99\", StringType(), True),\n            StructField(\"prob_99\", StringType(), True),\n            StructField(\"cs_99\", StringType(), True),\n            StructField(\"cs_specific_prob_99\", StringType(), True),\n            StructField(\"low_purity_99\", StringType(), True),\n            StructField(\"lead_r2_99\", StringType(), True),\n            StructField(\"alpha1\", DoubleType(), True),\n            StructField(\"alpha2\", DoubleType(), True),\n            StructField(\"alpha3\", DoubleType(), True),\n            StructField(\"alpha4\", DoubleType(), True),\n            StructField(\"alpha5\", DoubleType(), True),\n            StructField(\"alpha6\", DoubleType(), True),\n            StructField(\"alpha7\", DoubleType(), True),\n            StructField(\"alpha8\", DoubleType(), True),\n            StructField(\"alpha9\", DoubleType(), True),\n            StructField(\"alpha10\", DoubleType(), True),\n            StructField(\"mean1\", StringType(), True),\n            StructField(\"mean2\", StringType(), True),\n            StructField(\"mean3\", StringType(), True),\n            StructField(\"mean4\", StringType(), True),\n            StructField(\"mean5\", StringType(), True),\n            StructField(\"mean6\", StringType(), True),\n            StructField(\"mean7\", StringType(), True),\n            StructField(\"mean8\", StringType(), True),\n            StructField(\"mean9\", StringType(), True),\n            StructField(\"mean10\", StringType(), True),\n            StructField(\"sd1\", StringType(), True),\n            StructField(\"sd2\", StringType(), True),\n            StructField(\"sd3\", StringType(), True),\n            StructField(\"sd4\", StringType(), True),\n            StructField(\"sd5\", StringType(), True),\n            StructField(\"sd6\", StringType(), True),\n            StructField(\"sd7\", StringType(), True),\n            StructField(\"sd8\", StringType(), True),\n            StructField(\"sd9\", StringType(), True),\n            StructField(\"sd10\", StringType(), True),\n            StructField(\"lbf_variable1\", DoubleType(), True),\n            StructField(\"lbf_variable2\", DoubleType(), True),\n            StructField(\"lbf_variable3\", DoubleType(), True),\n            StructField(\"lbf_variable4\", DoubleType(), True),\n            StructField(\"lbf_variable5\", DoubleType(), True),\n            StructField(\"lbf_variable6\", DoubleType(), True),\n            StructField(\"lbf_variable7\", DoubleType(), True),\n            StructField(\"lbf_variable8\", DoubleType(), True),\n            StructField(\"lbf_variable9\", DoubleType(), True),\n            StructField(\"lbf_variable10\", DoubleType(), True),\n        ]\n    )\n\n    summary_schema: t.StructType = StructType(\n        [\n            StructField(\"trait\", StringType(), True),\n            StructField(\"region\", StringType(), True),\n            StructField(\"cs\", StringType(), True),\n            StructField(\"cs_log10bf\", DoubleType(), True),\n            StructField(\"cs_avg_r2\", DoubleType(), True),\n            StructField(\"cs_min_r2\", DoubleType(), True),\n        ]\n    )\n\n    raw_hail_shema: hl.tstruct = hl.tstruct(\n        trait=hl.tstr,\n        region=hl.tstr,\n        v=hl.tstr,\n        rsid=hl.tstr,\n        chromosome=hl.tstr,\n        position=hl.tstr,\n        allele1=hl.tstr,\n        allele2=hl.tstr,\n        maf=hl.tstr,\n        beta=hl.tstr,\n        se=hl.tstr,\n        p=hl.tstr,\n        mean=hl.tstr,\n        sd=hl.tstr,\n        prob=hl.tstr,\n        cs=hl.tstr,\n        cs_specific_prob=hl.tfloat64,\n        low_purity=hl.tstr,\n        lead_r2=hl.tstr,\n        mean_99=hl.tstr,\n        sd_99=hl.tstr,\n        prob_99=hl.tstr,\n        cs_99=hl.tstr,\n        cs_specific_prob_99=hl.tstr,\n        low_purity_99=hl.tstr,\n        lead_r2_99=hl.tstr,\n        alpha1=hl.tfloat64,\n        alpha2=hl.tfloat64,\n        alpha3=hl.tfloat64,\n        alpha4=hl.tfloat64,\n        alpha5=hl.tfloat64,\n        alpha6=hl.tfloat64,\n        alpha7=hl.tfloat64,\n        alpha8=hl.tfloat64,\n        alpha9=hl.tfloat64,\n        alpha10=hl.tfloat64,\n        mean1=hl.tstr,\n        mean2=hl.tstr,\n        mean3=hl.tstr,\n        mean4=hl.tstr,\n        mean5=hl.tstr,\n        mean6=hl.tstr,\n        mean7=hl.tstr,\n        mean8=hl.tstr,\n        mean9=hl.tstr,\n        mean10=hl.tstr,\n        sd1=hl.tstr,\n        sd2=hl.tstr,\n        sd3=hl.tstr,\n        sd4=hl.tstr,\n        sd5=hl.tstr,\n        sd6=hl.tstr,\n        sd7=hl.tstr,\n        sd8=hl.tstr,\n        sd9=hl.tstr,\n        sd10=hl.tstr,\n        lbf_variable1=hl.tfloat64,\n        lbf_variable2=hl.tfloat64,\n        lbf_variable3=hl.tfloat64,\n        lbf_variable4=hl.tfloat64,\n        lbf_variable5=hl.tfloat64,\n        lbf_variable6=hl.tfloat64,\n        lbf_variable7=hl.tfloat64,\n        lbf_variable8=hl.tfloat64,\n        lbf_variable9=hl.tfloat64,\n        lbf_variable10=hl.tfloat64,\n    )\n\n    summary_hail_schema: hl.tstruct = hl.tstruct(\n        trait=hl.tstr,\n        region=hl.tstr,\n        cs=hl.tstr,\n        cs_log10bf=hl.tfloat64,\n        cs_avg_r2=hl.tfloat64,\n        cs_min_r2=hl.tfloat64,\n    )\n\n    @staticmethod\n    def _infer_block_gzip_compression(paths: str | list[str]) -&gt; bool:\n        \"\"\"Naively infer compression type based on the file extension.\n\n        Args:\n            paths (str | list[str]): File path(s).\n\n        Returns:\n            bool: True if block gzipped, False otherwise.\n        \"\"\"\n        if isinstance(paths, str):\n            return paths.endswith(\".bgz\")\n        return all(path.endswith(\".bgz\") for path in paths)\n\n    @classmethod\n    def from_finngen_susie_finemapping(\n        cls: type[FinnGenFinemapping],\n        spark: SparkSession,\n        finngen_susie_finemapping_snp_files: (str | list[str]),\n        finngen_susie_finemapping_cs_summary_files: (str | list[str]),\n        finngen_release_prefix: str,\n        credset_lbf_threshold: float = 0.8685889638065036,\n    ) -&gt; StudyLocus:\n        \"\"\"Process the SuSIE finemapping output for FinnGen studies.\n\n        The finngen_susie_finemapping_snp_files are files that contain variant summaries with credible set information with following shema:\n            - trait: phenotype\n            - region: region for which the fine-mapping was run.\n            - v, rsid: variant ids\n            - chromosome\n            - position\n            - allele1\n            - allele2\n            - maf: minor allele frequency\n            - beta: original marginal beta\n            - se: original se\n            - p: original p\n            - mean: posterior mean beta after fine-mapping\n            - sd: posterior standard deviation after fine-mapping.\n            - prob: posterior inclusion probability\n            - cs: credible set index within region\n            - lead_r2: r2 value to a lead variant (the one with maximum PIP) in a credible set\n            - alphax: posterior inclusion probability for the x-th single effect (x := 1..L where L is the number of single effects (causal variants) specified; default: L = 10).\n            - lbfx: log-Bayes Factor for each variable and single effect (i.e credible set).\n            - meanx: posterior mean for each variable and single effect (i.e credible set).\n            - sdx: posterior sd of mean for each variable and single effect (i.e credible set).\n        As for r11 finngen release these files are ingested from `https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/` by\n            - *.snp.bgz\n            - *.snp.bgz.tbi\n        Each file contains index (.tbi) file that is required to read the block gzipped compressed snp file. These files needs to be\n        downloaded, transfromed from block gzipped to plain gzipped and then uploaded to the storage bucket, before they can be read by spark or read by hail directly as import table.\n\n        The finngen_susie_finemapping_cs_summary_files are files that Contains credible set summaries from SuSiE fine-mapping for all genome-wide significant regions with following schema:\n            - trait: phenotype\n            - region: region for which the fine-mapping was run.\n            - cs: running number for independent credible sets in a region, assigned to 95% PIP\n            - cs_log10bf: Log10 bayes factor of comparing the solution of this model (cs independent credible sets) to cs -1 credible sets\n            - cs_avg_r2: Average correlation R2 between variants in the credible set\n            - cs_min_r2: minimum r2 between variants in the credible set\n            - low_purity: boolean (TRUE,FALSE) indicator if the CS is low purity (low min r2)\n            - cs_size: how many snps does this credible set contain\n            - good_cs: boolean (TRUE,FALSE) indicator if this CS is considered reliable. IF this is FALSE then top variant reported for the CS will be chosen based on minimum p-value in the credible set, otherwise top variant is chosen by maximum PIP\n            - cs_id:\n            - v: top variant (chr:pos:ref:alt)\n            - p: top variant p-value\n            - beta: top variant beta\n            - sd: top variant standard deviation\n            - prob: overall PIP of the variant in the region\n            - cs_specific_prob: PIP of the variant in the current credible set (this and previous are typically almost identical)\n            - 0..n: configured annotation columns. Typical default most_severe,gene_most_severe giving consequence and gene of top variant\n        These files needs to be downloaded from the `https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/summary/` by *.cred.summary.tsv pattern,\n\n        Args:\n            spark (SparkSession): SparkSession object.\n            finngen_susie_finemapping_snp_files (str | list[str]): SuSIE finemapping output filename(s).\n            finngen_susie_finemapping_cs_summary_files (str | list[str]): filename of SuSIE finemapping credible set summaries.\n            finngen_release_prefix (str): Finngen project release prefix. Should look like FINNGEN_R*.\n            credset_lbf_threshold (float, optional): Filter out credible sets below, Default 0.8685889638065036 == np.log10(np.exp(2)), this is threshold from publication.\n\n        Returns:\n            StudyLocus: Processed SuSIE finemapping output in StudyLocus format.\n        \"\"\"\n        # NOTE: hail allows for importing block gzipped files, spark does not without external libraries.\n        # check https://github.com/projectglow/glow/blob/36bf6121fbc4ccc33a13b028deb87b63faeba7a9/core/src/main/scala/io/projectglow/vcf/VCFFileFormat.scala#L274\n        # how it could be implemented with spark.\n        bgzip_compressed_snps = cls._infer_block_gzip_compression(\n            finngen_susie_finemapping_snp_files\n        )\n\n        # NOTE: fallback to spark read if not block gzipped file in the input\n        if bgzip_compressed_snps:\n            snps_df = hl.import_table(\n                finngen_susie_finemapping_snp_files,\n                delimiter=\"\\t\",\n                types=cls.raw_hail_shema,\n            ).to_spark()\n        else:\n            snps_df = (\n                spark.read.schema(cls.raw_schema)\n                .option(\"delimiter\", \"\\t\")\n                .option(\"compression\", \"gzip\")\n                .csv(finngen_susie_finemapping_snp_files, header=True)\n            )\n\n        processed_finngen_finemapping_df = (\n            # Drop rows which don't have proper position.\n            snps_df.filter(f.col(\"position\").cast(t.IntegerType()).isNotNull())\n            # Drop non credible set SNPs:\n            .filter(f.col(\"cs\").cast(t.IntegerType()) &gt; 0)\n            .select(\n                # Add study idenfitier.\n                f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"trait\"))\n                .cast(t.StringType())\n                .alias(\"studyId\"),\n                f.col(\"region\"),\n                # Add variant information.\n                f.regexp_replace(f.col(\"v\"), \":\", \"_\").alias(\"variantId\"),\n                f.col(\"cs\").cast(\"integer\").alias(\"credibleSetIndex\"),\n                f.regexp_replace(f.col(\"chromosome\"), \"^chr\", \"\")\n                .cast(t.StringType())\n                .alias(\"chromosome\"),\n                f.col(\"position\").cast(t.IntegerType()),\n                f.col(\"allele1\").cast(t.StringType()).alias(\"ref\"),\n                f.col(\"allele2\").cast(t.StringType()).alias(\"alt\"),\n                # Parse p-value into mantissa and exponent.\n                *parse_pvalue(f.col(\"p\")),\n                # Add standard error, and allele frequency information.\n                f.col(\"se\").cast(\"double\").alias(\"standardError\"),\n                f.col(\"maf\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n                f.lit(FinemappingMethod.SUSIE.value).alias(\"finemappingMethod\"),\n                *[\n                    f.col(f\"alpha{i}\").cast(t.DoubleType()).alias(f\"alpha_{i}\")\n                    for i in range(1, 11)\n                ],\n                *[\n                    f.col(f\"lbf_variable{i}\").cast(t.DoubleType()).alias(f\"lbf_{i}\")\n                    for i in range(1, 11)\n                ],\n                *[\n                    f.col(f\"mean{i}\").cast(t.DoubleType()).alias(f\"beta_{i}\")\n                    for i in range(1, 11)\n                ],\n            )\n            .withColumn(\n                \"posteriorProbability\",\n                f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"alpha_1\"))\n                .when(f.col(\"credibleSetIndex\") == 2, f.col(\"alpha_2\"))\n                .when(f.col(\"credibleSetIndex\") == 3, f.col(\"alpha_3\"))\n                .when(f.col(\"credibleSetIndex\") == 4, f.col(\"alpha_4\"))\n                .when(f.col(\"credibleSetIndex\") == 5, f.col(\"alpha_5\"))\n                .when(f.col(\"credibleSetIndex\") == 6, f.col(\"alpha_6\"))\n                .when(f.col(\"credibleSetIndex\") == 7, f.col(\"alpha_7\"))\n                .when(f.col(\"credibleSetIndex\") == 8, f.col(\"alpha_8\"))\n                .when(f.col(\"credibleSetIndex\") == 9, f.col(\"alpha_9\"))\n                .when(f.col(\"credibleSetIndex\") == 10, f.col(\"alpha_10\")),\n            )\n            .drop(\n                \"alpha_1\",\n                \"alpha_2\",\n                \"alpha_3\",\n                \"alpha_4\",\n                \"alpha_5\",\n                \"alpha_6\",\n                \"alpha_7\",\n                \"alpha_8\",\n                \"alpha_9\",\n                \"alpha_10\",\n            )\n            .withColumn(\n                \"logBF\",\n                f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"lbf_1\"))\n                .when(f.col(\"credibleSetIndex\") == 2, f.col(\"lbf_2\"))\n                .when(f.col(\"credibleSetIndex\") == 3, f.col(\"lbf_3\"))\n                .when(f.col(\"credibleSetIndex\") == 4, f.col(\"lbf_4\"))\n                .when(f.col(\"credibleSetIndex\") == 5, f.col(\"lbf_5\"))\n                .when(f.col(\"credibleSetIndex\") == 6, f.col(\"lbf_6\"))\n                .when(f.col(\"credibleSetIndex\") == 7, f.col(\"lbf_7\"))\n                .when(f.col(\"credibleSetIndex\") == 8, f.col(\"lbf_8\"))\n                .when(f.col(\"credibleSetIndex\") == 9, f.col(\"lbf_9\"))\n                .when(f.col(\"credibleSetIndex\") == 10, f.col(\"lbf_10\")),\n            )\n            .drop(\n                \"lbf_1\",\n                \"lbf_2\",\n                \"lbf_3\",\n                \"lbf_4\",\n                \"lbf_5\",\n                \"lbf_6\",\n                \"lbf_7\",\n                \"lbf_8\",\n                \"lbf_9\",\n                \"lbf_10\",\n            )\n            .withColumn(\n                \"beta\",\n                f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"beta_1\"))\n                .when(f.col(\"credibleSetIndex\") == 2, f.col(\"beta_2\"))\n                .when(f.col(\"credibleSetIndex\") == 3, f.col(\"beta_3\"))\n                .when(f.col(\"credibleSetIndex\") == 4, f.col(\"beta_4\"))\n                .when(f.col(\"credibleSetIndex\") == 5, f.col(\"beta_5\"))\n                .when(f.col(\"credibleSetIndex\") == 6, f.col(\"beta_6\"))\n                .when(f.col(\"credibleSetIndex\") == 7, f.col(\"beta_7\"))\n                .when(f.col(\"credibleSetIndex\") == 8, f.col(\"beta_8\"))\n                .when(f.col(\"credibleSetIndex\") == 9, f.col(\"beta_9\"))\n                .when(f.col(\"credibleSetIndex\") == 10, f.col(\"beta_10\")),\n            )\n            .drop(\n                \"beta_1\",\n                \"beta_2\",\n                \"beta_3\",\n                \"beta_4\",\n                \"beta_5\",\n                \"beta_6\",\n                \"beta_7\",\n                \"beta_8\",\n                \"beta_9\",\n                \"beta_10\",\n            )\n        )\n\n        bgzip_compressed_cs_summaries = cls._infer_block_gzip_compression(\n            finngen_susie_finemapping_cs_summary_files\n        )\n\n        # NOTE: fallback to spark read if not block gzipped file in the input\n        # in case we want to use the raw files from the\n        # https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/*.cred.gz\n        if bgzip_compressed_cs_summaries:\n            cs_summary_df = hl.import_table(\n                finngen_susie_finemapping_cs_summary_files,\n                delimiter=\"\\t\",\n                types=cls.summary_hail_schema,\n            ).to_spark()\n        else:\n            cs_summary_df = (\n                spark.read.schema(cls.summary_schema)\n                .option(\"delimiter\", \"\\t\")\n                .csv(finngen_susie_finemapping_cs_summary_files, header=True)\n            )\n\n        # drop credible sets where logbf &gt; 2. Except when there's only one credible set in region:\n        # 0.8685889638065036 corresponds to np.log10(np.exp(2)), to match the orginal threshold in publication.\n        finngen_finemapping_summaries_df = (\n            # Read credible set level lbf, it is output as a different file which is not ideal.\n            cs_summary_df.select(\n                f.col(\"region\"),\n                f.col(\"trait\"),\n                f.col(\"cs\").cast(\"integer\").alias(\"credibleSetIndex\"),\n                f.col(\"cs_log10bf\").cast(\"double\").alias(\"credibleSetlog10BF\"),\n                f.col(\"cs_avg_r2\").cast(\"double\").alias(\"purityMeanR2\"),\n                f.col(\"cs_min_r2\").cast(\"double\").alias(\"purityMinR2\"),\n            )\n            .filter(\n                (f.col(\"credibleSetlog10BF\") &gt; credset_lbf_threshold)\n                | (f.col(\"credibleSetIndex\") == 1)\n            )\n            .withColumn(\n                \"studyId\",\n                f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"trait\")),\n            )\n        )\n\n        processed_finngen_finemapping_df = processed_finngen_finemapping_df.join(\n            finngen_finemapping_summaries_df,\n            on=[\"studyId\", \"region\", \"credibleSetIndex\"],\n            how=\"inner\",\n        )\n\n        toploci_df = get_top_ranked_in_window(\n            processed_finngen_finemapping_df,\n            Window.partitionBy(\"studyId\", \"region\", \"credibleSetIndex\").orderBy(\n                f.desc(\"posteriorProbability\")\n            ),\n        ).select(\n            \"variantId\",\n            \"chromosome\",\n            \"position\",\n            \"studyId\",\n            \"beta\",\n            \"pValueMantissa\",\n            \"pValueExponent\",\n            \"effectAlleleFrequencyFromSource\",\n            \"standardError\",\n            \"region\",\n            \"credibleSetIndex\",\n            \"finemappingMethod\",\n            \"credibleSetlog10BF\",\n            \"purityMeanR2\",\n            \"purityMinR2\",\n        )\n\n        processed_finngen_finemapping_df = (\n            processed_finngen_finemapping_df.groupBy(\n                \"studyId\", \"region\", \"credibleSetIndex\"\n            )\n            .agg(\n                f.collect_list(\n                    f.struct(\n                        f.col(\"variantId\").cast(\"string\").alias(\"variantId\"),\n                        f.col(\"posteriorProbability\")\n                        .cast(\"double\")\n                        .alias(\"posteriorProbability\"),\n                        f.col(\"logBF\").cast(\"double\").alias(\"logBF\"),\n                        f.col(\"pValueMantissa\").cast(\"float\").alias(\"pValueMantissa\"),\n                        f.col(\"pValueExponent\").cast(\"integer\").alias(\"pValueExponent\"),\n                        f.col(\"beta\").cast(\"double\").alias(\"beta\"),\n                        f.col(\"standardError\").cast(\"double\").alias(\"standardError\"),\n                    )\n                ).alias(\"locus\"),\n            )\n            .select(\n                \"studyId\",\n                \"region\",\n                \"credibleSetIndex\",\n                \"locus\",\n            )\n            .join(\n                toploci_df,\n                on=[\"studyId\", \"region\", \"credibleSetIndex\"],\n                how=\"inner\",\n            )\n            .withColumns(\n                {\n                    \"locusStart\": f.split(f.split(\"region\", \":\")[1], \"-\")[0].cast(\n                        \"int\"\n                    ),\n                    \"locusEnd\": f.split(f.split(\"region\", \":\")[1], \"-\")[1].cast(\"int\"),\n                }\n            )\n        ).withColumn(\n            \"studyLocusId\",\n            StudyLocus.assign_study_locus_id(\n                [\"studyId\", \"variantId\", \"finemappingMethod\"]\n            ),\n        )\n\n        return StudyLocus(\n            _df=processed_finngen_finemapping_df,\n            _schema=StudyLocus.get_schema(),\n        ).annotate_credible_sets()\n</code></pre>"},{"location":"python_api/datasources/finngen/finemapping/#gentropy.datasource.finngen.finemapping.FinnGenFinemapping.from_finngen_susie_finemapping","title":"<code>from_finngen_susie_finemapping(spark: SparkSession, finngen_susie_finemapping_snp_files: str | list[str], finngen_susie_finemapping_cs_summary_files: str | list[str], finngen_release_prefix: str, credset_lbf_threshold: float = 0.8685889638065036) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Process the SuSIE finemapping output for FinnGen studies.</p> The finngen_susie_finemapping_snp_files are files that contain variant summaries with credible set information with following shema <ul> <li>trait: phenotype</li> <li>region: region for which the fine-mapping was run.</li> <li>v, rsid: variant ids</li> <li>chromosome</li> <li>position</li> <li>allele1</li> <li>allele2</li> <li>maf: minor allele frequency</li> <li>beta: original marginal beta</li> <li>se: original se</li> <li>p: original p</li> <li>mean: posterior mean beta after fine-mapping</li> <li>sd: posterior standard deviation after fine-mapping.</li> <li>prob: posterior inclusion probability</li> <li>cs: credible set index within region</li> <li>lead_r2: r2 value to a lead variant (the one with maximum PIP) in a credible set</li> <li>alphax: posterior inclusion probability for the x-th single effect (x := 1..L where L is the number of single effects (causal variants) specified; default: L = 10).</li> <li>lbfx: log-Bayes Factor for each variable and single effect (i.e credible set).</li> <li>meanx: posterior mean for each variable and single effect (i.e credible set).</li> <li>sdx: posterior sd of mean for each variable and single effect (i.e credible set).</li> </ul> <p>As for r11 finngen release these files are ingested from <code>https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/</code> by     - .snp.bgz     - .snp.bgz.tbi Each file contains index (.tbi) file that is required to read the block gzipped compressed snp file. These files needs to be downloaded, transfromed from block gzipped to plain gzipped and then uploaded to the storage bucket, before they can be read by spark or read by hail directly as import table.</p> The finngen_susie_finemapping_cs_summary_files are files that Contains credible set summaries from SuSiE fine-mapping for all genome-wide significant regions with following schema <ul> <li>trait: phenotype</li> <li>region: region for which the fine-mapping was run.</li> <li>cs: running number for independent credible sets in a region, assigned to 95% PIP</li> <li>cs_log10bf: Log10 bayes factor of comparing the solution of this model (cs independent credible sets) to cs -1 credible sets</li> <li>cs_avg_r2: Average correlation R2 between variants in the credible set</li> <li>cs_min_r2: minimum r2 between variants in the credible set</li> <li>low_purity: boolean (TRUE,FALSE) indicator if the CS is low purity (low min r2)</li> <li>cs_size: how many snps does this credible set contain</li> <li>good_cs: boolean (TRUE,FALSE) indicator if this CS is considered reliable. IF this is FALSE then top variant reported for the CS will be chosen based on minimum p-value in the credible set, otherwise top variant is chosen by maximum PIP</li> <li>cs_id:</li> <li>v: top variant (chr:pos:ref:alt)</li> <li>p: top variant p-value</li> <li>beta: top variant beta</li> <li>sd: top variant standard deviation</li> <li>prob: overall PIP of the variant in the region</li> <li>cs_specific_prob: PIP of the variant in the current credible set (this and previous are typically almost identical)</li> <li>0..n: configured annotation columns. Typical default most_severe,gene_most_severe giving consequence and gene of top variant</li> </ul> <p>These files needs to be downloaded from the <code>https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/summary/</code> by *.cred.summary.tsv pattern,</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>SparkSession object.</p> required <code>finngen_susie_finemapping_snp_files</code> <code>str | list[str]</code> <p>SuSIE finemapping output filename(s).</p> required <code>finngen_susie_finemapping_cs_summary_files</code> <code>str | list[str]</code> <p>filename of SuSIE finemapping credible set summaries.</p> required <code>finngen_release_prefix</code> <code>str</code> <p>Finngen project release prefix. Should look like FINNGEN_R*.</p> required <code>credset_lbf_threshold</code> <code>float</code> <p>Filter out credible sets below, Default 0.8685889638065036 == np.log10(np.exp(2)), this is threshold from publication.</p> <code>0.8685889638065036</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Processed SuSIE finemapping output in StudyLocus format.</p> Source code in <code>src/gentropy/datasource/finngen/finemapping.py</code> <pre><code>@classmethod\ndef from_finngen_susie_finemapping(\n    cls: type[FinnGenFinemapping],\n    spark: SparkSession,\n    finngen_susie_finemapping_snp_files: (str | list[str]),\n    finngen_susie_finemapping_cs_summary_files: (str | list[str]),\n    finngen_release_prefix: str,\n    credset_lbf_threshold: float = 0.8685889638065036,\n) -&gt; StudyLocus:\n    \"\"\"Process the SuSIE finemapping output for FinnGen studies.\n\n    The finngen_susie_finemapping_snp_files are files that contain variant summaries with credible set information with following shema:\n        - trait: phenotype\n        - region: region for which the fine-mapping was run.\n        - v, rsid: variant ids\n        - chromosome\n        - position\n        - allele1\n        - allele2\n        - maf: minor allele frequency\n        - beta: original marginal beta\n        - se: original se\n        - p: original p\n        - mean: posterior mean beta after fine-mapping\n        - sd: posterior standard deviation after fine-mapping.\n        - prob: posterior inclusion probability\n        - cs: credible set index within region\n        - lead_r2: r2 value to a lead variant (the one with maximum PIP) in a credible set\n        - alphax: posterior inclusion probability for the x-th single effect (x := 1..L where L is the number of single effects (causal variants) specified; default: L = 10).\n        - lbfx: log-Bayes Factor for each variable and single effect (i.e credible set).\n        - meanx: posterior mean for each variable and single effect (i.e credible set).\n        - sdx: posterior sd of mean for each variable and single effect (i.e credible set).\n    As for r11 finngen release these files are ingested from `https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/` by\n        - *.snp.bgz\n        - *.snp.bgz.tbi\n    Each file contains index (.tbi) file that is required to read the block gzipped compressed snp file. These files needs to be\n    downloaded, transfromed from block gzipped to plain gzipped and then uploaded to the storage bucket, before they can be read by spark or read by hail directly as import table.\n\n    The finngen_susie_finemapping_cs_summary_files are files that Contains credible set summaries from SuSiE fine-mapping for all genome-wide significant regions with following schema:\n        - trait: phenotype\n        - region: region for which the fine-mapping was run.\n        - cs: running number for independent credible sets in a region, assigned to 95% PIP\n        - cs_log10bf: Log10 bayes factor of comparing the solution of this model (cs independent credible sets) to cs -1 credible sets\n        - cs_avg_r2: Average correlation R2 between variants in the credible set\n        - cs_min_r2: minimum r2 between variants in the credible set\n        - low_purity: boolean (TRUE,FALSE) indicator if the CS is low purity (low min r2)\n        - cs_size: how many snps does this credible set contain\n        - good_cs: boolean (TRUE,FALSE) indicator if this CS is considered reliable. IF this is FALSE then top variant reported for the CS will be chosen based on minimum p-value in the credible set, otherwise top variant is chosen by maximum PIP\n        - cs_id:\n        - v: top variant (chr:pos:ref:alt)\n        - p: top variant p-value\n        - beta: top variant beta\n        - sd: top variant standard deviation\n        - prob: overall PIP of the variant in the region\n        - cs_specific_prob: PIP of the variant in the current credible set (this and previous are typically almost identical)\n        - 0..n: configured annotation columns. Typical default most_severe,gene_most_severe giving consequence and gene of top variant\n    These files needs to be downloaded from the `https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/summary/` by *.cred.summary.tsv pattern,\n\n    Args:\n        spark (SparkSession): SparkSession object.\n        finngen_susie_finemapping_snp_files (str | list[str]): SuSIE finemapping output filename(s).\n        finngen_susie_finemapping_cs_summary_files (str | list[str]): filename of SuSIE finemapping credible set summaries.\n        finngen_release_prefix (str): Finngen project release prefix. Should look like FINNGEN_R*.\n        credset_lbf_threshold (float, optional): Filter out credible sets below, Default 0.8685889638065036 == np.log10(np.exp(2)), this is threshold from publication.\n\n    Returns:\n        StudyLocus: Processed SuSIE finemapping output in StudyLocus format.\n    \"\"\"\n    # NOTE: hail allows for importing block gzipped files, spark does not without external libraries.\n    # check https://github.com/projectglow/glow/blob/36bf6121fbc4ccc33a13b028deb87b63faeba7a9/core/src/main/scala/io/projectglow/vcf/VCFFileFormat.scala#L274\n    # how it could be implemented with spark.\n    bgzip_compressed_snps = cls._infer_block_gzip_compression(\n        finngen_susie_finemapping_snp_files\n    )\n\n    # NOTE: fallback to spark read if not block gzipped file in the input\n    if bgzip_compressed_snps:\n        snps_df = hl.import_table(\n            finngen_susie_finemapping_snp_files,\n            delimiter=\"\\t\",\n            types=cls.raw_hail_shema,\n        ).to_spark()\n    else:\n        snps_df = (\n            spark.read.schema(cls.raw_schema)\n            .option(\"delimiter\", \"\\t\")\n            .option(\"compression\", \"gzip\")\n            .csv(finngen_susie_finemapping_snp_files, header=True)\n        )\n\n    processed_finngen_finemapping_df = (\n        # Drop rows which don't have proper position.\n        snps_df.filter(f.col(\"position\").cast(t.IntegerType()).isNotNull())\n        # Drop non credible set SNPs:\n        .filter(f.col(\"cs\").cast(t.IntegerType()) &gt; 0)\n        .select(\n            # Add study idenfitier.\n            f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"trait\"))\n            .cast(t.StringType())\n            .alias(\"studyId\"),\n            f.col(\"region\"),\n            # Add variant information.\n            f.regexp_replace(f.col(\"v\"), \":\", \"_\").alias(\"variantId\"),\n            f.col(\"cs\").cast(\"integer\").alias(\"credibleSetIndex\"),\n            f.regexp_replace(f.col(\"chromosome\"), \"^chr\", \"\")\n            .cast(t.StringType())\n            .alias(\"chromosome\"),\n            f.col(\"position\").cast(t.IntegerType()),\n            f.col(\"allele1\").cast(t.StringType()).alias(\"ref\"),\n            f.col(\"allele2\").cast(t.StringType()).alias(\"alt\"),\n            # Parse p-value into mantissa and exponent.\n            *parse_pvalue(f.col(\"p\")),\n            # Add standard error, and allele frequency information.\n            f.col(\"se\").cast(\"double\").alias(\"standardError\"),\n            f.col(\"maf\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n            f.lit(FinemappingMethod.SUSIE.value).alias(\"finemappingMethod\"),\n            *[\n                f.col(f\"alpha{i}\").cast(t.DoubleType()).alias(f\"alpha_{i}\")\n                for i in range(1, 11)\n            ],\n            *[\n                f.col(f\"lbf_variable{i}\").cast(t.DoubleType()).alias(f\"lbf_{i}\")\n                for i in range(1, 11)\n            ],\n            *[\n                f.col(f\"mean{i}\").cast(t.DoubleType()).alias(f\"beta_{i}\")\n                for i in range(1, 11)\n            ],\n        )\n        .withColumn(\n            \"posteriorProbability\",\n            f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"alpha_1\"))\n            .when(f.col(\"credibleSetIndex\") == 2, f.col(\"alpha_2\"))\n            .when(f.col(\"credibleSetIndex\") == 3, f.col(\"alpha_3\"))\n            .when(f.col(\"credibleSetIndex\") == 4, f.col(\"alpha_4\"))\n            .when(f.col(\"credibleSetIndex\") == 5, f.col(\"alpha_5\"))\n            .when(f.col(\"credibleSetIndex\") == 6, f.col(\"alpha_6\"))\n            .when(f.col(\"credibleSetIndex\") == 7, f.col(\"alpha_7\"))\n            .when(f.col(\"credibleSetIndex\") == 8, f.col(\"alpha_8\"))\n            .when(f.col(\"credibleSetIndex\") == 9, f.col(\"alpha_9\"))\n            .when(f.col(\"credibleSetIndex\") == 10, f.col(\"alpha_10\")),\n        )\n        .drop(\n            \"alpha_1\",\n            \"alpha_2\",\n            \"alpha_3\",\n            \"alpha_4\",\n            \"alpha_5\",\n            \"alpha_6\",\n            \"alpha_7\",\n            \"alpha_8\",\n            \"alpha_9\",\n            \"alpha_10\",\n        )\n        .withColumn(\n            \"logBF\",\n            f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"lbf_1\"))\n            .when(f.col(\"credibleSetIndex\") == 2, f.col(\"lbf_2\"))\n            .when(f.col(\"credibleSetIndex\") == 3, f.col(\"lbf_3\"))\n            .when(f.col(\"credibleSetIndex\") == 4, f.col(\"lbf_4\"))\n            .when(f.col(\"credibleSetIndex\") == 5, f.col(\"lbf_5\"))\n            .when(f.col(\"credibleSetIndex\") == 6, f.col(\"lbf_6\"))\n            .when(f.col(\"credibleSetIndex\") == 7, f.col(\"lbf_7\"))\n            .when(f.col(\"credibleSetIndex\") == 8, f.col(\"lbf_8\"))\n            .when(f.col(\"credibleSetIndex\") == 9, f.col(\"lbf_9\"))\n            .when(f.col(\"credibleSetIndex\") == 10, f.col(\"lbf_10\")),\n        )\n        .drop(\n            \"lbf_1\",\n            \"lbf_2\",\n            \"lbf_3\",\n            \"lbf_4\",\n            \"lbf_5\",\n            \"lbf_6\",\n            \"lbf_7\",\n            \"lbf_8\",\n            \"lbf_9\",\n            \"lbf_10\",\n        )\n        .withColumn(\n            \"beta\",\n            f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"beta_1\"))\n            .when(f.col(\"credibleSetIndex\") == 2, f.col(\"beta_2\"))\n            .when(f.col(\"credibleSetIndex\") == 3, f.col(\"beta_3\"))\n            .when(f.col(\"credibleSetIndex\") == 4, f.col(\"beta_4\"))\n            .when(f.col(\"credibleSetIndex\") == 5, f.col(\"beta_5\"))\n            .when(f.col(\"credibleSetIndex\") == 6, f.col(\"beta_6\"))\n            .when(f.col(\"credibleSetIndex\") == 7, f.col(\"beta_7\"))\n            .when(f.col(\"credibleSetIndex\") == 8, f.col(\"beta_8\"))\n            .when(f.col(\"credibleSetIndex\") == 9, f.col(\"beta_9\"))\n            .when(f.col(\"credibleSetIndex\") == 10, f.col(\"beta_10\")),\n        )\n        .drop(\n            \"beta_1\",\n            \"beta_2\",\n            \"beta_3\",\n            \"beta_4\",\n            \"beta_5\",\n            \"beta_6\",\n            \"beta_7\",\n            \"beta_8\",\n            \"beta_9\",\n            \"beta_10\",\n        )\n    )\n\n    bgzip_compressed_cs_summaries = cls._infer_block_gzip_compression(\n        finngen_susie_finemapping_cs_summary_files\n    )\n\n    # NOTE: fallback to spark read if not block gzipped file in the input\n    # in case we want to use the raw files from the\n    # https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/*.cred.gz\n    if bgzip_compressed_cs_summaries:\n        cs_summary_df = hl.import_table(\n            finngen_susie_finemapping_cs_summary_files,\n            delimiter=\"\\t\",\n            types=cls.summary_hail_schema,\n        ).to_spark()\n    else:\n        cs_summary_df = (\n            spark.read.schema(cls.summary_schema)\n            .option(\"delimiter\", \"\\t\")\n            .csv(finngen_susie_finemapping_cs_summary_files, header=True)\n        )\n\n    # drop credible sets where logbf &gt; 2. Except when there's only one credible set in region:\n    # 0.8685889638065036 corresponds to np.log10(np.exp(2)), to match the orginal threshold in publication.\n    finngen_finemapping_summaries_df = (\n        # Read credible set level lbf, it is output as a different file which is not ideal.\n        cs_summary_df.select(\n            f.col(\"region\"),\n            f.col(\"trait\"),\n            f.col(\"cs\").cast(\"integer\").alias(\"credibleSetIndex\"),\n            f.col(\"cs_log10bf\").cast(\"double\").alias(\"credibleSetlog10BF\"),\n            f.col(\"cs_avg_r2\").cast(\"double\").alias(\"purityMeanR2\"),\n            f.col(\"cs_min_r2\").cast(\"double\").alias(\"purityMinR2\"),\n        )\n        .filter(\n            (f.col(\"credibleSetlog10BF\") &gt; credset_lbf_threshold)\n            | (f.col(\"credibleSetIndex\") == 1)\n        )\n        .withColumn(\n            \"studyId\",\n            f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"trait\")),\n        )\n    )\n\n    processed_finngen_finemapping_df = processed_finngen_finemapping_df.join(\n        finngen_finemapping_summaries_df,\n        on=[\"studyId\", \"region\", \"credibleSetIndex\"],\n        how=\"inner\",\n    )\n\n    toploci_df = get_top_ranked_in_window(\n        processed_finngen_finemapping_df,\n        Window.partitionBy(\"studyId\", \"region\", \"credibleSetIndex\").orderBy(\n            f.desc(\"posteriorProbability\")\n        ),\n    ).select(\n        \"variantId\",\n        \"chromosome\",\n        \"position\",\n        \"studyId\",\n        \"beta\",\n        \"pValueMantissa\",\n        \"pValueExponent\",\n        \"effectAlleleFrequencyFromSource\",\n        \"standardError\",\n        \"region\",\n        \"credibleSetIndex\",\n        \"finemappingMethod\",\n        \"credibleSetlog10BF\",\n        \"purityMeanR2\",\n        \"purityMinR2\",\n    )\n\n    processed_finngen_finemapping_df = (\n        processed_finngen_finemapping_df.groupBy(\n            \"studyId\", \"region\", \"credibleSetIndex\"\n        )\n        .agg(\n            f.collect_list(\n                f.struct(\n                    f.col(\"variantId\").cast(\"string\").alias(\"variantId\"),\n                    f.col(\"posteriorProbability\")\n                    .cast(\"double\")\n                    .alias(\"posteriorProbability\"),\n                    f.col(\"logBF\").cast(\"double\").alias(\"logBF\"),\n                    f.col(\"pValueMantissa\").cast(\"float\").alias(\"pValueMantissa\"),\n                    f.col(\"pValueExponent\").cast(\"integer\").alias(\"pValueExponent\"),\n                    f.col(\"beta\").cast(\"double\").alias(\"beta\"),\n                    f.col(\"standardError\").cast(\"double\").alias(\"standardError\"),\n                )\n            ).alias(\"locus\"),\n        )\n        .select(\n            \"studyId\",\n            \"region\",\n            \"credibleSetIndex\",\n            \"locus\",\n        )\n        .join(\n            toploci_df,\n            on=[\"studyId\", \"region\", \"credibleSetIndex\"],\n            how=\"inner\",\n        )\n        .withColumns(\n            {\n                \"locusStart\": f.split(f.split(\"region\", \":\")[1], \"-\")[0].cast(\n                    \"int\"\n                ),\n                \"locusEnd\": f.split(f.split(\"region\", \":\")[1], \"-\")[1].cast(\"int\"),\n            }\n        )\n    ).withColumn(\n        \"studyLocusId\",\n        StudyLocus.assign_study_locus_id(\n            [\"studyId\", \"variantId\", \"finemappingMethod\"]\n        ),\n    )\n\n    return StudyLocus(\n        _df=processed_finngen_finemapping_df,\n        _schema=StudyLocus.get_schema(),\n    ).annotate_credible_sets()\n</code></pre>"},{"location":"python_api/datasources/finngen/study_index/","title":"Study Index","text":""},{"location":"python_api/datasources/finngen/study_index/#gentropy.datasource.finngen.study_index.FinnGenStudyIndex","title":"<code>gentropy.datasource.finngen.study_index.FinnGenStudyIndex</code>","text":"<p>Study index dataset from FinnGen.</p> <p>The following information is aggregated/extracted:</p> <ul> <li>Study ID in the special format (e.g. FINNGEN_R11_*)</li> <li>Trait name (for example, Amoebiasis)</li> <li>Number of cases and controls</li> <li>Link to the summary statistics location</li> <li>EFO mapping from curated EFO mapping file</li> </ul> <p>Some fields are also populated as constants, such as study type and the initial sample size.</p> Source code in <code>src/gentropy/datasource/finngen/study_index.py</code> <pre><code>class FinnGenStudyIndex:\n    \"\"\"Study index dataset from FinnGen.\n\n    The following information is aggregated/extracted:\n\n    - Study ID in the special format (e.g. FINNGEN_R11_*)\n    - Trait name (for example, Amoebiasis)\n    - Number of cases and controls\n    - Link to the summary statistics location\n    - EFO mapping from curated EFO mapping file\n\n    Some fields are also populated as constants, such as study type and the initial sample size.\n    \"\"\"\n\n    @staticmethod\n    def validate_release_prefix(release_prefix: str) -&gt; FinngenPrefixMatch:\n        \"\"\"Validate release prefix passed to finngen StudyIndex.\n\n        Args:\n            release_prefix (str): Finngen release prefix, should be a string like FINNGEN_R*.\n\n        Returns:\n            FinngenPrefixMatch: Object containing valid prefix and release strings.\n\n        Raises:\n            ValueError: when incorrect release prefix is provided.\n\n        This method ensures that the trailing underscore is removed from prefix.\n        \"\"\"\n        pattern = re.compile(r\"FINNGEN_(?P&lt;release&gt;R\\d+){1}_?\")\n        pattern_match = pattern.match(release_prefix)\n        if not pattern_match:\n            raise ValueError(\n                f\"Invalid FinnGen release prefix: {release_prefix}, use the format FINNGEN_R*\"\n            )\n        release = pattern_match.group(\"release\").upper()\n        if release_prefix.endswith(\"_\"):\n            release_prefix = release_prefix[:-1]\n        return FinngenPrefixMatch(prefix=release_prefix, release=release)\n\n    @staticmethod\n    def read_efo_curation(session: SparkSession, url: str) -&gt; DataFrame:\n        \"\"\"Read efo curation from provided url.\n\n        Args:\n            session (SparkSession): Session to use when reading the mapping file.\n            url (str): Url to the mapping file. The file provided should be a tsv file.\n\n        Returns:\n            DataFrame: DataFrame with EFO mappings.\n\n        Example of the file can be found in https://raw.githubusercontent.com/opentargets/curation/refs/heads/master/mappings/disease/manual_string.tsv.\n        \"\"\"\n        csv_data = urlopen(url).readlines()\n        csv_rows = [row.decode(\"utf8\") for row in csv_data]\n        rdd = session.sparkContext.parallelize(csv_rows)\n        # NOTE: type annotations for spark.read.csv miss the fact that the first param can be [RDD[str]]\n        efo_curation_mapping_df = session.read.csv(rdd, header=True, sep=\"\\t\")\n        return efo_curation_mapping_df\n\n    @staticmethod\n    def join_efo_mapping(\n        study_index: StudyIndex,\n        efo_curation_mapping: DataFrame,\n        finngen_release: str,\n    ) -&gt; StudyIndex:\n        \"\"\"Add EFO mapping to the Finngen study index table.\n\n        This function performs inner join on table of EFO mappings to the study index table by trait name.\n        All studies without EFO traits are dropped. The EFO mappings are then aggregated into lists per\n        studyId.\n\n        NOTE: preserve all studyId entries even if they don't have EFO mappings.\n        This is to avoid discrepancies between `study_index` and `credible_set` `studyId` column.\n        The rows with missing EFO mappings will be dropped in the study_index validation step.\n\n        Args:\n            study_index (StudyIndex): Study index table.\n            efo_curation_mapping (DataFrame): Dataframe with EFO mappings.\n            finngen_release (str): FinnGen release.\n\n        Returns:\n            StudyIndex: Study index table with added EFO mappings.\n        \"\"\"\n        efo_mappings = (\n            efo_curation_mapping.withColumn(\"STUDY\", f.upper(f.col(\"STUDY\")))\n            .filter(f.col(\"STUDY\").contains(\"FINNGEN\"))\n            .filter(f.upper(f.col(\"STUDY\")).contains(finngen_release))\n            .select(\n                f.regexp_replace(f.col(\"SEMANTIC_TAG\"), r\"^.*/\", \"\").alias(\n                    \"traitFromSourceMappedId\"\n                ),\n                f.col(\"PROPERTY_VALUE\").alias(\"traitFromSource\"),\n            )\n        )\n\n        si_df = study_index.df.join(\n            efo_mappings, on=\"traitFromSource\", how=\"left_outer\"\n        )\n        common_cols = [c for c in si_df.columns if c != \"traitFromSourceMappedId\"]\n        si_df = si_df.groupby(common_cols).agg(\n            f.collect_list(\"traitFromSourceMappedId\").alias(\"traitFromSourceMappedIds\")\n        )\n        return StudyIndex(_df=si_df, _schema=StudyIndex.get_schema())\n\n    @classmethod\n    def from_source(\n        cls: type[FinnGenStudyIndex],\n        spark: SparkSession,\n        finngen_phenotype_table_url: str,\n        finngen_release_prefix: str,\n        finngen_summary_stats_url_prefix: str,\n        finngen_summary_stats_url_suffix: str,\n        sample_size: int,\n    ) -&gt; StudyIndex:\n        \"\"\"This function ingests study level metadata from FinnGen.\n\n        Args:\n            spark (SparkSession): Spark session object.\n            finngen_phenotype_table_url (str): URL to the FinnGen phenotype table.\n            finngen_release_prefix (str): FinnGen release prefix.\n            finngen_summary_stats_url_prefix (str): FinnGen summary stats URL prefix.\n            finngen_summary_stats_url_suffix (str): FinnGen summary stats URL suffix.\n            sample_size (int): Number of individuals participated in sample collection.\n\n        Returns:\n            StudyIndex: Parsed and annotated FinnGen study table.\n        \"\"\"\n        json_data = urlopen(finngen_phenotype_table_url).read().decode(\"utf-8\")\n        rdd = spark.sparkContext.parallelize([json_data])\n        raw_df = spark.read.json(rdd)\n\n        return StudyIndex(\n            _df=raw_df.select(\n                f.concat(\n                    f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"phenocode\"))\n                ).alias(\"studyId\"),\n                f.col(\"phenostring\").alias(\"traitFromSource\"),\n                f.col(\"num_cases\").cast(\"integer\").alias(\"nCases\"),\n                f.col(\"num_controls\").cast(\"integer\").alias(\"nControls\"),\n                (f.col(\"num_cases\") + f.col(\"num_controls\"))\n                .cast(\"integer\")\n                .alias(\"nSamples\"),\n                f.lit(finngen_release_prefix).alias(\"projectId\"),\n                f.lit(\"gwas\").alias(\"studyType\"),\n                f.lit(True).alias(\"hasSumstats\"),\n                f.lit(\"500,348 (282,064 females and 218,284 males)\").alias(\n                    \"initialSampleSize\"\n                ),\n                f.array(\n                    f.struct(\n                        f.lit(sample_size).cast(\"integer\").alias(\"sampleSize\"),\n                        f.lit(\"Finnish\").alias(\"ancestry\"),\n                    )\n                ).alias(\"discoverySamples\"),\n                # Cohort label is consistent with GWAS Catalog curation.\n                f.array(f.lit(\"FinnGen\")).alias(\"cohorts\"),\n                f.concat(\n                    f.lit(finngen_summary_stats_url_prefix),\n                    f.col(\"phenocode\"),\n                    f.lit(finngen_summary_stats_url_suffix),\n                ).alias(\"summarystatsLocation\"),\n            ).withColumn(\n                \"ldPopulationStructure\",\n                StudyIndex.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n            ),\n            _schema=StudyIndex.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/finngen/study_index/#gentropy.datasource.finngen.study_index.FinnGenStudyIndex.from_source","title":"<code>from_source(spark: SparkSession, finngen_phenotype_table_url: str, finngen_release_prefix: str, finngen_summary_stats_url_prefix: str, finngen_summary_stats_url_suffix: str, sample_size: int) -&gt; StudyIndex</code>  <code>classmethod</code>","text":"<p>This function ingests study level metadata from FinnGen.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>finngen_phenotype_table_url</code> <code>str</code> <p>URL to the FinnGen phenotype table.</p> required <code>finngen_release_prefix</code> <code>str</code> <p>FinnGen release prefix.</p> required <code>finngen_summary_stats_url_prefix</code> <code>str</code> <p>FinnGen summary stats URL prefix.</p> required <code>finngen_summary_stats_url_suffix</code> <code>str</code> <p>FinnGen summary stats URL suffix.</p> required <code>sample_size</code> <code>int</code> <p>Number of individuals participated in sample collection.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Parsed and annotated FinnGen study table.</p> Source code in <code>src/gentropy/datasource/finngen/study_index.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[FinnGenStudyIndex],\n    spark: SparkSession,\n    finngen_phenotype_table_url: str,\n    finngen_release_prefix: str,\n    finngen_summary_stats_url_prefix: str,\n    finngen_summary_stats_url_suffix: str,\n    sample_size: int,\n) -&gt; StudyIndex:\n    \"\"\"This function ingests study level metadata from FinnGen.\n\n    Args:\n        spark (SparkSession): Spark session object.\n        finngen_phenotype_table_url (str): URL to the FinnGen phenotype table.\n        finngen_release_prefix (str): FinnGen release prefix.\n        finngen_summary_stats_url_prefix (str): FinnGen summary stats URL prefix.\n        finngen_summary_stats_url_suffix (str): FinnGen summary stats URL suffix.\n        sample_size (int): Number of individuals participated in sample collection.\n\n    Returns:\n        StudyIndex: Parsed and annotated FinnGen study table.\n    \"\"\"\n    json_data = urlopen(finngen_phenotype_table_url).read().decode(\"utf-8\")\n    rdd = spark.sparkContext.parallelize([json_data])\n    raw_df = spark.read.json(rdd)\n\n    return StudyIndex(\n        _df=raw_df.select(\n            f.concat(\n                f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"phenocode\"))\n            ).alias(\"studyId\"),\n            f.col(\"phenostring\").alias(\"traitFromSource\"),\n            f.col(\"num_cases\").cast(\"integer\").alias(\"nCases\"),\n            f.col(\"num_controls\").cast(\"integer\").alias(\"nControls\"),\n            (f.col(\"num_cases\") + f.col(\"num_controls\"))\n            .cast(\"integer\")\n            .alias(\"nSamples\"),\n            f.lit(finngen_release_prefix).alias(\"projectId\"),\n            f.lit(\"gwas\").alias(\"studyType\"),\n            f.lit(True).alias(\"hasSumstats\"),\n            f.lit(\"500,348 (282,064 females and 218,284 males)\").alias(\n                \"initialSampleSize\"\n            ),\n            f.array(\n                f.struct(\n                    f.lit(sample_size).cast(\"integer\").alias(\"sampleSize\"),\n                    f.lit(\"Finnish\").alias(\"ancestry\"),\n                )\n            ).alias(\"discoverySamples\"),\n            # Cohort label is consistent with GWAS Catalog curation.\n            f.array(f.lit(\"FinnGen\")).alias(\"cohorts\"),\n            f.concat(\n                f.lit(finngen_summary_stats_url_prefix),\n                f.col(\"phenocode\"),\n                f.lit(finngen_summary_stats_url_suffix),\n            ).alias(\"summarystatsLocation\"),\n        ).withColumn(\n            \"ldPopulationStructure\",\n            StudyIndex.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n        ),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen/study_index/#gentropy.datasource.finngen.study_index.FinnGenStudyIndex.join_efo_mapping","title":"<code>join_efo_mapping(study_index: StudyIndex, efo_curation_mapping: DataFrame, finngen_release: str) -&gt; StudyIndex</code>  <code>staticmethod</code>","text":"<p>Add EFO mapping to the Finngen study index table.</p> <p>This function performs inner join on table of EFO mappings to the study index table by trait name. All studies without EFO traits are dropped. The EFO mappings are then aggregated into lists per studyId.</p> <p>NOTE: preserve all studyId entries even if they don't have EFO mappings. This is to avoid discrepancies between <code>study_index</code> and <code>credible_set</code> <code>studyId</code> column. The rows with missing EFO mappings will be dropped in the study_index validation step.</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>Study index table.</p> required <code>efo_curation_mapping</code> <code>DataFrame</code> <p>Dataframe with EFO mappings.</p> required <code>finngen_release</code> <code>str</code> <p>FinnGen release.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Study index table with added EFO mappings.</p> Source code in <code>src/gentropy/datasource/finngen/study_index.py</code> <pre><code>@staticmethod\ndef join_efo_mapping(\n    study_index: StudyIndex,\n    efo_curation_mapping: DataFrame,\n    finngen_release: str,\n) -&gt; StudyIndex:\n    \"\"\"Add EFO mapping to the Finngen study index table.\n\n    This function performs inner join on table of EFO mappings to the study index table by trait name.\n    All studies without EFO traits are dropped. The EFO mappings are then aggregated into lists per\n    studyId.\n\n    NOTE: preserve all studyId entries even if they don't have EFO mappings.\n    This is to avoid discrepancies between `study_index` and `credible_set` `studyId` column.\n    The rows with missing EFO mappings will be dropped in the study_index validation step.\n\n    Args:\n        study_index (StudyIndex): Study index table.\n        efo_curation_mapping (DataFrame): Dataframe with EFO mappings.\n        finngen_release (str): FinnGen release.\n\n    Returns:\n        StudyIndex: Study index table with added EFO mappings.\n    \"\"\"\n    efo_mappings = (\n        efo_curation_mapping.withColumn(\"STUDY\", f.upper(f.col(\"STUDY\")))\n        .filter(f.col(\"STUDY\").contains(\"FINNGEN\"))\n        .filter(f.upper(f.col(\"STUDY\")).contains(finngen_release))\n        .select(\n            f.regexp_replace(f.col(\"SEMANTIC_TAG\"), r\"^.*/\", \"\").alias(\n                \"traitFromSourceMappedId\"\n            ),\n            f.col(\"PROPERTY_VALUE\").alias(\"traitFromSource\"),\n        )\n    )\n\n    si_df = study_index.df.join(\n        efo_mappings, on=\"traitFromSource\", how=\"left_outer\"\n    )\n    common_cols = [c for c in si_df.columns if c != \"traitFromSourceMappedId\"]\n    si_df = si_df.groupby(common_cols).agg(\n        f.collect_list(\"traitFromSourceMappedId\").alias(\"traitFromSourceMappedIds\")\n    )\n    return StudyIndex(_df=si_df, _schema=StudyIndex.get_schema())\n</code></pre>"},{"location":"python_api/datasources/finngen/study_index/#gentropy.datasource.finngen.study_index.FinnGenStudyIndex.read_efo_curation","title":"<code>read_efo_curation(session: SparkSession, url: str) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>Read efo curation from provided url.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SparkSession</code> <p>Session to use when reading the mapping file.</p> required <code>url</code> <code>str</code> <p>Url to the mapping file. The file provided should be a tsv file.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with EFO mappings.</p> <p>Example of the file can be found in https://raw.githubusercontent.com/opentargets/curation/refs/heads/master/mappings/disease/manual_string.tsv.</p> Source code in <code>src/gentropy/datasource/finngen/study_index.py</code> <pre><code>@staticmethod\ndef read_efo_curation(session: SparkSession, url: str) -&gt; DataFrame:\n    \"\"\"Read efo curation from provided url.\n\n    Args:\n        session (SparkSession): Session to use when reading the mapping file.\n        url (str): Url to the mapping file. The file provided should be a tsv file.\n\n    Returns:\n        DataFrame: DataFrame with EFO mappings.\n\n    Example of the file can be found in https://raw.githubusercontent.com/opentargets/curation/refs/heads/master/mappings/disease/manual_string.tsv.\n    \"\"\"\n    csv_data = urlopen(url).readlines()\n    csv_rows = [row.decode(\"utf8\") for row in csv_data]\n    rdd = session.sparkContext.parallelize(csv_rows)\n    # NOTE: type annotations for spark.read.csv miss the fact that the first param can be [RDD[str]]\n    efo_curation_mapping_df = session.read.csv(rdd, header=True, sep=\"\\t\")\n    return efo_curation_mapping_df\n</code></pre>"},{"location":"python_api/datasources/finngen/study_index/#gentropy.datasource.finngen.study_index.FinnGenStudyIndex.validate_release_prefix","title":"<code>validate_release_prefix(release_prefix: str) -&gt; FinngenPrefixMatch</code>  <code>staticmethod</code>","text":"<p>Validate release prefix passed to finngen StudyIndex.</p> <p>Parameters:</p> Name Type Description Default <code>release_prefix</code> <code>str</code> <p>Finngen release prefix, should be a string like FINNGEN_R*.</p> required <p>Returns:</p> Name Type Description <code>FinngenPrefixMatch</code> <code>FinngenPrefixMatch</code> <p>Object containing valid prefix and release strings.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>when incorrect release prefix is provided.</p> <p>This method ensures that the trailing underscore is removed from prefix.</p> Source code in <code>src/gentropy/datasource/finngen/study_index.py</code> <pre><code>@staticmethod\ndef validate_release_prefix(release_prefix: str) -&gt; FinngenPrefixMatch:\n    \"\"\"Validate release prefix passed to finngen StudyIndex.\n\n    Args:\n        release_prefix (str): Finngen release prefix, should be a string like FINNGEN_R*.\n\n    Returns:\n        FinngenPrefixMatch: Object containing valid prefix and release strings.\n\n    Raises:\n        ValueError: when incorrect release prefix is provided.\n\n    This method ensures that the trailing underscore is removed from prefix.\n    \"\"\"\n    pattern = re.compile(r\"FINNGEN_(?P&lt;release&gt;R\\d+){1}_?\")\n    pattern_match = pattern.match(release_prefix)\n    if not pattern_match:\n        raise ValueError(\n            f\"Invalid FinnGen release prefix: {release_prefix}, use the format FINNGEN_R*\"\n        )\n    release = pattern_match.group(\"release\").upper()\n    if release_prefix.endswith(\"_\"):\n        release_prefix = release_prefix[:-1]\n    return FinngenPrefixMatch(prefix=release_prefix, release=release)\n</code></pre>"},{"location":"python_api/datasources/finngen/summary_stats/","title":"Study Index","text":""},{"location":"python_api/datasources/finngen/summary_stats/#gentropy.datasource.finngen.summary_stats.FinnGenSummaryStats","title":"<code>gentropy.datasource.finngen.summary_stats.FinnGenSummaryStats</code>  <code>dataclass</code>","text":"<p>Summary statistics dataset for FinnGen.</p> Source code in <code>src/gentropy/datasource/finngen/summary_stats.py</code> <pre><code>@dataclass\nclass FinnGenSummaryStats:\n    \"\"\"Summary statistics dataset for FinnGen.\"\"\"\n\n    raw_schema: t.StructType = StructType(\n        [\n            StructField(\"#chrom\", StringType(), True),\n            StructField(\"pos\", StringType(), True),\n            StructField(\"ref\", StringType(), True),\n            StructField(\"alt\", StringType(), True),\n            StructField(\"rsids\", StringType(), True),\n            StructField(\"nearest_genes\", StringType(), True),\n            StructField(\"pval\", StringType(), True),\n            StructField(\"mlogp\", StringType(), True),\n            StructField(\"beta\", StringType(), True),\n            StructField(\"sebeta\", StringType(), True),\n            StructField(\"af_alt\", StringType(), True),\n            StructField(\"af_alt_cases\", StringType(), True),\n            StructField(\"af_alt_controls\", StringType(), True),\n        ]\n    )\n\n    @classmethod\n    def from_source(\n        cls: type[FinnGenSummaryStats],\n        spark: SparkSession,\n        raw_file: str,\n    ) -&gt; SummaryStatistics:\n        \"\"\"Ingests all summary statst for all FinnGen studies.\n\n        Args:\n            spark (SparkSession): Spark session object.\n            raw_file (str): Path to raw summary statistics .gz files.\n\n        Returns:\n            SummaryStatistics: Processed summary statistics dataset\n        \"\"\"\n        processed_summary_stats_df = (\n            spark.read.schema(cls.raw_schema)\n            .option(\"delimiter\", \"\\t\")\n            .csv(raw_file, header=True)\n            # Drop rows which don't have proper position.\n            .filter(f.col(\"pos\").cast(t.IntegerType()).isNotNull())\n            .select(\n                # From the full path, extracts just the filename, and converts to upper case to get the study ID.\n                f.upper(\n                    f.regexp_extract(\n                        f.input_file_name(), r\"([^/]+)(\\.tsv\\.gz|\\.gz|\\.tsv)\", 1\n                    )\n                ).alias(\"studyId\"),\n                # Add variant information.\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"#chrom\"),\n                    f.col(\"pos\"),\n                    f.col(\"ref\"),\n                    f.col(\"alt\"),\n                ).alias(\"variantId\"),\n                f.col(\"#chrom\").alias(\"chromosome\"),\n                f.col(\"pos\").cast(t.IntegerType()).alias(\"position\"),\n                # Parse p-value into mantissa and exponent.\n                *parse_pvalue(f.col(\"pval\")),\n                # Add beta, standard error, and allele frequency information.\n                f.col(\"beta\").cast(\"double\"),\n                f.col(\"sebeta\").cast(\"double\").alias(\"standardError\"),\n                f.col(\"af_alt\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n            )\n            # Calculating the confidence intervals.\n            .filter(\n                f.col(\"pos\").cast(t.IntegerType()).isNotNull() &amp; (f.col(\"beta\") != 0)\n            )\n            # Average ~20Mb partitions with 30 partitions per study\n            .repartitionByRange(30, \"chromosome\", \"position\")\n            .sortWithinPartitions(\"chromosome\", \"position\")\n        )\n\n        # Initializing summary statistics object:\n        return SummaryStatistics(\n            _df=processed_summary_stats_df,\n            _schema=SummaryStatistics.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/finngen/summary_stats/#gentropy.datasource.finngen.summary_stats.FinnGenSummaryStats.from_source","title":"<code>from_source(spark: SparkSession, raw_file: str) -&gt; SummaryStatistics</code>  <code>classmethod</code>","text":"<p>Ingests all summary statst for all FinnGen studies.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>raw_file</code> <code>str</code> <p>Path to raw summary statistics .gz files.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>Processed summary statistics dataset</p> Source code in <code>src/gentropy/datasource/finngen/summary_stats.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[FinnGenSummaryStats],\n    spark: SparkSession,\n    raw_file: str,\n) -&gt; SummaryStatistics:\n    \"\"\"Ingests all summary statst for all FinnGen studies.\n\n    Args:\n        spark (SparkSession): Spark session object.\n        raw_file (str): Path to raw summary statistics .gz files.\n\n    Returns:\n        SummaryStatistics: Processed summary statistics dataset\n    \"\"\"\n    processed_summary_stats_df = (\n        spark.read.schema(cls.raw_schema)\n        .option(\"delimiter\", \"\\t\")\n        .csv(raw_file, header=True)\n        # Drop rows which don't have proper position.\n        .filter(f.col(\"pos\").cast(t.IntegerType()).isNotNull())\n        .select(\n            # From the full path, extracts just the filename, and converts to upper case to get the study ID.\n            f.upper(\n                f.regexp_extract(\n                    f.input_file_name(), r\"([^/]+)(\\.tsv\\.gz|\\.gz|\\.tsv)\", 1\n                )\n            ).alias(\"studyId\"),\n            # Add variant information.\n            f.concat_ws(\n                \"_\",\n                f.col(\"#chrom\"),\n                f.col(\"pos\"),\n                f.col(\"ref\"),\n                f.col(\"alt\"),\n            ).alias(\"variantId\"),\n            f.col(\"#chrom\").alias(\"chromosome\"),\n            f.col(\"pos\").cast(t.IntegerType()).alias(\"position\"),\n            # Parse p-value into mantissa and exponent.\n            *parse_pvalue(f.col(\"pval\")),\n            # Add beta, standard error, and allele frequency information.\n            f.col(\"beta\").cast(\"double\"),\n            f.col(\"sebeta\").cast(\"double\").alias(\"standardError\"),\n            f.col(\"af_alt\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n        )\n        # Calculating the confidence intervals.\n        .filter(\n            f.col(\"pos\").cast(t.IntegerType()).isNotNull() &amp; (f.col(\"beta\") != 0)\n        )\n        # Average ~20Mb partitions with 30 partitions per study\n        .repartitionByRange(30, \"chromosome\", \"position\")\n        .sortWithinPartitions(\"chromosome\", \"position\")\n    )\n\n    # Initializing summary statistics object:\n    return SummaryStatistics(\n        _df=processed_summary_stats_df,\n        _schema=SummaryStatistics.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gnomad/_gnomad/","title":"GnomAD","text":"<p>GnomAD (Genome Aggregation Database) is a comprehensive resource that provides aggregated genomic data from large-scale sequencing projects. It encompasses variants from diverse populations and is widely used for variant annotation and population genetics studies.</p> <p>We use GnomAD v4.0 as a source for variant annotation, offering detailed information about the prevalence and distribution of genetic variants across different populations. This version of GnomAD provides valuable insights into the genomic landscape, aiding in the interpretation of genetic variants and their potential functional implications.</p> <p>Additionally, GnomAD v2.1.1 is utilized as a source for linkage disequilibrium (LD) information.</p>"},{"location":"python_api/datasources/gnomad/gnomad_ld/","title":"LD Matrix","text":""},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix","title":"<code>gentropy.datasource.gnomad.ld.GnomADLDMatrix</code>","text":"<p>Toolset ot interact with GnomAD LD dataset (version: r2.1.1).</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>class GnomADLDMatrix:\n    \"\"\"Toolset ot interact with GnomAD LD dataset (version: r2.1.1).\"\"\"\n\n    def __init__(\n        self,\n        ld_matrix_template: str = LDIndexConfig().ld_matrix_template,\n        ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template,\n        grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path,\n        ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations,\n        liftover_ht_path: str = LDIndexConfig().liftover_ht_path,\n    ):\n        \"\"\"Initialize.\n\n        Datasets are accessed in Hail's native format, as provided by the [GnomAD consortium](https://gnomad.broadinstitute.org/downloads/#v2-linkage-disequilibrium).\n\n        Args:\n            ld_matrix_template (str): Template for the LD matrix path.\n            ld_index_raw_template (str): Template for the LD index path.\n            grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates.\n            ld_populations (list[LD_Population | str]): List of populations to use to build the LDIndex.\n            liftover_ht_path (str): Path to the liftover ht file.\n\n        Default values are set in LDIndexConfig.\n        \"\"\"\n        self.ld_matrix_template = ld_matrix_template\n        self.ld_index_raw_template = ld_index_raw_template\n        self.grch37_to_grch38_chain_path = grch37_to_grch38_chain_path\n        self.ld_populations = ld_populations\n        self.liftover_ht_path = liftover_ht_path\n\n    @staticmethod\n    def _aggregate_ld_index_across_populations(\n        unaggregated_ld_index: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"Aggregate LDIndex across populations.\n\n        Args:\n            unaggregated_ld_index (DataFrame): Unaggregate LDIndex index dataframe  each row is a variant pair in a population\n\n        Returns:\n            DataFrame: Aggregated LDIndex index dataframe  each row is a variant with the LD set across populations\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"1.0\", \"var1\", \"X\", \"var1\", \"pop1\"), (\"1.0\", \"X\", \"var2\", \"var2\", \"pop1\"),\n            ...         (\"0.5\", \"var1\", \"X\", \"var2\", \"pop1\"), (\"0.5\", \"var1\", \"X\", \"var2\", \"pop2\"),\n            ...         (\"0.5\", \"var2\", \"X\", \"var1\", \"pop1\"), (\"0.5\", \"X\", \"var2\", \"var1\", \"pop2\")]\n            &gt;&gt;&gt; df = spark.createDataFrame(data, [\"r\", \"variantId\", \"chromosome\", \"tagvariantId\", \"population\"])\n            &gt;&gt;&gt; GnomADLDMatrix._aggregate_ld_index_across_populations(df).printSchema()\n            root\n             |-- variantId: string (nullable = true)\n             |-- chromosome: string (nullable = true)\n             |-- ldSet: array (nullable = false)\n             |    |-- element: struct (containsNull = false)\n             |    |    |-- tagVariantId: string (nullable = true)\n             |    |    |-- rValues: array (nullable = false)\n             |    |    |    |-- element: struct (containsNull = false)\n             |    |    |    |    |-- population: string (nullable = true)\n             |    |    |    |    |-- r: string (nullable = true)\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return (\n            unaggregated_ld_index\n            # First level of aggregation: get r/population for each variant/tagVariant pair\n            .withColumn(\"r_pop_struct\", f.struct(\"population\", \"r\"))\n            .groupBy(\"chromosome\", \"variantId\", \"tagVariantId\")\n            .agg(\n                f.collect_set(\"r_pop_struct\").alias(\"rValues\"),\n            )\n            # Second level of aggregation: get r/population for each variant\n            .withColumn(\"r_pop_tag_struct\", f.struct(\"tagVariantId\", \"rValues\"))\n            .groupBy(\"variantId\", \"chromosome\")\n            .agg(\n                f.collect_set(\"r_pop_tag_struct\").alias(\"ldSet\"),\n            )\n        )\n\n    @staticmethod\n    def _convert_ld_matrix_to_table(\n        block_matrix: BlockMatrix, min_r2: float\n    ) -&gt; DataFrame:\n        \"\"\"Convert LD matrix to table.\n\n        Args:\n            block_matrix (BlockMatrix): LD matrix\n            min_r2 (float): Minimum r2 value to keep in the table\n\n        Returns:\n            DataFrame: LD matrix as a Spark DataFrame\n        \"\"\"\n        table = block_matrix.entries(keyed=False)\n        return (\n            table.filter(hl.abs(table.entry) &gt;= min_r2**0.5)\n            .to_spark()\n            .withColumnRenamed(\"entry\", \"r\")\n        )\n\n    @staticmethod\n    def _create_ldindex_for_population(\n        population_id: str,\n        ld_matrix_path: str,\n        ld_index_raw_path: str,\n        grch37_to_grch38_chain_path: str,\n        min_r2: float,\n    ) -&gt; DataFrame:\n        \"\"\"Create LDIndex for a specific population.\n\n        Args:\n            population_id (str): Population ID\n            ld_matrix_path (str): Path to the LD matrix\n            ld_index_raw_path (str): Path to the LD index\n            grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates\n            min_r2 (float): Minimum r2 value to keep in the table\n\n        Returns:\n            DataFrame: LDIndex for a specific population\n        \"\"\"\n        # Prepare LD Block matrix\n        ld_matrix = GnomADLDMatrix._convert_ld_matrix_to_table(\n            BlockMatrix.read(ld_matrix_path), min_r2\n        )\n\n        # Prepare table with variant indices\n        ld_index = GnomADLDMatrix._process_variant_indices(\n            hl.read_table(ld_index_raw_path),\n            grch37_to_grch38_chain_path,\n        )\n\n        return GnomADLDMatrix._resolve_variant_indices(ld_index, ld_matrix).select(\n            \"*\",\n            f.lit(population_id).alias(\"population\"),\n        )\n\n    @staticmethod\n    def _process_variant_indices(\n        ld_index_raw: hl.Table, grch37_to_grch38_chain_path: str\n    ) -&gt; DataFrame:\n        \"\"\"Creates a look up table between variants and their coordinates in the LD Matrix.\n\n        !!! info \"Gnomad's LD Matrix and Index are based on GRCh37 coordinates. This function will lift over the coordinates to GRCh38 to build the lookup table.\"\n\n        Args:\n            ld_index_raw (hl.Table): LD index table from GnomAD\n            grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates\n\n        Returns:\n            DataFrame: Look up table between variants in build hg38 and their coordinates in the LD Matrix\n        \"\"\"\n        ld_index_38 = _liftover_loci(\n            ld_index_raw, grch37_to_grch38_chain_path, \"GRCh38\"\n        )\n\n        return (\n            ld_index_38.to_spark()\n            # Filter out variants where the liftover failed\n            .filter(f.col(\"`locus_GRCh38.position`\").isNotNull())\n            .select(\n                f.regexp_replace(\"`locus_GRCh38.contig`\", \"chr\", \"\").alias(\n                    \"chromosome\"\n                ),\n                f.col(\"`locus_GRCh38.position`\").alias(\"position\"),\n                f.concat_ws(\n                    \"_\",\n                    f.regexp_replace(\"`locus_GRCh38.contig`\", \"chr\", \"\"),\n                    f.col(\"`locus_GRCh38.position`\"),\n                    f.col(\"`alleles`\").getItem(0),\n                    f.col(\"`alleles`\").getItem(1),\n                ).alias(\"variantId\"),\n                f.col(\"idx\"),\n            )\n            # Filter out ambiguous liftover results: multiple indices for the same variant\n            .withColumn(\"count\", f.count(\"*\").over(Window.partitionBy([\"variantId\"])))\n            .filter(f.col(\"count\") == 1)\n            .drop(\"count\")\n        )\n\n    @staticmethod\n    def _resolve_variant_indices(\n        ld_index: DataFrame, ld_matrix: DataFrame\n    ) -&gt; DataFrame:\n        \"\"\"Resolve the `i` and `j` indices of the block matrix to variant IDs (build 38).\n\n        Args:\n            ld_index (DataFrame): Dataframe with resolved variant indices\n            ld_matrix (DataFrame): Dataframe with the filtered LD matrix\n\n        Returns:\n            DataFrame: Dataframe with variant IDs instead of `i` and `j` indices\n        \"\"\"\n        ld_index_i = ld_index.selectExpr(\n            \"idx as i\", \"variantId as variantIdI\", \"chromosome\"\n        )\n        ld_index_j = ld_index.selectExpr(\"idx as j\", \"variantId as variantIdJ\")\n        return (\n            ld_matrix.join(ld_index_i, on=\"i\", how=\"inner\")\n            .join(ld_index_j, on=\"j\", how=\"inner\")\n            .drop(\"i\", \"j\")\n        )\n\n    @staticmethod\n    def _transpose_ld_matrix(ld_matrix: DataFrame) -&gt; DataFrame:\n        \"\"\"Transpose LD matrix to a square matrix format.\n\n        Args:\n            ld_matrix (DataFrame): Triangular LD matrix converted to a Spark DataFrame\n\n        Returns:\n            DataFrame: Square LD matrix without diagonal duplicates\n\n        Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     [\n        ...         (1, 1, 1.0, \"1\", \"AFR\"),\n        ...         (1, 2, 0.5, \"1\", \"AFR\"),\n        ...         (2, 2, 1.0, \"1\", \"AFR\"),\n        ...     ],\n        ...     [\"variantIdI\", \"variantIdJ\", \"r\", \"chromosome\", \"population\"],\n        ... )\n        &gt;&gt;&gt; GnomADLDMatrix._transpose_ld_matrix(df).show()\n        +----------+----------+---+----------+----------+\n        |variantIdI|variantIdJ|  r|chromosome|population|\n        +----------+----------+---+----------+----------+\n        |         1|         2|0.5|         1|       AFR|\n        |         1|         1|1.0|         1|       AFR|\n        |         2|         1|0.5|         1|       AFR|\n        |         2|         2|1.0|         1|       AFR|\n        +----------+----------+---+----------+----------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        ld_matrix_transposed = ld_matrix.selectExpr(\n            \"variantIdI as variantIdJ\",\n            \"variantIdJ as variantIdI\",\n            \"r\",\n            \"chromosome\",\n            \"population\",\n        )\n        return ld_matrix.filter(f.col(\"variantIdI\") != f.col(\"variantIdJ\")).unionByName(\n            ld_matrix_transposed\n        )\n\n    def as_ld_index(\n        self: GnomADLDMatrix,\n        min_r2: float,\n    ) -&gt; LDIndex:\n        \"\"\"Create LDIndex dataset aggregating the LD information across a set of populations.\n\n        **The basic steps to generate the LDIndex are:**\n\n        1. Convert LD matrix to a Spark DataFrame.\n        2. Resolve the matrix indices to variant IDs by lifting over the coordinates to GRCh38.\n        3. Aggregate the LD information across populations.\n\n        Args:\n            min_r2 (float): Minimum r2 value to keep in the table\n\n        Returns:\n            LDIndex: LDIndex dataset\n        \"\"\"\n        ld_indices_unaggregated = []\n        for pop in self.ld_populations:\n            try:\n                ld_matrix_path = self.ld_matrix_template.format(POP=pop)\n                ld_index_raw_path = self.ld_index_raw_template.format(POP=pop)\n                pop_ld_index = self._create_ldindex_for_population(\n                    pop,\n                    ld_matrix_path,\n                    ld_index_raw_path.format(pop),\n                    self.grch37_to_grch38_chain_path,\n                    min_r2,\n                )\n                ld_indices_unaggregated.append(pop_ld_index)\n            except Exception as e:\n                print(f\"Failed to create LDIndex for population {pop}: {e}\")  # noqa: T201\n                sys.exit(1)\n\n        ld_index_unaggregated = (\n            GnomADLDMatrix._transpose_ld_matrix(\n                reduce(lambda df1, df2: df1.unionByName(df2), ld_indices_unaggregated)\n            )\n            .withColumnRenamed(\"variantIdI\", \"variantId\")\n            .withColumnRenamed(\"variantIdJ\", \"tagVariantId\")\n        )\n        return LDIndex(\n            _df=self._aggregate_ld_index_across_populations(ld_index_unaggregated),\n            _schema=LDIndex.get_schema(),\n        )\n\n    def get_ld_variants(\n        self: GnomADLDMatrix,\n        gnomad_ancestry: str,\n        chromosome: str,\n        start: int,\n        end: int,\n    ) -&gt; DataFrame | None:\n        \"\"\"Return melted LD table with resolved variant id based on ancestry and genomic location.\n\n        Args:\n            gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n            chromosome (str): chromosome label\n            start (int): window upper bound\n            end (int): window lower bound\n\n        Returns:\n            DataFrame | None: LD table with resolved variant id based on ancestry and genomic location\n        \"\"\"\n        # Extracting locus:\n        ld_index_df = (\n            self._process_variant_indices(\n                hl.read_table(self.ld_index_raw_template.format(POP=gnomad_ancestry)),\n                self.grch37_to_grch38_chain_path,\n            )\n            .filter(\n                (f.col(\"chromosome\") == chromosome)\n                &amp; (f.col(\"position\") &gt;= start)\n                &amp; (f.col(\"position\") &lt;= end)\n            )\n            .select(\"chromosome\", \"position\", \"variantId\", \"idx\")\n        )\n\n        if ld_index_df.limit(1).count() == 0:\n            # If the returned slice from the ld index is empty, return None\n            return None\n\n        # Compute start and end indices\n        start_index = get_value_from_row(\n            get_top_ranked_in_window(\n                ld_index_df, Window.partitionBy().orderBy(f.col(\"position\").asc())\n            ).collect()[0],\n            \"idx\",\n        )\n        end_index = get_value_from_row(\n            get_top_ranked_in_window(\n                ld_index_df, Window.partitionBy().orderBy(f.col(\"position\").desc())\n            ).collect()[0],\n            \"idx\",\n        )\n\n        return self._extract_square_matrix(\n            ld_index_df, gnomad_ancestry, start_index, end_index\n        )\n\n    def _extract_square_matrix(\n        self: GnomADLDMatrix,\n        ld_index_df: DataFrame,\n        gnomad_ancestry: str,\n        start_index: int,\n        end_index: int,\n    ) -&gt; DataFrame:\n        \"\"\"Return LD square matrix for a region where coordinates are normalised.\n\n        Args:\n            ld_index_df (DataFrame): Look up table between a variantId and its index in the LD matrix\n            gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n            start_index (int): start index of the slice\n            end_index (int): end index of the slice\n\n        Returns:\n            DataFrame: square LD matrix resolved to variants.\n        \"\"\"\n        return (\n            self.get_ld_matrix_slice(\n                gnomad_ancestry, start_index=start_index, end_index=end_index\n            )\n            .join(\n                ld_index_df.select(\n                    f.col(\"idx\").alias(\"idx_i\"),\n                    f.col(\"variantId\").alias(\"variantIdI\"),\n                ),\n                on=\"idx_i\",\n                how=\"inner\",\n            )\n            .join(\n                ld_index_df.select(\n                    f.col(\"idx\").alias(\"idx_j\"),\n                    f.col(\"variantId\").alias(\"variantIdJ\"),\n                ),\n                on=\"idx_j\",\n                how=\"inner\",\n            )\n            .select(\"variantIdI\", \"variantIdJ\", \"r\")\n        )\n\n    def get_ld_matrix_slice(\n        self: GnomADLDMatrix,\n        gnomad_ancestry: str,\n        start_index: int,\n        end_index: int,\n    ) -&gt; DataFrame:\n        \"\"\"Extract a slice of the LD matrix based on the provided ancestry and stop and end indices.\n\n        - The half matrix is completed into a full square.\n        - The returned indices are adjusted based on the start index.\n\n        Args:\n            gnomad_ancestry (str): LD population label eg. `nfe`\n            start_index (int): start index of the slice\n            end_index (int): end index of the slice\n\n        Returns:\n            DataFrame: square slice of the LD matrix melted as dataframe with idx_i, idx_j and r columns\n        \"\"\"\n        # Extracting block matrix slice:\n        half_matrix = BlockMatrix.read(\n            self.ld_matrix_template.format(POP=gnomad_ancestry)\n        ).filter(range(start_index, end_index + 1), range(start_index, end_index + 1))\n\n        # Return converted Dataframe:\n        return (\n            (half_matrix + half_matrix.T)\n            .entries()\n            .to_spark()\n            .select(\n                (f.col(\"i\") + start_index).alias(\"idx_i\"),\n                (f.col(\"j\") + start_index).alias(\"idx_j\"),\n                f.when(f.col(\"i\") == f.col(\"j\"), f.col(\"entry\") / 2)\n                .otherwise(f.col(\"entry\"))\n                .alias(\"r\"),\n            )\n        )\n\n    def get_locus_index(\n        self: GnomADLDMatrix,\n        study_locus_row: Row,\n        radius: int = 500_000,\n        major_population: str = \"nfe\",\n    ) -&gt; DataFrame:\n        \"\"\"Extract hail matrix index from StudyLocus rows.\n\n        Args:\n            study_locus_row (Row): Study-locus row\n            radius (int): Locus radius to extract from gnomad matrix\n            major_population (str): Major population to extract from gnomad matrix, default is \"nfe\"\n\n        Returns:\n            DataFrame: Returns the index of the gnomad matrix for the locus\n\n        \"\"\"\n        chromosome = str(\"chr\" + study_locus_row[\"chromosome\"])\n        start = study_locus_row[\"position\"] - radius\n        end = study_locus_row[\"position\"] + radius\n\n        liftover_ht = hl.read_table(self.liftover_ht_path)\n        liftover_ht = (\n            liftover_ht.filter(\n                (liftover_ht.locus.contig == chromosome)\n                &amp; (liftover_ht.locus.position &gt;= start)\n                &amp; (liftover_ht.locus.position &lt;= end)\n            )\n            .key_by()\n            .select(\"locus\", \"alleles\", \"original_locus\")\n            .key_by(\"original_locus\", \"alleles\")\n            .naive_coalesce(20)\n        )\n\n        hail_index = hl.read_table(\n            self.ld_index_raw_template.format(POP=major_population)\n        )\n\n        joined_index = (\n            liftover_ht.join(hail_index, how=\"inner\").order_by(\"idx\").to_spark()\n        )\n\n        return joined_index\n\n    @staticmethod\n    def get_numpy_matrix(\n        locus_index: DataFrame,\n        gnomad_ancestry: str = \"nfe\",\n    ) -&gt; np.ndarray:\n        \"\"\"Extract the LD block matrix for a locus.\n\n        Args:\n            locus_index (DataFrame): hail matrix variant index table\n            gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n\n        Returns:\n            np.ndarray: LD block matrix for the locus\n        \"\"\"\n        idx = [row[\"idx\"] for row in locus_index.select(\"idx\").collect()]\n\n        half_matrix = (\n            BlockMatrix.read(\n                GnomADLDMatrix().ld_matrix_template.format(POP=gnomad_ancestry)\n            )\n            .filter(idx, idx)\n            .to_numpy()\n        )\n\n        return (half_matrix + half_matrix.T) - np.diag(np.diag(half_matrix))\n\n    def get_locus_index_boundaries(\n        self: GnomADLDMatrix,\n        study_locus_row: Row,\n        major_population: str = \"nfe\",\n    ) -&gt; DataFrame:\n        \"\"\"Extract hail matrix index from StudyLocus rows.\n\n        Args:\n            study_locus_row (Row): Study-locus row\n            major_population (str): Major population to extract from gnomad matrix, default is \"nfe\"\n\n        Returns:\n            DataFrame: Returns the index of the gnomad matrix for the locus\n\n        \"\"\"\n        chromosome = str(\"chr\" + study_locus_row[\"chromosome\"])\n        start = int(study_locus_row[\"locusStart\"])\n        end = int(study_locus_row[\"locusEnd\"])\n\n        liftover_ht = hl.read_table(self.liftover_ht_path)\n        liftover_ht = (\n            liftover_ht.filter(\n                (liftover_ht.locus.contig == chromosome)\n                &amp; (liftover_ht.locus.position &gt;= start)\n                &amp; (liftover_ht.locus.position &lt;= end)\n            )\n            .key_by()\n            .select(\"locus\", \"alleles\", \"original_locus\")\n            .key_by(\"original_locus\", \"alleles\")\n            .naive_coalesce(20)\n        )\n\n        hail_index = hl.read_table(\n            self.ld_index_raw_template.format(POP=major_population)\n        )\n\n        joined_index = (\n            liftover_ht.join(hail_index, how=\"inner\").order_by(\"idx\").to_spark()\n        )\n\n        return joined_index\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.__init__","title":"<code>__init__(ld_matrix_template: str = LDIndexConfig().ld_matrix_template, ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template, grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path, ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations, liftover_ht_path: str = LDIndexConfig().liftover_ht_path)</code>","text":"<p>Initialize.</p> <p>Datasets are accessed in Hail's native format, as provided by the GnomAD consortium.</p> <p>Parameters:</p> Name Type Description Default <code>ld_matrix_template</code> <code>str</code> <p>Template for the LD matrix path.</p> <code>ld_matrix_template</code> <code>ld_index_raw_template</code> <code>str</code> <p>Template for the LD index path.</p> <code>ld_index_raw_template</code> <code>grch37_to_grch38_chain_path</code> <code>str</code> <p>Path to the chain file used to lift over the coordinates.</p> <code>grch37_to_grch38_chain_path</code> <code>ld_populations</code> <code>list[LD_Population | str]</code> <p>List of populations to use to build the LDIndex.</p> <code>ld_populations</code> <code>liftover_ht_path</code> <code>str</code> <p>Path to the liftover ht file.</p> <code>liftover_ht_path</code> <p>Default values are set in LDIndexConfig.</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def __init__(\n    self,\n    ld_matrix_template: str = LDIndexConfig().ld_matrix_template,\n    ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template,\n    grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path,\n    ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations,\n    liftover_ht_path: str = LDIndexConfig().liftover_ht_path,\n):\n    \"\"\"Initialize.\n\n    Datasets are accessed in Hail's native format, as provided by the [GnomAD consortium](https://gnomad.broadinstitute.org/downloads/#v2-linkage-disequilibrium).\n\n    Args:\n        ld_matrix_template (str): Template for the LD matrix path.\n        ld_index_raw_template (str): Template for the LD index path.\n        grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates.\n        ld_populations (list[LD_Population | str]): List of populations to use to build the LDIndex.\n        liftover_ht_path (str): Path to the liftover ht file.\n\n    Default values are set in LDIndexConfig.\n    \"\"\"\n    self.ld_matrix_template = ld_matrix_template\n    self.ld_index_raw_template = ld_index_raw_template\n    self.grch37_to_grch38_chain_path = grch37_to_grch38_chain_path\n    self.ld_populations = ld_populations\n    self.liftover_ht_path = liftover_ht_path\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.as_ld_index","title":"<code>as_ld_index(min_r2: float) -&gt; LDIndex</code>","text":"<p>Create LDIndex dataset aggregating the LD information across a set of populations.</p> <p>The basic steps to generate the LDIndex are:</p> <ol> <li>Convert LD matrix to a Spark DataFrame.</li> <li>Resolve the matrix indices to variant IDs by lifting over the coordinates to GRCh38.</li> <li>Aggregate the LD information across populations.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>min_r2</code> <code>float</code> <p>Minimum r2 value to keep in the table</p> required <p>Returns:</p> Name Type Description <code>LDIndex</code> <code>LDIndex</code> <p>LDIndex dataset</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def as_ld_index(\n    self: GnomADLDMatrix,\n    min_r2: float,\n) -&gt; LDIndex:\n    \"\"\"Create LDIndex dataset aggregating the LD information across a set of populations.\n\n    **The basic steps to generate the LDIndex are:**\n\n    1. Convert LD matrix to a Spark DataFrame.\n    2. Resolve the matrix indices to variant IDs by lifting over the coordinates to GRCh38.\n    3. Aggregate the LD information across populations.\n\n    Args:\n        min_r2 (float): Minimum r2 value to keep in the table\n\n    Returns:\n        LDIndex: LDIndex dataset\n    \"\"\"\n    ld_indices_unaggregated = []\n    for pop in self.ld_populations:\n        try:\n            ld_matrix_path = self.ld_matrix_template.format(POP=pop)\n            ld_index_raw_path = self.ld_index_raw_template.format(POP=pop)\n            pop_ld_index = self._create_ldindex_for_population(\n                pop,\n                ld_matrix_path,\n                ld_index_raw_path.format(pop),\n                self.grch37_to_grch38_chain_path,\n                min_r2,\n            )\n            ld_indices_unaggregated.append(pop_ld_index)\n        except Exception as e:\n            print(f\"Failed to create LDIndex for population {pop}: {e}\")  # noqa: T201\n            sys.exit(1)\n\n    ld_index_unaggregated = (\n        GnomADLDMatrix._transpose_ld_matrix(\n            reduce(lambda df1, df2: df1.unionByName(df2), ld_indices_unaggregated)\n        )\n        .withColumnRenamed(\"variantIdI\", \"variantId\")\n        .withColumnRenamed(\"variantIdJ\", \"tagVariantId\")\n    )\n    return LDIndex(\n        _df=self._aggregate_ld_index_across_populations(ld_index_unaggregated),\n        _schema=LDIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_ld_matrix_slice","title":"<code>get_ld_matrix_slice(gnomad_ancestry: str, start_index: int, end_index: int) -&gt; DataFrame</code>","text":"<p>Extract a slice of the LD matrix based on the provided ancestry and stop and end indices.</p> <ul> <li>The half matrix is completed into a full square.</li> <li>The returned indices are adjusted based on the start index.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>gnomad_ancestry</code> <code>str</code> <p>LD population label eg. <code>nfe</code></p> required <code>start_index</code> <code>int</code> <p>start index of the slice</p> required <code>end_index</code> <code>int</code> <p>end index of the slice</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>square slice of the LD matrix melted as dataframe with idx_i, idx_j and r columns</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def get_ld_matrix_slice(\n    self: GnomADLDMatrix,\n    gnomad_ancestry: str,\n    start_index: int,\n    end_index: int,\n) -&gt; DataFrame:\n    \"\"\"Extract a slice of the LD matrix based on the provided ancestry and stop and end indices.\n\n    - The half matrix is completed into a full square.\n    - The returned indices are adjusted based on the start index.\n\n    Args:\n        gnomad_ancestry (str): LD population label eg. `nfe`\n        start_index (int): start index of the slice\n        end_index (int): end index of the slice\n\n    Returns:\n        DataFrame: square slice of the LD matrix melted as dataframe with idx_i, idx_j and r columns\n    \"\"\"\n    # Extracting block matrix slice:\n    half_matrix = BlockMatrix.read(\n        self.ld_matrix_template.format(POP=gnomad_ancestry)\n    ).filter(range(start_index, end_index + 1), range(start_index, end_index + 1))\n\n    # Return converted Dataframe:\n    return (\n        (half_matrix + half_matrix.T)\n        .entries()\n        .to_spark()\n        .select(\n            (f.col(\"i\") + start_index).alias(\"idx_i\"),\n            (f.col(\"j\") + start_index).alias(\"idx_j\"),\n            f.when(f.col(\"i\") == f.col(\"j\"), f.col(\"entry\") / 2)\n            .otherwise(f.col(\"entry\"))\n            .alias(\"r\"),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_ld_variants","title":"<code>get_ld_variants(gnomad_ancestry: str, chromosome: str, start: int, end: int) -&gt; DataFrame | None</code>","text":"<p>Return melted LD table with resolved variant id based on ancestry and genomic location.</p> <p>Parameters:</p> Name Type Description Default <code>gnomad_ancestry</code> <code>str</code> <p>GnomAD major ancestry label eg. <code>nfe</code></p> required <code>chromosome</code> <code>str</code> <p>chromosome label</p> required <code>start</code> <code>int</code> <p>window upper bound</p> required <code>end</code> <code>int</code> <p>window lower bound</p> required <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>DataFrame | None: LD table with resolved variant id based on ancestry and genomic location</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def get_ld_variants(\n    self: GnomADLDMatrix,\n    gnomad_ancestry: str,\n    chromosome: str,\n    start: int,\n    end: int,\n) -&gt; DataFrame | None:\n    \"\"\"Return melted LD table with resolved variant id based on ancestry and genomic location.\n\n    Args:\n        gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n        chromosome (str): chromosome label\n        start (int): window upper bound\n        end (int): window lower bound\n\n    Returns:\n        DataFrame | None: LD table with resolved variant id based on ancestry and genomic location\n    \"\"\"\n    # Extracting locus:\n    ld_index_df = (\n        self._process_variant_indices(\n            hl.read_table(self.ld_index_raw_template.format(POP=gnomad_ancestry)),\n            self.grch37_to_grch38_chain_path,\n        )\n        .filter(\n            (f.col(\"chromosome\") == chromosome)\n            &amp; (f.col(\"position\") &gt;= start)\n            &amp; (f.col(\"position\") &lt;= end)\n        )\n        .select(\"chromosome\", \"position\", \"variantId\", \"idx\")\n    )\n\n    if ld_index_df.limit(1).count() == 0:\n        # If the returned slice from the ld index is empty, return None\n        return None\n\n    # Compute start and end indices\n    start_index = get_value_from_row(\n        get_top_ranked_in_window(\n            ld_index_df, Window.partitionBy().orderBy(f.col(\"position\").asc())\n        ).collect()[0],\n        \"idx\",\n    )\n    end_index = get_value_from_row(\n        get_top_ranked_in_window(\n            ld_index_df, Window.partitionBy().orderBy(f.col(\"position\").desc())\n        ).collect()[0],\n        \"idx\",\n    )\n\n    return self._extract_square_matrix(\n        ld_index_df, gnomad_ancestry, start_index, end_index\n    )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_locus_index","title":"<code>get_locus_index(study_locus_row: Row, radius: int = 500000, major_population: str = 'nfe') -&gt; DataFrame</code>","text":"<p>Extract hail matrix index from StudyLocus rows.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus_row</code> <code>Row</code> <p>Study-locus row</p> required <code>radius</code> <code>int</code> <p>Locus radius to extract from gnomad matrix</p> <code>500000</code> <code>major_population</code> <code>str</code> <p>Major population to extract from gnomad matrix, default is \"nfe\"</p> <code>'nfe'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Returns the index of the gnomad matrix for the locus</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def get_locus_index(\n    self: GnomADLDMatrix,\n    study_locus_row: Row,\n    radius: int = 500_000,\n    major_population: str = \"nfe\",\n) -&gt; DataFrame:\n    \"\"\"Extract hail matrix index from StudyLocus rows.\n\n    Args:\n        study_locus_row (Row): Study-locus row\n        radius (int): Locus radius to extract from gnomad matrix\n        major_population (str): Major population to extract from gnomad matrix, default is \"nfe\"\n\n    Returns:\n        DataFrame: Returns the index of the gnomad matrix for the locus\n\n    \"\"\"\n    chromosome = str(\"chr\" + study_locus_row[\"chromosome\"])\n    start = study_locus_row[\"position\"] - radius\n    end = study_locus_row[\"position\"] + radius\n\n    liftover_ht = hl.read_table(self.liftover_ht_path)\n    liftover_ht = (\n        liftover_ht.filter(\n            (liftover_ht.locus.contig == chromosome)\n            &amp; (liftover_ht.locus.position &gt;= start)\n            &amp; (liftover_ht.locus.position &lt;= end)\n        )\n        .key_by()\n        .select(\"locus\", \"alleles\", \"original_locus\")\n        .key_by(\"original_locus\", \"alleles\")\n        .naive_coalesce(20)\n    )\n\n    hail_index = hl.read_table(\n        self.ld_index_raw_template.format(POP=major_population)\n    )\n\n    joined_index = (\n        liftover_ht.join(hail_index, how=\"inner\").order_by(\"idx\").to_spark()\n    )\n\n    return joined_index\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_locus_index_boundaries","title":"<code>get_locus_index_boundaries(study_locus_row: Row, major_population: str = 'nfe') -&gt; DataFrame</code>","text":"<p>Extract hail matrix index from StudyLocus rows.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus_row</code> <code>Row</code> <p>Study-locus row</p> required <code>major_population</code> <code>str</code> <p>Major population to extract from gnomad matrix, default is \"nfe\"</p> <code>'nfe'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Returns the index of the gnomad matrix for the locus</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def get_locus_index_boundaries(\n    self: GnomADLDMatrix,\n    study_locus_row: Row,\n    major_population: str = \"nfe\",\n) -&gt; DataFrame:\n    \"\"\"Extract hail matrix index from StudyLocus rows.\n\n    Args:\n        study_locus_row (Row): Study-locus row\n        major_population (str): Major population to extract from gnomad matrix, default is \"nfe\"\n\n    Returns:\n        DataFrame: Returns the index of the gnomad matrix for the locus\n\n    \"\"\"\n    chromosome = str(\"chr\" + study_locus_row[\"chromosome\"])\n    start = int(study_locus_row[\"locusStart\"])\n    end = int(study_locus_row[\"locusEnd\"])\n\n    liftover_ht = hl.read_table(self.liftover_ht_path)\n    liftover_ht = (\n        liftover_ht.filter(\n            (liftover_ht.locus.contig == chromosome)\n            &amp; (liftover_ht.locus.position &gt;= start)\n            &amp; (liftover_ht.locus.position &lt;= end)\n        )\n        .key_by()\n        .select(\"locus\", \"alleles\", \"original_locus\")\n        .key_by(\"original_locus\", \"alleles\")\n        .naive_coalesce(20)\n    )\n\n    hail_index = hl.read_table(\n        self.ld_index_raw_template.format(POP=major_population)\n    )\n\n    joined_index = (\n        liftover_ht.join(hail_index, how=\"inner\").order_by(\"idx\").to_spark()\n    )\n\n    return joined_index\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_numpy_matrix","title":"<code>get_numpy_matrix(locus_index: DataFrame, gnomad_ancestry: str = 'nfe') -&gt; np.ndarray</code>  <code>staticmethod</code>","text":"<p>Extract the LD block matrix for a locus.</p> <p>Parameters:</p> Name Type Description Default <code>locus_index</code> <code>DataFrame</code> <p>hail matrix variant index table</p> required <code>gnomad_ancestry</code> <code>str</code> <p>GnomAD major ancestry label eg. <code>nfe</code></p> <code>'nfe'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: LD block matrix for the locus</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>@staticmethod\ndef get_numpy_matrix(\n    locus_index: DataFrame,\n    gnomad_ancestry: str = \"nfe\",\n) -&gt; np.ndarray:\n    \"\"\"Extract the LD block matrix for a locus.\n\n    Args:\n        locus_index (DataFrame): hail matrix variant index table\n        gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n\n    Returns:\n        np.ndarray: LD block matrix for the locus\n    \"\"\"\n    idx = [row[\"idx\"] for row in locus_index.select(\"idx\").collect()]\n\n    half_matrix = (\n        BlockMatrix.read(\n            GnomADLDMatrix().ld_matrix_template.format(POP=gnomad_ancestry)\n        )\n        .filter(idx, idx)\n        .to_numpy()\n    )\n\n    return (half_matrix + half_matrix.T) - np.diag(np.diag(half_matrix))\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_variants/","title":"Variants","text":""},{"location":"python_api/datasources/gnomad/gnomad_variants/#gentropy.datasource.gnomad.variants.GnomADVariants","title":"<code>gentropy.datasource.gnomad.variants.GnomADVariants</code>","text":"<p>GnomAD variants included in the GnomAD genomes dataset.</p> Source code in <code>src/gentropy/datasource/gnomad/variants.py</code> <pre><code>class GnomADVariants:\n    \"\"\"GnomAD variants included in the GnomAD genomes dataset.\"\"\"\n\n    def __init__(\n        self,\n        gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path,\n        gnomad_variant_populations: list[\n            VariantPopulation | str\n        ] = GnomadVariantConfig().gnomad_variant_populations,\n        hash_threshold: int = VariantIndexConfig().hash_threshold,\n    ):\n        \"\"\"Initialize.\n\n        Args:\n            gnomad_genomes_path (str): Path to gnomAD genomes hail table.\n            gnomad_variant_populations (list[VariantPopulation | str]): List of populations to include.\n            hash_threshold (int): longer variant ids will be hashed.\n\n        All defaults are stored in GnomadVariantConfig.\n        \"\"\"\n        self.gnomad_genomes_path = gnomad_genomes_path\n        self.gnomad_variant_populations = gnomad_variant_populations\n        self.lenght_threshold = hash_threshold\n\n    def as_variant_index(self: GnomADVariants) -&gt; VariantIndex:\n        \"\"\"Generate variant annotation dataset from gnomAD.\n\n        Some relevant modifications to the original dataset are:\n\n        1. The transcript consequences features provided by VEP are filtered to only refer to the Ensembl canonical transcript.\n        2. Genome coordinates are liftovered from GRCh38 to GRCh37 to keep as annotation.\n        3. Field names are converted to camel case to follow the convention.\n\n        Returns:\n            VariantIndex: GnomaAD variants dataset.\n        \"\"\"\n        # Load variants dataset\n        ht = hl.read_table(\n            self.gnomad_genomes_path,\n            _load_refs=False,\n        )\n\n        # Drop non biallelic variants\n        ht = ht.filter(ht.alleles.length() == 2)\n\n        # Select relevant fields and nested records to create class\n        return VariantIndex(\n            _df=(\n                ht.select(\n                    # Extract mandatory fields:\n                    variantId=hl.str(\"_\").join(\n                        [\n                            ht.locus.contig.replace(\"chr\", \"\"),\n                            hl.str(ht.locus.position),\n                            ht.alleles[0],\n                            ht.alleles[1],\n                        ]\n                    ),\n                    chromosome=ht.locus.contig.replace(\"chr\", \"\"),\n                    position=ht.locus.position,\n                    referenceAllele=ht.alleles[0],\n                    alternateAllele=ht.alleles[1],\n                    # Extract allele frequencies from populations of interest:\n                    alleleFrequencies=hl.set(\n                        [f\"{pop}_adj\" for pop in self.gnomad_variant_populations]\n                    ).map(\n                        lambda p: hl.struct(\n                            populationName=p,\n                            alleleFrequency=ht.joint.freq[\n                                ht.joint_globals.freq_index_dict[p]\n                            ].AF,\n                        )\n                    ),\n                    # Extract cross references to GnomAD:\n                    dbXrefs=hl.array(\n                        [\n                            hl.struct(\n                                id=hl.str(\"-\").join(\n                                    [\n                                        ht.locus.contig.replace(\"chr\", \"\"),\n                                        hl.str(ht.locus.position),\n                                        ht.alleles[0],\n                                        ht.alleles[1],\n                                    ]\n                                ),\n                                source=hl.str(\"gnomad\"),\n                            )\n                        ]\n                    ),\n                )\n                .key_by(\"chromosome\", \"position\")\n                .drop(\"locus\", \"alleles\")\n                .select_globals()\n                .to_spark(flatten=False)\n                .withColumns(\n                    {\n                        # Generate a variantId that is hashed for long variant ids:\n                        \"variantId\": VariantIndex.hash_long_variant_ids(\n                            f.col(\"variantId\"),\n                            f.col(\"chromosome\"),\n                            f.col(\"position\"),\n                            self.lenght_threshold,\n                        ),\n                        # We are not capturing the most severe consequence from GnomAD, but this column needed for the schema:\n                        \"mostSevereConsequenceId\": f.lit(None).cast(t.StringType()),\n                    }\n                )\n            ),\n            _schema=VariantIndex.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_variants/#gentropy.datasource.gnomad.variants.GnomADVariants.__init__","title":"<code>__init__(gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path, gnomad_variant_populations: list[VariantPopulation | str] = GnomadVariantConfig().gnomad_variant_populations, hash_threshold: int = VariantIndexConfig().hash_threshold)</code>","text":"<p>Initialize.</p> <p>Parameters:</p> Name Type Description Default <code>gnomad_genomes_path</code> <code>str</code> <p>Path to gnomAD genomes hail table.</p> <code>gnomad_genomes_path</code> <code>gnomad_variant_populations</code> <code>list[VariantPopulation | str]</code> <p>List of populations to include.</p> <code>gnomad_variant_populations</code> <code>hash_threshold</code> <code>int</code> <p>longer variant ids will be hashed.</p> <code>hash_threshold</code> <p>All defaults are stored in GnomadVariantConfig.</p> Source code in <code>src/gentropy/datasource/gnomad/variants.py</code> <pre><code>def __init__(\n    self,\n    gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path,\n    gnomad_variant_populations: list[\n        VariantPopulation | str\n    ] = GnomadVariantConfig().gnomad_variant_populations,\n    hash_threshold: int = VariantIndexConfig().hash_threshold,\n):\n    \"\"\"Initialize.\n\n    Args:\n        gnomad_genomes_path (str): Path to gnomAD genomes hail table.\n        gnomad_variant_populations (list[VariantPopulation | str]): List of populations to include.\n        hash_threshold (int): longer variant ids will be hashed.\n\n    All defaults are stored in GnomadVariantConfig.\n    \"\"\"\n    self.gnomad_genomes_path = gnomad_genomes_path\n    self.gnomad_variant_populations = gnomad_variant_populations\n    self.lenght_threshold = hash_threshold\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_variants/#gentropy.datasource.gnomad.variants.GnomADVariants.as_variant_index","title":"<code>as_variant_index() -&gt; VariantIndex</code>","text":"<p>Generate variant annotation dataset from gnomAD.</p> <p>Some relevant modifications to the original dataset are:</p> <ol> <li>The transcript consequences features provided by VEP are filtered to only refer to the Ensembl canonical transcript.</li> <li>Genome coordinates are liftovered from GRCh38 to GRCh37 to keep as annotation.</li> <li>Field names are converted to camel case to follow the convention.</li> </ol> <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>GnomaAD variants dataset.</p> Source code in <code>src/gentropy/datasource/gnomad/variants.py</code> <pre><code>def as_variant_index(self: GnomADVariants) -&gt; VariantIndex:\n    \"\"\"Generate variant annotation dataset from gnomAD.\n\n    Some relevant modifications to the original dataset are:\n\n    1. The transcript consequences features provided by VEP are filtered to only refer to the Ensembl canonical transcript.\n    2. Genome coordinates are liftovered from GRCh38 to GRCh37 to keep as annotation.\n    3. Field names are converted to camel case to follow the convention.\n\n    Returns:\n        VariantIndex: GnomaAD variants dataset.\n    \"\"\"\n    # Load variants dataset\n    ht = hl.read_table(\n        self.gnomad_genomes_path,\n        _load_refs=False,\n    )\n\n    # Drop non biallelic variants\n    ht = ht.filter(ht.alleles.length() == 2)\n\n    # Select relevant fields and nested records to create class\n    return VariantIndex(\n        _df=(\n            ht.select(\n                # Extract mandatory fields:\n                variantId=hl.str(\"_\").join(\n                    [\n                        ht.locus.contig.replace(\"chr\", \"\"),\n                        hl.str(ht.locus.position),\n                        ht.alleles[0],\n                        ht.alleles[1],\n                    ]\n                ),\n                chromosome=ht.locus.contig.replace(\"chr\", \"\"),\n                position=ht.locus.position,\n                referenceAllele=ht.alleles[0],\n                alternateAllele=ht.alleles[1],\n                # Extract allele frequencies from populations of interest:\n                alleleFrequencies=hl.set(\n                    [f\"{pop}_adj\" for pop in self.gnomad_variant_populations]\n                ).map(\n                    lambda p: hl.struct(\n                        populationName=p,\n                        alleleFrequency=ht.joint.freq[\n                            ht.joint_globals.freq_index_dict[p]\n                        ].AF,\n                    )\n                ),\n                # Extract cross references to GnomAD:\n                dbXrefs=hl.array(\n                    [\n                        hl.struct(\n                            id=hl.str(\"-\").join(\n                                [\n                                    ht.locus.contig.replace(\"chr\", \"\"),\n                                    hl.str(ht.locus.position),\n                                    ht.alleles[0],\n                                    ht.alleles[1],\n                                ]\n                            ),\n                            source=hl.str(\"gnomad\"),\n                        )\n                    ]\n                ),\n            )\n            .key_by(\"chromosome\", \"position\")\n            .drop(\"locus\", \"alleles\")\n            .select_globals()\n            .to_spark(flatten=False)\n            .withColumns(\n                {\n                    # Generate a variantId that is hashed for long variant ids:\n                    \"variantId\": VariantIndex.hash_long_variant_ids(\n                        f.col(\"variantId\"),\n                        f.col(\"chromosome\"),\n                        f.col(\"position\"),\n                        self.lenght_threshold,\n                    ),\n                    # We are not capturing the most severe consequence from GnomAD, but this column needed for the schema:\n                    \"mostSevereConsequenceId\": f.lit(None).cast(t.StringType()),\n                }\n            )\n        ),\n        _schema=VariantIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/_gwas_catalog/","title":"GWAS Catalog","text":"GWAS Catalog <p>The GWAS Catalog is a comprehensive resource that aims to provide a curated collection of Genome-Wide Association Studies (GWAS) (including harmonized full GWAS summary statistics) across various traits and diseases in humans.</p> <p>It serves as a valuable repository of genetic associations identified in diverse populations, offering insights into the genetic basis of complex traits and diseases.</p> <p>We rely on the GWAS Catalog for a rich source of genetic associations, utilizing the data for analysis and interpretation.</p> <p>For detailed information on specific genetic associations, their significance, and associated studies, refer to the GWAS Catalog.</p> <p>Within our analyses, we leverage two different types of studies from the GWAS Catalog:</p> <ol> <li> <p>Studies with (full) GWAS summary stats</p> </li> <li> <p>Studies with top hits only - GWAS curated studies</p> </li> </ol>"},{"location":"python_api/datasources/gwas_catalog/associations/","title":"Associations","text":""},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser","title":"<code>gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser</code>  <code>dataclass</code>","text":"<p>GWAS Catalog curated associations parser.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@dataclass\nclass GWASCatalogCuratedAssociationsParser:\n    \"\"\"GWAS Catalog curated associations parser.\"\"\"\n\n    @staticmethod\n    def convert_gnomad_position_to_ensembl(\n        position: Column, reference: Column, alternate: Column\n    ) -&gt; Column:\n        \"\"\"Convert GnomAD variant position to Ensembl variant position.\n\n        For indels (the reference or alternate allele is longer than 1), then adding 1 to the position, for SNPs,\n        the position is unchanged. More info about the problem: https://www.biostars.org/p/84686/\n\n        Args:\n            position (Column): Position of the variant in GnomAD's coordinates system.\n            reference (Column): The reference allele in GnomAD's coordinates system.\n            alternate (Column): The alternate allele in GnomAD's coordinates system.\n\n        Returns:\n            Column: The position of the variant in the Ensembl genome.\n\n        Examples:\n            &gt;&gt;&gt; d = [(1, \"A\", \"C\"), (2, \"AA\", \"C\"), (3, \"A\", \"AA\")]\n            &gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"position\", \"reference\", \"alternate\")\n            &gt;&gt;&gt; df.withColumn(\"new_position\", GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl(f.col(\"position\"), f.col(\"reference\"), f.col(\"alternate\"))).show()\n            +--------+---------+---------+------------+\n            |position|reference|alternate|new_position|\n            +--------+---------+---------+------------+\n            |       1|        A|        C|           1|\n            |       2|       AA|        C|           3|\n            |       3|        A|       AA|           4|\n            +--------+---------+---------+------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.when(\n            (f.length(reference) &gt; 1) | (f.length(alternate) &gt; 1), position + 1\n        ).otherwise(position)\n\n    @staticmethod\n    def _parse_pvalue(pvalue: Column) -&gt; tuple[Column, Column]:\n        \"\"\"Parse p-value column.\n\n        Args:\n            pvalue (Column): p-value [string]\n\n        Returns:\n            tuple[Column, Column]: p-value mantissa and exponent\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [(\"1.0\"), (\"0.5\"), (\"1E-20\"), (\"3E-3\"), (\"1E-1000\")]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, t.StringType())\n            &gt;&gt;&gt; df.select('value',*GWASCatalogCuratedAssociationsParser._parse_pvalue(f.col('value'))).show()\n            +-------+--------------+--------------+\n            |  value|pValueMantissa|pValueExponent|\n            +-------+--------------+--------------+\n            |    1.0|           1.0|             1|\n            |    0.5|           0.5|             1|\n            |  1E-20|           1.0|           -20|\n            |   3E-3|           3.0|            -3|\n            |1E-1000|           1.0|         -1000|\n            +-------+--------------+--------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        split = f.split(pvalue, \"E\")\n        return split.getItem(0).cast(\"float\").alias(\"pValueMantissa\"), f.coalesce(\n            split.getItem(1).cast(\"integer\"), f.lit(1)\n        ).alias(\"pValueExponent\")\n\n    @staticmethod\n    def _normalise_pvaluetext(p_value_text: Column) -&gt; Column:\n        \"\"\"Normalised p-value text column to a standardised format.\n\n        For cases where there is no mapping, the value is set to null.\n\n        Args:\n            p_value_text (Column): `pValueText` column from GWASCatalog\n\n        Returns:\n            Column: Array column after using GWAS Catalog mappings. There might be multiple mappings for a single p-value text.\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [(\"European Ancestry\"), (\"African ancestry\"), (\"Alzheimer\u2019s Disease\"), (\"(progression)\"), (\"\"), (None)]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, t.StringType())\n            &gt;&gt;&gt; df.withColumn('normalised', GWASCatalogCuratedAssociationsParser._normalise_pvaluetext(f.col('value'))).show()\n            +-------------------+----------+\n            |              value|normalised|\n            +-------------------+----------+\n            |  European Ancestry|      [EA]|\n            |   African ancestry|      [AA]|\n            |Alzheimer\u2019s Disease|      [AD]|\n            |      (progression)|      NULL|\n            |                   |      NULL|\n            |               NULL|      NULL|\n            +-------------------+----------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        # GWAS Catalog to p-value mapping\n        json_dict = json.loads(\n            pkg_resources.read_text(data, \"gwas_pValueText_map.json\", encoding=\"utf-8\")\n        )\n        map_expr = f.create_map(*[f.lit(x) for x in chain(*json_dict.items())])\n\n        splitted_col = f.split(f.regexp_replace(p_value_text, r\"[\\(\\)]\", \"\"), \",\")\n        mapped_col = f.transform(splitted_col, lambda x: map_expr[x])\n        return f.when(f.forall(mapped_col, lambda x: x.isNull()), None).otherwise(\n            mapped_col\n        )\n\n    @staticmethod\n    def _extract_risk_allele(risk_allele: Column) -&gt; Column:\n        \"\"\"Extract risk allele from provided \"STRONGEST SNP-RISK ALLELE\" input column.\n\n        If multiple risk alleles are present, the first one is returned.\n\n        Args:\n            risk_allele (Column): `riskAllele` column from GWASCatalog\n\n        Returns:\n            Column: mapped using GWAS Catalog mapping\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [(\"rs1234-A-G\"), (\"rs1234-A\"), (\"rs1234-A; rs1235-G\")]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, t.StringType())\n            &gt;&gt;&gt; df.withColumn('normalised', GWASCatalogCuratedAssociationsParser._extract_risk_allele(f.col('value'))).show()\n            +------------------+----------+\n            |             value|normalised|\n            +------------------+----------+\n            |        rs1234-A-G|         A|\n            |          rs1234-A|         A|\n            |rs1234-A; rs1235-G|         A|\n            +------------------+----------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # GWAS Catalog to risk allele mapping\n        return f.split(f.split(risk_allele, \"; \").getItem(0), \"-\").getItem(1)\n\n    @staticmethod\n    def _collect_rsids(\n        snp_id: Column, snp_id_current: Column, risk_allele: Column\n    ) -&gt; Column:\n        \"\"\"It takes three columns, and returns an array of distinct values from those columns.\n\n        Args:\n            snp_id (Column): The original snp id from the GWAS catalog.\n            snp_id_current (Column): The current snp id field is just a number at the moment (stored as a string). Adding 'rs' prefix if looks good.\n            risk_allele (Column): The risk allele for the SNP.\n\n        Returns:\n            Column: An array of distinct values.\n        \"\"\"\n        # The current snp id field is just a number at the moment (stored as a string). Adding 'rs' prefix if looks good.\n        snp_id_current = f.when(\n            snp_id_current.rlike(\"^[0-9]*$\"),\n            f.format_string(\"rs%s\", snp_id_current),\n        )\n        # Cleaning risk allele:\n        risk_allele = f.split(risk_allele, \"-\").getItem(0)\n\n        # Collecting all values:\n        return f.array_distinct(f.array(snp_id, snp_id_current, risk_allele))\n\n    @staticmethod\n    def _map_variants_to_gnomad_variants(\n        gwas_associations: DataFrame, variant_index: VariantIndex\n    ) -&gt; DataFrame:\n        \"\"\"Add variant metadata in associations.\n\n        Args:\n            gwas_associations (DataFrame): raw GWAS Catalog associations.\n            variant_index (VariantIndex): GnomaAD variants dataset with allele frequencies.\n\n        Returns:\n            DataFrame: GWAS Catalog associations data including `variantId`, `referenceAllele`,\n            `alternateAllele`, `chromosome`, `position` with variant metadata\n        \"\"\"\n        # Subset of GWAS Catalog associations required for resolving variant IDs:\n        gwas_associations_subset = gwas_associations.select(\n            \"rowId\",\n            f.col(\"CHR_ID\").alias(\"chromosome\"),\n            # The positions from GWAS Catalog are from ensembl that causes discrepancy for indels:\n            f.col(\"CHR_POS\").cast(IntegerType()).alias(\"ensemblPosition\"),\n            # List of all SNPs associated with the variant\n            GWASCatalogCuratedAssociationsParser._collect_rsids(\n                f.split(f.col(\"SNPS\"), \"; \").getItem(0),\n                f.col(\"SNP_ID_CURRENT\"),\n                f.split(f.col(\"STRONGEST SNP-RISK ALLELE\"), \"; \").getItem(0),\n            ).alias(\"rsIdsGwasCatalog\"),\n            GWASCatalogCuratedAssociationsParser._extract_risk_allele(\n                f.col(\"STRONGEST SNP-RISK ALLELE\")\n            ).alias(\"riskAllele\"),\n        )\n\n        # Subset of variant annotation required for GWAS Catalog annotations:\n        va_subset = variant_index.df.select(\n            \"variantId\",\n            \"chromosome\",\n            # Calculate the position in Ensembl coordinates for indels:\n            GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl(\n                f.col(\"position\"),\n                f.col(\"referenceAllele\"),\n                f.col(\"alternateAllele\"),\n            ).alias(\"ensemblPosition\"),\n            # Keeping GnomAD position:\n            \"position\",\n            f.col(\"rsIds\").alias(\"rsIdsGnomad\"),\n            \"referenceAllele\",\n            \"alternateAllele\",\n            \"alleleFrequencies\",\n            variant_index.max_maf().alias(\"maxMaf\"),\n        ).join(\n            gwas_associations_subset.select(\"chromosome\", \"ensemblPosition\").distinct(),\n            on=[\"chromosome\", \"ensemblPosition\"],\n            how=\"inner\",\n        )\n\n        # Semi-resolved ids (still contains duplicates when conclusion was not possible to make\n        # based on rsIds or allele concordance)\n        filtered_associations = (\n            gwas_associations_subset.join(\n                va_subset,\n                on=[\"chromosome\", \"ensemblPosition\"],\n                how=\"left\",\n            )\n            .withColumn(\n                \"rsIdFilter\",\n                GWASCatalogCuratedAssociationsParser._flag_mappings_to_retain(\n                    f.col(\"rowId\"),\n                    GWASCatalogCuratedAssociationsParser._compare_rsids(\n                        f.col(\"rsIdsGnomad\"), f.col(\"rsIdsGwasCatalog\")\n                    ),\n                ),\n            )\n            .withColumn(\n                \"concordanceFilter\",\n                GWASCatalogCuratedAssociationsParser._flag_mappings_to_retain(\n                    f.col(\"rowId\"),\n                    GWASCatalogCuratedAssociationsParser._check_concordance(\n                        f.col(\"riskAllele\"),\n                        f.col(\"referenceAllele\"),\n                        f.col(\"alternateAllele\"),\n                    ),\n                ),\n            )\n            .filter(\n                # Filter out rows where GWAS Catalog rsId does not match with GnomAD rsId,\n                # but there is corresponding variant for the same association\n                f.col(\"rsIdFilter\")\n                # or filter out rows where GWAS Catalog alleles are not concordant with GnomAD alleles,\n                # but there is corresponding variant for the same association\n                | f.col(\"concordanceFilter\")\n            )\n        )\n\n        # Keep only highest maxMaf variant per rowId\n        fully_mapped_associations = get_record_with_maximum_value(\n            filtered_associations, grouping_col=\"rowId\", sorting_col=\"maxMaf\"\n        ).select(\n            \"rowId\",\n            \"variantId\",\n            \"referenceAllele\",\n            \"alternateAllele\",\n            \"chromosome\",\n            \"position\",\n        )\n\n        return gwas_associations.join(fully_mapped_associations, on=\"rowId\", how=\"left\")\n\n    @staticmethod\n    def _compare_rsids(gnomad: Column, gwas: Column) -&gt; Column:\n        \"\"\"If the intersection of the two arrays is greater than 0, return True, otherwise return False.\n\n        Args:\n            gnomad (Column): rsids from gnomad\n            gwas (Column): rsids from the GWAS Catalog\n\n        Returns:\n            Column: A boolean column that is true if the GnomAD rsIDs can be found in the GWAS rsIDs.\n\n        Examples:\n            &gt;&gt;&gt; d = [\n            ...    (1, [\"rs123\", \"rs523\"], [\"rs123\"]),\n            ...    (2, [], [\"rs123\"]),\n            ...    (3, [\"rs123\", \"rs523\"], []),\n            ...    (4, [], []),\n            ... ]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, ['associationId', 'gnomad', 'gwas'])\n            &gt;&gt;&gt; df.withColumn(\"rsid_matches\", GWASCatalogCuratedAssociationsParser._compare_rsids(f.col(\"gnomad\"),f.col('gwas'))).show()\n            +-------------+--------------+-------+------------+\n            |associationId|        gnomad|   gwas|rsid_matches|\n            +-------------+--------------+-------+------------+\n            |            1|[rs123, rs523]|[rs123]|        true|\n            |            2|            []|[rs123]|       false|\n            |            3|[rs123, rs523]|     []|       false|\n            |            4|            []|     []|       false|\n            +-------------+--------------+-------+------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return f.when(f.size(f.array_intersect(gnomad, gwas)) &gt; 0, True).otherwise(\n            False\n        )\n\n    @staticmethod\n    def _flag_mappings_to_retain(\n        association_id: Column, filter_column: Column\n    ) -&gt; Column:\n        \"\"\"Flagging mappings to drop for each association.\n\n        Some associations have multiple mappings. Some has matching rsId others don't. We only\n        want to drop the non-matching mappings, when a matching is available for the given association.\n        This logic can be generalised for other measures eg. allele concordance.\n\n        Args:\n            association_id (Column): association identifier column\n            filter_column (Column): boolean col indicating to keep a mapping\n\n        Returns:\n            Column: A column with a boolean value.\n\n        Examples:\n        &gt;&gt;&gt; d = [\n        ...    (1, False),\n        ...    (1, False),\n        ...    (2, False),\n        ...    (2, True),\n        ...    (3, True),\n        ...    (3, True),\n        ... ]\n        &gt;&gt;&gt; df = spark.createDataFrame(d, ['associationId', 'filter'])\n        &gt;&gt;&gt; df.withColumn(\"isConcordant\", GWASCatalogCuratedAssociationsParser._flag_mappings_to_retain(f.col(\"associationId\"),f.col('filter'))).show()\n        +-------------+------+------------+\n        |associationId|filter|isConcordant|\n        +-------------+------+------------+\n        |            1| false|        true|\n        |            1| false|        true|\n        |            2| false|       false|\n        |            2|  true|        true|\n        |            3|  true|        true|\n        |            3|  true|        true|\n        +-------------+------+------------+\n        &lt;BLANKLINE&gt;\n\n        \"\"\"\n        w = Window.partitionBy(association_id)\n\n        # Generating a boolean column informing if the filter column contains true anywhere for the association:\n        aggregated_filter = f.when(\n            f.array_contains(f.collect_set(filter_column).over(w), True), True\n        ).otherwise(False)\n\n        # Generate a filter column:\n        return f.when(aggregated_filter &amp; (~filter_column), False).otherwise(True)\n\n    @staticmethod\n    def _check_concordance(\n        risk_allele: Column, reference_allele: Column, alternate_allele: Column\n    ) -&gt; Column:\n        \"\"\"A function to check if the risk allele is concordant with the alt or ref allele.\n\n        If the risk allele is the same as the reference or alternate allele, or if the reverse complement of\n        the risk allele is the same as the reference or alternate allele, then the allele is concordant.\n        If no mapping is available (ref/alt is null), the function returns True.\n\n        Args:\n            risk_allele (Column): The allele that is associated with the risk of the disease.\n            reference_allele (Column): The reference allele from the GWAS catalog\n            alternate_allele (Column): The alternate allele of the variant.\n\n        Returns:\n            Column: A boolean column that is True if the risk allele is the same as the reference or alternate allele,\n            or if the reverse complement of the risk allele is the same as the reference or alternate allele.\n\n        Examples:\n            &gt;&gt;&gt; d = [\n            ...     ('A', 'A', 'G'),\n            ...     ('A', 'T', 'G'),\n            ...     ('A', 'C', 'G'),\n            ...     ('A', 'A', '?'),\n            ...     (None, None, 'A'),\n            ... ]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, ['riskAllele', 'referenceAllele', 'alternateAllele'])\n            &gt;&gt;&gt; df.withColumn(\"isConcordant\", GWASCatalogCuratedAssociationsParser._check_concordance(f.col(\"riskAllele\"),f.col('referenceAllele'), f.col('alternateAllele'))).show()\n            +----------+---------------+---------------+------------+\n            |riskAllele|referenceAllele|alternateAllele|isConcordant|\n            +----------+---------------+---------------+------------+\n            |         A|              A|              G|        true|\n            |         A|              T|              G|        true|\n            |         A|              C|              G|       false|\n            |         A|              A|              ?|        true|\n            |      NULL|           NULL|              A|        true|\n            +----------+---------------+---------------+------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        # Calculating the reverse complement of the risk allele:\n        risk_allele_reverse_complement = f.when(\n            risk_allele.rlike(r\"^[ACTG]+$\"),\n            f.reverse(f.translate(risk_allele, \"ACTG\", \"TGAC\")),\n        ).otherwise(risk_allele)\n\n        # OK, is the risk allele or the reverse complent is the same as the mapped alleles:\n        return (\n            f.when(\n                (risk_allele == reference_allele) | (risk_allele == alternate_allele),\n                True,\n            )\n            # If risk allele is found on the negative strand:\n            .when(\n                (risk_allele_reverse_complement == reference_allele)\n                | (risk_allele_reverse_complement == alternate_allele),\n                True,\n            )\n            # If risk allele is ambiguous, still accepted: &lt; This condition could be reconsidered\n            .when(risk_allele == \"?\", True)\n            # If the association could not be mapped we keep it:\n            .when(reference_allele.isNull(), True)\n            # Allele is discordant:\n            .otherwise(False)\n        )\n\n    @staticmethod\n    def _get_reverse_complement(allele_col: Column) -&gt; Column:\n        \"\"\"A function to return the reverse complement of an allele column.\n\n        It takes a string and returns the reverse complement of that string if it's a DNA sequence,\n        otherwise it returns the original string. Assumes alleles in upper case.\n\n        Args:\n            allele_col (Column): The column containing the allele to reverse complement.\n\n        Returns:\n            Column: A column that is the reverse complement of the allele column.\n\n        Examples:\n            &gt;&gt;&gt; d = [{\"allele\": 'A'}, {\"allele\": 'T'},{\"allele\": 'G'}, {\"allele\": 'C'},{\"allele\": 'AC'}, {\"allele\": 'GTaatc'},{\"allele\": '?'}, {\"allele\": None}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d)\n            &gt;&gt;&gt; df.withColumn(\"revcom_allele\", GWASCatalogCuratedAssociationsParser._get_reverse_complement(f.col(\"allele\"))).show()\n            +------+-------------+\n            |allele|revcom_allele|\n            +------+-------------+\n            |     A|            T|\n            |     T|            A|\n            |     G|            C|\n            |     C|            G|\n            |    AC|           GT|\n            |GTaatc|       GATTAC|\n            |     ?|            ?|\n            |  NULL|         NULL|\n            +------+-------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        allele_col = f.upper(allele_col)\n        return f.when(\n            allele_col.rlike(\"[ACTG]+\"),\n            f.reverse(f.translate(allele_col, \"ACTG\", \"TGAC\")),\n        ).otherwise(allele_col)\n\n    @staticmethod\n    def _effect_needs_harmonisation(\n        risk_allele: Column, reference_allele: Column\n    ) -&gt; Column:\n        \"\"\"A function to check if the effect allele needs to be harmonised.\n\n        Args:\n            risk_allele (Column): Risk allele column\n            reference_allele (Column): Effect allele column\n\n        Returns:\n            Column: A boolean column indicating if the effect allele needs to be harmonised.\n\n        Examples:\n            &gt;&gt;&gt; d = [{\"risk\": 'A', \"reference\": 'A'}, {\"risk\": 'A', \"reference\": 'T'}, {\"risk\": 'AT', \"reference\": 'TA'}, {\"risk\": 'AT', \"reference\": 'AT'}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d)\n            &gt;&gt;&gt; df.withColumn(\"needs_harmonisation\", GWASCatalogCuratedAssociationsParser._effect_needs_harmonisation(f.col(\"risk\"), f.col(\"reference\"))).show()\n            +---------+----+-------------------+\n            |reference|risk|needs_harmonisation|\n            +---------+----+-------------------+\n            |        A|   A|               true|\n            |        T|   A|               true|\n            |       TA|  AT|              false|\n            |       AT|  AT|               true|\n            +---------+----+-------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return (risk_allele == reference_allele) | (\n            risk_allele\n            == GWASCatalogCuratedAssociationsParser._get_reverse_complement(\n                reference_allele\n            )\n        )\n\n    @staticmethod\n    def _are_alleles_palindromic(\n        reference_allele: Column, alternate_allele: Column\n    ) -&gt; Column:\n        \"\"\"A function to check if the alleles are palindromic.\n\n        Args:\n            reference_allele (Column): Reference allele column\n            alternate_allele (Column): Alternate allele column\n\n        Returns:\n            Column: A boolean column indicating if the alleles are palindromic.\n\n        Examples:\n            &gt;&gt;&gt; d = [{\"reference\": 'A', \"alternate\": 'T'}, {\"reference\": 'AT', \"alternate\": 'AG'}, {\"reference\": 'AT', \"alternate\": 'AT'}, {\"reference\": 'CATATG', \"alternate\": 'CATATG'}, {\"reference\": '-', \"alternate\": None}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d)\n            &gt;&gt;&gt; df.withColumn(\"is_palindromic\", GWASCatalogCuratedAssociationsParser._are_alleles_palindromic(f.col(\"reference\"), f.col(\"alternate\"))).show()\n            +---------+---------+--------------+\n            |alternate|reference|is_palindromic|\n            +---------+---------+--------------+\n            |        T|        A|          true|\n            |       AG|       AT|         false|\n            |       AT|       AT|          true|\n            |   CATATG|   CATATG|          true|\n            |     NULL|        -|         false|\n            +---------+---------+--------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        revcomp = GWASCatalogCuratedAssociationsParser._get_reverse_complement(\n            alternate_allele\n        )\n        return (\n            f.when(reference_allele == revcomp, True)\n            .when(revcomp.isNull(), False)\n            .otherwise(False)\n        )\n\n    @staticmethod\n    def _harmonise_beta(\n        effect_size: Column,\n        confidence_interval: Column,\n        flipping_needed: Column,\n    ) -&gt; Column:\n        \"\"\"A function to extract the beta value from the effect size and confidence interval and harmonises for the alternate allele.\n\n        If the confidence interval contains the word \"increase\" or \"decrease\" it indicates, we are dealing with betas.\n        If it's \"increase\" and the effect size needs to be harmonized, then multiply the effect size by -1.\n        The sign of the effect size is flipped if the confidence interval contains \"decrease\".\n\n        eg. if the reported value is 0.5, and the confidence interval tells \"decrease\"? -&gt; beta is -0.5\n\n        Args:\n            effect_size (Column): GWAS Catalog effect size column.\n            confidence_interval (Column): GWAS Catalog confidence interval column to know the direction of the effect.\n            flipping_needed (Column): Boolean flag indicating if effect needs to be flipped based on the alleles.\n\n        Returns:\n            Column: A column containing the beta value.\n\n        Examples:\n            &gt;&gt;&gt; d = [\n            ...    # positive effect -no flipping:\n            ...    (0.5, 'increase', False),\n            ...    # Positive effect - flip:\n            ...    (0.5, 'decrease', False),\n            ...    # Positive effect - flip:\n            ...    (0.5, 'decrease', True),\n            ...    # Negative effect - no flip:\n            ...    (0.5, 'increase', True),\n            ...    # Negative effect - flip:\n            ...    (0.5, 'decrease', False),\n            ... ]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(d, ['effect', 'ci_text', 'flip'])\n            ...    .select(\"effect\", \"ci_text\", 'flip', GWASCatalogCuratedAssociationsParser._harmonise_beta(f.col(\"effect\"), f.col(\"ci_text\"), f.lit(False)).alias(\"beta\"))\n            ...    .show()\n            ... )\n            +------+--------+-----+----+\n            |effect| ci_text| flip|beta|\n            +------+--------+-----+----+\n            |   0.5|increase|false| 0.5|\n            |   0.5|decrease|false|-0.5|\n            |   0.5|decrease| true|-0.5|\n            |   0.5|increase| true| 0.5|\n            |   0.5|decrease|false|-0.5|\n            +------+--------+-----+----+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return (\n            f.when(\n                (flipping_needed &amp; confidence_interval.contains(\"increase\"))\n                | (~flipping_needed &amp; confidence_interval.contains(\"decrease\")),\n                -effect_size,\n            )\n            .otherwise(effect_size)\n            .cast(DoubleType())\n        )\n\n    @staticmethod\n    def _harmonise_odds_ratio(\n        effect_size: Column,\n        flipping_needed: Column,\n    ) -&gt; Column:\n        \"\"\"Odds ratio is either propagated as is, or flipped if indicated, meaning returning a reciprocal value.\n\n        Args:\n            effect_size (Column): containing effect size,\n            flipping_needed (Column): Boolean flag indicating if effect needs to be flipped\n\n        Returns:\n            Column: A column with the odds ratio, or 1/odds_ratio if harmonization required.\n\n        Examples:\n        &gt;&gt;&gt; d = [(0.5, False), (0.5, True), (0.0, False), (0.0, True)]\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame(d, ['effect', 'flip'])\n        ...    .select(\"effect\", \"flip\", GWASCatalogCuratedAssociationsParser._harmonise_odds_ratio(f.col(\"effect\"), f.col(\"flip\")).alias(\"odds_ratio\"))\n        ...    .show()\n        ... )\n        +------+-----+----------+\n        |effect| flip|odds_ratio|\n        +------+-----+----------+\n        |   0.5|false|       0.5|\n        |   0.5| true|       2.0|\n        |   0.0|false|       0.0|\n        |   0.0| true|      NULL|\n        +------+-----+----------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        return (\n            # We are not flipping zero effect size:\n            f.when((effect_size.cast(DoubleType()) == 0) &amp; flipping_needed, f.lit(None))\n            .when(\n                flipping_needed,\n                1 / effect_size,\n            )\n            .otherwise(effect_size)\n            .cast(DoubleType())\n        )\n\n    @staticmethod\n    def _concatenate_substudy_description(\n        association_trait: Column, pvalue_text: Column, mapped_trait_uri: Column\n    ) -&gt; Column:\n        \"\"\"Substudy description parsing. Complex string containing metadata about the substudy (e.g. QTL, specific EFO, etc.).\n\n        Args:\n            association_trait (Column): GWAS Catalog association trait column\n            pvalue_text (Column): GWAS Catalog p-value text column\n            mapped_trait_uri (Column): GWAS Catalog mapped trait URI column\n\n        Returns:\n            Column: A column with the substudy description in the shape trait|pvaluetext1_pvaluetext2|EFO1_EFO2.\n\n        Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([\n        ...    (\"Height\", \"http://www.ebi.ac.uk/efo/EFO_0000001,http://www.ebi.ac.uk/efo/EFO_0000002\", \"European Ancestry\"),\n        ...    (\"Schizophrenia\", \"http://www.ebi.ac.uk/efo/MONDO_0005090\", None)],\n        ...    [\"association_trait\", \"mapped_trait_uri\", \"pvalue_text\"]\n        ... )\n        &gt;&gt;&gt; df.withColumn('substudy_description', GWASCatalogCuratedAssociationsParser._concatenate_substudy_description(df.association_trait, df.pvalue_text, df.mapped_trait_uri)).show(truncate=False)\n        +-----------------+-------------------------------------------------------------------------+-----------------+------------------------------------------+\n        |association_trait|mapped_trait_uri                                                         |pvalue_text      |substudy_description                      |\n        +-----------------+-------------------------------------------------------------------------+-----------------+------------------------------------------+\n        |Height           |http://www.ebi.ac.uk/efo/EFO_0000001,http://www.ebi.ac.uk/efo/EFO_0000002|European Ancestry|Height|EA|EFO_0000001/EFO_0000002         |\n        |Schizophrenia    |http://www.ebi.ac.uk/efo/MONDO_0005090                                   |NULL             |Schizophrenia|no_pvalue_text|MONDO_0005090|\n        +-----------------+-------------------------------------------------------------------------+-----------------+------------------------------------------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        p_value_text = f.coalesce(\n            GWASCatalogCuratedAssociationsParser._normalise_pvaluetext(pvalue_text),\n            f.array(f.lit(\"no_pvalue_text\")),\n        )\n        return f.concat_ws(\n            \"|\",\n            association_trait,\n            f.concat_ws(\n                \"/\",\n                p_value_text,\n            ),\n            f.concat_ws(\n                \"/\",\n                parse_efos(mapped_trait_uri),\n            ),\n        )\n\n    @staticmethod\n    def _qc_all(\n        qc: Column,\n        chromosome: Column,\n        position: Column,\n        reference_allele: Column,\n        alternate_allele: Column,\n        strongest_snp_risk_allele: Column,\n        p_value_mantissa: Column,\n        p_value_exponent: Column,\n        p_value_cutoff: float,\n    ) -&gt; Column:\n        \"\"\"Flag associations that fail any QC.\n\n        Args:\n            qc (Column): QC column\n            chromosome (Column): Chromosome column\n            position (Column): Position column\n            reference_allele (Column): Reference allele column\n            alternate_allele (Column): Alternate allele column\n            strongest_snp_risk_allele (Column): Strongest SNP risk allele column\n            p_value_mantissa (Column): P-value mantissa column\n            p_value_exponent (Column): P-value exponent column\n            p_value_cutoff (float): P-value cutoff\n\n        Returns:\n            Column: Updated QC column with flag.\n        \"\"\"\n        qc = GWASCatalogCuratedAssociationsParser._qc_variant_interactions(\n            qc, strongest_snp_risk_allele\n        )\n        qc = StudyLocus._qc_subsignificant_associations(\n            qc, p_value_mantissa, p_value_exponent, p_value_cutoff\n        )\n        qc = GWASCatalogCuratedAssociationsParser._qc_genomic_location(\n            qc, chromosome, position\n        )\n        qc = GWASCatalogCuratedAssociationsParser._qc_variant_inconsistencies(\n            qc, chromosome, position, strongest_snp_risk_allele\n        )\n        qc = GWASCatalogCuratedAssociationsParser._qc_unmapped_variants(\n            qc, alternate_allele\n        )\n        qc = GWASCatalogCuratedAssociationsParser._qc_palindromic_alleles(\n            qc, reference_allele, alternate_allele\n        )\n        return qc\n\n    @staticmethod\n    def _qc_variant_interactions(\n        qc: Column, strongest_snp_risk_allele: Column\n    ) -&gt; Column:\n        \"\"\"Flag associations based on variant x variant interactions.\n\n        Args:\n            qc (Column): QC column\n            strongest_snp_risk_allele (Column): Column with the strongest SNP risk allele\n\n        Returns:\n            Column: Updated QC column with flag.\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            strongest_snp_risk_allele.contains(\";\"),\n            StudyLocusQualityCheck.COMPOSITE_FLAG,\n        )\n\n    @staticmethod\n    def _qc_genomic_location(\n        qc: Column, chromosome: Column, position: Column\n    ) -&gt; Column:\n        \"\"\"Flag associations without genomic location in GWAS Catalog.\n\n        Args:\n            qc (Column): QC column\n            chromosome (Column): Chromosome column in GWAS Catalog\n            position (Column): Position column in GWAS Catalog\n\n        Returns:\n            Column: Updated QC column with flag.\n\n        Examples:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [{'qc': None, 'chromosome': None, 'position': None}, {'qc': None, 'chromosome': '1', 'position': None}, {'qc': None, 'chromosome': None, 'position': 1}, {'qc': None, 'chromosome': '1', 'position': 1}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, schema=t.StructType([t.StructField('qc', t.ArrayType(t.StringType()), True), t.StructField('chromosome', t.StringType()), t.StructField('position', t.IntegerType())]))\n            &gt;&gt;&gt; df.withColumn('qc', GWASCatalogCuratedAssociationsParser._qc_genomic_location(df.qc, df.chromosome, df.position)).show(truncate=False)\n            +----------------------------+----------+--------+\n            |qc                          |chromosome|position|\n            +----------------------------+----------+--------+\n            |[Incomplete genomic mapping]|NULL      |NULL    |\n            |[Incomplete genomic mapping]|1         |NULL    |\n            |[Incomplete genomic mapping]|NULL      |1       |\n            |[]                          |1         |1       |\n            +----------------------------+----------+--------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            position.isNull() | chromosome.isNull(),\n            StudyLocusQualityCheck.NO_GENOMIC_LOCATION_FLAG,\n        )\n\n    @staticmethod\n    def _qc_variant_inconsistencies(\n        qc: Column,\n        chromosome: Column,\n        position: Column,\n        strongest_snp_risk_allele: Column,\n    ) -&gt; Column:\n        \"\"\"Flag associations with inconsistencies in the variant annotation.\n\n        Args:\n            qc (Column): QC column\n            chromosome (Column): Chromosome column in GWAS Catalog\n            position (Column): Position column in GWAS Catalog\n            strongest_snp_risk_allele (Column): Strongest SNP risk allele column in GWAS Catalog\n\n        Returns:\n            Column: Updated QC column with flag.\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            # Number of chromosomes does not correspond to the number of positions:\n            (f.size(f.split(chromosome, \";\")) != f.size(f.split(position, \";\")))\n            # Number of chromosome values different from riskAllele values:\n            | (\n                f.size(f.split(chromosome, \";\"))\n                != f.size(f.split(strongest_snp_risk_allele, \";\"))\n            ),\n            StudyLocusQualityCheck.INCONSISTENCY_FLAG,\n        )\n\n    @staticmethod\n    def _qc_unmapped_variants(qc: Column, alternate_allele: Column) -&gt; Column:\n        \"\"\"Flag associations with variants not mapped to variantAnnotation.\n\n        Args:\n            qc (Column): QC column\n            alternate_allele (Column): alternate allele\n\n        Returns:\n            Column: Updated QC column with flag.\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [{'alternate_allele': 'A', 'qc': None}, {'alternate_allele': None, 'qc': None}]\n            &gt;&gt;&gt; schema = t.StructType([t.StructField('alternate_allele', t.StringType(), True), t.StructField('qc', t.ArrayType(t.StringType()), True)])\n            &gt;&gt;&gt; df = spark.createDataFrame(data=d, schema=schema)\n            &gt;&gt;&gt; df.withColumn(\"new_qc\", GWASCatalogCuratedAssociationsParser._qc_unmapped_variants(f.col(\"qc\"), f.col(\"alternate_allele\"))).show()\n            +----------------+----+--------------------+\n            |alternate_allele|  qc|              new_qc|\n            +----------------+----+--------------------+\n            |               A|NULL|                  []|\n            |            NULL|NULL|[No mapping in Gn...|\n            +----------------+----+--------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            alternate_allele.isNull(),\n            StudyLocusQualityCheck.NON_MAPPED_VARIANT_FLAG,\n        )\n\n    @staticmethod\n    def _qc_palindromic_alleles(\n        qc: Column, reference_allele: Column, alternate_allele: Column\n    ) -&gt; Column:\n        \"\"\"Flag associations with palindromic variants which effects can not be harmonised.\n\n        Args:\n            qc (Column): QC column\n            reference_allele (Column): reference allele\n            alternate_allele (Column): alternate allele\n\n        Returns:\n            Column: Updated QC column with flag.\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; schema = t.StructType([t.StructField('reference_allele', t.StringType(), True), t.StructField('alternate_allele', t.StringType(), True), t.StructField('qc', t.ArrayType(t.StringType()), True)])\n            &gt;&gt;&gt; d = [{'reference_allele': 'A', 'alternate_allele': 'T', 'qc': None}, {'reference_allele': 'AT', 'alternate_allele': 'TA', 'qc': None}, {'reference_allele': 'AT', 'alternate_allele': 'AT', 'qc': None}]\n            &gt;&gt;&gt; df = spark.createDataFrame(data=d, schema=schema)\n            &gt;&gt;&gt; df.withColumn(\"qc\", GWASCatalogCuratedAssociationsParser._qc_palindromic_alleles(f.col(\"qc\"), f.col(\"reference_allele\"), f.col(\"alternate_allele\"))).show(truncate=False)\n            +----------------+----------------+---------------------------------------+\n            |reference_allele|alternate_allele|qc                                     |\n            +----------------+----------------+---------------------------------------+\n            |A               |T               |[Palindrome alleles - cannot harmonize]|\n            |AT              |TA              |[]                                     |\n            |AT              |AT              |[Palindrome alleles - cannot harmonize]|\n            +----------------+----------------+---------------------------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            GWASCatalogCuratedAssociationsParser._are_alleles_palindromic(\n                reference_allele, alternate_allele\n            ),\n            StudyLocusQualityCheck.PALINDROMIC_ALLELE_FLAG,\n        )\n\n    @staticmethod\n    def _get_effect_type(ci_text: Column) -&gt; Column:\n        \"\"\"Extracts the effect type from the 95% CI text.\n\n        The GWAS Catalog confidence interval column contains text that can be used to infer the effect type.\n        If the text contains \"increase\" or \"decrease\", the effect type is beta, otherwise it is odds ratio.\n        Null columns return null as the effect type.\n\n        Args:\n            ci_text (Column): Column containing the 95% CI text.\n\n        Returns:\n            Column: A column containing the effect type.\n\n        Examples:\n            &gt;&gt;&gt; data = [{\"ci_text\": \"95% CI: [0.1-0.2]\"}, {\"ci_text\": \"95% CI: [0.1-0.2] increase\"}, {\"ci_text\": \"95% CI: [0.1-0.2] decrease\"}, {\"ci_text\": None}]\n            &gt;&gt;&gt; spark.createDataFrame(data).select('ci_text', GWASCatalogCuratedAssociationsParser._get_effect_type(f.col('ci_text')).alias('effect_type')).show(truncate=False)\n            +--------------------------+-----------+\n            |ci_text                   |effect_type|\n            +--------------------------+-----------+\n            |95% CI: [0.1-0.2]         |odds_ratio |\n            |95% CI: [0.1-0.2] increase|beta       |\n            |95% CI: [0.1-0.2] decrease|beta       |\n            |NULL                      |NULL       |\n            +--------------------------+-----------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return f.when(\n            f.lower(ci_text).contains(\"increase\")\n            | f.lower(ci_text).contains(\"decrease\"),\n            f.lit(\"beta\"),\n        ).when(ci_text.isNotNull(), f.lit(\"odds_ratio\"))\n\n    @staticmethod\n    def harmonise_association_effect_to_beta(\n        df: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"Harmonise effect to beta value.\n\n        The harmonisation process has a number of steps:\n        - Extracting the reported effect allele.\n        - Flagging palindromic alleles.\n        - Flagging associations where the effect direction needs to be flipped.\n        - Flagging the effect type.\n        - Getting the standard error from the confidence interval text.\n        - Harmonising both beta and odds ratio.\n        - Converting the odds ratio to beta.\n\n        Args:\n            df (DataFrame): DataFrame with the following columns:\n\n        Returns:\n            DataFrame: DataFrame with the following columns:\n\n        Raises:\n            ValueError: If any of the required columns are missing.\n\n        Examples:\n            &gt;&gt;&gt; data = [\n            ...    # Flagged as palindromic:\n            ...    ('rs123-T', 'A', 'T', '0.1', '[0.08-0.12] unit increase'),\n            ...    # Not palindromic, beta needs to be flipped:\n            ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12] unit increase'),\n            ...    # Beta is not flipped:\n            ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12] unit increase'),\n            ...    # odds ratio:\n            ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12]'),\n            ...    # odds ratio flipped:\n            ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12]'),\n            ... ]\n            &gt;&gt;&gt; schema = [\"STRONGEST SNP-RISK ALLELE\", \"referenceAllele\", \"alternateAllele\", \"OR or BETA\", \"95% CI (TEXT)\"]\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; GWASCatalogCuratedAssociationsParser.harmonise_association_effect_to_beta(df).show()\n            +-------------------------+---------------+---------------+----------+--------------------+-------------------+--------------------+\n            |STRONGEST SNP-RISK ALLELE|referenceAllele|alternateAllele|OR or BETA|       95% CI (TEXT)|               beta|       standardError|\n            +-------------------------+---------------+---------------+----------+--------------------+-------------------+--------------------+\n            |                  rs123-T|              A|              T|       0.1|[0.08-0.12] unit ...|               NULL|                NULL|\n            |                  rs123-C|              G|              T|       0.1|[0.08-0.12] unit ...|               -0.1|0.010204081404574064|\n            |                  rs123-T|              C|              T|       0.1|[0.08-0.12] unit ...|                0.1|0.010204081404574064|\n            |                  rs123-T|              C|              T|       0.1|         [0.08-0.12]|-2.3025850929940455|                NULL|\n            |                  rs123-C|              G|              T|       0.1|         [0.08-0.12]|  2.302585092994046|                NULL|\n            +-------------------------+---------------+---------------+----------+--------------------+-------------------+--------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # Testing if all columns are in the dataframe:\n        required_columns = [\n            \"STRONGEST SNP-RISK ALLELE\",\n            \"referenceAllele\",\n            \"alternateAllele\",\n            \"OR or BETA\",\n            \"95% CI (TEXT)\",\n        ]\n\n        for column in required_columns:\n            if column not in df.columns:\n                raise ValueError(\n                    f\"Column {column} is required for harmonising effect to beta value.\"\n                )\n\n        return (\n            df.withColumn(\n                \"reportedRiskAllele\",\n                GWASCatalogCuratedAssociationsParser._extract_risk_allele(\n                    f.col(\"STRONGEST SNP-RISK ALLELE\")\n                ),\n            )\n            .withColumns(\n                {\n                    # Flag palindromic alleles:\n                    \"isAllelePalindromic\": GWASCatalogCuratedAssociationsParser._are_alleles_palindromic(\n                        f.col(\"referenceAllele\"), f.col(\"alternateAllele\")\n                    ),\n                    # Flag associations, where the effect direction needs to be flipped:\n                    \"needsFlipping\": GWASCatalogCuratedAssociationsParser._effect_needs_harmonisation(\n                        f.col(\"reportedRiskAllele\"), f.col(\"referenceAllele\")\n                    ),\n                    # Flag effect type:\n                    \"effectType\": GWASCatalogCuratedAssociationsParser._get_effect_type(\n                        f.col(\"95% CI (TEXT)\")\n                    ),\n                    # Get standard error from confidence interval text:\n                    \"standardError\": get_standard_error_from_confidence_interval(\n                        f.regexp_extract(\n                            \"95% CI (TEXT)\", r\"\\[(\\d+\\.*\\d*)-\\d+\\.*\\d*\\]\", 1\n                        ).cast(FloatType()),\n                        f.regexp_extract(\n                            \"95% CI (TEXT)\", r\"\\[\\d+\\.*\\d*-(\\d+\\.*\\d*)\\]\", 1\n                        ).cast(FloatType()),\n                    ),\n                }\n            )\n            # Harmonise both beta and odds ratio:\n            .withColumns(\n                {  # Normalise beta value of the association:\n                    \"effect_beta\": f.when(\n                        (f.col(\"effectType\") == \"beta\")\n                        &amp; (~f.col(\"isAllelePalindromic\")),\n                        GWASCatalogCuratedAssociationsParser._harmonise_beta(\n                            f.col(\"OR or BETA\"),\n                            f.col(\"95% CI (TEXT)\"),\n                            f.col(\"needsFlipping\"),\n                        ),\n                    ),\n                    # Normalise odds ratio of the association:\n                    \"effect_odds_ratio\": f.when(\n                        (f.col(\"effectType\") == \"odds_ratio\")\n                        &amp; (~f.col(\"isAllelePalindromic\")),\n                        GWASCatalogCuratedAssociationsParser._harmonise_odds_ratio(\n                            f.col(\"OR or BETA\"),\n                            f.col(\"needsFlipping\"),\n                        ),\n                    ),\n                },\n            )\n            .select(\n                *df.columns,\n                # Harmonise OR effect to beta:\n                *convert_odds_ratio_to_beta(\n                    f.col(\"effect_beta\"),\n                    f.col(\"effect_odds_ratio\"),\n                    f.col(\"standardError\"),\n                ),\n            )\n        )\n\n    @classmethod\n    def from_source(\n        cls: type[GWASCatalogCuratedAssociationsParser],\n        gwas_associations: DataFrame,\n        gnomad_variants: VariantIndex,\n        pvalue_threshold: float = WindowBasedClumpingStepConfig.gwas_significance,\n    ) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Read GWASCatalog associations.\n\n        It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and\n        applies some pre-defined filters on the data:\n\n        Args:\n            gwas_associations (DataFrame): GWAS Catalog raw associations dataset.\n            gnomad_variants (VariantIndex): Variant dataset from GnomAD, with allele frequencies.\n            pvalue_threshold (float): P-value threshold for flagging associations.\n\n        Returns:\n            StudyLocusGWASCatalog: GWASCatalogAssociations dataset\n\n        pvalue_threshold is keeped in sync with the WindowBasedClumpingStep gwas_significance.\n        \"\"\"\n        return StudyLocusGWASCatalog(\n            _df=gwas_associations.withColumn(\n                # temporary column\n                \"rowId\",\n                f.monotonically_increasing_id().cast(StringType()),\n            )\n            .transform(\n                # Map/harmonise variants to variant annotation dataset:\n                # This function adds columns: variantId, referenceAllele, alternateAllele, chromosome, position\n                lambda df: GWASCatalogCuratedAssociationsParser._map_variants_to_gnomad_variants(\n                    df, gnomad_variants\n                )\n            )\n            .withColumns(\n                # Perform all quality control checks:\n                {\n                    \"qualityControls\": GWASCatalogCuratedAssociationsParser._qc_all(\n                        f.array().alias(\"qualityControls\"),\n                        f.col(\"CHR_ID\"),\n                        f.col(\"CHR_POS\").cast(IntegerType()),\n                        f.col(\"referenceAllele\"),\n                        f.col(\"alternateAllele\"),\n                        f.col(\"STRONGEST SNP-RISK ALLELE\"),\n                        *GWASCatalogCuratedAssociationsParser._parse_pvalue(\n                            f.col(\"P-VALUE\")\n                        ),\n                        pvalue_threshold,\n                    )\n                }\n            )\n            # Harmonising effect to beta value and flip effect if needed:\n            .transform(cls.harmonise_association_effect_to_beta)\n            .withColumnRenamed(\"STUDY ACCESSION\", \"studyId\")\n            # Adding study-locus id:\n            .withColumn(\n                \"studyLocusId\",\n                StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n            )\n            .select(\n                # INSIDE STUDY-LOCUS SCHEMA:\n                \"studyLocusId\",\n                \"variantId\",\n                # Mapped genomic location of the variant (; separated list)\n                \"chromosome\",\n                \"position\",\n                \"studyId\",\n                # p-value of the association, string: split into exponent and mantissa.\n                *GWASCatalogCuratedAssociationsParser._parse_pvalue(f.col(\"P-VALUE\")),\n                # Capturing phenotype granularity at the association level\n                GWASCatalogCuratedAssociationsParser._concatenate_substudy_description(\n                    f.col(\"DISEASE/TRAIT\"),\n                    f.col(\"P-VALUE (TEXT)\"),\n                    f.col(\"MAPPED_TRAIT_URI\"),\n                ).alias(\"subStudyDescription\"),\n                # Quality controls (array of strings)\n                \"qualityControls\",\n                \"beta\",\n                \"standardError\",\n            ),\n            _schema=StudyLocusGWASCatalog.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl","title":"<code>convert_gnomad_position_to_ensembl(position: Column, reference: Column, alternate: Column) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Convert GnomAD variant position to Ensembl variant position.</p> <p>For indels (the reference or alternate allele is longer than 1), then adding 1 to the position, for SNPs, the position is unchanged. More info about the problem: https://www.biostars.org/p/84686/</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>Column</code> <p>Position of the variant in GnomAD's coordinates system.</p> required <code>reference</code> <code>Column</code> <p>The reference allele in GnomAD's coordinates system.</p> required <code>alternate</code> <code>Column</code> <p>The alternate allele in GnomAD's coordinates system.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>The position of the variant in the Ensembl genome.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = [(1, \"A\", \"C\"), (2, \"AA\", \"C\"), (3, \"A\", \"AA\")]\n&gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"position\", \"reference\", \"alternate\")\n&gt;&gt;&gt; df.withColumn(\"new_position\", GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl(f.col(\"position\"), f.col(\"reference\"), f.col(\"alternate\"))).show()\n+--------+---------+---------+------------+\n|position|reference|alternate|new_position|\n+--------+---------+---------+------------+\n|       1|        A|        C|           1|\n|       2|       AA|        C|           3|\n|       3|        A|       AA|           4|\n+--------+---------+---------+------------+\n</code></pre> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@staticmethod\ndef convert_gnomad_position_to_ensembl(\n    position: Column, reference: Column, alternate: Column\n) -&gt; Column:\n    \"\"\"Convert GnomAD variant position to Ensembl variant position.\n\n    For indels (the reference or alternate allele is longer than 1), then adding 1 to the position, for SNPs,\n    the position is unchanged. More info about the problem: https://www.biostars.org/p/84686/\n\n    Args:\n        position (Column): Position of the variant in GnomAD's coordinates system.\n        reference (Column): The reference allele in GnomAD's coordinates system.\n        alternate (Column): The alternate allele in GnomAD's coordinates system.\n\n    Returns:\n        Column: The position of the variant in the Ensembl genome.\n\n    Examples:\n        &gt;&gt;&gt; d = [(1, \"A\", \"C\"), (2, \"AA\", \"C\"), (3, \"A\", \"AA\")]\n        &gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"position\", \"reference\", \"alternate\")\n        &gt;&gt;&gt; df.withColumn(\"new_position\", GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl(f.col(\"position\"), f.col(\"reference\"), f.col(\"alternate\"))).show()\n        +--------+---------+---------+------------+\n        |position|reference|alternate|new_position|\n        +--------+---------+---------+------------+\n        |       1|        A|        C|           1|\n        |       2|       AA|        C|           3|\n        |       3|        A|       AA|           4|\n        +--------+---------+---------+------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.when(\n        (f.length(reference) &gt; 1) | (f.length(alternate) &gt; 1), position + 1\n    ).otherwise(position)\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser.from_source","title":"<code>from_source(gwas_associations: DataFrame, gnomad_variants: VariantIndex, pvalue_threshold: float = WindowBasedClumpingStepConfig.gwas_significance) -&gt; StudyLocusGWASCatalog</code>  <code>classmethod</code>","text":"<p>Read GWASCatalog associations.</p> <p>It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data:</p> <p>Parameters:</p> Name Type Description Default <code>gwas_associations</code> <code>DataFrame</code> <p>GWAS Catalog raw associations dataset.</p> required <code>gnomad_variants</code> <code>VariantIndex</code> <p>Variant dataset from GnomAD, with allele frequencies.</p> required <code>pvalue_threshold</code> <code>float</code> <p>P-value threshold for flagging associations.</p> <code>gwas_significance</code> <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>GWASCatalogAssociations dataset</p> <p>pvalue_threshold is keeped in sync with the WindowBasedClumpingStep gwas_significance.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[GWASCatalogCuratedAssociationsParser],\n    gwas_associations: DataFrame,\n    gnomad_variants: VariantIndex,\n    pvalue_threshold: float = WindowBasedClumpingStepConfig.gwas_significance,\n) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Read GWASCatalog associations.\n\n    It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and\n    applies some pre-defined filters on the data:\n\n    Args:\n        gwas_associations (DataFrame): GWAS Catalog raw associations dataset.\n        gnomad_variants (VariantIndex): Variant dataset from GnomAD, with allele frequencies.\n        pvalue_threshold (float): P-value threshold for flagging associations.\n\n    Returns:\n        StudyLocusGWASCatalog: GWASCatalogAssociations dataset\n\n    pvalue_threshold is keeped in sync with the WindowBasedClumpingStep gwas_significance.\n    \"\"\"\n    return StudyLocusGWASCatalog(\n        _df=gwas_associations.withColumn(\n            # temporary column\n            \"rowId\",\n            f.monotonically_increasing_id().cast(StringType()),\n        )\n        .transform(\n            # Map/harmonise variants to variant annotation dataset:\n            # This function adds columns: variantId, referenceAllele, alternateAllele, chromosome, position\n            lambda df: GWASCatalogCuratedAssociationsParser._map_variants_to_gnomad_variants(\n                df, gnomad_variants\n            )\n        )\n        .withColumns(\n            # Perform all quality control checks:\n            {\n                \"qualityControls\": GWASCatalogCuratedAssociationsParser._qc_all(\n                    f.array().alias(\"qualityControls\"),\n                    f.col(\"CHR_ID\"),\n                    f.col(\"CHR_POS\").cast(IntegerType()),\n                    f.col(\"referenceAllele\"),\n                    f.col(\"alternateAllele\"),\n                    f.col(\"STRONGEST SNP-RISK ALLELE\"),\n                    *GWASCatalogCuratedAssociationsParser._parse_pvalue(\n                        f.col(\"P-VALUE\")\n                    ),\n                    pvalue_threshold,\n                )\n            }\n        )\n        # Harmonising effect to beta value and flip effect if needed:\n        .transform(cls.harmonise_association_effect_to_beta)\n        .withColumnRenamed(\"STUDY ACCESSION\", \"studyId\")\n        # Adding study-locus id:\n        .withColumn(\n            \"studyLocusId\",\n            StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n        )\n        .select(\n            # INSIDE STUDY-LOCUS SCHEMA:\n            \"studyLocusId\",\n            \"variantId\",\n            # Mapped genomic location of the variant (; separated list)\n            \"chromosome\",\n            \"position\",\n            \"studyId\",\n            # p-value of the association, string: split into exponent and mantissa.\n            *GWASCatalogCuratedAssociationsParser._parse_pvalue(f.col(\"P-VALUE\")),\n            # Capturing phenotype granularity at the association level\n            GWASCatalogCuratedAssociationsParser._concatenate_substudy_description(\n                f.col(\"DISEASE/TRAIT\"),\n                f.col(\"P-VALUE (TEXT)\"),\n                f.col(\"MAPPED_TRAIT_URI\"),\n            ).alias(\"subStudyDescription\"),\n            # Quality controls (array of strings)\n            \"qualityControls\",\n            \"beta\",\n            \"standardError\",\n        ),\n        _schema=StudyLocusGWASCatalog.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser.harmonise_association_effect_to_beta","title":"<code>harmonise_association_effect_to_beta(df: DataFrame) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>Harmonise effect to beta value.</p> <p>The harmonisation process has a number of steps: - Extracting the reported effect allele. - Flagging palindromic alleles. - Flagging associations where the effect direction needs to be flipped. - Flagging the effect type. - Getting the standard error from the confidence interval text. - Harmonising both beta and odds ratio. - Converting the odds ratio to beta.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with the following columns:</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with the following columns:</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the required columns are missing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [\n...    # Flagged as palindromic:\n...    ('rs123-T', 'A', 'T', '0.1', '[0.08-0.12] unit increase'),\n...    # Not palindromic, beta needs to be flipped:\n...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12] unit increase'),\n...    # Beta is not flipped:\n...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12] unit increase'),\n...    # odds ratio:\n...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12]'),\n...    # odds ratio flipped:\n...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12]'),\n... ]\n&gt;&gt;&gt; schema = [\"STRONGEST SNP-RISK ALLELE\", \"referenceAllele\", \"alternateAllele\", \"OR or BETA\", \"95% CI (TEXT)\"]\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; GWASCatalogCuratedAssociationsParser.harmonise_association_effect_to_beta(df).show()\n+-------------------------+---------------+---------------+----------+--------------------+-------------------+--------------------+\n|STRONGEST SNP-RISK ALLELE|referenceAllele|alternateAllele|OR or BETA|       95% CI (TEXT)|               beta|       standardError|\n+-------------------------+---------------+---------------+----------+--------------------+-------------------+--------------------+\n|                  rs123-T|              A|              T|       0.1|[0.08-0.12] unit ...|               NULL|                NULL|\n|                  rs123-C|              G|              T|       0.1|[0.08-0.12] unit ...|               -0.1|0.010204081404574064|\n|                  rs123-T|              C|              T|       0.1|[0.08-0.12] unit ...|                0.1|0.010204081404574064|\n|                  rs123-T|              C|              T|       0.1|         [0.08-0.12]|-2.3025850929940455|                NULL|\n|                  rs123-C|              G|              T|       0.1|         [0.08-0.12]|  2.302585092994046|                NULL|\n+-------------------------+---------------+---------------+----------+--------------------+-------------------+--------------------+\n</code></pre> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@staticmethod\ndef harmonise_association_effect_to_beta(\n    df: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"Harmonise effect to beta value.\n\n    The harmonisation process has a number of steps:\n    - Extracting the reported effect allele.\n    - Flagging palindromic alleles.\n    - Flagging associations where the effect direction needs to be flipped.\n    - Flagging the effect type.\n    - Getting the standard error from the confidence interval text.\n    - Harmonising both beta and odds ratio.\n    - Converting the odds ratio to beta.\n\n    Args:\n        df (DataFrame): DataFrame with the following columns:\n\n    Returns:\n        DataFrame: DataFrame with the following columns:\n\n    Raises:\n        ValueError: If any of the required columns are missing.\n\n    Examples:\n        &gt;&gt;&gt; data = [\n        ...    # Flagged as palindromic:\n        ...    ('rs123-T', 'A', 'T', '0.1', '[0.08-0.12] unit increase'),\n        ...    # Not palindromic, beta needs to be flipped:\n        ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12] unit increase'),\n        ...    # Beta is not flipped:\n        ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12] unit increase'),\n        ...    # odds ratio:\n        ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12]'),\n        ...    # odds ratio flipped:\n        ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12]'),\n        ... ]\n        &gt;&gt;&gt; schema = [\"STRONGEST SNP-RISK ALLELE\", \"referenceAllele\", \"alternateAllele\", \"OR or BETA\", \"95% CI (TEXT)\"]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; GWASCatalogCuratedAssociationsParser.harmonise_association_effect_to_beta(df).show()\n        +-------------------------+---------------+---------------+----------+--------------------+-------------------+--------------------+\n        |STRONGEST SNP-RISK ALLELE|referenceAllele|alternateAllele|OR or BETA|       95% CI (TEXT)|               beta|       standardError|\n        +-------------------------+---------------+---------------+----------+--------------------+-------------------+--------------------+\n        |                  rs123-T|              A|              T|       0.1|[0.08-0.12] unit ...|               NULL|                NULL|\n        |                  rs123-C|              G|              T|       0.1|[0.08-0.12] unit ...|               -0.1|0.010204081404574064|\n        |                  rs123-T|              C|              T|       0.1|[0.08-0.12] unit ...|                0.1|0.010204081404574064|\n        |                  rs123-T|              C|              T|       0.1|         [0.08-0.12]|-2.3025850929940455|                NULL|\n        |                  rs123-C|              G|              T|       0.1|         [0.08-0.12]|  2.302585092994046|                NULL|\n        +-------------------------+---------------+---------------+----------+--------------------+-------------------+--------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    # Testing if all columns are in the dataframe:\n    required_columns = [\n        \"STRONGEST SNP-RISK ALLELE\",\n        \"referenceAllele\",\n        \"alternateAllele\",\n        \"OR or BETA\",\n        \"95% CI (TEXT)\",\n    ]\n\n    for column in required_columns:\n        if column not in df.columns:\n            raise ValueError(\n                f\"Column {column} is required for harmonising effect to beta value.\"\n            )\n\n    return (\n        df.withColumn(\n            \"reportedRiskAllele\",\n            GWASCatalogCuratedAssociationsParser._extract_risk_allele(\n                f.col(\"STRONGEST SNP-RISK ALLELE\")\n            ),\n        )\n        .withColumns(\n            {\n                # Flag palindromic alleles:\n                \"isAllelePalindromic\": GWASCatalogCuratedAssociationsParser._are_alleles_palindromic(\n                    f.col(\"referenceAllele\"), f.col(\"alternateAllele\")\n                ),\n                # Flag associations, where the effect direction needs to be flipped:\n                \"needsFlipping\": GWASCatalogCuratedAssociationsParser._effect_needs_harmonisation(\n                    f.col(\"reportedRiskAllele\"), f.col(\"referenceAllele\")\n                ),\n                # Flag effect type:\n                \"effectType\": GWASCatalogCuratedAssociationsParser._get_effect_type(\n                    f.col(\"95% CI (TEXT)\")\n                ),\n                # Get standard error from confidence interval text:\n                \"standardError\": get_standard_error_from_confidence_interval(\n                    f.regexp_extract(\n                        \"95% CI (TEXT)\", r\"\\[(\\d+\\.*\\d*)-\\d+\\.*\\d*\\]\", 1\n                    ).cast(FloatType()),\n                    f.regexp_extract(\n                        \"95% CI (TEXT)\", r\"\\[\\d+\\.*\\d*-(\\d+\\.*\\d*)\\]\", 1\n                    ).cast(FloatType()),\n                ),\n            }\n        )\n        # Harmonise both beta and odds ratio:\n        .withColumns(\n            {  # Normalise beta value of the association:\n                \"effect_beta\": f.when(\n                    (f.col(\"effectType\") == \"beta\")\n                    &amp; (~f.col(\"isAllelePalindromic\")),\n                    GWASCatalogCuratedAssociationsParser._harmonise_beta(\n                        f.col(\"OR or BETA\"),\n                        f.col(\"95% CI (TEXT)\"),\n                        f.col(\"needsFlipping\"),\n                    ),\n                ),\n                # Normalise odds ratio of the association:\n                \"effect_odds_ratio\": f.when(\n                    (f.col(\"effectType\") == \"odds_ratio\")\n                    &amp; (~f.col(\"isAllelePalindromic\")),\n                    GWASCatalogCuratedAssociationsParser._harmonise_odds_ratio(\n                        f.col(\"OR or BETA\"),\n                        f.col(\"needsFlipping\"),\n                    ),\n                ),\n            },\n        )\n        .select(\n            *df.columns,\n            # Harmonise OR effect to beta:\n            *convert_odds_ratio_to_beta(\n                f.col(\"effect_beta\"),\n                f.col(\"effect_odds_ratio\"),\n                f.col(\"standardError\"),\n            ),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog","title":"<code>gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog</code>  <code>dataclass</code>","text":"<p>               Bases: <code>StudyLocus</code></p> <p>Study locus Dataset for GWAS Catalog curated associations.</p> <p>A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@dataclass\nclass StudyLocusGWASCatalog(StudyLocus):\n    \"\"\"Study locus Dataset for GWAS Catalog curated associations.\n\n    A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.\n    \"\"\"\n\n    def update_study_id(\n        self: StudyLocusGWASCatalog, study_annotation: DataFrame\n    ) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Update final studyId and studyLocusId with a dataframe containing study annotation.\n\n        Args:\n            study_annotation (DataFrame): Dataframe containing `updatedStudyId` and key columns `studyId` and `subStudyDescription`.\n\n        Returns:\n            StudyLocusGWASCatalog: Updated study locus with new `studyId` and `studyLocusId`.\n        \"\"\"\n        self.df = (\n            self._df.join(\n                study_annotation, on=[\"studyId\", \"subStudyDescription\"], how=\"left\"\n            )\n            .withColumn(\"studyId\", f.coalesce(\"updatedStudyId\", \"studyId\"))\n            .drop(\"subStudyDescription\", \"updatedStudyId\")\n        ).withColumn(\n            \"studyLocusId\",\n            StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n        )\n        return self\n\n    def qc_ambiguous_study(self: StudyLocusGWASCatalog) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Flag associations with variants that can not be unambiguously associated with one study.\n\n        Returns:\n            StudyLocusGWASCatalog: Updated study locus.\n        \"\"\"\n        assoc_ambiguity_window = Window.partitionBy(\n            f.col(\"studyId\"), f.col(\"variantId\")\n        )\n\n        self._df.withColumn(\n            \"qualityControls\",\n            StudyLocus.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.count(f.col(\"variantId\")).over(assoc_ambiguity_window) &gt; 1,\n                StudyLocusQualityCheck.AMBIGUOUS_STUDY,\n            ),\n        )\n        return self\n\n    def qc_flag_all_tophits(self: StudyLocusGWASCatalog) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Flag all associations as top hits.\n\n        Returns:\n            StudyLocusGWASCatalog: Updated study locus.\n        \"\"\"\n        return StudyLocusGWASCatalog(\n            _df=self._df.withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.lit(True),\n                    StudyLocusQualityCheck.TOP_HIT,\n                ),\n            ),\n            _schema=StudyLocusGWASCatalog.get_schema(),\n        )\n\n    def apply_inclusion_list(\n        self: StudyLocusGWASCatalog, inclusion_list: DataFrame\n    ) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Restricting GWAS Catalog studies based on a list of accpected study ids.\n\n        Args:\n            inclusion_list (DataFrame): List of accepted GWAS Catalog study identifiers\n\n        Returns:\n            StudyLocusGWASCatalog: Filtered dataset.\n        \"\"\"\n        return StudyLocusGWASCatalog(\n            _df=self.df.join(inclusion_list, on=\"studyId\", how=\"inner\"),\n            _schema=StudyLocusGWASCatalog.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog.apply_inclusion_list","title":"<code>apply_inclusion_list(inclusion_list: DataFrame) -&gt; StudyLocusGWASCatalog</code>","text":"<p>Restricting GWAS Catalog studies based on a list of accpected study ids.</p> <p>Parameters:</p> Name Type Description Default <code>inclusion_list</code> <code>DataFrame</code> <p>List of accepted GWAS Catalog study identifiers</p> required <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>Filtered dataset.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>def apply_inclusion_list(\n    self: StudyLocusGWASCatalog, inclusion_list: DataFrame\n) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Restricting GWAS Catalog studies based on a list of accpected study ids.\n\n    Args:\n        inclusion_list (DataFrame): List of accepted GWAS Catalog study identifiers\n\n    Returns:\n        StudyLocusGWASCatalog: Filtered dataset.\n    \"\"\"\n    return StudyLocusGWASCatalog(\n        _df=self.df.join(inclusion_list, on=\"studyId\", how=\"inner\"),\n        _schema=StudyLocusGWASCatalog.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog.qc_ambiguous_study","title":"<code>qc_ambiguous_study() -&gt; StudyLocusGWASCatalog</code>","text":"<p>Flag associations with variants that can not be unambiguously associated with one study.</p> <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>Updated study locus.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>def qc_ambiguous_study(self: StudyLocusGWASCatalog) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Flag associations with variants that can not be unambiguously associated with one study.\n\n    Returns:\n        StudyLocusGWASCatalog: Updated study locus.\n    \"\"\"\n    assoc_ambiguity_window = Window.partitionBy(\n        f.col(\"studyId\"), f.col(\"variantId\")\n    )\n\n    self._df.withColumn(\n        \"qualityControls\",\n        StudyLocus.update_quality_flag(\n            f.col(\"qualityControls\"),\n            f.count(f.col(\"variantId\")).over(assoc_ambiguity_window) &gt; 1,\n            StudyLocusQualityCheck.AMBIGUOUS_STUDY,\n        ),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog.qc_flag_all_tophits","title":"<code>qc_flag_all_tophits() -&gt; StudyLocusGWASCatalog</code>","text":"<p>Flag all associations as top hits.</p> <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>Updated study locus.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>def qc_flag_all_tophits(self: StudyLocusGWASCatalog) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Flag all associations as top hits.\n\n    Returns:\n        StudyLocusGWASCatalog: Updated study locus.\n    \"\"\"\n    return StudyLocusGWASCatalog(\n        _df=self._df.withColumn(\n            \"qualityControls\",\n            StudyLocus.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.lit(True),\n                StudyLocusQualityCheck.TOP_HIT,\n            ),\n        ),\n        _schema=StudyLocusGWASCatalog.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog.update_study_id","title":"<code>update_study_id(study_annotation: DataFrame) -&gt; StudyLocusGWASCatalog</code>","text":"<p>Update final studyId and studyLocusId with a dataframe containing study annotation.</p> <p>Parameters:</p> Name Type Description Default <code>study_annotation</code> <code>DataFrame</code> <p>Dataframe containing <code>updatedStudyId</code> and key columns <code>studyId</code> and <code>subStudyDescription</code>.</p> required <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>Updated study locus with new <code>studyId</code> and <code>studyLocusId</code>.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>def update_study_id(\n    self: StudyLocusGWASCatalog, study_annotation: DataFrame\n) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Update final studyId and studyLocusId with a dataframe containing study annotation.\n\n    Args:\n        study_annotation (DataFrame): Dataframe containing `updatedStudyId` and key columns `studyId` and `subStudyDescription`.\n\n    Returns:\n        StudyLocusGWASCatalog: Updated study locus with new `studyId` and `studyLocusId`.\n    \"\"\"\n    self.df = (\n        self._df.join(\n            study_annotation, on=[\"studyId\", \"subStudyDescription\"], how=\"left\"\n        )\n        .withColumn(\"studyId\", f.coalesce(\"updatedStudyId\", \"studyId\"))\n        .drop(\"subStudyDescription\", \"updatedStudyId\")\n    ).withColumn(\n        \"studyLocusId\",\n        StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/","title":"Study Index","text":""},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalogParser","title":"<code>gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalogParser</code>  <code>dataclass</code>","text":"<p>GWAS Catalog study index parser.</p> <p>The following information is harmonised from the GWAS Catalog:</p> <ul> <li>All publication related information retained.</li> <li>Mapped measured and background traits parsed.</li> <li>Flagged if harmonized summary statistics datasets available.</li> <li>If available, the ftp path to these files presented.</li> <li>Ancestries from the discovery and replication stages are structured with sample counts.</li> <li>Case/control counts extracted.</li> <li>The number of samples with European ancestry extracted.</li> </ul> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>@dataclass\nclass StudyIndexGWASCatalogParser:\n    \"\"\"GWAS Catalog study index parser.\n\n    The following information is harmonised from the GWAS Catalog:\n\n    - All publication related information retained.\n    - Mapped measured and background traits parsed.\n    - Flagged if harmonized summary statistics datasets available.\n    - If available, the ftp path to these files presented.\n    - Ancestries from the discovery and replication stages are structured with sample counts.\n    - Case/control counts extracted.\n    - The number of samples with European ancestry extracted.\n\n    \"\"\"\n\n    @staticmethod\n    def _parse_discovery_samples(discovery_samples: Column) -&gt; Column:\n        \"\"\"Parse discovery sample sizes from GWAS Catalog.\n\n        This is a curated field. From publication sometimes it is not clear how the samples were split\n        across the reported ancestries. In such cases we are assuming the ancestries were evenly presented\n        and the total sample size is split:\n\n        [\"European, African\", 100] -&gt; [\"European, 50], [\"African\", 50]\n\n        Args:\n            discovery_samples (Column): Raw discovery sample sizes\n\n        Returns:\n            Column: Parsed and de-duplicated list of discovery ancestries with sample size.\n\n        Examples:\n            &gt;&gt;&gt; data = [('s1', \"European\", 10), ('s1', \"African\", 10), ('s2', \"European, African, Asian\", 100), ('s2', \"European\", 50)]\n            &gt;&gt;&gt; df = (\n            ...    spark.createDataFrame(data, ['studyId', 'ancestry', 'sampleSize'])\n            ...    .groupBy('studyId')\n            ...    .agg(\n            ...        f.collect_set(\n            ...            f.struct('ancestry', 'sampleSize')\n            ...        ).alias('discoverySampleSize')\n            ...    )\n            ...    .orderBy('studyId')\n            ...    .withColumn('discoverySampleSize', StudyIndexGWASCatalogParser._parse_discovery_samples(f.col('discoverySampleSize')))\n            ...    .select('discoverySampleSize')\n            ...    .show(truncate=False)\n            ... )\n            +--------------------------------------------+\n            |discoverySampleSize                         |\n            +--------------------------------------------+\n            |[{African, 10}, {European, 10}]             |\n            |[{European, 83}, {African, 33}, {Asian, 33}]|\n            +--------------------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # To initialize return objects for aggregate functions, schema has to be defined:\n        schema = t.ArrayType(\n            t.StructType(\n                [\n                    t.StructField(\"ancestry\", t.StringType(), True),\n                    t.StructField(\"sampleSize\", t.IntegerType(), True),\n                ]\n            )\n        )\n\n        # Splitting comma separated ancestries:\n        exploded_ancestries = f.transform(\n            discovery_samples,\n            lambda sample: f.split(sample.ancestry, r\",\\s(?![^()]*\\))\"),\n        )\n\n        # Initialize discoverySample object from unique list of ancestries:\n        unique_ancestries = f.transform(\n            f.aggregate(\n                exploded_ancestries,\n                f.array().cast(t.ArrayType(t.StringType())),\n                lambda x, y: f.array_union(x, y),\n                f.array_distinct,\n            ),\n            lambda ancestry: f.struct(\n                ancestry.alias(\"ancestry\"),\n                f.lit(0).alias(\"sampleSize\"),\n            ),\n        )\n\n        # Computing sample sizes for ancestries when splitting is needed:\n        resolved_sample_count = f.transform(\n            f.arrays_zip(\n                f.transform(exploded_ancestries, lambda pop: f.size(pop)).alias(\n                    \"pop_size\"\n                ),\n                f.transform(discovery_samples, lambda pop: pop.sampleSize).alias(\n                    \"pop_count\"\n                ),\n            ),\n            lambda pop: (pop.pop_count / pop.pop_size).cast(t.IntegerType()),\n        )\n\n        # Flattening out ancestries with sample sizes:\n        parsed_sample_size = f.aggregate(\n            f.transform(\n                f.arrays_zip(\n                    exploded_ancestries.alias(\"ancestries\"),\n                    resolved_sample_count.alias(\"sample_count\"),\n                ),\n                StudyIndexGWASCatalogParser._merge_ancestries_and_counts,\n            ),\n            f.array().cast(schema),\n            lambda x, y: f.array_union(x, y),\n        )\n\n        # Normalize ancestries:\n        return f.aggregate(\n            parsed_sample_size,\n            unique_ancestries,\n            StudyIndexGWASCatalogParser._normalize_ancestries,\n        )\n\n    @staticmethod\n    def _normalize_ancestries(merged: Column, ancestry: Column) -&gt; Column:\n        \"\"\"Normalize ancestries from a list of structs.\n\n        As some ancestry label might be repeated with different sample counts,\n        these counts need to be collected.\n\n        Args:\n            merged (Column): Resulting list of struct with unique ancestries.\n            ancestry (Column): One ancestry object coming from raw.\n\n        Returns:\n            Column: Unique list of ancestries with the sample counts.\n        \"\"\"\n        # Iterating over the list of unique ancestries and adding the sample size if label matches:\n        return f.transform(\n            merged,\n            lambda a: f.when(\n                a.ancestry == ancestry.ancestry,\n                f.struct(\n                    a.ancestry.alias(\"ancestry\"),\n                    (a.sampleSize + ancestry.sampleSize)\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                ),\n            ).otherwise(a),\n        )\n\n    @staticmethod\n    def _merge_ancestries_and_counts(ancestry_group: Column) -&gt; Column:\n        \"\"\"Merge ancestries with sample sizes.\n\n        After splitting ancestry annotations, all resulting ancestries needs to be assigned\n        with the proper sample size.\n\n        Args:\n            ancestry_group (Column): Each element is a struct with `sample_count` (int) and `ancestries` (list)\n\n        Returns:\n            Column: a list of structs with `ancestry` and `sampleSize` fields.\n\n        Examples:\n            &gt;&gt;&gt; data = [(12, ['African', 'European']),(12, ['African'])]\n            &gt;&gt;&gt; (\n            ...     spark.createDataFrame(data, ['sample_count', 'ancestries'])\n            ...     .select(StudyIndexGWASCatalogParser._merge_ancestries_and_counts(f.struct('sample_count', 'ancestries')).alias('test'))\n            ...     .show(truncate=False)\n            ... )\n            +-------------------------------+\n            |test                           |\n            +-------------------------------+\n            |[{African, 12}, {European, 12}]|\n            |[{African, 12}]                |\n            +-------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # Extract sample size for the ancestry group:\n        count = ancestry_group.sample_count\n\n        # We need to loop through the ancestries:\n        return f.transform(\n            ancestry_group.ancestries,\n            lambda ancestry: f.struct(\n                ancestry.alias(\"ancestry\"),\n                count.alias(\"sampleSize\"),\n            ),\n        )\n\n    @staticmethod\n    def parse_cohorts(raw_cohort: Column) -&gt; Column:\n        \"\"\"Return a list of unique cohort labels from pipe separated list if provided.\n\n        Args:\n            raw_cohort (Column): Cohort list column, where labels are separated by `|` sign.\n\n        Returns:\n            Column: an array colun with string elements.\n\n        Examples:\n        &gt;&gt;&gt; data = [('BioME|CaPS|Estonia|FHS|UKB|GERA|GERA|GERA',),(None,),]\n        &gt;&gt;&gt; spark.createDataFrame(data, ['cohorts']).select(StudyIndexGWASCatalogParser.parse_cohorts(f.col('cohorts')).alias('parsedCohorts')).show(truncate=False)\n        +--------------------------------------+\n        |parsedCohorts                         |\n        +--------------------------------------+\n        |[BioME, CaPS, Estonia, FHS, UKB, GERA]|\n        |NULL                                  |\n        +--------------------------------------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.when(\n            (raw_cohort.isNotNull()) &amp; (raw_cohort != \"\"),\n            f.array_distinct(f.split(raw_cohort, r\"\\|\")),\n        )\n\n    @classmethod\n    def _parse_study_table(\n        cls: type[StudyIndexGWASCatalogParser], catalog_studies: DataFrame\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Harmonise GWASCatalog study table with `StudyIndex` schema.\n\n        Args:\n            catalog_studies (DataFrame): GWAS Catalog study table\n\n        Returns:\n            StudyIndexGWASCatalog: Parsed and annotated GWAS Catalog study table.\n        \"\"\"\n        return StudyIndexGWASCatalog(\n            _df=catalog_studies.select(\n                f.coalesce(\n                    f.col(\"STUDY ACCESSION\"), f.monotonically_increasing_id()\n                ).alias(\"studyId\"),\n                f.lit(\"GCST\").alias(\"projectId\"),\n                f.lit(\"gwas\").alias(\"studyType\"),\n                f.col(\"PUBMED ID\").alias(\"pubmedId\"),\n                f.col(\"FIRST AUTHOR\").alias(\"publicationFirstAuthor\"),\n                f.col(\"DATE\").alias(\"publicationDate\"),\n                f.col(\"JOURNAL\").alias(\"publicationJournal\"),\n                f.col(\"STUDY\").alias(\"publicationTitle\"),\n                f.coalesce(f.col(\"DISEASE/TRAIT\"), f.lit(\"Unreported\")).alias(\n                    \"traitFromSource\"\n                ),\n                f.col(\"INITIAL SAMPLE SIZE\").alias(\"initialSampleSize\"),\n                parse_efos(f.col(\"MAPPED_TRAIT_URI\")).alias(\"traitFromSourceMappedIds\"),\n                parse_efos(f.col(\"MAPPED BACKGROUND TRAIT URI\")).alias(\n                    \"backgroundTraitFromSourceMappedIds\"\n                ),\n                cls.parse_cohorts(f.col(\"COHORT\")).alias(\"cohorts\"),\n            ),\n            _schema=StudyIndexGWASCatalog.get_schema(),\n        )\n\n    @classmethod\n    def from_source(\n        cls: type[StudyIndexGWASCatalogParser],\n        catalog_studies: DataFrame,\n        ancestry_file: DataFrame,\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Ingests study level metadata from the GWAS Catalog.\n\n        Args:\n            catalog_studies (DataFrame): GWAS Catalog raw study table\n            ancestry_file (DataFrame): GWAS Catalog ancestry table.\n\n        Returns:\n            StudyIndexGWASCatalog: Parsed and annotated GWAS Catalog study table.\n        \"\"\"\n        # Read GWAS Catalogue raw data\n        return (\n            cls._parse_study_table(catalog_studies)\n            .annotate_ancestries(ancestry_file)\n            .annotate_discovery_sample_sizes()\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalogParser.from_source","title":"<code>from_source(catalog_studies: DataFrame, ancestry_file: DataFrame) -&gt; StudyIndexGWASCatalog</code>  <code>classmethod</code>","text":"<p>Ingests study level metadata from the GWAS Catalog.</p> <p>Parameters:</p> Name Type Description Default <code>catalog_studies</code> <code>DataFrame</code> <p>GWAS Catalog raw study table</p> required <code>ancestry_file</code> <code>DataFrame</code> <p>GWAS Catalog ancestry table.</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Parsed and annotated GWAS Catalog study table.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[StudyIndexGWASCatalogParser],\n    catalog_studies: DataFrame,\n    ancestry_file: DataFrame,\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Ingests study level metadata from the GWAS Catalog.\n\n    Args:\n        catalog_studies (DataFrame): GWAS Catalog raw study table\n        ancestry_file (DataFrame): GWAS Catalog ancestry table.\n\n    Returns:\n        StudyIndexGWASCatalog: Parsed and annotated GWAS Catalog study table.\n    \"\"\"\n    # Read GWAS Catalogue raw data\n    return (\n        cls._parse_study_table(catalog_studies)\n        .annotate_ancestries(ancestry_file)\n        .annotate_discovery_sample_sizes()\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalogParser.parse_cohorts","title":"<code>parse_cohorts(raw_cohort: Column) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Return a list of unique cohort labels from pipe separated list if provided.</p> <p>Parameters:</p> Name Type Description Default <code>raw_cohort</code> <code>Column</code> <p>Cohort list column, where labels are separated by <code>|</code> sign.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>an array colun with string elements.</p> <p>Examples:</p> <p>data = [('BioME|CaPS|Estonia|FHS|UKB|GERA|GERA|GERA',),(None,),] spark.createDataFrame(data, ['cohorts']).select(StudyIndexGWASCatalogParser.parse_cohorts(f.col('cohorts')).alias('parsedCohorts')).show(truncate=False) +--------------------------------------+ |parsedCohorts                         | +--------------------------------------+ |[BioME, CaPS, Estonia, FHS, UKB, GERA]| |NULL                                  | +--------------------------------------+  Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>@staticmethod\ndef parse_cohorts(raw_cohort: Column) -&gt; Column:\n    \"\"\"Return a list of unique cohort labels from pipe separated list if provided.\n\n    Args:\n        raw_cohort (Column): Cohort list column, where labels are separated by `|` sign.\n\n    Returns:\n        Column: an array colun with string elements.\n\n    Examples:\n    &gt;&gt;&gt; data = [('BioME|CaPS|Estonia|FHS|UKB|GERA|GERA|GERA',),(None,),]\n    &gt;&gt;&gt; spark.createDataFrame(data, ['cohorts']).select(StudyIndexGWASCatalogParser.parse_cohorts(f.col('cohorts')).alias('parsedCohorts')).show(truncate=False)\n    +--------------------------------------+\n    |parsedCohorts                         |\n    +--------------------------------------+\n    |[BioME, CaPS, Estonia, FHS, UKB, GERA]|\n    |NULL                                  |\n    +--------------------------------------+\n    &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.when(\n        (raw_cohort.isNotNull()) &amp; (raw_cohort != \"\"),\n        f.array_distinct(f.split(raw_cohort, r\"\\|\")),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog","title":"<code>gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog</code>  <code>dataclass</code>","text":"<p>               Bases: <code>StudyIndex</code></p> <p>Study index dataset from GWAS Catalog.</p> <p>A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>@dataclass\nclass StudyIndexGWASCatalog(StudyIndex):\n    \"\"\"Study index dataset from GWAS Catalog.\n\n    A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.\n    \"\"\"\n\n    def update_study_id(\n        self: StudyIndexGWASCatalog, study_annotation: DataFrame\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Update studyId with a dataframe containing study.\n\n        Args:\n            study_annotation (DataFrame): Dataframe containing `updatedStudyId`, `traitFromSource`, `traitFromSourceMappedIds` and key column `studyId`.\n\n        Returns:\n            StudyIndexGWASCatalog: Updated study table.\n        \"\"\"\n        self.df = (\n            self._df.join(\n                study_annotation.select(\n                    *[\n                        f.col(c).alias(f\"updated{c}\")\n                        if c not in [\"studyId\", \"updatedStudyId\"]\n                        else f.col(c)\n                        for c in study_annotation.columns\n                    ]\n                ),\n                on=\"studyId\",\n                how=\"left\",\n            )\n            .withColumn(\n                \"studyId\",\n                f.coalesce(f.col(\"updatedStudyId\"), f.col(\"studyId\")),\n            )\n            .withColumn(\n                \"traitFromSource\",\n                f.coalesce(f.col(\"updatedtraitFromSource\"), f.col(\"traitFromSource\")),\n            )\n            .withColumn(\n                \"traitFromSourceMappedIds\",\n                f.coalesce(\n                    f.col(\"updatedtraitFromSourceMappedIds\"),\n                    f.col(\"traitFromSourceMappedIds\"),\n                ),\n            )\n            .select(self._df.columns)\n        )\n\n        return self\n\n    def annotate_from_study_curation(\n        self: StudyIndexGWASCatalog, curation_table: DataFrame | None\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Annotating study index with curation.\n\n        Args:\n            curation_table (DataFrame | None): Curated GWAS Catalog studies with summary statistics\n\n        Returns:\n            StudyIndexGWASCatalog: Updated study index\n        \"\"\"\n        # Providing curation table is optional. However once this method is called, the quality and studyFlag columns are added.\n        if curation_table is None:\n            return self\n\n        studies = self.df\n\n        if \"qualityControls\" not in studies.columns:\n            studies = studies.withColumn(\"qualityControls\", f.array())\n\n        if \"analysisFlags\" not in studies.columns:\n            studies = studies.withColumn(\"analysisFlags\", f.array())\n\n        # Adding prefix to columns in the curation table:\n        curation_table = curation_table.select(\n            *[\n                f.col(column).alias(f\"curation_{column}\")\n                if column != \"studyId\"\n                else f.col(column)\n                for column in curation_table.columns\n            ]\n        )\n\n        # Based on the curation table, columns needs to be updated:\n        curated_df = (\n            studies.join(\n                curation_table.withColumn(\"isCurated\", f.lit(True)),\n                on=\"studyId\",\n                how=\"left\",\n            )\n            .withColumn(\"isCurated\", f.coalesce(f.col(\"isCurated\"), f.lit(False)))\n            # Updating study type:\n            .withColumn(\n                \"studyType\", f.coalesce(f.col(\"curation_studyType\"), f.col(\"studyType\"))\n            )\n            # Updating study annotation flags:\n            .withColumn(\n                \"analysisFlags\",\n                f.array_union(f.col(\"analysisFlags\"), f.col(\"curation_analysisFlags\")),\n            )\n            .withColumn(\"analysisFlags\", f.coalesce(f.col(\"analysisFlags\"), f.array()))\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~f.col(\"isCurated\"),\n                    StudyQualityCheck.NO_OT_CURATION,\n                ),\n            )\n            # Dropping columns coming from the curation table:\n            .select(*studies.columns)\n        )\n        return StudyIndexGWASCatalog(\n            _df=curated_df, _schema=StudyIndexGWASCatalog.get_schema()\n        )\n\n    def extract_studies_for_curation(\n        self: StudyIndexGWASCatalog, curation: DataFrame | None\n    ) -&gt; DataFrame:\n        \"\"\"Extract studies for curation.\n\n        Args:\n            curation (DataFrame | None): Dataframe with curation.\n\n        Returns:\n            DataFrame: Updated curation table. New studies are have the `isCurated` False.\n        \"\"\"\n        # If no curation table provided, assume all studies needs curation:\n        if curation is None:\n            return (\n                self.df\n                # Curation only applyed on studies with summary statistics:\n                .filter(f.col(\"hasSumstats\"))\n                # Adding columns expected in the curation table - array columns aready flattened:\n                .withColumn(\"studyType\", f.lit(None).cast(t.StringType()))\n                .withColumn(\"analysisFlag\", f.lit(None).cast(t.StringType()))\n                .withColumn(\"qualityControl\", f.lit(None).cast(t.StringType()))\n                .withColumn(\"isCurated\", f.lit(False).cast(t.StringType()))\n            )\n\n        # Adding prefix to columns in the curation table:\n        curation = curation.select(\n            *[\n                f.col(column).alias(f\"curation_{column}\")\n                if column != \"studyId\"\n                else f.col(column)\n                for column in curation.columns\n            ]\n        )\n\n        return (\n            self.df\n            # Curation only applyed on studies with summary statistics:\n            .filter(f.col(\"hasSumstats\"))\n            .join(curation, on=\"studyId\", how=\"left\")\n            .select(\n                \"studyId\",\n                # Propagate existing curation - array columns are being flattened:\n                f.col(\"curation_studyType\").alias(\"studyType\"),\n                f.array_join(f.col(\"curation_analysisFlags\"), \"|\").alias(\n                    \"analysisFlag\"\n                ),\n                f.array_join(f.col(\"curation_qualityControls\"), \"|\").alias(\n                    \"qualityControl\"\n                ),\n                # This boolean flag needs to be casted to string, because saving to tsv would fail otherwise:\n                f.coalesce(f.col(\"curation_isCurated\"), f.lit(False))\n                .cast(t.StringType())\n                .alias(\"isCurated\"),\n                # The following columns are propagated to make curation easier:\n                \"pubmedId\",\n                \"publicationTitle\",\n                \"traitFromSource\",\n            )\n        )\n\n    def annotate_ancestries(\n        self: StudyIndexGWASCatalog, ancestry_lut: DataFrame\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Extracting sample sizes and ancestry information.\n\n        This function parses the ancestry data. Also get counts for the europeans in the same\n        discovery stage.\n\n        Args:\n            ancestry_lut (DataFrame): Ancestry table as downloaded from the GWAS Catalog\n\n        Returns:\n            StudyIndexGWASCatalog: Slimmed and cleaned version of the ancestry annotation.\n        \"\"\"\n        from gentropy.datasource.gwas_catalog.study_index import (\n            StudyIndexGWASCatalogParser as GWASCatalogStudyIndexParser,\n        )\n\n        ancestry = (\n            ancestry_lut\n            # Convert column headers to camelcase:\n            .transform(\n                lambda df: df.select(\n                    *[f.expr(column2camel_case(x)) for x in df.columns]\n                )\n            ).withColumnRenamed(\n                \"studyAccession\", \"studyId\"\n            )  # studyId has not been split yet\n        )\n\n        # Get a high resolution dataset on experimental stage:\n        ancestry_stages = (\n            ancestry.groupBy(\"studyId\")\n            .pivot(\"stage\")\n            .agg(\n                f.collect_set(\n                    f.struct(\n                        f.col(\"broadAncestralCategory\").alias(\"ancestry\"),\n                        f.col(\"numberOfIndividuals\")\n                        .cast(t.IntegerType())\n                        .alias(\"sampleSize\"),\n                    )\n                )\n            )\n            .withColumn(\n                \"discoverySamples\",\n                GWASCatalogStudyIndexParser._parse_discovery_samples(f.col(\"initial\")),\n            )\n            .withColumnRenamed(\"replication\", \"replicationSamples\")\n            # Mapping discovery stage ancestries to LD reference:\n            .withColumn(\n                \"ldPopulationStructure\",\n                self.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n            )\n            .drop(\"initial\")\n            .persist()\n        )\n\n        # Generate information on the ancestry composition of the discovery stage, and calculate\n        # the proportion of the Europeans:\n        europeans_deconvoluted = (\n            ancestry\n            # Focus on discovery stage:\n            .filter(f.col(\"stage\") == \"initial\")\n            # Sorting ancestries if European:\n            .withColumn(\n                \"ancestryFlag\",\n                # Excluding finnish:\n                f.when(\n                    f.col(\"initialSampleDescription\").contains(\"Finnish\"),\n                    f.lit(\"other\"),\n                )\n                # Excluding Icelandic population:\n                .when(\n                    f.col(\"initialSampleDescription\").contains(\"Icelandic\"),\n                    f.lit(\"other\"),\n                )\n                # Including European ancestry:\n                .when(f.col(\"broadAncestralCategory\") == \"European\", f.lit(\"european\"))\n                # Exclude all other population:\n                .otherwise(\"other\"),\n            )\n            # Grouping by study accession and initial sample description:\n            .groupBy(\"studyId\")\n            .pivot(\"ancestryFlag\")\n            .agg(\n                # Summarizing sample sizes for all ancestries:\n                f.sum(f.col(\"numberOfIndividuals\"))\n            )\n            # Do arithmetics to make sure we have the right proportion of european in the set:\n            .withColumn(\n                \"initialSampleCountEuropean\",\n                f.when(f.col(\"european\").isNull(), f.lit(0)).otherwise(\n                    f.col(\"european\")\n                ),\n            )\n            .withColumn(\n                \"initialSampleCountOther\",\n                f.when(f.col(\"other\").isNull(), f.lit(0)).otherwise(f.col(\"other\")),\n            )\n            .withColumn(\n                \"initialSampleCount\",\n                f.col(\"initialSampleCountEuropean\") + f.col(\"other\"),\n            )\n            .drop(\n                \"european\",\n                \"other\",\n                \"initialSampleCount\",\n                \"initialSampleCountEuropean\",\n                \"initialSampleCountOther\",\n            )\n        )\n\n        parsed_ancestry_lut = ancestry_stages.join(\n            europeans_deconvoluted, on=\"studyId\", how=\"outer\"\n        ).select(\n            \"studyId\", \"discoverySamples\", \"ldPopulationStructure\", \"replicationSamples\"\n        )\n        self.df = self.df.join(parsed_ancestry_lut, on=\"studyId\", how=\"left\")\n        return self\n\n    def annotate_discovery_sample_sizes(\n        self: StudyIndexGWASCatalog,\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog.\n\n        For some studies that measure quantitative traits, nCases and nControls can't be extracted. Therefore, we assume these are 0.\n\n        Returns:\n            StudyIndexGWASCatalog: object with columns `nCases`, `nControls`, and `nSamples` per `studyId` correctly extracted.\n        \"\"\"\n        sample_size_lut = (\n            self.df.select(\n                \"studyId\",\n                f.explode_outer(f.split(f.col(\"initialSampleSize\"), r\",\\s+\")).alias(\n                    \"samples\"\n                ),\n            )\n            # Extracting the sample size from the string:\n            .withColumn(\n                \"sampleSize\",\n                f.regexp_extract(\n                    f.regexp_replace(f.col(\"samples\"), \",\", \"\"), r\"[0-9,]+\", 0\n                ).cast(t.IntegerType()),\n            )\n            .select(\n                \"studyId\",\n                \"sampleSize\",\n                f.when(f.col(\"samples\").contains(\"cases\"), f.col(\"sampleSize\"))\n                .otherwise(f.lit(0))\n                .alias(\"nCases\"),\n                f.when(f.col(\"samples\").contains(\"controls\"), f.col(\"sampleSize\"))\n                .otherwise(f.lit(0))\n                .alias(\"nControls\"),\n            )\n            # Aggregating sample sizes for all ancestries:\n            .groupBy(\"studyId\")  # studyId has not been split yet\n            .agg(\n                f.sum(\"nCases\").cast(\"integer\").alias(\"nCases\"),\n                f.sum(\"nControls\").cast(\"integer\").alias(\"nControls\"),\n                f.sum(\"sampleSize\").cast(\"integer\").alias(\"nSamples\"),\n            )\n        )\n        self.df = self.df.join(sample_size_lut, on=\"studyId\", how=\"left\")\n        return self\n\n    def apply_inclusion_list(\n        self: StudyIndexGWASCatalog, inclusion_list: DataFrame\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Restricting GWAS Catalog studies based on a list of accepted study identifiers.\n\n        Args:\n            inclusion_list (DataFrame): List of accepted GWAS Catalog study identifiers\n\n        Returns:\n            StudyIndexGWASCatalog: Filtered dataset.\n        \"\"\"\n        return StudyIndexGWASCatalog(\n            _df=self.df.join(inclusion_list, on=\"studyId\", how=\"inner\"),\n            _schema=StudyIndexGWASCatalog.get_schema(),\n        )\n\n    def add_no_sumstats_flag(self: StudyIndexGWASCatalog) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Add a flag to the study index if no summary statistics are available.\n\n        Returns:\n            StudyIndexGWASCatalog: Updated study index.\n        \"\"\"\n        self.df = self.df.withColumn(\n            \"qualityControls\",\n            f.array(f.lit(StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value)),\n        )\n        return self\n\n    @staticmethod\n    def _parse_gwas_catalog_study_id(sumstats_path_column: str) -&gt; Column:\n        \"\"\"Extract GWAS Catalog study accession from the summary statistics path.\n\n        Args:\n            sumstats_path_column (str): column *name* for the summary statistics path\n\n        Returns:\n            Column: GWAS Catalog study accession.\n\n        Examples:\n            &gt;&gt;&gt; data = [\n            ... ('./GCST90086001-GCST90087000/GCST90086758/harmonised/35078996-GCST90086758-EFO_0007937.h.tsv.gz',),\n            ...    ('gs://open-targets-gwas-summary-stats/harmonised/GCST000568.parquet/',),\n            ...    (None,)\n            ... ]\n            &gt;&gt;&gt; spark.createDataFrame(data, ['testColumn']).select(StudyIndexGWASCatalog._parse_gwas_catalog_study_id('testColumn').alias('accessions')).collect()\n            [Row(accessions='GCST90086758'), Row(accessions='GCST000568'), Row(accessions=None)]\n        \"\"\"\n        accesions = f.expr(rf\"regexp_extract_all({sumstats_path_column}, '(GCST\\\\d+)')\")\n        return accesions[f.size(accesions) - 1]\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.add_no_sumstats_flag","title":"<code>add_no_sumstats_flag() -&gt; StudyIndexGWASCatalog</code>","text":"<p>Add a flag to the study index if no summary statistics are available.</p> <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Updated study index.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def add_no_sumstats_flag(self: StudyIndexGWASCatalog) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Add a flag to the study index if no summary statistics are available.\n\n    Returns:\n        StudyIndexGWASCatalog: Updated study index.\n    \"\"\"\n    self.df = self.df.withColumn(\n        \"qualityControls\",\n        f.array(f.lit(StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value)),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.annotate_ancestries","title":"<code>annotate_ancestries(ancestry_lut: DataFrame) -&gt; StudyIndexGWASCatalog</code>","text":"<p>Extracting sample sizes and ancestry information.</p> <p>This function parses the ancestry data. Also get counts for the europeans in the same discovery stage.</p> <p>Parameters:</p> Name Type Description Default <code>ancestry_lut</code> <code>DataFrame</code> <p>Ancestry table as downloaded from the GWAS Catalog</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Slimmed and cleaned version of the ancestry annotation.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def annotate_ancestries(\n    self: StudyIndexGWASCatalog, ancestry_lut: DataFrame\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Extracting sample sizes and ancestry information.\n\n    This function parses the ancestry data. Also get counts for the europeans in the same\n    discovery stage.\n\n    Args:\n        ancestry_lut (DataFrame): Ancestry table as downloaded from the GWAS Catalog\n\n    Returns:\n        StudyIndexGWASCatalog: Slimmed and cleaned version of the ancestry annotation.\n    \"\"\"\n    from gentropy.datasource.gwas_catalog.study_index import (\n        StudyIndexGWASCatalogParser as GWASCatalogStudyIndexParser,\n    )\n\n    ancestry = (\n        ancestry_lut\n        # Convert column headers to camelcase:\n        .transform(\n            lambda df: df.select(\n                *[f.expr(column2camel_case(x)) for x in df.columns]\n            )\n        ).withColumnRenamed(\n            \"studyAccession\", \"studyId\"\n        )  # studyId has not been split yet\n    )\n\n    # Get a high resolution dataset on experimental stage:\n    ancestry_stages = (\n        ancestry.groupBy(\"studyId\")\n        .pivot(\"stage\")\n        .agg(\n            f.collect_set(\n                f.struct(\n                    f.col(\"broadAncestralCategory\").alias(\"ancestry\"),\n                    f.col(\"numberOfIndividuals\")\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                )\n            )\n        )\n        .withColumn(\n            \"discoverySamples\",\n            GWASCatalogStudyIndexParser._parse_discovery_samples(f.col(\"initial\")),\n        )\n        .withColumnRenamed(\"replication\", \"replicationSamples\")\n        # Mapping discovery stage ancestries to LD reference:\n        .withColumn(\n            \"ldPopulationStructure\",\n            self.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n        )\n        .drop(\"initial\")\n        .persist()\n    )\n\n    # Generate information on the ancestry composition of the discovery stage, and calculate\n    # the proportion of the Europeans:\n    europeans_deconvoluted = (\n        ancestry\n        # Focus on discovery stage:\n        .filter(f.col(\"stage\") == \"initial\")\n        # Sorting ancestries if European:\n        .withColumn(\n            \"ancestryFlag\",\n            # Excluding finnish:\n            f.when(\n                f.col(\"initialSampleDescription\").contains(\"Finnish\"),\n                f.lit(\"other\"),\n            )\n            # Excluding Icelandic population:\n            .when(\n                f.col(\"initialSampleDescription\").contains(\"Icelandic\"),\n                f.lit(\"other\"),\n            )\n            # Including European ancestry:\n            .when(f.col(\"broadAncestralCategory\") == \"European\", f.lit(\"european\"))\n            # Exclude all other population:\n            .otherwise(\"other\"),\n        )\n        # Grouping by study accession and initial sample description:\n        .groupBy(\"studyId\")\n        .pivot(\"ancestryFlag\")\n        .agg(\n            # Summarizing sample sizes for all ancestries:\n            f.sum(f.col(\"numberOfIndividuals\"))\n        )\n        # Do arithmetics to make sure we have the right proportion of european in the set:\n        .withColumn(\n            \"initialSampleCountEuropean\",\n            f.when(f.col(\"european\").isNull(), f.lit(0)).otherwise(\n                f.col(\"european\")\n            ),\n        )\n        .withColumn(\n            \"initialSampleCountOther\",\n            f.when(f.col(\"other\").isNull(), f.lit(0)).otherwise(f.col(\"other\")),\n        )\n        .withColumn(\n            \"initialSampleCount\",\n            f.col(\"initialSampleCountEuropean\") + f.col(\"other\"),\n        )\n        .drop(\n            \"european\",\n            \"other\",\n            \"initialSampleCount\",\n            \"initialSampleCountEuropean\",\n            \"initialSampleCountOther\",\n        )\n    )\n\n    parsed_ancestry_lut = ancestry_stages.join(\n        europeans_deconvoluted, on=\"studyId\", how=\"outer\"\n    ).select(\n        \"studyId\", \"discoverySamples\", \"ldPopulationStructure\", \"replicationSamples\"\n    )\n    self.df = self.df.join(parsed_ancestry_lut, on=\"studyId\", how=\"left\")\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.annotate_discovery_sample_sizes","title":"<code>annotate_discovery_sample_sizes() -&gt; StudyIndexGWASCatalog</code>","text":"<p>Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog.</p> <p>For some studies that measure quantitative traits, nCases and nControls can't be extracted. Therefore, we assume these are 0.</p> <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>object with columns <code>nCases</code>, <code>nControls</code>, and <code>nSamples</code> per <code>studyId</code> correctly extracted.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def annotate_discovery_sample_sizes(\n    self: StudyIndexGWASCatalog,\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog.\n\n    For some studies that measure quantitative traits, nCases and nControls can't be extracted. Therefore, we assume these are 0.\n\n    Returns:\n        StudyIndexGWASCatalog: object with columns `nCases`, `nControls`, and `nSamples` per `studyId` correctly extracted.\n    \"\"\"\n    sample_size_lut = (\n        self.df.select(\n            \"studyId\",\n            f.explode_outer(f.split(f.col(\"initialSampleSize\"), r\",\\s+\")).alias(\n                \"samples\"\n            ),\n        )\n        # Extracting the sample size from the string:\n        .withColumn(\n            \"sampleSize\",\n            f.regexp_extract(\n                f.regexp_replace(f.col(\"samples\"), \",\", \"\"), r\"[0-9,]+\", 0\n            ).cast(t.IntegerType()),\n        )\n        .select(\n            \"studyId\",\n            \"sampleSize\",\n            f.when(f.col(\"samples\").contains(\"cases\"), f.col(\"sampleSize\"))\n            .otherwise(f.lit(0))\n            .alias(\"nCases\"),\n            f.when(f.col(\"samples\").contains(\"controls\"), f.col(\"sampleSize\"))\n            .otherwise(f.lit(0))\n            .alias(\"nControls\"),\n        )\n        # Aggregating sample sizes for all ancestries:\n        .groupBy(\"studyId\")  # studyId has not been split yet\n        .agg(\n            f.sum(\"nCases\").cast(\"integer\").alias(\"nCases\"),\n            f.sum(\"nControls\").cast(\"integer\").alias(\"nControls\"),\n            f.sum(\"sampleSize\").cast(\"integer\").alias(\"nSamples\"),\n        )\n    )\n    self.df = self.df.join(sample_size_lut, on=\"studyId\", how=\"left\")\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.annotate_from_study_curation","title":"<code>annotate_from_study_curation(curation_table: DataFrame | None) -&gt; StudyIndexGWASCatalog</code>","text":"<p>Annotating study index with curation.</p> <p>Parameters:</p> Name Type Description Default <code>curation_table</code> <code>DataFrame | None</code> <p>Curated GWAS Catalog studies with summary statistics</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Updated study index</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def annotate_from_study_curation(\n    self: StudyIndexGWASCatalog, curation_table: DataFrame | None\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Annotating study index with curation.\n\n    Args:\n        curation_table (DataFrame | None): Curated GWAS Catalog studies with summary statistics\n\n    Returns:\n        StudyIndexGWASCatalog: Updated study index\n    \"\"\"\n    # Providing curation table is optional. However once this method is called, the quality and studyFlag columns are added.\n    if curation_table is None:\n        return self\n\n    studies = self.df\n\n    if \"qualityControls\" not in studies.columns:\n        studies = studies.withColumn(\"qualityControls\", f.array())\n\n    if \"analysisFlags\" not in studies.columns:\n        studies = studies.withColumn(\"analysisFlags\", f.array())\n\n    # Adding prefix to columns in the curation table:\n    curation_table = curation_table.select(\n        *[\n            f.col(column).alias(f\"curation_{column}\")\n            if column != \"studyId\"\n            else f.col(column)\n            for column in curation_table.columns\n        ]\n    )\n\n    # Based on the curation table, columns needs to be updated:\n    curated_df = (\n        studies.join(\n            curation_table.withColumn(\"isCurated\", f.lit(True)),\n            on=\"studyId\",\n            how=\"left\",\n        )\n        .withColumn(\"isCurated\", f.coalesce(f.col(\"isCurated\"), f.lit(False)))\n        # Updating study type:\n        .withColumn(\n            \"studyType\", f.coalesce(f.col(\"curation_studyType\"), f.col(\"studyType\"))\n        )\n        # Updating study annotation flags:\n        .withColumn(\n            \"analysisFlags\",\n            f.array_union(f.col(\"analysisFlags\"), f.col(\"curation_analysisFlags\")),\n        )\n        .withColumn(\"analysisFlags\", f.coalesce(f.col(\"analysisFlags\"), f.array()))\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~f.col(\"isCurated\"),\n                StudyQualityCheck.NO_OT_CURATION,\n            ),\n        )\n        # Dropping columns coming from the curation table:\n        .select(*studies.columns)\n    )\n    return StudyIndexGWASCatalog(\n        _df=curated_df, _schema=StudyIndexGWASCatalog.get_schema()\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.apply_inclusion_list","title":"<code>apply_inclusion_list(inclusion_list: DataFrame) -&gt; StudyIndexGWASCatalog</code>","text":"<p>Restricting GWAS Catalog studies based on a list of accepted study identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>inclusion_list</code> <code>DataFrame</code> <p>List of accepted GWAS Catalog study identifiers</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Filtered dataset.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def apply_inclusion_list(\n    self: StudyIndexGWASCatalog, inclusion_list: DataFrame\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Restricting GWAS Catalog studies based on a list of accepted study identifiers.\n\n    Args:\n        inclusion_list (DataFrame): List of accepted GWAS Catalog study identifiers\n\n    Returns:\n        StudyIndexGWASCatalog: Filtered dataset.\n    \"\"\"\n    return StudyIndexGWASCatalog(\n        _df=self.df.join(inclusion_list, on=\"studyId\", how=\"inner\"),\n        _schema=StudyIndexGWASCatalog.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.extract_studies_for_curation","title":"<code>extract_studies_for_curation(curation: DataFrame | None) -&gt; DataFrame</code>","text":"<p>Extract studies for curation.</p> <p>Parameters:</p> Name Type Description Default <code>curation</code> <code>DataFrame | None</code> <p>Dataframe with curation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Updated curation table. New studies are have the <code>isCurated</code> False.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def extract_studies_for_curation(\n    self: StudyIndexGWASCatalog, curation: DataFrame | None\n) -&gt; DataFrame:\n    \"\"\"Extract studies for curation.\n\n    Args:\n        curation (DataFrame | None): Dataframe with curation.\n\n    Returns:\n        DataFrame: Updated curation table. New studies are have the `isCurated` False.\n    \"\"\"\n    # If no curation table provided, assume all studies needs curation:\n    if curation is None:\n        return (\n            self.df\n            # Curation only applyed on studies with summary statistics:\n            .filter(f.col(\"hasSumstats\"))\n            # Adding columns expected in the curation table - array columns aready flattened:\n            .withColumn(\"studyType\", f.lit(None).cast(t.StringType()))\n            .withColumn(\"analysisFlag\", f.lit(None).cast(t.StringType()))\n            .withColumn(\"qualityControl\", f.lit(None).cast(t.StringType()))\n            .withColumn(\"isCurated\", f.lit(False).cast(t.StringType()))\n        )\n\n    # Adding prefix to columns in the curation table:\n    curation = curation.select(\n        *[\n            f.col(column).alias(f\"curation_{column}\")\n            if column != \"studyId\"\n            else f.col(column)\n            for column in curation.columns\n        ]\n    )\n\n    return (\n        self.df\n        # Curation only applyed on studies with summary statistics:\n        .filter(f.col(\"hasSumstats\"))\n        .join(curation, on=\"studyId\", how=\"left\")\n        .select(\n            \"studyId\",\n            # Propagate existing curation - array columns are being flattened:\n            f.col(\"curation_studyType\").alias(\"studyType\"),\n            f.array_join(f.col(\"curation_analysisFlags\"), \"|\").alias(\n                \"analysisFlag\"\n            ),\n            f.array_join(f.col(\"curation_qualityControls\"), \"|\").alias(\n                \"qualityControl\"\n            ),\n            # This boolean flag needs to be casted to string, because saving to tsv would fail otherwise:\n            f.coalesce(f.col(\"curation_isCurated\"), f.lit(False))\n            .cast(t.StringType())\n            .alias(\"isCurated\"),\n            # The following columns are propagated to make curation easier:\n            \"pubmedId\",\n            \"publicationTitle\",\n            \"traitFromSource\",\n        )\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.update_study_id","title":"<code>update_study_id(study_annotation: DataFrame) -&gt; StudyIndexGWASCatalog</code>","text":"<p>Update studyId with a dataframe containing study.</p> <p>Parameters:</p> Name Type Description Default <code>study_annotation</code> <code>DataFrame</code> <p>Dataframe containing <code>updatedStudyId</code>, <code>traitFromSource</code>, <code>traitFromSourceMappedIds</code> and key column <code>studyId</code>.</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Updated study table.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def update_study_id(\n    self: StudyIndexGWASCatalog, study_annotation: DataFrame\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Update studyId with a dataframe containing study.\n\n    Args:\n        study_annotation (DataFrame): Dataframe containing `updatedStudyId`, `traitFromSource`, `traitFromSourceMappedIds` and key column `studyId`.\n\n    Returns:\n        StudyIndexGWASCatalog: Updated study table.\n    \"\"\"\n    self.df = (\n        self._df.join(\n            study_annotation.select(\n                *[\n                    f.col(c).alias(f\"updated{c}\")\n                    if c not in [\"studyId\", \"updatedStudyId\"]\n                    else f.col(c)\n                    for c in study_annotation.columns\n                ]\n            ),\n            on=\"studyId\",\n            how=\"left\",\n        )\n        .withColumn(\n            \"studyId\",\n            f.coalesce(f.col(\"updatedStudyId\"), f.col(\"studyId\")),\n        )\n        .withColumn(\n            \"traitFromSource\",\n            f.coalesce(f.col(\"updatedtraitFromSource\"), f.col(\"traitFromSource\")),\n        )\n        .withColumn(\n            \"traitFromSourceMappedIds\",\n            f.coalesce(\n                f.col(\"updatedtraitFromSourceMappedIds\"),\n                f.col(\"traitFromSourceMappedIds\"),\n            ),\n        )\n        .select(self._df.columns)\n    )\n\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_splitter/","title":"Study Splitter","text":""},{"location":"python_api/datasources/gwas_catalog/study_splitter/#gentropy.datasource.gwas_catalog.study_splitter.GWASCatalogStudySplitter","title":"<code>gentropy.datasource.gwas_catalog.study_splitter.GWASCatalogStudySplitter</code>","text":"<p>Splitting multi-trait GWAS Catalog studies.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_splitter.py</code> <pre><code>class GWASCatalogStudySplitter:\n    \"\"\"Splitting multi-trait GWAS Catalog studies.\"\"\"\n\n    @staticmethod\n    def _resolve_trait(\n        study_trait: Column, association_trait: Column, p_value_text: Column\n    ) -&gt; Column:\n        \"\"\"Resolve trait names by consolidating association-level and study-level trait names.\n\n        Args:\n            study_trait (Column): Study-level trait name.\n            association_trait (Column): Association-level trait name.\n            p_value_text (Column): P-value text.\n\n        Returns:\n            Column: Resolved trait name.\n        \"\"\"\n        return (\n            f.when(\n                (p_value_text.isNotNull()) &amp; (p_value_text != (\"no_pvalue_text\")),\n                f.concat(\n                    association_trait,\n                    f.lit(\" [\"),\n                    p_value_text,\n                    f.lit(\"]\"),\n                ),\n            )\n            .when(\n                association_trait.isNotNull(),\n                association_trait,\n            )\n            .otherwise(study_trait)\n        )\n\n    @staticmethod\n    def _resolve_efo(association_efo: Column, study_efo: Column) -&gt; Column:\n        \"\"\"Resolve EFOs by consolidating association-level and study-level EFOs.\n\n        Args:\n            association_efo (Column): EFO column from the association table.\n            study_efo (Column): EFO column from the study table.\n\n        Returns:\n            Column: Consolidated EFO column.\n        \"\"\"\n        return f.coalesce(f.split(association_efo, r\"\\/\"), study_efo)\n\n    @staticmethod\n    def _resolve_study_id(study_id: Column, sub_study_description: Column) -&gt; Column:\n        \"\"\"Resolve study IDs by exploding association-level information (e.g. pvalue_text, EFO).\n\n        Args:\n            study_id (Column): Study ID column.\n            sub_study_description (Column): Sub-study description column from the association table.\n\n        Returns:\n            Column: Resolved study ID column.\n        \"\"\"\n        split_w = Window.partitionBy(study_id).orderBy(sub_study_description)\n        row_number = f.dense_rank().over(split_w)\n        substudy_count = f.approx_count_distinct(row_number).over(split_w)\n        return f.when(substudy_count == 1, study_id).otherwise(\n            f.concat_ws(\"_\", study_id, row_number)\n        )\n\n    @classmethod\n    def split(\n        cls: type[GWASCatalogStudySplitter],\n        studies: StudyIndexGWASCatalog,\n        associations: StudyLocusGWASCatalog,\n    ) -&gt; tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]:\n        \"\"\"Splitting multi-trait GWAS Catalog studies.\n\n        If assigned disease of the study and the association don't agree, we assume the study needs to be split.\n        Then disease EFOs, trait names and study ID are consolidated\n\n        Args:\n            studies (StudyIndexGWASCatalog): GWAS Catalog studies.\n            associations (StudyLocusGWASCatalog): GWAS Catalog associations.\n\n        Returns:\n            tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]: Split studies and associations.\n        \"\"\"\n        # Composite of studies and associations to resolve scattered information\n        st_ass = (\n            associations.df.join(f.broadcast(studies.df), on=\"studyId\", how=\"inner\")\n            .select(\n                \"studyId\",\n                \"subStudyDescription\",\n                cls._resolve_study_id(\n                    f.col(\"studyId\"), f.col(\"subStudyDescription\")\n                ).alias(\"updatedStudyId\"),\n                cls._resolve_trait(\n                    f.col(\"traitFromSource\"),\n                    f.split(\"subStudyDescription\", r\"\\|\").getItem(0),\n                    f.split(\"subStudyDescription\", r\"\\|\").getItem(1),\n                ).alias(\"traitFromSource\"),\n                cls._resolve_efo(\n                    f.split(\"subStudyDescription\", r\"\\|\").getItem(2),\n                    f.col(\"traitFromSourceMappedIds\"),\n                ).alias(\"traitFromSourceMappedIds\"),\n            )\n            .persist()\n        )\n\n        return (\n            studies.update_study_id(\n                st_ass.select(\n                    \"studyId\",\n                    \"updatedStudyId\",\n                    \"traitFromSource\",\n                    \"traitFromSourceMappedIds\",\n                ).distinct()\n            ),\n            associations.update_study_id(\n                st_ass.select(\n                    \"updatedStudyId\", \"studyId\", \"subStudyDescription\"\n                ).distinct()\n            )\n            .qc_ambiguous_study()\n            .qc_flag_all_tophits(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_splitter/#gentropy.datasource.gwas_catalog.study_splitter.GWASCatalogStudySplitter.split","title":"<code>split(studies: StudyIndexGWASCatalog, associations: StudyLocusGWASCatalog) -&gt; tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]</code>  <code>classmethod</code>","text":"<p>Splitting multi-trait GWAS Catalog studies.</p> <p>If assigned disease of the study and the association don't agree, we assume the study needs to be split. Then disease EFOs, trait names and study ID are consolidated</p> <p>Parameters:</p> Name Type Description Default <code>studies</code> <code>StudyIndexGWASCatalog</code> <p>GWAS Catalog studies.</p> required <code>associations</code> <code>StudyLocusGWASCatalog</code> <p>GWAS Catalog associations.</p> required <p>Returns:</p> Type Description <code>tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]</code> <p>tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]: Split studies and associations.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_splitter.py</code> <pre><code>@classmethod\ndef split(\n    cls: type[GWASCatalogStudySplitter],\n    studies: StudyIndexGWASCatalog,\n    associations: StudyLocusGWASCatalog,\n) -&gt; tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]:\n    \"\"\"Splitting multi-trait GWAS Catalog studies.\n\n    If assigned disease of the study and the association don't agree, we assume the study needs to be split.\n    Then disease EFOs, trait names and study ID are consolidated\n\n    Args:\n        studies (StudyIndexGWASCatalog): GWAS Catalog studies.\n        associations (StudyLocusGWASCatalog): GWAS Catalog associations.\n\n    Returns:\n        tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]: Split studies and associations.\n    \"\"\"\n    # Composite of studies and associations to resolve scattered information\n    st_ass = (\n        associations.df.join(f.broadcast(studies.df), on=\"studyId\", how=\"inner\")\n        .select(\n            \"studyId\",\n            \"subStudyDescription\",\n            cls._resolve_study_id(\n                f.col(\"studyId\"), f.col(\"subStudyDescription\")\n            ).alias(\"updatedStudyId\"),\n            cls._resolve_trait(\n                f.col(\"traitFromSource\"),\n                f.split(\"subStudyDescription\", r\"\\|\").getItem(0),\n                f.split(\"subStudyDescription\", r\"\\|\").getItem(1),\n            ).alias(\"traitFromSource\"),\n            cls._resolve_efo(\n                f.split(\"subStudyDescription\", r\"\\|\").getItem(2),\n                f.col(\"traitFromSourceMappedIds\"),\n            ).alias(\"traitFromSourceMappedIds\"),\n        )\n        .persist()\n    )\n\n    return (\n        studies.update_study_id(\n            st_ass.select(\n                \"studyId\",\n                \"updatedStudyId\",\n                \"traitFromSource\",\n                \"traitFromSourceMappedIds\",\n            ).distinct()\n        ),\n        associations.update_study_id(\n            st_ass.select(\n                \"updatedStudyId\", \"studyId\", \"subStudyDescription\"\n            ).distinct()\n        )\n        .qc_ambiguous_study()\n        .qc_flag_all_tophits(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/summary_statistics/","title":"Summary statistics","text":""},{"location":"python_api/datasources/gwas_catalog/summary_statistics/#gentropy.datasource.gwas_catalog.summary_statistics.GWASCatalogSummaryStatistics","title":"<code>gentropy.datasource.gwas_catalog.summary_statistics.GWASCatalogSummaryStatistics</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SummaryStatistics</code></p> <p>GWAS Catalog Summary Statistics reader.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/summary_statistics.py</code> <pre><code>@dataclass\nclass GWASCatalogSummaryStatistics(SummaryStatistics):\n    \"\"\"GWAS Catalog Summary Statistics reader.\"\"\"\n\n    @classmethod\n    def from_gwas_harmonized_summary_stats(\n        cls: type[GWASCatalogSummaryStatistics],\n        spark: SparkSession,\n        sumstats_file: str,\n    ) -&gt; GWASCatalogSummaryStatistics:\n        \"\"\"Create summary statistics object from summary statistics flatfile, harmonized by the GWAS Catalog.\n\n        Things got slightly complicated given the GWAS Catalog harmonization pipelines changed recently so we had to accomodate to\n        both formats.\n\n        Args:\n            spark (SparkSession): spark session\n            sumstats_file (str): list of GWAS Catalog summary stat files, with study ids in them.\n\n        Returns:\n            GWASCatalogSummaryStatistics: Summary statistics object.\n        \"\"\"\n        sumstats_df = spark.read.csv(sumstats_file, sep=\"\\t\", header=True).withColumn(\n            # Parsing GWAS Catalog study identifier from filename:\n            \"studyId\",\n            f.lit(filename_to_study_identifier(sumstats_file)),\n        )\n\n        # Parsing variant id fields:\n        chromosome = (\n            f.col(\"hm_chrom\")\n            if \"hm_chrom\" in sumstats_df.columns\n            else f.col(\"chromosome\")\n        ).cast(t.StringType())\n        position = (\n            f.col(\"hm_pos\")\n            if \"hm_pos\" in sumstats_df.columns\n            else f.col(\"base_pair_location\")\n        ).cast(t.IntegerType())\n        ref_allele = (\n            f.col(\"hm_other_allele\")\n            if \"hm_other_allele\" in sumstats_df.columns\n            else f.col(\"other_allele\")\n        )\n        alt_allele = (\n            f.col(\"hm_effect_allele\")\n            if \"hm_effect_allele\" in sumstats_df.columns\n            else f.col(\"effect_allele\")\n        )\n\n        # Parsing p-value (get a tuple with mantissa and exponent):\n        p_value_expression = (\n            parse_pvalue(f.col(\"p_value\"))\n            if \"p_value\" in sumstats_df.columns\n            else neglog_pvalue_to_mantissa_and_exponent(f.col(\"neg_log_10_p_value\"))\n        )\n\n        # The effect allele frequency is an optional column, we have to test if it is there:\n        allele_frequency = (\n            f.col(\"effect_allele_frequency\")\n            if \"effect_allele_frequency\" in sumstats_df.columns\n            else f.lit(None)\n        ).cast(t.FloatType())\n\n        # Do we have sample size? This expression captures 99.7% of sample size columns.\n        sample_size = (f.col(\"n\") if \"n\" in sumstats_df.columns else f.lit(None)).cast(\n            t.IntegerType()\n        )\n\n        # Depending on the input, we might have beta, but the column might not be there at all also old format calls differently:\n        beta_expression = (\n            f.col(\"hm_beta\")\n            if \"hm_beta\" in sumstats_df.columns\n            else f.col(\"beta\")\n            if \"beta\" in sumstats_df.columns\n            # If no column, create one:\n            else f.lit(None)\n        ).cast(t.DoubleType())\n\n        # We might have odds ratio or hazard ratio, wich are basically the same:\n        odds_ratio_expression = (\n            f.col(\"hm_odds_ratio\")\n            if \"hm_odds_ratio\" in sumstats_df.columns\n            else f.col(\"odds_ratio\")\n            if \"odds_ratio\" in sumstats_df.columns\n            else f.col(\"hazard_ratio\")\n            if \"hazard_ratio\" in sumstats_df.columns\n            # If no column, create one:\n            else f.lit(None)\n        ).cast(t.DoubleType())\n\n        # Does the file have standard error column?\n        standard_error = (\n            f.col(\"standard_error\")\n            if \"standard_error\" in sumstats_df.columns\n            else f.lit(None)\n        ).cast(t.DoubleType())\n\n        # Processing columns of interest:\n        processed_sumstats_df = (\n            sumstats_df\n            # Dropping rows which doesn't have proper position:\n            .select(\n                \"studyId\",\n                # Adding variant identifier:\n                f.concat_ws(\n                    \"_\",\n                    chromosome,\n                    position,\n                    ref_allele,\n                    alt_allele,\n                ).alias(\"variantId\"),\n                chromosome.alias(\"chromosome\"),\n                position.alias(\"position\"),\n                # Parsing p-value mantissa and exponent:\n                *p_value_expression,\n                # Converting/calculating effect and confidence interval:\n                *convert_odds_ratio_to_beta(\n                    beta_expression,\n                    odds_ratio_expression,\n                    standard_error,\n                ),\n                allele_frequency.alias(\"effectAlleleFrequencyFromSource\"),\n                sample_size.alias(\"sampleSize\"),\n            )\n            .filter(\n                # Dropping associations where no harmonized position is available:\n                f.col(\"position\").isNotNull()\n                &amp;\n                # We are not interested in associations with zero effect:\n                (f.col(\"beta\") != 0)\n            )\n            .orderBy(f.col(\"chromosome\"), f.col(\"position\"))\n            # median study size is 200Mb, max is 2.6Gb\n            .repartition(20)\n        )\n\n        # Initializing summary statistics object:\n        return cls(\n            _df=processed_sumstats_df,\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/summary_statistics/#gentropy.datasource.gwas_catalog.summary_statistics.GWASCatalogSummaryStatistics.from_gwas_harmonized_summary_stats","title":"<code>from_gwas_harmonized_summary_stats(spark: SparkSession, sumstats_file: str) -&gt; GWASCatalogSummaryStatistics</code>  <code>classmethod</code>","text":"<p>Create summary statistics object from summary statistics flatfile, harmonized by the GWAS Catalog.</p> <p>Things got slightly complicated given the GWAS Catalog harmonization pipelines changed recently so we had to accomodate to both formats.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>spark session</p> required <code>sumstats_file</code> <code>str</code> <p>list of GWAS Catalog summary stat files, with study ids in them.</p> required <p>Returns:</p> Name Type Description <code>GWASCatalogSummaryStatistics</code> <code>GWASCatalogSummaryStatistics</code> <p>Summary statistics object.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/summary_statistics.py</code> <pre><code>@classmethod\ndef from_gwas_harmonized_summary_stats(\n    cls: type[GWASCatalogSummaryStatistics],\n    spark: SparkSession,\n    sumstats_file: str,\n) -&gt; GWASCatalogSummaryStatistics:\n    \"\"\"Create summary statistics object from summary statistics flatfile, harmonized by the GWAS Catalog.\n\n    Things got slightly complicated given the GWAS Catalog harmonization pipelines changed recently so we had to accomodate to\n    both formats.\n\n    Args:\n        spark (SparkSession): spark session\n        sumstats_file (str): list of GWAS Catalog summary stat files, with study ids in them.\n\n    Returns:\n        GWASCatalogSummaryStatistics: Summary statistics object.\n    \"\"\"\n    sumstats_df = spark.read.csv(sumstats_file, sep=\"\\t\", header=True).withColumn(\n        # Parsing GWAS Catalog study identifier from filename:\n        \"studyId\",\n        f.lit(filename_to_study_identifier(sumstats_file)),\n    )\n\n    # Parsing variant id fields:\n    chromosome = (\n        f.col(\"hm_chrom\")\n        if \"hm_chrom\" in sumstats_df.columns\n        else f.col(\"chromosome\")\n    ).cast(t.StringType())\n    position = (\n        f.col(\"hm_pos\")\n        if \"hm_pos\" in sumstats_df.columns\n        else f.col(\"base_pair_location\")\n    ).cast(t.IntegerType())\n    ref_allele = (\n        f.col(\"hm_other_allele\")\n        if \"hm_other_allele\" in sumstats_df.columns\n        else f.col(\"other_allele\")\n    )\n    alt_allele = (\n        f.col(\"hm_effect_allele\")\n        if \"hm_effect_allele\" in sumstats_df.columns\n        else f.col(\"effect_allele\")\n    )\n\n    # Parsing p-value (get a tuple with mantissa and exponent):\n    p_value_expression = (\n        parse_pvalue(f.col(\"p_value\"))\n        if \"p_value\" in sumstats_df.columns\n        else neglog_pvalue_to_mantissa_and_exponent(f.col(\"neg_log_10_p_value\"))\n    )\n\n    # The effect allele frequency is an optional column, we have to test if it is there:\n    allele_frequency = (\n        f.col(\"effect_allele_frequency\")\n        if \"effect_allele_frequency\" in sumstats_df.columns\n        else f.lit(None)\n    ).cast(t.FloatType())\n\n    # Do we have sample size? This expression captures 99.7% of sample size columns.\n    sample_size = (f.col(\"n\") if \"n\" in sumstats_df.columns else f.lit(None)).cast(\n        t.IntegerType()\n    )\n\n    # Depending on the input, we might have beta, but the column might not be there at all also old format calls differently:\n    beta_expression = (\n        f.col(\"hm_beta\")\n        if \"hm_beta\" in sumstats_df.columns\n        else f.col(\"beta\")\n        if \"beta\" in sumstats_df.columns\n        # If no column, create one:\n        else f.lit(None)\n    ).cast(t.DoubleType())\n\n    # We might have odds ratio or hazard ratio, wich are basically the same:\n    odds_ratio_expression = (\n        f.col(\"hm_odds_ratio\")\n        if \"hm_odds_ratio\" in sumstats_df.columns\n        else f.col(\"odds_ratio\")\n        if \"odds_ratio\" in sumstats_df.columns\n        else f.col(\"hazard_ratio\")\n        if \"hazard_ratio\" in sumstats_df.columns\n        # If no column, create one:\n        else f.lit(None)\n    ).cast(t.DoubleType())\n\n    # Does the file have standard error column?\n    standard_error = (\n        f.col(\"standard_error\")\n        if \"standard_error\" in sumstats_df.columns\n        else f.lit(None)\n    ).cast(t.DoubleType())\n\n    # Processing columns of interest:\n    processed_sumstats_df = (\n        sumstats_df\n        # Dropping rows which doesn't have proper position:\n        .select(\n            \"studyId\",\n            # Adding variant identifier:\n            f.concat_ws(\n                \"_\",\n                chromosome,\n                position,\n                ref_allele,\n                alt_allele,\n            ).alias(\"variantId\"),\n            chromosome.alias(\"chromosome\"),\n            position.alias(\"position\"),\n            # Parsing p-value mantissa and exponent:\n            *p_value_expression,\n            # Converting/calculating effect and confidence interval:\n            *convert_odds_ratio_to_beta(\n                beta_expression,\n                odds_ratio_expression,\n                standard_error,\n            ),\n            allele_frequency.alias(\"effectAlleleFrequencyFromSource\"),\n            sample_size.alias(\"sampleSize\"),\n        )\n        .filter(\n            # Dropping associations where no harmonized position is available:\n            f.col(\"position\").isNotNull()\n            &amp;\n            # We are not interested in associations with zero effect:\n            (f.col(\"beta\") != 0)\n        )\n        .orderBy(f.col(\"chromosome\"), f.col(\"position\"))\n        # median study size is 200Mb, max is 2.6Gb\n        .repartition(20)\n    )\n\n    # Initializing summary statistics object:\n    return cls(\n        _df=processed_sumstats_df,\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/_intervals/","title":"List of Interaction and Interval-based Studies","text":"<p>In this section, we provide a list of studies that focus on interaction and interval-based investigations, shedding light on the intricate relationships between genetic elements and their functional implications.</p> <ol> <li> <p>Promoter Capture Hi-C (Javierre et al., 2016): Title: \"Lineage-Specific Genome Architecture Links Enhancers and Non-coding Disease Variants to Target Gene Promoters\".    This study presents evidence linking genetic variation to genes through the application of Promoter Capture Hi-C across each of the 17 human primary hematopoietic cell types. The method captures interactions between promoters and distal regulatory elements, providing valuable insights into the three-dimensional chromatin architecture. DOI: 10.1016/j.cell.2016.09.037</p> </li> <li> <p>Enhancer-TSS Correlation (Andersson et al., 2014): Title: \"An Atlas of Active Enhancers across Human Cell Types and Tissues\".    This study explores genetic variation's impact on genes by examining the correlation between the transcriptional activity of enhancers and transcription start sites. The findings are documented in the FANTOM5 CAGE expression atlas, offering a comprehensive view of the regulatory landscape. DOI: 10.1038/nature12787</p> </li> <li> <p>DHS-Promoter Correlation (Thurman et al., 2012): Title: \"The accessible chromatin landscape of the human genome\".    Investigating genetic variation's connection to genes, this study employs the correlation of DNase I hypersensitive sites (DHS) and gene promoters. The analysis spans 125 cell and tissue types from the ENCODE project, providing a broad understanding of the regulatory interactions across diverse biological contexts. DOI: 10.1038/nature11232</p> </li> <li> <p>Promoter Capture Hi-C (Jung et al., 2019): Title: \"A compendium of promoter-centered long-range chromatin interactions in the human genome\".    This study compiles a compendium of promoter-centered long-range chromatin interactions in the human genome. By focusing on the three-dimensional organization of chromatin, the research contributes to our understanding of the spatial arrangement of genetic elements and their implications in gene regulation. DOI: 10.1038/s41588-019-0494-8</p> </li> </ol> <p>For in-depth details on each study, you may refer to the respective publications.</p>"},{"location":"python_api/datasources/intervals/andersson/","title":"Andersson et al.","text":""},{"location":"python_api/datasources/intervals/andersson/#gentropy.datasource.intervals.andersson.IntervalsAndersson","title":"<code>gentropy.datasource.intervals.andersson.IntervalsAndersson</code>","text":"<p>Interval dataset from Andersson et al. 2014.</p> Source code in <code>src/gentropy/datasource/intervals/andersson.py</code> <pre><code>class IntervalsAndersson:\n    \"\"\"Interval dataset from Andersson et al. 2014.\"\"\"\n\n    @staticmethod\n    def read(spark: SparkSession, path: str) -&gt; DataFrame:\n        \"\"\"Read andersson2014 dataset.\n\n        Args:\n            spark (SparkSession): Spark session\n            path (str): Path to the dataset\n\n        Returns:\n            DataFrame: Raw Andersson et al. dataframe\n        \"\"\"\n        input_schema = t.StructType.fromJson(\n            json.loads(\n                pkg_resources.read_text(schemas, \"andersson2014.json\", encoding=\"utf-8\")\n            )\n        )\n        return (\n            spark.read.option(\"delimiter\", \"\\t\")\n            .option(\"mode\", \"DROPMALFORMED\")\n            .option(\"header\", \"true\")\n            .schema(input_schema)\n            .csv(path)\n        )\n\n    @classmethod\n    def parse(\n        cls: type[IntervalsAndersson],\n        raw_anderson_df: DataFrame,\n        target_index: TargetIndex,\n        lift: LiftOverSpark,\n    ) -&gt; Intervals:\n        \"\"\"Parse Andersson et al. 2014 dataset.\n\n        Args:\n            raw_anderson_df (DataFrame): Raw Andersson et al. dataset\n            target_index (TargetIndex): Target index\n            lift (LiftOverSpark): LiftOverSpark instance\n\n        Returns:\n            Intervals: Intervals dataset\n        \"\"\"\n        # Constant values:\n        dataset_name = \"andersson2014\"\n        experiment_type = \"fantom5\"\n        pmid = \"24670763\"\n        bio_feature = \"aggregate\"\n        twosided_threshold = 2.45e6  # &lt;-  this needs to phased out. Filter by percentile instead of absolute value.\n\n        # Read the anderson file:\n        parsed_anderson_df = (\n            raw_anderson_df\n            # Parsing score column and casting as float:\n            .withColumn(\"score\", f.col(\"score\").cast(\"float\") / f.lit(1000))\n            # Parsing the 'name' column:\n            .withColumn(\"parsedName\", f.split(f.col(\"name\"), \";\"))\n            .withColumn(\"gene_symbol\", f.col(\"parsedName\")[2])\n            .withColumn(\"location\", f.col(\"parsedName\")[0])\n            .withColumn(\n                \"chrom\",\n                f.regexp_replace(f.split(f.col(\"location\"), \":|-\")[0], \"chr\", \"\"),\n            )\n            .withColumn(\n                \"start\", f.split(f.col(\"location\"), \":|-\")[1].cast(t.IntegerType())\n            )\n            .withColumn(\n                \"end\", f.split(f.col(\"location\"), \":|-\")[2].cast(t.IntegerType())\n            )\n            # Select relevant columns:\n            .select(\"chrom\", \"start\", \"end\", \"gene_symbol\", \"score\")\n            # Drop rows with non-canonical chromosomes:\n            .filter(\n                f.col(\"chrom\").isin([str(x) for x in range(1, 23)] + [\"X\", \"Y\", \"MT\"])\n            )\n            # For each region/gene, keep only one row with the highest score:\n            .groupBy(\"chrom\", \"start\", \"end\", \"gene_symbol\")\n            .agg(f.max(\"score\").alias(\"resourceScore\"))\n            .orderBy(\"chrom\", \"start\")\n        )\n\n        return Intervals(\n            _df=(\n                # Lift over the intervals:\n                lift.convert_intervals(parsed_anderson_df, \"chrom\", \"start\", \"end\")\n                .drop(\"start\", \"end\")\n                .withColumnRenamed(\"mapped_start\", \"start\")\n                .withColumnRenamed(\"mapped_end\", \"end\")\n                .distinct()\n                # Joining with the target index\n                .alias(\"intervals\")\n                .join(\n                    target_index.symbols_lut().alias(\"genes\"),\n                    on=[\n                        f.col(\"intervals.gene_symbol\") == f.col(\"genes.geneSymbol\"),\n                        # Drop rows where the TSS is far from the start of the region\n                        f.abs(\n                            (f.col(\"intervals.start\") + f.col(\"intervals.end\")) / 2\n                            - f.col(\"tss\")\n                        )\n                        &lt;= twosided_threshold,\n                    ],\n                    how=\"left\",\n                )\n                # Select relevant columns:\n                .select(\n                    f.col(\"chrom\").alias(\"chromosome\"),\n                    f.col(\"intervals.start\").alias(\"start\"),\n                    f.col(\"intervals.end\").alias(\"end\"),\n                    \"geneId\",\n                    \"resourceScore\",\n                    f.lit(dataset_name).alias(\"datasourceId\"),\n                    f.lit(experiment_type).alias(\"datatypeId\"),\n                    f.lit(pmid).alias(\"pmid\"),\n                    f.lit(bio_feature).alias(\"biofeature\"),\n                )\n            ),\n            _schema=Intervals.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/intervals/andersson/#gentropy.datasource.intervals.andersson.IntervalsAndersson.parse","title":"<code>parse(raw_anderson_df: DataFrame, target_index: TargetIndex, lift: LiftOverSpark) -&gt; Intervals</code>  <code>classmethod</code>","text":"<p>Parse Andersson et al. 2014 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raw_anderson_df</code> <code>DataFrame</code> <p>Raw Andersson et al. dataset</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index</p> required <code>lift</code> <code>LiftOverSpark</code> <p>LiftOverSpark instance</p> required <p>Returns:</p> Name Type Description <code>Intervals</code> <code>Intervals</code> <p>Intervals dataset</p> Source code in <code>src/gentropy/datasource/intervals/andersson.py</code> <pre><code>@classmethod\ndef parse(\n    cls: type[IntervalsAndersson],\n    raw_anderson_df: DataFrame,\n    target_index: TargetIndex,\n    lift: LiftOverSpark,\n) -&gt; Intervals:\n    \"\"\"Parse Andersson et al. 2014 dataset.\n\n    Args:\n        raw_anderson_df (DataFrame): Raw Andersson et al. dataset\n        target_index (TargetIndex): Target index\n        lift (LiftOverSpark): LiftOverSpark instance\n\n    Returns:\n        Intervals: Intervals dataset\n    \"\"\"\n    # Constant values:\n    dataset_name = \"andersson2014\"\n    experiment_type = \"fantom5\"\n    pmid = \"24670763\"\n    bio_feature = \"aggregate\"\n    twosided_threshold = 2.45e6  # &lt;-  this needs to phased out. Filter by percentile instead of absolute value.\n\n    # Read the anderson file:\n    parsed_anderson_df = (\n        raw_anderson_df\n        # Parsing score column and casting as float:\n        .withColumn(\"score\", f.col(\"score\").cast(\"float\") / f.lit(1000))\n        # Parsing the 'name' column:\n        .withColumn(\"parsedName\", f.split(f.col(\"name\"), \";\"))\n        .withColumn(\"gene_symbol\", f.col(\"parsedName\")[2])\n        .withColumn(\"location\", f.col(\"parsedName\")[0])\n        .withColumn(\n            \"chrom\",\n            f.regexp_replace(f.split(f.col(\"location\"), \":|-\")[0], \"chr\", \"\"),\n        )\n        .withColumn(\n            \"start\", f.split(f.col(\"location\"), \":|-\")[1].cast(t.IntegerType())\n        )\n        .withColumn(\n            \"end\", f.split(f.col(\"location\"), \":|-\")[2].cast(t.IntegerType())\n        )\n        # Select relevant columns:\n        .select(\"chrom\", \"start\", \"end\", \"gene_symbol\", \"score\")\n        # Drop rows with non-canonical chromosomes:\n        .filter(\n            f.col(\"chrom\").isin([str(x) for x in range(1, 23)] + [\"X\", \"Y\", \"MT\"])\n        )\n        # For each region/gene, keep only one row with the highest score:\n        .groupBy(\"chrom\", \"start\", \"end\", \"gene_symbol\")\n        .agg(f.max(\"score\").alias(\"resourceScore\"))\n        .orderBy(\"chrom\", \"start\")\n    )\n\n    return Intervals(\n        _df=(\n            # Lift over the intervals:\n            lift.convert_intervals(parsed_anderson_df, \"chrom\", \"start\", \"end\")\n            .drop(\"start\", \"end\")\n            .withColumnRenamed(\"mapped_start\", \"start\")\n            .withColumnRenamed(\"mapped_end\", \"end\")\n            .distinct()\n            # Joining with the target index\n            .alias(\"intervals\")\n            .join(\n                target_index.symbols_lut().alias(\"genes\"),\n                on=[\n                    f.col(\"intervals.gene_symbol\") == f.col(\"genes.geneSymbol\"),\n                    # Drop rows where the TSS is far from the start of the region\n                    f.abs(\n                        (f.col(\"intervals.start\") + f.col(\"intervals.end\")) / 2\n                        - f.col(\"tss\")\n                    )\n                    &lt;= twosided_threshold,\n                ],\n                how=\"left\",\n            )\n            # Select relevant columns:\n            .select(\n                f.col(\"chrom\").alias(\"chromosome\"),\n                f.col(\"intervals.start\").alias(\"start\"),\n                f.col(\"intervals.end\").alias(\"end\"),\n                \"geneId\",\n                \"resourceScore\",\n                f.lit(dataset_name).alias(\"datasourceId\"),\n                f.lit(experiment_type).alias(\"datatypeId\"),\n                f.lit(pmid).alias(\"pmid\"),\n                f.lit(bio_feature).alias(\"biofeature\"),\n            )\n        ),\n        _schema=Intervals.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/andersson/#gentropy.datasource.intervals.andersson.IntervalsAndersson.read","title":"<code>read(spark: SparkSession, path: str) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>Read andersson2014 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>path</code> <code>str</code> <p>Path to the dataset</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Raw Andersson et al. dataframe</p> Source code in <code>src/gentropy/datasource/intervals/andersson.py</code> <pre><code>@staticmethod\ndef read(spark: SparkSession, path: str) -&gt; DataFrame:\n    \"\"\"Read andersson2014 dataset.\n\n    Args:\n        spark (SparkSession): Spark session\n        path (str): Path to the dataset\n\n    Returns:\n        DataFrame: Raw Andersson et al. dataframe\n    \"\"\"\n    input_schema = t.StructType.fromJson(\n        json.loads(\n            pkg_resources.read_text(schemas, \"andersson2014.json\", encoding=\"utf-8\")\n        )\n    )\n    return (\n        spark.read.option(\"delimiter\", \"\\t\")\n        .option(\"mode\", \"DROPMALFORMED\")\n        .option(\"header\", \"true\")\n        .schema(input_schema)\n        .csv(path)\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/javierre/","title":"Javierre et al.","text":""},{"location":"python_api/datasources/intervals/javierre/#gentropy.datasource.intervals.javierre.IntervalsJavierre","title":"<code>gentropy.datasource.intervals.javierre.IntervalsJavierre</code>","text":"<p>Interval dataset from Javierre et al. 2016.</p> Source code in <code>src/gentropy/datasource/intervals/javierre.py</code> <pre><code>class IntervalsJavierre:\n    \"\"\"Interval dataset from Javierre et al. 2016.\"\"\"\n\n    @staticmethod\n    def read(spark: SparkSession, path: str) -&gt; DataFrame:\n        \"\"\"Read Javierre dataset.\n\n        Args:\n            spark (SparkSession): Spark session\n            path (str): Path to dataset\n\n        Returns:\n            DataFrame: Raw Javierre dataset\n        \"\"\"\n        return spark.read.parquet(path)\n\n    @classmethod\n    def parse(\n        cls: type[IntervalsJavierre],\n        javierre_raw: DataFrame,\n        target_index: TargetIndex,\n        lift: LiftOverSpark,\n    ) -&gt; Intervals:\n        \"\"\"Parse Javierre et al. 2016 dataset.\n\n        Args:\n            javierre_raw (DataFrame): Raw Javierre data\n            target_index (TargetIndex): Target index\n            lift (LiftOverSpark): LiftOverSpark instance\n\n        Returns:\n            Intervals: Javierre et al. 2016 interval data\n        \"\"\"\n        # Constant values:\n        dataset_name = \"javierre2016\"\n        experiment_type = \"pchic\"\n        pmid = \"27863249\"\n        twosided_threshold = 2.45e6\n\n        # Read Javierre data:\n        javierre_parsed = (\n            javierre_raw\n            # Splitting name column into chromosome, start, end, and score:\n            .withColumn(\"name_split\", f.split(f.col(\"name\"), r\":|-|,\"))\n            .withColumn(\n                \"name_chr\",\n                f.regexp_replace(f.col(\"name_split\")[0], \"chr\", \"\").cast(\n                    t.StringType()\n                ),\n            )\n            .withColumn(\"name_start\", f.col(\"name_split\")[1].cast(t.IntegerType()))\n            .withColumn(\"name_end\", f.col(\"name_split\")[2].cast(t.IntegerType()))\n            .withColumn(\"name_score\", f.col(\"name_split\")[3].cast(t.FloatType()))\n            # Cleaning up chromosome:\n            .withColumn(\n                \"chrom\",\n                f.regexp_replace(f.col(\"chrom\"), \"chr\", \"\").cast(t.StringType()),\n            )\n            .drop(\"name_split\", \"name\", \"annotation\")\n            # Keep canonical chromosomes and consistent chromosomes with scores:\n            .filter(\n                (f.col(\"name_score\").isNotNull())\n                &amp; (f.col(\"chrom\") == f.col(\"name_chr\"))\n                &amp; f.col(\"name_chr\").isin(\n                    [f\"{x}\" for x in range(1, 23)] + [\"X\", \"Y\", \"MT\"]\n                )\n            )\n        )\n\n        # Lifting over intervals:\n        javierre_remapped = (\n            javierre_parsed\n            # Lifting over to GRCh38 interval 1:\n            .transform(lambda df: lift.convert_intervals(df, \"chrom\", \"start\", \"end\"))\n            .drop(\"start\", \"end\")\n            .withColumnRenamed(\"mapped_chrom\", \"chrom\")\n            .withColumnRenamed(\"mapped_start\", \"start\")\n            .withColumnRenamed(\"mapped_end\", \"end\")\n            # Lifting over interval 2 to GRCh38:\n            .transform(\n                lambda df: lift.convert_intervals(\n                    df, \"name_chr\", \"name_start\", \"name_end\"\n                )\n            )\n            .drop(\"name_start\", \"name_end\")\n            .withColumnRenamed(\"mapped_name_chr\", \"name_chr\")\n            .withColumnRenamed(\"mapped_name_start\", \"name_start\")\n            .withColumnRenamed(\"mapped_name_end\", \"name_end\")\n        )\n\n        # Once the intervals are lifted, extracting the unique intervals:\n        unique_intervals_with_genes = (\n            javierre_remapped.select(\n                f.col(\"chrom\"),\n                f.col(\"start\").cast(t.IntegerType()),\n                f.col(\"end\").cast(t.IntegerType()),\n            )\n            .distinct()\n            .alias(\"intervals\")\n            .join(\n                target_index.locations_lut().alias(\"genes\"),\n                on=[\n                    f.col(\"intervals.chrom\") == f.col(\"genes.chromosome\"),\n                    (\n                        (f.col(\"intervals.start\") &gt;= f.col(\"genes.start\"))\n                        &amp; (f.col(\"intervals.start\") &lt;= f.col(\"genes.end\"))\n                    )\n                    | (\n                        (f.col(\"intervals.end\") &gt;= f.col(\"genes.start\"))\n                        &amp; (f.col(\"intervals.end\") &lt;= f.col(\"genes.end\"))\n                    ),\n                ],\n                how=\"left\",\n            )\n            .select(\n                f.col(\"intervals.chrom\").alias(\"chrom\"),\n                f.col(\"intervals.start\").alias(\"start\"),\n                f.col(\"intervals.end\").alias(\"end\"),\n                f.col(\"genes.geneId\").alias(\"geneId\"),\n                f.col(\"genes.tss\").alias(\"tss\"),\n            )\n        )\n\n        # Joining back the data:\n        return Intervals(\n            _df=(\n                javierre_remapped.join(\n                    unique_intervals_with_genes,\n                    on=[\"chrom\", \"start\", \"end\"],\n                    how=\"left\",\n                )\n                .filter(\n                    # Drop rows where the TSS is far from the start of the region\n                    f.abs((f.col(\"start\") + f.col(\"end\")) / 2 - f.col(\"tss\"))\n                    &lt;= twosided_threshold\n                )\n                # For each gene, keep only the highest scoring interval:\n                .groupBy(\"name_chr\", \"name_start\", \"name_end\", \"geneId\", \"bio_feature\")\n                .agg(f.max(f.col(\"name_score\")).alias(\"resourceScore\"))\n                # Create the output:\n                .select(\n                    f.col(\"name_chr\").alias(\"chromosome\"),\n                    f.col(\"name_start\").alias(\"start\"),\n                    f.col(\"name_end\").alias(\"end\"),\n                    f.col(\"resourceScore\").cast(t.DoubleType()),\n                    f.col(\"geneId\"),\n                    f.col(\"bio_feature\").alias(\"biofeature\"),\n                    f.lit(dataset_name).alias(\"datasourceId\"),\n                    f.lit(experiment_type).alias(\"datatypeId\"),\n                    f.lit(pmid).alias(\"pmid\"),\n                )\n            ),\n            _schema=Intervals.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/intervals/javierre/#gentropy.datasource.intervals.javierre.IntervalsJavierre.parse","title":"<code>parse(javierre_raw: DataFrame, target_index: TargetIndex, lift: LiftOverSpark) -&gt; Intervals</code>  <code>classmethod</code>","text":"<p>Parse Javierre et al. 2016 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>javierre_raw</code> <code>DataFrame</code> <p>Raw Javierre data</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index</p> required <code>lift</code> <code>LiftOverSpark</code> <p>LiftOverSpark instance</p> required <p>Returns:</p> Name Type Description <code>Intervals</code> <code>Intervals</code> <p>Javierre et al. 2016 interval data</p> Source code in <code>src/gentropy/datasource/intervals/javierre.py</code> <pre><code>@classmethod\ndef parse(\n    cls: type[IntervalsJavierre],\n    javierre_raw: DataFrame,\n    target_index: TargetIndex,\n    lift: LiftOverSpark,\n) -&gt; Intervals:\n    \"\"\"Parse Javierre et al. 2016 dataset.\n\n    Args:\n        javierre_raw (DataFrame): Raw Javierre data\n        target_index (TargetIndex): Target index\n        lift (LiftOverSpark): LiftOverSpark instance\n\n    Returns:\n        Intervals: Javierre et al. 2016 interval data\n    \"\"\"\n    # Constant values:\n    dataset_name = \"javierre2016\"\n    experiment_type = \"pchic\"\n    pmid = \"27863249\"\n    twosided_threshold = 2.45e6\n\n    # Read Javierre data:\n    javierre_parsed = (\n        javierre_raw\n        # Splitting name column into chromosome, start, end, and score:\n        .withColumn(\"name_split\", f.split(f.col(\"name\"), r\":|-|,\"))\n        .withColumn(\n            \"name_chr\",\n            f.regexp_replace(f.col(\"name_split\")[0], \"chr\", \"\").cast(\n                t.StringType()\n            ),\n        )\n        .withColumn(\"name_start\", f.col(\"name_split\")[1].cast(t.IntegerType()))\n        .withColumn(\"name_end\", f.col(\"name_split\")[2].cast(t.IntegerType()))\n        .withColumn(\"name_score\", f.col(\"name_split\")[3].cast(t.FloatType()))\n        # Cleaning up chromosome:\n        .withColumn(\n            \"chrom\",\n            f.regexp_replace(f.col(\"chrom\"), \"chr\", \"\").cast(t.StringType()),\n        )\n        .drop(\"name_split\", \"name\", \"annotation\")\n        # Keep canonical chromosomes and consistent chromosomes with scores:\n        .filter(\n            (f.col(\"name_score\").isNotNull())\n            &amp; (f.col(\"chrom\") == f.col(\"name_chr\"))\n            &amp; f.col(\"name_chr\").isin(\n                [f\"{x}\" for x in range(1, 23)] + [\"X\", \"Y\", \"MT\"]\n            )\n        )\n    )\n\n    # Lifting over intervals:\n    javierre_remapped = (\n        javierre_parsed\n        # Lifting over to GRCh38 interval 1:\n        .transform(lambda df: lift.convert_intervals(df, \"chrom\", \"start\", \"end\"))\n        .drop(\"start\", \"end\")\n        .withColumnRenamed(\"mapped_chrom\", \"chrom\")\n        .withColumnRenamed(\"mapped_start\", \"start\")\n        .withColumnRenamed(\"mapped_end\", \"end\")\n        # Lifting over interval 2 to GRCh38:\n        .transform(\n            lambda df: lift.convert_intervals(\n                df, \"name_chr\", \"name_start\", \"name_end\"\n            )\n        )\n        .drop(\"name_start\", \"name_end\")\n        .withColumnRenamed(\"mapped_name_chr\", \"name_chr\")\n        .withColumnRenamed(\"mapped_name_start\", \"name_start\")\n        .withColumnRenamed(\"mapped_name_end\", \"name_end\")\n    )\n\n    # Once the intervals are lifted, extracting the unique intervals:\n    unique_intervals_with_genes = (\n        javierre_remapped.select(\n            f.col(\"chrom\"),\n            f.col(\"start\").cast(t.IntegerType()),\n            f.col(\"end\").cast(t.IntegerType()),\n        )\n        .distinct()\n        .alias(\"intervals\")\n        .join(\n            target_index.locations_lut().alias(\"genes\"),\n            on=[\n                f.col(\"intervals.chrom\") == f.col(\"genes.chromosome\"),\n                (\n                    (f.col(\"intervals.start\") &gt;= f.col(\"genes.start\"))\n                    &amp; (f.col(\"intervals.start\") &lt;= f.col(\"genes.end\"))\n                )\n                | (\n                    (f.col(\"intervals.end\") &gt;= f.col(\"genes.start\"))\n                    &amp; (f.col(\"intervals.end\") &lt;= f.col(\"genes.end\"))\n                ),\n            ],\n            how=\"left\",\n        )\n        .select(\n            f.col(\"intervals.chrom\").alias(\"chrom\"),\n            f.col(\"intervals.start\").alias(\"start\"),\n            f.col(\"intervals.end\").alias(\"end\"),\n            f.col(\"genes.geneId\").alias(\"geneId\"),\n            f.col(\"genes.tss\").alias(\"tss\"),\n        )\n    )\n\n    # Joining back the data:\n    return Intervals(\n        _df=(\n            javierre_remapped.join(\n                unique_intervals_with_genes,\n                on=[\"chrom\", \"start\", \"end\"],\n                how=\"left\",\n            )\n            .filter(\n                # Drop rows where the TSS is far from the start of the region\n                f.abs((f.col(\"start\") + f.col(\"end\")) / 2 - f.col(\"tss\"))\n                &lt;= twosided_threshold\n            )\n            # For each gene, keep only the highest scoring interval:\n            .groupBy(\"name_chr\", \"name_start\", \"name_end\", \"geneId\", \"bio_feature\")\n            .agg(f.max(f.col(\"name_score\")).alias(\"resourceScore\"))\n            # Create the output:\n            .select(\n                f.col(\"name_chr\").alias(\"chromosome\"),\n                f.col(\"name_start\").alias(\"start\"),\n                f.col(\"name_end\").alias(\"end\"),\n                f.col(\"resourceScore\").cast(t.DoubleType()),\n                f.col(\"geneId\"),\n                f.col(\"bio_feature\").alias(\"biofeature\"),\n                f.lit(dataset_name).alias(\"datasourceId\"),\n                f.lit(experiment_type).alias(\"datatypeId\"),\n                f.lit(pmid).alias(\"pmid\"),\n            )\n        ),\n        _schema=Intervals.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/javierre/#gentropy.datasource.intervals.javierre.IntervalsJavierre.read","title":"<code>read(spark: SparkSession, path: str) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>Read Javierre dataset.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>path</code> <code>str</code> <p>Path to dataset</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Raw Javierre dataset</p> Source code in <code>src/gentropy/datasource/intervals/javierre.py</code> <pre><code>@staticmethod\ndef read(spark: SparkSession, path: str) -&gt; DataFrame:\n    \"\"\"Read Javierre dataset.\n\n    Args:\n        spark (SparkSession): Spark session\n        path (str): Path to dataset\n\n    Returns:\n        DataFrame: Raw Javierre dataset\n    \"\"\"\n    return spark.read.parquet(path)\n</code></pre>"},{"location":"python_api/datasources/intervals/jung/","title":"Jung et al.","text":""},{"location":"python_api/datasources/intervals/jung/#gentropy.datasource.intervals.jung.IntervalsJung","title":"<code>gentropy.datasource.intervals.jung.IntervalsJung</code>","text":"<p>Interval dataset from Jung et al. 2019.</p> Source code in <code>src/gentropy/datasource/intervals/jung.py</code> <pre><code>class IntervalsJung:\n    \"\"\"Interval dataset from Jung et al. 2019.\"\"\"\n\n    @staticmethod\n    def read(spark: SparkSession, path: str) -&gt; DataFrame:\n        \"\"\"Read jung dataset.\n\n        Args:\n            spark (SparkSession): Spark session\n            path (str): Path to dataset\n\n        Returns:\n            DataFrame: DataFrame with raw jung data\n        \"\"\"\n        return spark.read.csv(path, sep=\",\", header=True)\n\n    @classmethod\n    def parse(\n        cls: type[IntervalsJung],\n        jung_raw: DataFrame,\n        target_index: TargetIndex,\n        lift: LiftOverSpark,\n    ) -&gt; Intervals:\n        \"\"\"Parse the Jung et al. 2019 dataset.\n\n        Args:\n            jung_raw (DataFrame): raw Jung et al. 2019 dataset\n            target_index (TargetIndex): Target index\n            lift (LiftOverSpark): LiftOverSpark instance\n\n        Returns:\n            Intervals: Interval dataset containing Jung et al. 2019 data\n        \"\"\"\n        dataset_name = \"jung2019\"\n        experiment_type = \"pchic\"\n        pmid = \"31501517\"\n\n        # Lifting over the coordinates:\n        return Intervals(\n            _df=(\n                jung_raw.withColumn(\n                    \"interval\", f.split(f.col(\"Interacting_fragment\"), r\"\\.\")\n                )\n                .select(\n                    # Parsing intervals:\n                    f.regexp_replace(f.col(\"interval\")[0], \"chr\", \"\").alias(\"chrom\"),\n                    f.col(\"interval\")[1].cast(t.IntegerType()).alias(\"start\"),\n                    f.col(\"interval\")[2].cast(t.IntegerType()).alias(\"end\"),\n                    # Extract other columns:\n                    f.col(\"Promoter\").alias(\"gene_name\"),\n                    f.col(\"Tissue_type\").alias(\"tissue\"),\n                )\n                # Lifting over to GRCh38 interval 1:\n                .transform(\n                    lambda df: lift.convert_intervals(df, \"chrom\", \"start\", \"end\")\n                )\n                .select(\n                    \"chrom\",\n                    f.col(\"mapped_start\").alias(\"start\"),\n                    f.col(\"mapped_end\").alias(\"end\"),\n                    f.explode(f.split(f.col(\"gene_name\"), \";\")).alias(\"gene_name\"),\n                    \"tissue\",\n                )\n                .alias(\"intervals\")\n                # Joining with genes:\n                .join(\n                    target_index.symbols_lut().alias(\"genes\"),\n                    on=[f.col(\"intervals.gene_name\") == f.col(\"genes.geneSymbol\")],\n                    how=\"inner\",\n                )\n                # Finalize dataset:\n                .select(\n                    \"chromosome\",\n                    f.col(\"intervals.start\").alias(\"start\"),\n                    f.col(\"intervals.end\").alias(\"end\"),\n                    \"geneId\",\n                    f.col(\"tissue\").alias(\"biofeature\"),\n                    f.lit(1.0).alias(\"score\"),\n                    f.lit(dataset_name).alias(\"datasourceId\"),\n                    f.lit(experiment_type).alias(\"datatypeId\"),\n                    f.lit(pmid).alias(\"pmid\"),\n                )\n                .drop_duplicates()\n            ),\n            _schema=Intervals.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/intervals/jung/#gentropy.datasource.intervals.jung.IntervalsJung.parse","title":"<code>parse(jung_raw: DataFrame, target_index: TargetIndex, lift: LiftOverSpark) -&gt; Intervals</code>  <code>classmethod</code>","text":"<p>Parse the Jung et al. 2019 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>jung_raw</code> <code>DataFrame</code> <p>raw Jung et al. 2019 dataset</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index</p> required <code>lift</code> <code>LiftOverSpark</code> <p>LiftOverSpark instance</p> required <p>Returns:</p> Name Type Description <code>Intervals</code> <code>Intervals</code> <p>Interval dataset containing Jung et al. 2019 data</p> Source code in <code>src/gentropy/datasource/intervals/jung.py</code> <pre><code>@classmethod\ndef parse(\n    cls: type[IntervalsJung],\n    jung_raw: DataFrame,\n    target_index: TargetIndex,\n    lift: LiftOverSpark,\n) -&gt; Intervals:\n    \"\"\"Parse the Jung et al. 2019 dataset.\n\n    Args:\n        jung_raw (DataFrame): raw Jung et al. 2019 dataset\n        target_index (TargetIndex): Target index\n        lift (LiftOverSpark): LiftOverSpark instance\n\n    Returns:\n        Intervals: Interval dataset containing Jung et al. 2019 data\n    \"\"\"\n    dataset_name = \"jung2019\"\n    experiment_type = \"pchic\"\n    pmid = \"31501517\"\n\n    # Lifting over the coordinates:\n    return Intervals(\n        _df=(\n            jung_raw.withColumn(\n                \"interval\", f.split(f.col(\"Interacting_fragment\"), r\"\\.\")\n            )\n            .select(\n                # Parsing intervals:\n                f.regexp_replace(f.col(\"interval\")[0], \"chr\", \"\").alias(\"chrom\"),\n                f.col(\"interval\")[1].cast(t.IntegerType()).alias(\"start\"),\n                f.col(\"interval\")[2].cast(t.IntegerType()).alias(\"end\"),\n                # Extract other columns:\n                f.col(\"Promoter\").alias(\"gene_name\"),\n                f.col(\"Tissue_type\").alias(\"tissue\"),\n            )\n            # Lifting over to GRCh38 interval 1:\n            .transform(\n                lambda df: lift.convert_intervals(df, \"chrom\", \"start\", \"end\")\n            )\n            .select(\n                \"chrom\",\n                f.col(\"mapped_start\").alias(\"start\"),\n                f.col(\"mapped_end\").alias(\"end\"),\n                f.explode(f.split(f.col(\"gene_name\"), \";\")).alias(\"gene_name\"),\n                \"tissue\",\n            )\n            .alias(\"intervals\")\n            # Joining with genes:\n            .join(\n                target_index.symbols_lut().alias(\"genes\"),\n                on=[f.col(\"intervals.gene_name\") == f.col(\"genes.geneSymbol\")],\n                how=\"inner\",\n            )\n            # Finalize dataset:\n            .select(\n                \"chromosome\",\n                f.col(\"intervals.start\").alias(\"start\"),\n                f.col(\"intervals.end\").alias(\"end\"),\n                \"geneId\",\n                f.col(\"tissue\").alias(\"biofeature\"),\n                f.lit(1.0).alias(\"score\"),\n                f.lit(dataset_name).alias(\"datasourceId\"),\n                f.lit(experiment_type).alias(\"datatypeId\"),\n                f.lit(pmid).alias(\"pmid\"),\n            )\n            .drop_duplicates()\n        ),\n        _schema=Intervals.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/jung/#gentropy.datasource.intervals.jung.IntervalsJung.read","title":"<code>read(spark: SparkSession, path: str) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>Read jung dataset.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>path</code> <code>str</code> <p>Path to dataset</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with raw jung data</p> Source code in <code>src/gentropy/datasource/intervals/jung.py</code> <pre><code>@staticmethod\ndef read(spark: SparkSession, path: str) -&gt; DataFrame:\n    \"\"\"Read jung dataset.\n\n    Args:\n        spark (SparkSession): Spark session\n        path (str): Path to dataset\n\n    Returns:\n        DataFrame: DataFrame with raw jung data\n    \"\"\"\n    return spark.read.csv(path, sep=\",\", header=True)\n</code></pre>"},{"location":"python_api/datasources/intervals/thurman/","title":"Thurman et al.","text":""},{"location":"python_api/datasources/intervals/thurman/#gentropy.datasource.intervals.thurman.IntervalsThurman","title":"<code>gentropy.datasource.intervals.thurman.IntervalsThurman</code>","text":"<p>Interval dataset from Thurman et al. 2012.</p> Source code in <code>src/gentropy/datasource/intervals/thurman.py</code> <pre><code>class IntervalsThurman:\n    \"\"\"Interval dataset from Thurman et al. 2012.\"\"\"\n\n    @staticmethod\n    def read(spark: SparkSession, path: str) -&gt; DataFrame:\n        \"\"\"Read thurman dataset.\n\n        Args:\n            spark (SparkSession): Spark session\n            path (str): Path to dataset\n\n        Returns:\n            DataFrame: DataFrame with raw thurman data\n        \"\"\"\n        thurman_schema = t.StructType(\n            [\n                t.StructField(\"gene_chr\", t.StringType(), False),\n                t.StructField(\"gene_start\", t.IntegerType(), False),\n                t.StructField(\"gene_end\", t.IntegerType(), False),\n                t.StructField(\"gene_name\", t.StringType(), False),\n                t.StructField(\"chrom\", t.StringType(), False),\n                t.StructField(\"start\", t.IntegerType(), False),\n                t.StructField(\"end\", t.IntegerType(), False),\n                t.StructField(\"score\", t.FloatType(), False),\n            ]\n        )\n        return spark.read.csv(path, sep=\"\\t\", header=False, schema=thurman_schema)\n\n    @classmethod\n    def parse(\n        cls: type[IntervalsThurman],\n        thurman_raw: DataFrame,\n        target_index: TargetIndex,\n        lift: LiftOverSpark,\n    ) -&gt; Intervals:\n        \"\"\"Parse the Thurman et al. 2012 dataset.\n\n        Args:\n            thurman_raw (DataFrame): raw Thurman et al. 2019 dataset\n            target_index (TargetIndex): Target index\n            lift (LiftOverSpark): LiftOverSpark instance\n\n        Returns:\n            Intervals: Interval dataset containing Thurman et al. 2012 data\n        \"\"\"\n        dataset_name = \"thurman2012\"\n        experiment_type = \"dhscor\"\n        pmid = \"22955617\"\n\n        return Intervals(\n            _df=(\n                thurman_raw.select(\n                    f.regexp_replace(f.col(\"chrom\"), \"chr\", \"\").alias(\"chrom\"),\n                    \"start\",\n                    \"end\",\n                    \"gene_name\",\n                    \"score\",\n                )\n                # Lift over to the GRCh38 build:\n                .transform(\n                    lambda df: lift.convert_intervals(df, \"chrom\", \"start\", \"end\")\n                )\n                .alias(\"intervals\")\n                # Map gene names to gene IDs:\n                .join(\n                    target_index.symbols_lut().alias(\"genes\"),\n                    on=[\n                        f.col(\"intervals.gene_name\") == f.col(\"genes.geneSymbol\"),\n                        f.col(\"intervals.chrom\") == f.col(\"genes.chromosome\"),\n                    ],\n                    how=\"inner\",\n                )\n                # Select relevant columns and add constant columns:\n                .select(\n                    f.col(\"chrom\").alias(\"chromosome\"),\n                    f.col(\"mapped_start\").alias(\"start\"),\n                    f.col(\"mapped_end\").alias(\"end\"),\n                    \"geneId\",\n                    f.col(\"score\").cast(t.DoubleType()).alias(\"resourceScore\"),\n                    f.lit(dataset_name).alias(\"datasourceId\"),\n                    f.lit(experiment_type).alias(\"datatypeId\"),\n                    f.lit(pmid).alias(\"pmid\"),\n                )\n                .distinct()\n            ),\n            _schema=Intervals.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/intervals/thurman/#gentropy.datasource.intervals.thurman.IntervalsThurman.parse","title":"<code>parse(thurman_raw: DataFrame, target_index: TargetIndex, lift: LiftOverSpark) -&gt; Intervals</code>  <code>classmethod</code>","text":"<p>Parse the Thurman et al. 2012 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>thurman_raw</code> <code>DataFrame</code> <p>raw Thurman et al. 2019 dataset</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index</p> required <code>lift</code> <code>LiftOverSpark</code> <p>LiftOverSpark instance</p> required <p>Returns:</p> Name Type Description <code>Intervals</code> <code>Intervals</code> <p>Interval dataset containing Thurman et al. 2012 data</p> Source code in <code>src/gentropy/datasource/intervals/thurman.py</code> <pre><code>@classmethod\ndef parse(\n    cls: type[IntervalsThurman],\n    thurman_raw: DataFrame,\n    target_index: TargetIndex,\n    lift: LiftOverSpark,\n) -&gt; Intervals:\n    \"\"\"Parse the Thurman et al. 2012 dataset.\n\n    Args:\n        thurman_raw (DataFrame): raw Thurman et al. 2019 dataset\n        target_index (TargetIndex): Target index\n        lift (LiftOverSpark): LiftOverSpark instance\n\n    Returns:\n        Intervals: Interval dataset containing Thurman et al. 2012 data\n    \"\"\"\n    dataset_name = \"thurman2012\"\n    experiment_type = \"dhscor\"\n    pmid = \"22955617\"\n\n    return Intervals(\n        _df=(\n            thurman_raw.select(\n                f.regexp_replace(f.col(\"chrom\"), \"chr\", \"\").alias(\"chrom\"),\n                \"start\",\n                \"end\",\n                \"gene_name\",\n                \"score\",\n            )\n            # Lift over to the GRCh38 build:\n            .transform(\n                lambda df: lift.convert_intervals(df, \"chrom\", \"start\", \"end\")\n            )\n            .alias(\"intervals\")\n            # Map gene names to gene IDs:\n            .join(\n                target_index.symbols_lut().alias(\"genes\"),\n                on=[\n                    f.col(\"intervals.gene_name\") == f.col(\"genes.geneSymbol\"),\n                    f.col(\"intervals.chrom\") == f.col(\"genes.chromosome\"),\n                ],\n                how=\"inner\",\n            )\n            # Select relevant columns and add constant columns:\n            .select(\n                f.col(\"chrom\").alias(\"chromosome\"),\n                f.col(\"mapped_start\").alias(\"start\"),\n                f.col(\"mapped_end\").alias(\"end\"),\n                \"geneId\",\n                f.col(\"score\").cast(t.DoubleType()).alias(\"resourceScore\"),\n                f.lit(dataset_name).alias(\"datasourceId\"),\n                f.lit(experiment_type).alias(\"datatypeId\"),\n                f.lit(pmid).alias(\"pmid\"),\n            )\n            .distinct()\n        ),\n        _schema=Intervals.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/thurman/#gentropy.datasource.intervals.thurman.IntervalsThurman.read","title":"<code>read(spark: SparkSession, path: str) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>Read thurman dataset.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>path</code> <code>str</code> <p>Path to dataset</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with raw thurman data</p> Source code in <code>src/gentropy/datasource/intervals/thurman.py</code> <pre><code>@staticmethod\ndef read(spark: SparkSession, path: str) -&gt; DataFrame:\n    \"\"\"Read thurman dataset.\n\n    Args:\n        spark (SparkSession): Spark session\n        path (str): Path to dataset\n\n    Returns:\n        DataFrame: DataFrame with raw thurman data\n    \"\"\"\n    thurman_schema = t.StructType(\n        [\n            t.StructField(\"gene_chr\", t.StringType(), False),\n            t.StructField(\"gene_start\", t.IntegerType(), False),\n            t.StructField(\"gene_end\", t.IntegerType(), False),\n            t.StructField(\"gene_name\", t.StringType(), False),\n            t.StructField(\"chrom\", t.StringType(), False),\n            t.StructField(\"start\", t.IntegerType(), False),\n            t.StructField(\"end\", t.IntegerType(), False),\n            t.StructField(\"score\", t.FloatType(), False),\n        ]\n    )\n    return spark.read.csv(path, sep=\"\\t\", header=False, schema=thurman_schema)\n</code></pre>"},{"location":"python_api/datasources/open_targets/_open_targets/","title":"Open Targets","text":"<p>The Open Targets Platform is a comprehensive resource that aims to aggregate and harmonize various types of data to facilitate the identification, prioritization, and validation of drug targets. By integrating publicly available datasets, including data generated by the Open Targets consortium, the Platform builds and scores target-disease associations to assist in drug target identification and prioritization. It also integrates relevant annotation information about targets, diseases, phenotypes, and drugs, as well as their most relevant relationships.</p> <p>Within our analyses, we utilize Open Targets to infer two datasets:</p> <ol> <li> <p>The list of targets:    This dataset provides a compilation of targets. In the Open Targets Platform, a target is understood as any naturally-occurring molecule that can be targeted by a medicinal product. The EMBL-EBI Ensembl database serves as the source for human targets in the Platform, with the Ensembl gene ID as the primary identifier. For more details, refer to this link.</p> </li> <li> <p>The list of Gold Standard Positives:    We use this dataset for training the Locus-to-Gene model. The current list contains 496 Gold Standard Positives.</p> </li> </ol>"},{"location":"python_api/datasources/open_targets/l2g_gold_standard/","title":"L2G Gold Standard","text":""},{"location":"python_api/datasources/open_targets/l2g_gold_standard/#gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard","title":"<code>gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard</code>","text":"<p>Parser for OTGenetics locus to gene gold standards curation.</p> The curation is processed to generate a dataset with 2 labels <ul> <li>Gold Standard Positive (GSP): When the lead variant is part of a curated list of GWAS loci with known gene-trait associations.</li> <li>Gold Standard Negative (GSN): When the lead variant is not part of a curated list of GWAS loci with known gene-trait associations but is in the vicinity of a gene's TSS.</li> </ul> Source code in <code>src/gentropy/datasource/open_targets/l2g_gold_standard.py</code> <pre><code>class OpenTargetsL2GGoldStandard:\n    \"\"\"Parser for OTGenetics locus to gene gold standards curation.\n\n    The curation is processed to generate a dataset with 2 labels:\n        - Gold Standard Positive (GSP): When the lead variant is part of a curated list of GWAS loci with known gene-trait associations.\n        - Gold Standard Negative (GSN): When the lead variant is not part of a curated list of GWAS loci with known gene-trait associations but is in the vicinity of a gene's TSS.\n    \"\"\"\n\n    LOCUS_TO_GENE_WINDOW = 500_000\n\n    @classmethod\n    def parse_positive_curation(\n        cls: type[OpenTargetsL2GGoldStandard], gold_standard_curation: DataFrame\n    ) -&gt; DataFrame:\n        \"\"\"Parse positive set from gold standard curation.\n\n        Args:\n            gold_standard_curation (DataFrame): Gold standard curation dataframe\n\n        Returns:\n            DataFrame: Positive set\n        \"\"\"\n        return (\n            gold_standard_curation.filter(\n                f.col(\"gold_standard_info.highest_confidence\").isin([\"High\", \"Medium\"])\n            )\n            .select(\n                f.col(\"association_info.otg_id\").alias(\"studyId\"),\n                f.col(\"gold_standard_info.gene_id\").alias(\"geneId\"),\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n                    f.col(\"sentinel_variant.locus_GRCh38.position\"),\n                    f.col(\"sentinel_variant.alleles.reference\"),\n                    f.col(\"sentinel_variant.alleles.alternative\"),\n                ).alias(\"variantId\"),\n                f.col(\"metadata.set_label\").alias(\"source\"),\n            )\n            .withColumn(\n                \"studyLocusId\",\n                StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n            )\n            .groupBy(\"studyLocusId\", \"studyId\", \"variantId\", \"geneId\")\n            .agg(f.collect_set(\"source\").alias(\"sources\"))\n        )\n\n    @classmethod\n    def expand_gold_standard_with_negatives(\n        cls: type[OpenTargetsL2GGoldStandard],\n        positive_set: DataFrame,\n        variant_index: VariantIndex,\n    ) -&gt; DataFrame:\n        \"\"\"Create full set of positive and negative evidence of locus to gene associations.\n\n        Negative evidence consists of all genes within a window of 500kb of the lead variant that are not in the positive set.\n\n        Args:\n            positive_set (DataFrame): Positive set from curation\n            variant_index (VariantIndex): Variant index to get distance to gene\n\n        Returns:\n            DataFrame: Full set of positive and negative evidence of locus to gene associations\n        \"\"\"\n        return (\n            positive_set.withColumnRenamed(\"geneId\", \"curated_geneId\")\n            .join(\n                variant_index.get_distance_to_gene()\n                .selectExpr(\n                    \"variantId\",\n                    \"targetId as non_curated_geneId\",\n                    \"distanceFromTss\",\n                )\n                .filter(f.col(\"distanceFromTss\") &lt;= cls.LOCUS_TO_GENE_WINDOW),\n                on=\"variantId\",\n                how=\"left\",\n            )\n            .withColumn(\n                \"goldStandardSet\",\n                f.when(\n                    (f.col(\"curated_geneId\") == f.col(\"non_curated_geneId\"))\n                    # to keep the positives that are not part of the variant index\n                    | (f.col(\"non_curated_geneId\").isNull()),\n                    f.lit(L2GGoldStandard.GS_POSITIVE_LABEL),\n                ).otherwise(L2GGoldStandard.GS_NEGATIVE_LABEL),\n            )\n            .withColumn(\n                \"geneId\",\n                f.when(\n                    f.col(\"goldStandardSet\") == L2GGoldStandard.GS_POSITIVE_LABEL,\n                    f.col(\"curated_geneId\"),\n                ).otherwise(f.col(\"non_curated_geneId\")),\n            )\n            .drop(\"distanceFromTss\", \"curated_geneId\", \"non_curated_geneId\")\n        )\n\n    @classmethod\n    def as_l2g_gold_standard(\n        cls: type[OpenTargetsL2GGoldStandard],\n        gold_standard_curation: DataFrame,\n        variant_index: VariantIndex,\n    ) -&gt; L2GGoldStandard:\n        \"\"\"Initialise L2GGoldStandard from source dataset.\n\n        Args:\n            gold_standard_curation (DataFrame): Gold standard curation dataframe, extracted from https://github.com/opentargets/genetics-gold-standards\n            variant_index (VariantIndex): Dataset to bring distance between a variant and a gene's footprint\n\n        Returns:\n            L2GGoldStandard: L2G Gold Standard dataset. False negatives have not yet been removed.\n        \"\"\"\n        return L2GGoldStandard(\n            _df=cls.parse_positive_curation(gold_standard_curation).transform(\n                cls.expand_gold_standard_with_negatives, variant_index\n            ),\n            _schema=L2GGoldStandard.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/open_targets/l2g_gold_standard/#gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard.as_l2g_gold_standard","title":"<code>as_l2g_gold_standard(gold_standard_curation: DataFrame, variant_index: VariantIndex) -&gt; L2GGoldStandard</code>  <code>classmethod</code>","text":"<p>Initialise L2GGoldStandard from source dataset.</p> <p>Parameters:</p> Name Type Description Default <code>gold_standard_curation</code> <code>DataFrame</code> <p>Gold standard curation dataframe, extracted from https://github.com/opentargets/genetics-gold-standards</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Dataset to bring distance between a variant and a gene's footprint</p> required <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>L2G Gold Standard dataset. False negatives have not yet been removed.</p> Source code in <code>src/gentropy/datasource/open_targets/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef as_l2g_gold_standard(\n    cls: type[OpenTargetsL2GGoldStandard],\n    gold_standard_curation: DataFrame,\n    variant_index: VariantIndex,\n) -&gt; L2GGoldStandard:\n    \"\"\"Initialise L2GGoldStandard from source dataset.\n\n    Args:\n        gold_standard_curation (DataFrame): Gold standard curation dataframe, extracted from https://github.com/opentargets/genetics-gold-standards\n        variant_index (VariantIndex): Dataset to bring distance between a variant and a gene's footprint\n\n    Returns:\n        L2GGoldStandard: L2G Gold Standard dataset. False negatives have not yet been removed.\n    \"\"\"\n    return L2GGoldStandard(\n        _df=cls.parse_positive_curation(gold_standard_curation).transform(\n            cls.expand_gold_standard_with_negatives, variant_index\n        ),\n        _schema=L2GGoldStandard.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/open_targets/l2g_gold_standard/#gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard.expand_gold_standard_with_negatives","title":"<code>expand_gold_standard_with_negatives(positive_set: DataFrame, variant_index: VariantIndex) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Create full set of positive and negative evidence of locus to gene associations.</p> <p>Negative evidence consists of all genes within a window of 500kb of the lead variant that are not in the positive set.</p> <p>Parameters:</p> Name Type Description Default <code>positive_set</code> <code>DataFrame</code> <p>Positive set from curation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Variant index to get distance to gene</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Full set of positive and negative evidence of locus to gene associations</p> Source code in <code>src/gentropy/datasource/open_targets/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef expand_gold_standard_with_negatives(\n    cls: type[OpenTargetsL2GGoldStandard],\n    positive_set: DataFrame,\n    variant_index: VariantIndex,\n) -&gt; DataFrame:\n    \"\"\"Create full set of positive and negative evidence of locus to gene associations.\n\n    Negative evidence consists of all genes within a window of 500kb of the lead variant that are not in the positive set.\n\n    Args:\n        positive_set (DataFrame): Positive set from curation\n        variant_index (VariantIndex): Variant index to get distance to gene\n\n    Returns:\n        DataFrame: Full set of positive and negative evidence of locus to gene associations\n    \"\"\"\n    return (\n        positive_set.withColumnRenamed(\"geneId\", \"curated_geneId\")\n        .join(\n            variant_index.get_distance_to_gene()\n            .selectExpr(\n                \"variantId\",\n                \"targetId as non_curated_geneId\",\n                \"distanceFromTss\",\n            )\n            .filter(f.col(\"distanceFromTss\") &lt;= cls.LOCUS_TO_GENE_WINDOW),\n            on=\"variantId\",\n            how=\"left\",\n        )\n        .withColumn(\n            \"goldStandardSet\",\n            f.when(\n                (f.col(\"curated_geneId\") == f.col(\"non_curated_geneId\"))\n                # to keep the positives that are not part of the variant index\n                | (f.col(\"non_curated_geneId\").isNull()),\n                f.lit(L2GGoldStandard.GS_POSITIVE_LABEL),\n            ).otherwise(L2GGoldStandard.GS_NEGATIVE_LABEL),\n        )\n        .withColumn(\n            \"geneId\",\n            f.when(\n                f.col(\"goldStandardSet\") == L2GGoldStandard.GS_POSITIVE_LABEL,\n                f.col(\"curated_geneId\"),\n            ).otherwise(f.col(\"non_curated_geneId\")),\n        )\n        .drop(\"distanceFromTss\", \"curated_geneId\", \"non_curated_geneId\")\n    )\n</code></pre>"},{"location":"python_api/datasources/open_targets/l2g_gold_standard/#gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard.parse_positive_curation","title":"<code>parse_positive_curation(gold_standard_curation: DataFrame) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Parse positive set from gold standard curation.</p> <p>Parameters:</p> Name Type Description Default <code>gold_standard_curation</code> <code>DataFrame</code> <p>Gold standard curation dataframe</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Positive set</p> Source code in <code>src/gentropy/datasource/open_targets/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef parse_positive_curation(\n    cls: type[OpenTargetsL2GGoldStandard], gold_standard_curation: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Parse positive set from gold standard curation.\n\n    Args:\n        gold_standard_curation (DataFrame): Gold standard curation dataframe\n\n    Returns:\n        DataFrame: Positive set\n    \"\"\"\n    return (\n        gold_standard_curation.filter(\n            f.col(\"gold_standard_info.highest_confidence\").isin([\"High\", \"Medium\"])\n        )\n        .select(\n            f.col(\"association_info.otg_id\").alias(\"studyId\"),\n            f.col(\"gold_standard_info.gene_id\").alias(\"geneId\"),\n            f.concat_ws(\n                \"_\",\n                f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n                f.col(\"sentinel_variant.locus_GRCh38.position\"),\n                f.col(\"sentinel_variant.alleles.reference\"),\n                f.col(\"sentinel_variant.alleles.alternative\"),\n            ).alias(\"variantId\"),\n            f.col(\"metadata.set_label\").alias(\"source\"),\n        )\n        .withColumn(\n            \"studyLocusId\",\n            StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n        )\n        .groupBy(\"studyLocusId\", \"studyId\", \"variantId\", \"geneId\")\n        .agg(f.collect_set(\"source\").alias(\"sources\"))\n    )\n</code></pre>"},{"location":"python_api/datasources/ukb_ppp_eur/_ukb_ppp_eur/","title":"UK Biobank Pharma Proteomics Project (UKB-PPP) (EUR)","text":"<p>The UKB-PPP is a collaboration between the UK Biobank (UKB) and thirteen biopharmaceutical companies characterising the plasma proteomic profiles of 54,219 UKB participants.</p> <p>The original data is available here: https://www.synapse.org/Synapse:syn51364943/wiki/622119. The associated paper is https://www.nature.com/articles/s41586-023-06592-6.</p>"},{"location":"python_api/datasources/ukb_ppp_eur/study_index/","title":"Study Index","text":""},{"location":"python_api/datasources/ukb_ppp_eur/study_index/#gentropy.datasource.ukb_ppp_eur.study_index.UkbPppEurStudyIndex","title":"<code>gentropy.datasource.ukb_ppp_eur.study_index.UkbPppEurStudyIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>StudyIndex</code></p> <p>Study index dataset from UKB PPP (EUR).</p> Source code in <code>src/gentropy/datasource/ukb_ppp_eur/study_index.py</code> <pre><code>class UkbPppEurStudyIndex(StudyIndex):\n    \"\"\"Study index dataset from UKB PPP (EUR).\"\"\"\n\n    @classmethod\n    def from_source(\n        cls: type[UkbPppEurStudyIndex],\n        spark: SparkSession,\n        raw_study_index_path_from_tsv: str,\n        raw_summary_stats_path: str,\n    ) -&gt; StudyIndex:\n        \"\"\"This function ingests study level metadata from UKB PPP (EUR).\n\n        Args:\n            spark (SparkSession): Spark session object.\n            raw_study_index_path_from_tsv (str): Raw study index path.\n            raw_summary_stats_path (str): Raw summary stats path.\n\n        Returns:\n            StudyIndex: Parsed and annotated UKB PPP (EUR) study table.\n        \"\"\"\n        # In order to populate the nSamples column, we need to peek inside the summary stats dataframe.\n        num_of_samples = (\n            spark.read.parquet(raw_summary_stats_path)\n            .filter(f.col(\"chromosome\") == \"22\")\n            .groupBy(\"studyId\")\n            .agg(f.first(\"N\").cast(\"integer\").alias(\"nSamples\"))\n            .select(\"*\")\n        )\n        # Now we can read the raw study index and complete the processing.\n        study_index_df = (\n            spark.read.csv(raw_study_index_path_from_tsv, sep=\"\\t\", header=True)\n            .select(\n                f.lit(\"pqtl\").alias(\"studyType\"),\n                f.lit(\"UKB_PPP_EUR\").alias(\"projectId\"),\n                f.col(\"_gentropy_study_id\").alias(\"studyId\"),\n                f.col(\"UKBPPP_ProteinID\").alias(\"traitFromSource\"),\n                f.lit(\"UBERON_0001969\").alias(\"biosampleFromSourceId\"),\n                f.col(\"ensembl_id\").alias(\"geneId\"),\n                f.lit(True).alias(\"hasSumstats\"),\n                f.col(\"_gentropy_summary_stats_link\").alias(\"summarystatsLocation\"),\n            )\n            .join(num_of_samples, \"studyId\", \"inner\")\n        )\n        # Add population structure.\n        study_index_df = (\n            study_index_df.withColumn(\n                \"discoverySamples\",\n                f.array(\n                    f.struct(\n                        f.col(\"nSamples\").cast(\"integer\").alias(\"sampleSize\"),\n                        f.lit(\"European\").alias(\"ancestry\"),\n                    )\n                ),\n            )\n            .withColumn(\n                \"ldPopulationStructure\",\n                cls.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n            )\n            .withColumn(\"biosampleFromSourceId\", f.lit(\"UBERON_0001969\"))\n        )\n\n        return StudyIndex(\n            _df=study_index_df,\n            _schema=StudyIndex.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/ukb_ppp_eur/study_index/#gentropy.datasource.ukb_ppp_eur.study_index.UkbPppEurStudyIndex.from_source","title":"<code>from_source(spark: SparkSession, raw_study_index_path_from_tsv: str, raw_summary_stats_path: str) -&gt; StudyIndex</code>  <code>classmethod</code>","text":"<p>This function ingests study level metadata from UKB PPP (EUR).</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>raw_study_index_path_from_tsv</code> <code>str</code> <p>Raw study index path.</p> required <code>raw_summary_stats_path</code> <code>str</code> <p>Raw summary stats path.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Parsed and annotated UKB PPP (EUR) study table.</p> Source code in <code>src/gentropy/datasource/ukb_ppp_eur/study_index.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[UkbPppEurStudyIndex],\n    spark: SparkSession,\n    raw_study_index_path_from_tsv: str,\n    raw_summary_stats_path: str,\n) -&gt; StudyIndex:\n    \"\"\"This function ingests study level metadata from UKB PPP (EUR).\n\n    Args:\n        spark (SparkSession): Spark session object.\n        raw_study_index_path_from_tsv (str): Raw study index path.\n        raw_summary_stats_path (str): Raw summary stats path.\n\n    Returns:\n        StudyIndex: Parsed and annotated UKB PPP (EUR) study table.\n    \"\"\"\n    # In order to populate the nSamples column, we need to peek inside the summary stats dataframe.\n    num_of_samples = (\n        spark.read.parquet(raw_summary_stats_path)\n        .filter(f.col(\"chromosome\") == \"22\")\n        .groupBy(\"studyId\")\n        .agg(f.first(\"N\").cast(\"integer\").alias(\"nSamples\"))\n        .select(\"*\")\n    )\n    # Now we can read the raw study index and complete the processing.\n    study_index_df = (\n        spark.read.csv(raw_study_index_path_from_tsv, sep=\"\\t\", header=True)\n        .select(\n            f.lit(\"pqtl\").alias(\"studyType\"),\n            f.lit(\"UKB_PPP_EUR\").alias(\"projectId\"),\n            f.col(\"_gentropy_study_id\").alias(\"studyId\"),\n            f.col(\"UKBPPP_ProteinID\").alias(\"traitFromSource\"),\n            f.lit(\"UBERON_0001969\").alias(\"biosampleFromSourceId\"),\n            f.col(\"ensembl_id\").alias(\"geneId\"),\n            f.lit(True).alias(\"hasSumstats\"),\n            f.col(\"_gentropy_summary_stats_link\").alias(\"summarystatsLocation\"),\n        )\n        .join(num_of_samples, \"studyId\", \"inner\")\n    )\n    # Add population structure.\n    study_index_df = (\n        study_index_df.withColumn(\n            \"discoverySamples\",\n            f.array(\n                f.struct(\n                    f.col(\"nSamples\").cast(\"integer\").alias(\"sampleSize\"),\n                    f.lit(\"European\").alias(\"ancestry\"),\n                )\n            ),\n        )\n        .withColumn(\n            \"ldPopulationStructure\",\n            cls.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n        )\n        .withColumn(\"biosampleFromSourceId\", f.lit(\"UBERON_0001969\"))\n    )\n\n    return StudyIndex(\n        _df=study_index_df,\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/ukb_ppp_eur/summary_stats/","title":"Summary Statistics","text":""},{"location":"python_api/datasources/ukb_ppp_eur/summary_stats/#gentropy.datasource.ukb_ppp_eur.summary_stats.UkbPppEurSummaryStats","title":"<code>gentropy.datasource.ukb_ppp_eur.summary_stats.UkbPppEurSummaryStats</code>  <code>dataclass</code>","text":"<p>Summary statistics dataset for UKB PPP (EUR).</p> Source code in <code>src/gentropy/datasource/ukb_ppp_eur/summary_stats.py</code> <pre><code>@dataclass\nclass UkbPppEurSummaryStats:\n    \"\"\"Summary statistics dataset for UKB PPP (EUR).\"\"\"\n\n    @classmethod\n    def from_source(\n        cls: type[UkbPppEurSummaryStats],\n        spark: SparkSession,\n        raw_summary_stats_path: str,\n        tmp_variant_annotation_path: str,\n        chromosome: str,\n        study_index_path: str,\n    ) -&gt; SummaryStatistics:\n        \"\"\"Ingest and harmonise all summary stats for UKB PPP (EUR) data.\n\n        Args:\n            spark (SparkSession): Spark session object.\n            raw_summary_stats_path (str): Input raw summary stats path.\n            tmp_variant_annotation_path (str): Input variant annotation dataset path.\n            chromosome (str): Which chromosome to process.\n            study_index_path (str): The path to study index, which is necessary in some cases to populate the sample size column.\n\n        Returns:\n            SummaryStatistics: Processed summary statistics dataset for a given chromosome.\n        \"\"\"\n        df = harmonise_summary_stats(\n            spark,\n            raw_summary_stats_path,\n            tmp_variant_annotation_path,\n            chromosome,\n            colname_position=\"GENPOS\",\n            colname_allele0=\"ALLELE0\",\n            colname_allele1=\"ALLELE1\",\n            colname_a1freq=\"A1FREQ\",\n            colname_info=\"INFO\",\n            colname_beta=\"BETA\",\n            colname_se=\"SE\",\n            colname_mlog10p=\"LOG10P\",\n            colname_n=\"N\",\n        )\n\n        # Create the summary statistics object.\n        return SummaryStatistics(\n            _df=df,\n            _schema=SummaryStatistics.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/ukb_ppp_eur/summary_stats/#gentropy.datasource.ukb_ppp_eur.summary_stats.UkbPppEurSummaryStats.from_source","title":"<code>from_source(spark: SparkSession, raw_summary_stats_path: str, tmp_variant_annotation_path: str, chromosome: str, study_index_path: str) -&gt; SummaryStatistics</code>  <code>classmethod</code>","text":"<p>Ingest and harmonise all summary stats for UKB PPP (EUR) data.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>raw_summary_stats_path</code> <code>str</code> <p>Input raw summary stats path.</p> required <code>tmp_variant_annotation_path</code> <code>str</code> <p>Input variant annotation dataset path.</p> required <code>chromosome</code> <code>str</code> <p>Which chromosome to process.</p> required <code>study_index_path</code> <code>str</code> <p>The path to study index, which is necessary in some cases to populate the sample size column.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>Processed summary statistics dataset for a given chromosome.</p> Source code in <code>src/gentropy/datasource/ukb_ppp_eur/summary_stats.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[UkbPppEurSummaryStats],\n    spark: SparkSession,\n    raw_summary_stats_path: str,\n    tmp_variant_annotation_path: str,\n    chromosome: str,\n    study_index_path: str,\n) -&gt; SummaryStatistics:\n    \"\"\"Ingest and harmonise all summary stats for UKB PPP (EUR) data.\n\n    Args:\n        spark (SparkSession): Spark session object.\n        raw_summary_stats_path (str): Input raw summary stats path.\n        tmp_variant_annotation_path (str): Input variant annotation dataset path.\n        chromosome (str): Which chromosome to process.\n        study_index_path (str): The path to study index, which is necessary in some cases to populate the sample size column.\n\n    Returns:\n        SummaryStatistics: Processed summary statistics dataset for a given chromosome.\n    \"\"\"\n    df = harmonise_summary_stats(\n        spark,\n        raw_summary_stats_path,\n        tmp_variant_annotation_path,\n        chromosome,\n        colname_position=\"GENPOS\",\n        colname_allele0=\"ALLELE0\",\n        colname_allele1=\"ALLELE1\",\n        colname_a1freq=\"A1FREQ\",\n        colname_info=\"INFO\",\n        colname_beta=\"BETA\",\n        colname_se=\"SE\",\n        colname_mlog10p=\"LOG10P\",\n        colname_n=\"N\",\n    )\n\n    # Create the summary statistics object.\n    return SummaryStatistics(\n        _df=df,\n        _schema=SummaryStatistics.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/_methods/","title":"Methods","text":"<p>This section consists of all the methods available in the package. It provides detailed explanations and usage examples for each method. Developers can refer to this section to understand how to use the methods effectively in their code. The list of methods is constantly updated.</p>"},{"location":"python_api/methods/carma/","title":"CARMA","text":"<p>CARMA is the method of the fine-mapping and outlier detection, originally implemented in R (CARMA on GitHub).</p> <p>The full repository for the reimplementation of CARMA in Python can be found here.</p> <p>This is a simplified version of CARMA with the following features:</p> <ol> <li>It uses only Spike-slab effect size priors and Poisson model priors.</li> <li>C++ is re-implemented in Python.</li> <li>The way of storing the configuration list is changed. It uses a string with the list of indexes for causal SNPs instead of a sparse matrix.</li> <li>Fixed bugs in PIP calculation.</li> <li>No credible models.</li> <li>No credible sets, only PIPs.</li> <li>No functional annotations.</li> <li>Removed unnecessary parameters.</li> </ol>"},{"location":"python_api/methods/carma/#gentropy.method.carma.CARMA","title":"<code>gentropy.method.carma.CARMA</code>","text":"<p>Implementation of CARMA outlier detection method.</p> Source code in <code>src/gentropy/method/carma.py</code> <pre><code>class CARMA:\n    \"\"\"Implementation of CARMA outlier detection method.\"\"\"\n\n    @staticmethod\n    def time_limited_CARMA_spike_slab_noEM(\n        z: np.ndarray,\n        ld: np.ndarray,\n        sec_threshold: float = 600,\n        tau: float = 0.04,\n    ) -&gt; dict[str, Any]:\n        \"\"\"The wrapper for the CARMA_spike_slab_noEM function that runs the function in a separate thread and terminates it if it takes too long.\n\n        Args:\n            z (np.ndarray): Numeric vector representing z-scores.\n            ld (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n            sec_threshold (float): The time threshold in seconds.\n            tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix\n\n        Returns:\n            dict[str, Any]: A dictionary containing the following results:\n                - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs or None.\n                - B_list: A dataframe containing the marginal likelihoods and the corresponding model space or None.\n                - Outliers: A list of outlier SNPs or None.\n        \"\"\"\n        # Ignore pandas future warnings\n        warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n        try:\n            # Execute CARMA.CARMA_spike_slab_noEM with a timeout\n            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n                future = executor.submit(\n                    CARMA.CARMA_spike_slab_noEM, z=z, ld=ld, tau=tau\n                )\n                result = future.result(timeout=sec_threshold)\n        except concurrent.futures.TimeoutError:\n            # If execution exceeds the timeout, return None\n            result = {\"PIPs\": None, \"B_list\": None, \"Outliers\": None}\n\n        return result\n\n    @staticmethod\n    def CARMA_spike_slab_noEM(\n        z: np.ndarray,\n        ld: np.ndarray,\n        lambda_val: float = 1,\n        Max_Model_Dim: int = 200_000,\n        all_iter: int = 1,\n        all_inner_iter: int = 10,\n        epsilon_threshold: float = 1e-5,\n        num_causal: int = 10,\n        tau: float = 0.04,\n        outlier_switch: bool = True,\n        outlier_BF_index: float = 1 / 3.2,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Perform CARMA analysis using a Spike-and-Slab prior without Expectation-Maximization (EM).\n\n        Args:\n            z (np.ndarray): Numeric vector representing z-scores.\n            ld (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n            lambda_val (float): Regularization parameter controlling the strength of the L1 penalty.\n            Max_Model_Dim (int): Maximum allowed dimension for the causal models.\n            all_iter (int): The total number of iterations to run the CARMA analysis.\n            all_inner_iter (int): The number of inner iterations in each CARMA iteration.\n            epsilon_threshold (float): Threshold for convergence in CARMA iterations.\n            num_causal (int): Maximal number of causal variants to be selected in the final model.\n            tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix.\n            outlier_switch (bool): Whether to consider outlier detection in the analysis.\n            outlier_BF_index (float): Bayes Factor threshold for identifying outliers.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the following results:\n                - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs.\n                - B_list: A dataframe containing the marginal likelihoods and the corresponding model space.\n                - Outliers: A list of outlier SNPs.\n        \"\"\"\n        p_snp = len(z)\n        epsilon_list = epsilon_threshold * p_snp\n        all_epsilon_threshold = epsilon_threshold * p_snp\n\n        # Zero step\n        all_C_list = CARMA._MCS_modified(\n            z=z,\n            ld_matrix=ld,\n            epsilon=epsilon_list,\n            Max_Model_Dim=Max_Model_Dim,\n            lambda_val=lambda_val,\n            outlier_switch=outlier_switch,\n            tau=tau,\n            num_causal=num_causal,\n            inner_all_iter=all_inner_iter,\n            outlier_BF_index=outlier_BF_index,\n        )\n\n        # Main steps\n        for _ in range(0, all_iter):\n            ac1 = all_C_list[\"B_list\"][\"set_gamma_margin\"]\n            previous_result = np.mean(ac1[0 : round(len(ac1) / 4)])\n\n            all_C_list = CARMA._MCS_modified(\n                z=z,\n                ld_matrix=ld,\n                input_conditional_S_list=all_C_list[\"conditional_S_list\"],\n                Max_Model_Dim=Max_Model_Dim,\n                num_causal=num_causal,\n                epsilon=epsilon_list,\n                outlier_switch=outlier_switch,\n                tau=tau,\n                lambda_val=lambda_val,\n                inner_all_iter=all_inner_iter,\n                outlier_BF_index=outlier_BF_index,\n            )\n\n            ac1 = all_C_list[\"B_list\"][\"set_gamma_margin\"]\n            difference = np.abs(previous_result - np.mean(ac1[0 : round(len(ac1) / 4)]))\n            if difference &lt; all_epsilon_threshold:\n                break\n\n        # Calculate PIPs and Credible Set\n        pip = CARMA._PIP_func(\n            likeli=all_C_list[\"B_list\"][\"set_gamma_margin\"],\n            model_space=all_C_list[\"B_list\"][\"matrix_gamma\"],\n            p=p_snp,\n            num_causal=num_causal,\n        )\n\n        results_list = {\n            \"PIPs\": pip,\n            \"B_list\": all_C_list[\"B_list\"],\n            \"Outliers\": all_C_list[\"conditional_S_list\"],\n        }\n\n        return results_list\n\n    @staticmethod\n    def _ind_normal_sigma_fixed_marginal_fun_indi(\n        zSigmaz_S: np.ndarray, tau: float, p_S: int, det_S: float\n    ) -&gt; float:\n        \"\"\"Internal function for calculating the marginal likelihood of configuration model.\n\n        Args:\n            zSigmaz_S (np.ndarray): The zSigmaz_S value.\n            tau (float): The tau value.\n            p_S (int): The number of SNPs.\n            det_S (float): The det_S value.\n\n        Returns:\n            float: The marginal likelihood of a model.\n\n        Examples:\n            &gt;&gt;&gt; zSigmaz_S = 0.1\n            &gt;&gt;&gt; tau = 1 / 0.05**2\n            &gt;&gt;&gt; p_S = 3\n            &gt;&gt;&gt; det_S = 0.1\n            &gt;&gt;&gt; np.round(CARMA._ind_normal_sigma_fixed_marginal_fun_indi(zSigmaz_S, tau, p_S, det_S),decimals=5)\n            10.18849\n        \"\"\"\n        return p_S / 2.0 * np.log(tau) - 0.5 * np.log(det_S) + zSigmaz_S / 2.0\n\n    @staticmethod\n    def _ind_Normal_fixed_sigma_marginal_external(\n        index_vec_input: np.ndarray,\n        Sigma: np.ndarray,\n        z: np.ndarray,\n        tau: float,\n        p_S: int,\n    ) -&gt; float:\n        \"\"\"Marginal likelihood of configuration model.\n\n        Args:\n            index_vec_input (np.ndarray): The index vector.\n            Sigma (np.ndarray): The Sigma matrix.\n            z (np.ndarray): The z vector.\n            tau (float): The tau value.\n            p_S (int): The number of SNPs.\n\n        Returns:\n            float: The marginal likelihood of a model.\n\n        Examples:\n            &gt;&gt;&gt; index_vec_input = np.array([1, 2])\n            &gt;&gt;&gt; Sigma = np.array([[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]])\n            &gt;&gt;&gt; z = np.array([10, 11, 10])\n            &gt;&gt;&gt; tau = 1\n            &gt;&gt;&gt; p_S = 2\n            &gt;&gt;&gt; np.round(CARMA._ind_Normal_fixed_sigma_marginal_external(index_vec_input, Sigma, z, tau, p_S),decimals=5)\n            43.60579\n        \"\"\"\n        index_vec = index_vec_input - 1\n        Sigma_S = Sigma[np.ix_(index_vec, index_vec)]\n        A = tau * np.eye(p_S)\n\n        det_S = det(Sigma_S + A)\n        Sigma_S_inv = inv(Sigma_S + A)\n\n        sub_z = z[index_vec]\n        zSigmaz_S = np.dot(sub_z.T, np.dot(Sigma_S_inv, sub_z))\n\n        b = CARMA._ind_normal_sigma_fixed_marginal_fun_indi(zSigmaz_S, tau, p_S, det_S)\n\n        results = b\n\n        return results\n\n    @staticmethod\n    def _outlier_ind_Normal_marginal_external(\n        index_vec_input: np.ndarray,\n        Sigma: np.ndarray,\n        z: np.ndarray,\n        tau: float,\n        p_S: int,\n    ) -&gt; float:\n        \"\"\"Likehood of outlier model.\n\n        Args:\n            index_vec_input (np.ndarray): The index vector.\n            Sigma (np.ndarray): The Sigma matrix.\n            z (np.ndarray): The z vector.\n            tau (float): The tau value.\n            p_S (int): The number of SNPs.\n\n        Returns:\n            float: The likelihood of a model.\n\n        Examples:\n            &gt;&gt;&gt; index_vec_input = np.array([1, 2, 3])\n            &gt;&gt;&gt; Sigma = np.array([[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]])\n            &gt;&gt;&gt; z = np.array([0.1, 0.2, 0.3])\n            &gt;&gt;&gt; tau = 1 / 0.05**2\n            &gt;&gt;&gt; p_S = 3\n            &gt;&gt;&gt; np.round(CARMA._outlier_ind_Normal_marginal_external(index_vec_input, Sigma, z, tau, p_S),decimals=5)\n            -8.8497\n        \"\"\"\n        index_vec = index_vec_input - 1\n\n        Sigma_S = Sigma[np.ix_(index_vec, index_vec)]\n        A = tau * np.eye(p_S)\n\n        Sigma_S_I_inv = pinv(Sigma_S + A, rtol=0.00001)\n        Sigma_S_inv = pinv(Sigma_S, rtol=0.00001)\n\n        det_S = np.abs(det(Sigma_S_inv))\n        det_I_S = np.abs(det(Sigma_S_I_inv))\n\n        sub_z = z[index_vec]\n        zSigmaz_S = np.dot(sub_z, np.dot(Sigma_S_inv, sub_z))\n        zSigmaz_I_S = np.dot(sub_z, np.dot(Sigma_S_I_inv, sub_z))\n\n        b = 0.5 * (np.log(det_S) + np.log(det_I_S)) - 0.5 * (zSigmaz_S - zSigmaz_I_S)\n        results = b\n\n        return results\n\n    @staticmethod\n    def _add_function(S_sub: np.ndarray, y: Any) -&gt; np.ndarray:\n        \"\"\"Concatenate two arrays and sort the result.\n\n        Args:\n            S_sub (np.ndarray): The first array.\n            y (Any): The second array.\n\n        Returns:\n            np.ndarray: The concatenated and sorted array.\n\n        Examples:\n            &gt;&gt;&gt; S_sub = np.array([3, 4])\n            &gt;&gt;&gt; y = np.array([1, 2])\n            &gt;&gt;&gt; CARMA._add_function(S_sub, y)\n            array([[1, 2, 3],\n                   [1, 2, 4]])\n        \"\"\"\n        return np.array([np.sort(np.concatenate(([x], y))) for x in S_sub])\n\n    @staticmethod\n    def _set_gamma_func_base(S: Any, p: int) -&gt; dict[int, np.ndarray]:\n        \"\"\"Creates a dictionary of sets of configurations assuming no conditional set.\n\n        Args:\n            S (Any): The input set.\n            p (int): The number of SNPs.\n\n        Returns:\n            dict[int, np.ndarray]: A dictionary of sets of configurations.\n\n        Examples:\n        &gt;&gt;&gt; S = [0,1]\n        &gt;&gt;&gt; p = 4\n        &gt;&gt;&gt; CARMA._set_gamma_func_base(S, p)\n        {0: array([[0],\n               [1]]), 1: array([[0, 1, 2],\n               [0, 1, 3]]), 2: array([[0, 2],\n               [0, 3],\n               [1, 2],\n               [1, 3]])}\n\n        &gt;&gt;&gt; S = [0]\n        &gt;&gt;&gt; p = 2\n        &gt;&gt;&gt; CARMA._set_gamma_func_base(S, p)\n        {0: None, 1: array([[0, 1]]), 2: array([[1]])}\n\n        &gt;&gt;&gt; S = []\n        &gt;&gt;&gt; p = 2\n        &gt;&gt;&gt; CARMA._set_gamma_func_base(S, p)\n        {0: None, 1: array([[0],\n               [1]]), 2: None}\n        \"\"\"\n        set_gamma: dict[int, Any] = {}\n\n        if len(S) == 0:\n            set_gamma[0] = None\n            set_gamma[1] = np.arange(0, p).reshape(-1, 1)\n            set_gamma[2] = None\n\n        if len(S) == 1:\n            S_sub = np.setdiff1d(np.arange(0, p), S)\n            set_gamma[0] = None\n            set_gamma[1] = CARMA._add_function(S_sub, S)\n            set_gamma[2] = S_sub.reshape(-1, 1)\n\n        if len(S) &gt; 1:\n            S_sub = np.setdiff1d(np.arange(0, p), S)\n            S = np.sort(S)\n            set_gamma[0] = np.array(list(combinations(S, len(S) - 1)))\n            set_gamma[1] = CARMA._add_function(S_sub, S)\n            xs = np.vstack([CARMA._add_function(S_sub, row) for row in set_gamma[0]])\n            set_gamma[2] = xs\n\n        return set_gamma\n\n    @staticmethod\n    def _set_gamma_func_conditional(\n        input_S: Any, condition_index: list[int], p: int\n    ) -&gt; dict[int, np.ndarray]:\n        \"\"\"Creates a dictionary of sets of configurations assuming conditional set.\n\n        Args:\n            input_S (Any): The input set.\n            condition_index (list[int]): The conditional set.\n            p (int): The number of SNPs.\n\n        Returns:\n            dict[int, np.ndarray]: A dictionary of sets of configurations.\n\n        Examples:\n        &gt;&gt;&gt; input_S = [0,1,2]\n        &gt;&gt;&gt; condition_index = [2]\n        &gt;&gt;&gt; p = 4\n        &gt;&gt;&gt; CARMA._set_gamma_func_conditional(input_S, condition_index, p)\n        {0: array([[0],\n               [1]]), 1: array([[0, 1, 3]]), 2: array([[0, 3],\n               [1, 3]])}\n        \"\"\"\n        set_gamma: dict[int, Any] = {}\n        S = np.setdiff1d(input_S, condition_index)\n\n        # set of gamma-\n        if len(S) == 0:\n            S_sub = np.setdiff1d(np.arange(0, p), condition_index)\n            set_gamma[0] = None\n            set_gamma[1] = S_sub.reshape(-1, 1)\n            set_gamma[2] = None\n\n        if len(S) == 1:\n            S_sub = np.setdiff1d(np.arange(0, p), input_S)\n            set_gamma[0] = None\n            set_gamma[1] = CARMA._add_function(S_sub, S)\n            set_gamma[2] = S_sub.reshape(-1, 1)\n\n        if len(S) &gt; 1:\n            S_sub = np.setdiff1d(np.arange(0, p), input_S)\n            S = np.sort(S)\n            set_gamma[0] = np.array(list(combinations(S, len(S) - 1)))\n            set_gamma[1] = CARMA._add_function(S_sub, S)\n            xs = np.vstack([CARMA._add_function(S_sub, row) for row in set_gamma[0]])\n            set_gamma[2] = xs\n\n        return set_gamma\n\n    @staticmethod\n    def _set_gamma_func(\n        input_S: Any, p: int, condition_index: list[int] | None = None\n    ) -&gt; dict[int, np.ndarray]:\n        \"\"\"Creates a dictionary of sets of configurations.\n\n        Args:\n            input_S (Any): The input set.\n            p (int): The number of SNPs.\n            condition_index (list[int] | None): The conditional set. Defaults to None.\n\n        Returns:\n            dict[int, np.ndarray]: A dictionary of sets of configurations.\n\n        Examples:\n        &gt;&gt;&gt; input_S = [0,1,2]\n        &gt;&gt;&gt; condition_index=[2]\n        &gt;&gt;&gt; p = 4\n        &gt;&gt;&gt; CARMA._set_gamma_func(input_S, p, condition_index)\n        {0: array([[0],\n               [1]]), 1: array([[0, 1, 3]]), 2: array([[0, 3],\n               [1, 3]])}\n        \"\"\"\n        if condition_index is None:\n            results = CARMA._set_gamma_func_base(input_S, p)\n        else:\n            results = CARMA._set_gamma_func_conditional(input_S, condition_index, p)\n        return results\n\n    @staticmethod\n    def _index_fun_internal(x: np.ndarray) -&gt; str:\n        \"\"\"Convert an array of causal SNP indexes to comma-separated string.\n\n        Args:\n            x (np.ndarray): The input array.\n\n        Returns:\n            str: The comma-separated string.\n\n        Examples:\n        &gt;&gt;&gt; x = np.array([1,2,3])\n        &gt;&gt;&gt; CARMA._index_fun_internal(x)\n        '1,2,3'\n        \"\"\"\n        y = np.sort(x)\n        y = y.astype(str)\n        return \",\".join(y)\n\n    @staticmethod\n    def _index_fun(y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Convert an array of causal SNP indexes to comma-separated string.\n\n        Args:\n            y (np.ndarray): The input array.\n\n        Returns:\n            np.ndarray: The comma-separated string.\n\n        Examples:\n        &gt;&gt;&gt; y = np.array([[1,2,3],[4,5,6]])\n        &gt;&gt;&gt; CARMA._index_fun(y)\n        array(['1,2,3', '4,5,6'], dtype='&lt;U5')\n        \"\"\"\n        return np.array([CARMA._index_fun_internal(x) for x in y])\n\n    @staticmethod\n    def _ridge_fun(\n        x: float,\n        Sigma: np.ndarray,\n        modi_ld_S: np.ndarray,\n        test_S: np.ndarray,\n        z: np.ndarray,\n        outlier_tau: float,\n        outlier_likelihood: Any,\n    ) -&gt; float:\n        \"\"\"Estimate the matrix shrinkage parameter for outlier detection.\n\n        Args:\n            x (float): The input parameter.\n            Sigma (np.ndarray): The Sigma matrix.\n            modi_ld_S (np.ndarray): The modi_ld_S matrix.\n            test_S (np.ndarray): The test_S matrix.\n            z (np.ndarray): The z vector.\n            outlier_tau (float): The outlier_tau value.\n            outlier_likelihood (Any): The outlier_likelihood function.\n\n        Returns:\n            float: The estimated matrix shrinkage parameter.\n\n        Examples:\n        &gt;&gt;&gt; x = 0.5\n        &gt;&gt;&gt; Sigma = np.array([[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]])\n        &gt;&gt;&gt; modi_ld_S = np.array([[1, 0.5], [0.5, 1]])\n        &gt;&gt;&gt; test_S = np.array([1, 2])\n        &gt;&gt;&gt; z = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; outlier_tau = 1 / 0.05**2\n        &gt;&gt;&gt; outlier_likelihood = CARMA._outlier_ind_Normal_marginal_external\n        &gt;&gt;&gt; np.round(CARMA._ridge_fun(x, Sigma, modi_ld_S, test_S, z, outlier_tau, outlier_likelihood),decimals=5)\n        6.01486\n        \"\"\"\n        temp_Sigma = Sigma.copy()\n        temp_ld_S = x * modi_ld_S + (1 - x) * np.eye(len(modi_ld_S))\n        temp_Sigma[np.ix_(test_S, test_S)] = temp_ld_S\n        return -outlier_likelihood(\n            index_vec_input=test_S + 1,\n            Sigma=temp_Sigma,\n            z=z,\n            tau=outlier_tau,\n            p_S=len(test_S),\n        )\n\n    @staticmethod\n    def _prior_dist(t: str, lambda_val: float, p: int) -&gt; float:\n        \"\"\"Estimate the priors for the given configurations.\n\n        Args:\n            t (str): The input string for the given configuration.\n            lambda_val (float): The lambda value.\n            p (int): The number of SNPs.\n\n        Returns:\n            float: The estimated prior.\n\n        Examples:\n        &gt;&gt;&gt; t = \"1,2,3\"\n        &gt;&gt;&gt; lambda_val = 1\n        &gt;&gt;&gt; p = 4\n        &gt;&gt;&gt; np.round(CARMA._prior_dist(t, lambda_val, p),decimals=5)\n        -3.17805\n        \"\"\"\n        index_array = t.split(\",\")\n        dim_model = len(index_array)\n        if t == \"\":\n            dim_model = 0\n        return (\n            dim_model * np.log(lambda_val) + lgamma(p - dim_model + 1) - lgamma(p + 1)\n        )\n\n    @staticmethod\n    def _PIP_func(\n        likeli: pd.DataFrame, model_space: pd.DataFrame, p: int, num_causal: int\n    ) -&gt; np.ndarray:\n        \"\"\"Estimates the posterior inclusion probabilities (PIPs) for all SNPs.\n\n        Args:\n            likeli (pd.DataFrame): The marginal likelihoods.\n            model_space (pd.DataFrame): The corresponding model space.\n            p (int): The number of SNPs.\n            num_causal (int): The maximal number of causal SNPs.\n\n        Returns:\n            np.ndarray: The posterior inclusion probabilities (PIPs) for all SNPs.\n\n        Examples:\n        &gt;&gt;&gt; likeli = pd.DataFrame([10, 10, 5,11,0], columns=['likeli']).squeeze()\n        &gt;&gt;&gt; model_space = pd.DataFrame(['0', '1', '2','0,1',''], columns=['config']).squeeze()\n        &gt;&gt;&gt; p = 3\n        &gt;&gt;&gt; num_causal = 2\n        &gt;&gt;&gt; CARMA._PIP_func(likeli, model_space, p, num_causal)\n        array([0.7869271, 0.7869271, 0.001426 ])\n        \"\"\"\n        likeli = likeli.reset_index(drop=True)\n        model_space = model_space.reset_index(drop=True)\n\n        model_space_matrix = np.zeros((len(model_space), p), dtype=int)\n\n        for i in range(len(model_space)):\n            if model_space.iloc[i] != \"\":\n                ind = list(map(int, model_space.iloc[i].split(\",\")))\n                if len(ind) &gt; 0:\n                    model_space_matrix[i, ind] = 1\n\n        infi_index = np.where(np.isinf(likeli))[0]\n        if len(infi_index) != 0:\n            likeli = likeli.drop(infi_index).reset_index(drop=True)\n            model_space_matrix = np.delete(model_space_matrix, infi_index, axis=0)\n\n        na_index = np.where(np.isnan(likeli))[0]\n        if len(na_index) != 0:\n            likeli = likeli.drop(na_index).reset_index(drop=True)\n            model_space_matrix = np.delete(model_space_matrix, na_index, axis=0)\n\n        row_sums = np.sum(model_space_matrix, axis=1)\n        model_space_matrix = model_space_matrix[row_sums &lt;= num_causal]\n        likeli = likeli[row_sums &lt;= num_causal]\n\n        aa = likeli - max(likeli)\n        prob_sum = np.sum(np.exp(aa))\n\n        result_prob = np.zeros(p)\n        for i in range(p):\n            result_prob[i] = (\n                np.sum(np.exp(aa[model_space_matrix[:, i] == 1])) / prob_sum\n            )\n\n        return result_prob\n\n    @staticmethod\n    def _MCS_modified(  # noqa: C901\n        z: np.ndarray,\n        ld_matrix: np.ndarray,\n        Max_Model_Dim: int = 10_000,\n        lambda_val: float = 1,\n        num_causal: int = 10,\n        outlier_switch: bool = True,\n        input_conditional_S_list: list[int] | None = None,\n        tau: float = 1 / 0.05**2,\n        epsilon: float = 1e-3,\n        inner_all_iter: int = 10,\n        outlier_BF_index: float | None = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Modified Monte Carlo shotgun sampling (MCS) algorithm.\n\n        Args:\n            z (np.ndarray): Numeric vector representing z-scores.\n            ld_matrix (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n            Max_Model_Dim (int): Maximum allowed dimension for the causal models.\n            lambda_val (float): Regularization parameter controlling the strength of the L1 penalty.\n            num_causal (int): Maximal number of causal variants to be selected in the final model.\n            outlier_switch (bool): Whether to consider outlier detection in the analysis.\n            input_conditional_S_list (list[int] | None): The conditional set. Defaults to None.\n            tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix.\n            epsilon (float): Threshold for convergence in CARMA iterations.\n            inner_all_iter (int): The number of inner iterations in each CARMA iteration.\n            outlier_BF_index (float | None): Bayes Factor threshold for identifying outliers. Defaults to None.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the following results:\n                - B_list: A dataframe containing the marginal likelihoods and the corresponding model space.\n                - conditional_S_list: A list of outliers.\n\n        Examples:\n        &gt;&gt;&gt; z = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; ld_matrix = np.array([[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]])\n        &gt;&gt;&gt; Max_Model_Dim = 10_000\n        &gt;&gt;&gt; lambda_val = 1\n\n        &gt;&gt;&gt; num_causal = 10\n        &gt;&gt;&gt; outlier_switch = True\n        \"\"\"\n        p = len(z)\n        marginal_likelihood = CARMA._ind_Normal_fixed_sigma_marginal_external\n        tau_sample = tau\n        if outlier_switch:\n            outlier_likelihood = CARMA._outlier_ind_Normal_marginal_external\n            outlier_tau = tau\n\n        B = Max_Model_Dim\n        stored_bf = 0\n        Sigma = ld_matrix\n\n        S = []\n\n        null_model = \"\"\n        null_margin = CARMA._prior_dist(null_model, lambda_val=lambda_val, p=p)\n\n        B_list = pd.DataFrame({\"set_gamma_margin\": [null_margin], \"matrix_gamma\": [\"\"]})\n\n        if input_conditional_S_list is None:\n            conditional_S = []\n        else:\n            conditional_S = input_conditional_S_list\n            S = conditional_S\n\n        for _i in range(0, inner_all_iter):\n            for _j in range(0, 10):\n                set_gamma = CARMA._set_gamma_func(\n                    input_S=S, p=p, condition_index=conditional_S\n                )\n\n                if conditional_S is None:\n                    working_S = S\n                else:\n                    working_S = np.sort(np.setdiff1d(S, conditional_S)).astype(int)\n\n                set_gamma_margin: list[Any] = [None, None, None]\n                set_gamma_prior: list[Any] = [None, None, None]\n                matrix_gamma: list[Any] = [None, None, None]\n\n                for i in range(0, len(set_gamma)):\n                    if set_gamma[i] is not None:\n                        matrix_gamma[i] = CARMA._index_fun(set_gamma[i])\n                        p_S = set_gamma[i].shape[1]\n                        set_gamma_margin[i] = np.apply_along_axis(\n                            marginal_likelihood,\n                            1,\n                            set_gamma[i] + 1,\n                            Sigma=Sigma,\n                            z=z,\n                            tau=tau_sample,\n                            p_S=p_S,\n                        )\n                        set_gamma_prior[i] = np.array(\n                            [\n                                CARMA._prior_dist(model, lambda_val=lambda_val, p=p)\n                                for model in matrix_gamma[i]\n                            ]\n                        )\n                        set_gamma_margin[i] = set_gamma_prior[i] + set_gamma_margin[i]\n                    else:\n                        set_gamma_margin[i] = np.array(null_margin)\n                        set_gamma_prior[i] = 0\n                        matrix_gamma[i] = np.array(null_model)\n\n                columns = [\"set_gamma_margin\", \"matrix_gamma\"]\n                add_B = pd.DataFrame(columns=columns)\n\n                for i in range(len(set_gamma)):\n                    if isinstance(set_gamma_margin[i].tolist(), list):\n                        new_row = pd.DataFrame(\n                            {\n                                \"set_gamma_margin\": set_gamma_margin[i].tolist(),\n                                \"matrix_gamma\": matrix_gamma[i].tolist(),\n                            }\n                        )\n                        add_B = pd.concat([add_B, new_row], ignore_index=True)\n                    else:\n                        new_row = pd.DataFrame(\n                            {\n                                \"set_gamma_margin\": [set_gamma_margin[i].tolist()],\n                                \"matrix_gamma\": [matrix_gamma[i].tolist()],\n                            }\n                        )\n                        add_B = pd.concat([add_B, new_row], ignore_index=True)\n\n                # Add visited models into the storage space of models\n                B_list = pd.concat([B_list, add_B], ignore_index=True)\n                B_list = B_list.drop_duplicates(\n                    subset=\"matrix_gamma\", ignore_index=True\n                )\n                B_list = B_list.sort_values(\n                    by=\"set_gamma_margin\", ignore_index=True, ascending=False\n                )\n\n                if len(working_S) == 0:\n                    # Create a DataFrame set.star\n                    set_star = pd.DataFrame(\n                        {\n                            \"set_index\": [0, 1, 2],\n                            \"gamma_set_index\": [np.nan, np.nan, np.nan],\n                            \"margin\": [np.nan, np.nan, np.nan],\n                        }\n                    )\n\n                    # Assuming set.gamma.margin and current.log.margin are defined\n                    aa = set_gamma_margin[1]\n                    aa = aa - aa[np.argmax(aa)]\n\n                    min_half_len = min(len(aa), floor(p / 2))\n                    decr_ind = np.argsort(np.exp(aa))[::-1]\n                    decr_half_ind = decr_ind[:min_half_len]\n\n                    probs = np.exp(aa)[decr_half_ind]\n\n                    chosen_index = np.random.choice(\n                        decr_half_ind, 1, p=probs / np.sum(probs)\n                    )\n                    set_star.at[1, \"gamma_set_index\"] = chosen_index[0]\n                    set_star.at[1, \"margin\"] = set_gamma_margin[1][chosen_index[0]]\n\n                    S = set_gamma[1][chosen_index[0]].tolist()\n\n                else:\n                    set_star = pd.DataFrame(\n                        {\n                            \"set_index\": [0, 1, 2],\n                            \"gamma_set_index\": [np.nan, np.nan, np.nan],\n                            \"margin\": [np.nan, np.nan, np.nan],\n                        }\n                    )\n                    for i in range(0, 3):\n                        aa = set_gamma_margin[i]\n                        if np.size(aa) &gt; 1:\n                            aa = aa - aa[np.argmax(aa)]\n                            chosen_index = np.random.choice(\n                                range(0, np.size(set_gamma_margin[i])),\n                                1,\n                                p=np.exp(aa) / np.sum(np.exp(aa)),\n                            )\n                            set_star.at[i, \"gamma_set_index\"] = chosen_index\n                            set_star.at[i, \"margin\"] = set_gamma_margin[i][chosen_index]\n                        else:\n                            set_star.at[i, \"gamma_set_index\"] = 0\n                            set_star.at[i, \"margin\"] = set_gamma_margin[i]\n\n                    if outlier_switch:\n                        for i in range(1, len(set_gamma)):\n                            test_log_BF: float = 100\n                            while True:\n                                aa = set_gamma_margin[i]\n                                aa = aa - aa[np.argmax(aa)]\n                                chosen_index = np.random.choice(\n                                    range(0, np.size(set_gamma_margin[i])),\n                                    1,\n                                    p=np.exp(aa) / np.sum(np.exp(aa)),\n                                )\n                                set_star.at[i, \"gamma_set_index\"] = chosen_index\n                                set_star.at[i, \"margin\"] = set_gamma_margin[i][\n                                    chosen_index\n                                ]\n\n                                test_S = set_gamma[i][int(chosen_index), :]\n\n                                modi_Sigma = Sigma.copy()\n                                if np.size(test_S) &gt; 1:\n                                    modi_ld_S = modi_Sigma[test_S][:, test_S]\n\n                                    result = minimize_scalar(\n                                        CARMA._ridge_fun,\n                                        bounds=(0, 1),\n                                        args=(\n                                            Sigma,\n                                            modi_ld_S,\n                                            test_S,\n                                            z,\n                                            outlier_tau,\n                                            outlier_likelihood,\n                                        ),\n                                        method=\"bounded\",\n                                    )\n                                    modi_ld_S = result.x * modi_ld_S + (\n                                        1 - result.x\n                                    ) * np.eye(len(modi_ld_S))\n\n                                    modi_Sigma[np.ix_(test_S, test_S)] = modi_ld_S\n\n                                    test_log_BF = outlier_likelihood(\n                                        test_S + 1, Sigma, z, outlier_tau, len(test_S)\n                                    ) - outlier_likelihood(\n                                        test_S + 1,\n                                        modi_Sigma,\n                                        z,\n                                        outlier_tau,\n                                        len(test_S),\n                                    )\n                                    test_log_BF = -np.abs(test_log_BF)\n\n                                if np.exp(test_log_BF) &lt; outlier_BF_index:\n                                    set_gamma[i] = np.delete(\n                                        set_gamma[i],\n                                        int(set_star[\"gamma_set_index\"][i]),\n                                        axis=0,\n                                    )\n                                    set_gamma_margin[i] = np.delete(\n                                        set_gamma_margin[i],\n                                        int(set_star[\"gamma_set_index\"][i]),\n                                        axis=0,\n                                    )\n                                    conditional_S = np.concatenate(\n                                        [conditional_S, np.setdiff1d(test_S, working_S)]\n                                    )\n                                    conditional_S = (\n                                        np.unique(conditional_S).astype(int).tolist()\n                                    )\n                                else:\n                                    break\n\n                    if len(working_S) == num_causal:\n                        set_star = set_star.drop(1)\n                        aa = set_star[\"margin\"] - max(set_star[\"margin\"])\n                        sec_sample = np.random.choice(\n                            [0, 2], 1, p=np.exp(aa) / np.sum(np.exp(aa))\n                        )\n                        ind_sec = int(\n                            set_star[\"gamma_set_index\"][\n                                set_star[\"set_index\"] == int(sec_sample)\n                            ]\n                        )\n                        S = set_gamma[sec_sample[0]][ind_sec].tolist()\n                    else:\n                        aa = set_star[\"margin\"] - max(set_star[\"margin\"])\n                        sec_sample = np.random.choice(\n                            range(0, 3), 1, p=np.exp(aa) / np.sum(np.exp(aa))\n                        )\n                        if set_gamma[sec_sample[0]] is not None:\n                            S = set_gamma[sec_sample[0]][\n                                int(set_star[\"gamma_set_index\"][sec_sample[0]])\n                            ].tolist()\n                        else:\n                            sec_sample = np.random.choice(\n                                range(1, 3),\n                                1,\n                                p=np.exp(aa)[[1, 2]] / np.sum(np.exp(aa)[[1, 2]]),\n                            )\n                            S = set_gamma[sec_sample[0]][\n                                int(set_star[\"gamma_set_index\"][sec_sample[0]])\n                            ].tolist()\n\n                for item in conditional_S:\n                    if item not in S:\n                        S.append(item)\n            # END h_ind loop\n            #\n            if conditional_S is not None:\n                all_c_index = []\n                index_array = [s.split(\",\") for s in B_list[\"matrix_gamma\"]]\n                for tt in conditional_S:\n                    tt_str = str(tt)\n                    ind = [\n                        i for i, sublist in enumerate(index_array) if tt_str in sublist\n                    ]\n                    all_c_index.extend(ind)\n\n                all_c_index = list(set(all_c_index))\n\n                if len(all_c_index) &gt; 0:\n                    temp_B_list = B_list.copy()\n                    temp_B_list = B_list.drop(all_c_index)\n                else:\n                    temp_B_list = B_list.copy()\n            else:\n                temp_B_list = B_list.copy()\n\n            result_B_list = temp_B_list[: min(int(B), len(temp_B_list))]\n\n            rb1 = result_B_list[\"set_gamma_margin\"]\n\n            difference = abs(rb1[: (len(rb1) // 4)].mean() - stored_bf)\n\n            if difference &lt; epsilon:\n                break\n            else:\n                stored_bf = rb1[: (len(rb1) // 4)].mean()\n\n        out = {\"B_list\": result_B_list, \"conditional_S_list\": conditional_S}\n\n        return out\n</code></pre>"},{"location":"python_api/methods/carma/#gentropy.method.carma.CARMA.CARMA_spike_slab_noEM","title":"<code>CARMA_spike_slab_noEM(z: np.ndarray, ld: np.ndarray, lambda_val: float = 1, Max_Model_Dim: int = 200000, all_iter: int = 1, all_inner_iter: int = 10, epsilon_threshold: float = 1e-05, num_causal: int = 10, tau: float = 0.04, outlier_switch: bool = True, outlier_BF_index: float = 1 / 3.2) -&gt; dict[str, Any]</code>  <code>staticmethod</code>","text":"<p>Perform CARMA analysis using a Spike-and-Slab prior without Expectation-Maximization (EM).</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>ndarray</code> <p>Numeric vector representing z-scores.</p> required <code>ld</code> <code>ndarray</code> <p>Numeric matrix representing the linkage disequilibrium (LD) matrix.</p> required <code>lambda_val</code> <code>float</code> <p>Regularization parameter controlling the strength of the L1 penalty.</p> <code>1</code> <code>Max_Model_Dim</code> <code>int</code> <p>Maximum allowed dimension for the causal models.</p> <code>200000</code> <code>all_iter</code> <code>int</code> <p>The total number of iterations to run the CARMA analysis.</p> <code>1</code> <code>all_inner_iter</code> <code>int</code> <p>The number of inner iterations in each CARMA iteration.</p> <code>10</code> <code>epsilon_threshold</code> <code>float</code> <p>Threshold for convergence in CARMA iterations.</p> <code>1e-05</code> <code>num_causal</code> <code>int</code> <p>Maximal number of causal variants to be selected in the final model.</p> <code>10</code> <code>tau</code> <code>float</code> <p>Tuning parameter controlling the level of shrinkage of the LD matrix.</p> <code>0.04</code> <code>outlier_switch</code> <code>bool</code> <p>Whether to consider outlier detection in the analysis.</p> <code>True</code> <code>outlier_BF_index</code> <code>float</code> <p>Bayes Factor threshold for identifying outliers.</p> <code>1 / 3.2</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the following results: - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs. - B_list: A dataframe containing the marginal likelihoods and the corresponding model space. - Outliers: A list of outlier SNPs.</p> Source code in <code>src/gentropy/method/carma.py</code> <pre><code>@staticmethod\ndef CARMA_spike_slab_noEM(\n    z: np.ndarray,\n    ld: np.ndarray,\n    lambda_val: float = 1,\n    Max_Model_Dim: int = 200_000,\n    all_iter: int = 1,\n    all_inner_iter: int = 10,\n    epsilon_threshold: float = 1e-5,\n    num_causal: int = 10,\n    tau: float = 0.04,\n    outlier_switch: bool = True,\n    outlier_BF_index: float = 1 / 3.2,\n) -&gt; dict[str, Any]:\n    \"\"\"Perform CARMA analysis using a Spike-and-Slab prior without Expectation-Maximization (EM).\n\n    Args:\n        z (np.ndarray): Numeric vector representing z-scores.\n        ld (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n        lambda_val (float): Regularization parameter controlling the strength of the L1 penalty.\n        Max_Model_Dim (int): Maximum allowed dimension for the causal models.\n        all_iter (int): The total number of iterations to run the CARMA analysis.\n        all_inner_iter (int): The number of inner iterations in each CARMA iteration.\n        epsilon_threshold (float): Threshold for convergence in CARMA iterations.\n        num_causal (int): Maximal number of causal variants to be selected in the final model.\n        tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix.\n        outlier_switch (bool): Whether to consider outlier detection in the analysis.\n        outlier_BF_index (float): Bayes Factor threshold for identifying outliers.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the following results:\n            - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs.\n            - B_list: A dataframe containing the marginal likelihoods and the corresponding model space.\n            - Outliers: A list of outlier SNPs.\n    \"\"\"\n    p_snp = len(z)\n    epsilon_list = epsilon_threshold * p_snp\n    all_epsilon_threshold = epsilon_threshold * p_snp\n\n    # Zero step\n    all_C_list = CARMA._MCS_modified(\n        z=z,\n        ld_matrix=ld,\n        epsilon=epsilon_list,\n        Max_Model_Dim=Max_Model_Dim,\n        lambda_val=lambda_val,\n        outlier_switch=outlier_switch,\n        tau=tau,\n        num_causal=num_causal,\n        inner_all_iter=all_inner_iter,\n        outlier_BF_index=outlier_BF_index,\n    )\n\n    # Main steps\n    for _ in range(0, all_iter):\n        ac1 = all_C_list[\"B_list\"][\"set_gamma_margin\"]\n        previous_result = np.mean(ac1[0 : round(len(ac1) / 4)])\n\n        all_C_list = CARMA._MCS_modified(\n            z=z,\n            ld_matrix=ld,\n            input_conditional_S_list=all_C_list[\"conditional_S_list\"],\n            Max_Model_Dim=Max_Model_Dim,\n            num_causal=num_causal,\n            epsilon=epsilon_list,\n            outlier_switch=outlier_switch,\n            tau=tau,\n            lambda_val=lambda_val,\n            inner_all_iter=all_inner_iter,\n            outlier_BF_index=outlier_BF_index,\n        )\n\n        ac1 = all_C_list[\"B_list\"][\"set_gamma_margin\"]\n        difference = np.abs(previous_result - np.mean(ac1[0 : round(len(ac1) / 4)]))\n        if difference &lt; all_epsilon_threshold:\n            break\n\n    # Calculate PIPs and Credible Set\n    pip = CARMA._PIP_func(\n        likeli=all_C_list[\"B_list\"][\"set_gamma_margin\"],\n        model_space=all_C_list[\"B_list\"][\"matrix_gamma\"],\n        p=p_snp,\n        num_causal=num_causal,\n    )\n\n    results_list = {\n        \"PIPs\": pip,\n        \"B_list\": all_C_list[\"B_list\"],\n        \"Outliers\": all_C_list[\"conditional_S_list\"],\n    }\n\n    return results_list\n</code></pre>"},{"location":"python_api/methods/carma/#gentropy.method.carma.CARMA.time_limited_CARMA_spike_slab_noEM","title":"<code>time_limited_CARMA_spike_slab_noEM(z: np.ndarray, ld: np.ndarray, sec_threshold: float = 600, tau: float = 0.04) -&gt; dict[str, Any]</code>  <code>staticmethod</code>","text":"<p>The wrapper for the CARMA_spike_slab_noEM function that runs the function in a separate thread and terminates it if it takes too long.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>ndarray</code> <p>Numeric vector representing z-scores.</p> required <code>ld</code> <code>ndarray</code> <p>Numeric matrix representing the linkage disequilibrium (LD) matrix.</p> required <code>sec_threshold</code> <code>float</code> <p>The time threshold in seconds.</p> <code>600</code> <code>tau</code> <code>float</code> <p>Tuning parameter controlling the level of shrinkage of the LD matrix</p> <code>0.04</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the following results: - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs or None. - B_list: A dataframe containing the marginal likelihoods and the corresponding model space or None. - Outliers: A list of outlier SNPs or None.</p> Source code in <code>src/gentropy/method/carma.py</code> <pre><code>@staticmethod\ndef time_limited_CARMA_spike_slab_noEM(\n    z: np.ndarray,\n    ld: np.ndarray,\n    sec_threshold: float = 600,\n    tau: float = 0.04,\n) -&gt; dict[str, Any]:\n    \"\"\"The wrapper for the CARMA_spike_slab_noEM function that runs the function in a separate thread and terminates it if it takes too long.\n\n    Args:\n        z (np.ndarray): Numeric vector representing z-scores.\n        ld (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n        sec_threshold (float): The time threshold in seconds.\n        tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix\n\n    Returns:\n        dict[str, Any]: A dictionary containing the following results:\n            - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs or None.\n            - B_list: A dataframe containing the marginal likelihoods and the corresponding model space or None.\n            - Outliers: A list of outlier SNPs or None.\n    \"\"\"\n    # Ignore pandas future warnings\n    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n    try:\n        # Execute CARMA.CARMA_spike_slab_noEM with a timeout\n        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n            future = executor.submit(\n                CARMA.CARMA_spike_slab_noEM, z=z, ld=ld, tau=tau\n            )\n            result = future.result(timeout=sec_threshold)\n    except concurrent.futures.TimeoutError:\n        # If execution exceeds the timeout, return None\n        result = {\"PIPs\": None, \"B_list\": None, \"Outliers\": None}\n\n    return result\n</code></pre>"},{"location":"python_api/methods/clumping/","title":"Clumping","text":"<p>Clumping is a commonly used post-processing method that allows for the identification of independent association signals from GWAS summary statistics and curated associations. This process is critical because of the complex linkage disequilibrium (LD) structure in human populations, which can result in multiple statistically significant associations within the same genomic region. Clumping methods help reduce redundancy in GWAS results and ensure that each reported association represents an independent signal.</p> <p>We have implemented two clumping methods:</p> <ol> <li>Distance-based clumping: Uses genomic window to clump the significant SNPs into one hit.</li> <li>LD-based clumping: Uses genomic window and LD to clump the significant SNPs into one hit.</li> <li>Locus-breaker clumping: Applies a distance cutoff between baseline significant SNPs. Returns the start and end position of the locus as well.</li> </ol> <p>The algorithmic logic is similar to classic clumping approaches from PLINK (Reference: PLINK Clump Documentation). See details below:</p>"},{"location":"python_api/methods/clumping/#distance-based-clumping","title":"Distance-based clumping","text":""},{"location":"python_api/methods/clumping/#gentropy.method.window_based_clumping.WindowBasedClumping","title":"<code>gentropy.method.window_based_clumping.WindowBasedClumping</code>","text":"<p>Get semi-lead snps from summary statistics using a window based function.</p> Source code in <code>src/gentropy/method/window_based_clumping.py</code> <pre><code>class WindowBasedClumping:\n    \"\"\"Get semi-lead snps from summary statistics using a window based function.\"\"\"\n\n    @staticmethod\n    def _cluster_peaks(\n        study: Column, chromosome: Column, position: Column, window_length: int\n    ) -&gt; Column:\n        \"\"\"Cluster GWAS significant variants, were clusters are separated by a defined distance.\n\n        !! Important to note that the length of the clusters can be arbitrarily big.\n\n        Args:\n            study (Column): study identifier\n            chromosome (Column): chromosome identifier\n            position (Column): position of the variant\n            window_length (int): window length in basepair\n\n        Returns:\n            Column: containing cluster identifier\n\n        Examples:\n            &gt;&gt;&gt; data = [\n            ...     # Cluster 1:\n            ...     ('s1', 'chr1', 2),\n            ...     ('s1', 'chr1', 4),\n            ...     ('s1', 'chr1', 12),\n            ...     # Cluster 2 - Same chromosome:\n            ...     ('s1', 'chr1', 31),\n            ...     ('s1', 'chr1', 38),\n            ...     ('s1', 'chr1', 42),\n            ...     # Cluster 3 - New chromosome:\n            ...     ('s1', 'chr2', 41),\n            ...     ('s1', 'chr2', 44),\n            ...     ('s1', 'chr2', 50),\n            ...     # Cluster 4 - other study:\n            ...     ('s2', 'chr2', 55),\n            ...     ('s2', 'chr2', 62),\n            ...     ('s2', 'chr2', 70),\n            ... ]\n            &gt;&gt;&gt; window_length = 10\n            &gt;&gt;&gt; (\n            ...     spark.createDataFrame(data, ['studyId', 'chromosome', 'position'])\n            ...     .withColumn(\"cluster_id\",\n            ...         WindowBasedClumping._cluster_peaks(\n            ...             f.col('studyId'),\n            ...             f.col('chromosome'),\n            ...             f.col('position'),\n            ...             window_length\n            ...         )\n            ...     ).show()\n            ... )\n            +-------+----------+--------+----------+\n            |studyId|chromosome|position|cluster_id|\n            +-------+----------+--------+----------+\n            |     s1|      chr1|       2| s1_chr1_2|\n            |     s1|      chr1|       4| s1_chr1_2|\n            |     s1|      chr1|      12| s1_chr1_2|\n            |     s1|      chr1|      31|s1_chr1_31|\n            |     s1|      chr1|      38|s1_chr1_31|\n            |     s1|      chr1|      42|s1_chr1_31|\n            |     s1|      chr2|      41|s1_chr2_41|\n            |     s1|      chr2|      44|s1_chr2_41|\n            |     s1|      chr2|      50|s1_chr2_41|\n            |     s2|      chr2|      55|s2_chr2_55|\n            |     s2|      chr2|      62|s2_chr2_55|\n            |     s2|      chr2|      70|s2_chr2_55|\n            +-------+----------+--------+----------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        # By adding previous position, the cluster boundary can be identified:\n        previous_position = f.lag(position).over(\n            Window.partitionBy(study, chromosome).orderBy(position)\n        )\n        # We consider a cluster boudary if subsequent snps are further than the defined window:\n        cluster_id = f.when(\n            (previous_position.isNull())\n            | (position - previous_position &gt; window_length),\n            f.concat_ws(\"_\", study, chromosome, position),\n        )\n        # The cluster identifier is propagated across every variant of the cluster:\n        return f.when(\n            cluster_id.isNull(),\n            f.last(cluster_id, ignorenulls=True).over(\n                Window.partitionBy(study, chromosome)\n                .orderBy(position)\n                .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n            ),\n        ).otherwise(cluster_id)\n\n    @staticmethod\n    def _prune_peak(position: NDArray[np.float64], window_size: int) -&gt; DenseVector:\n        \"\"\"Establish lead snps based on their positions listed by p-value.\n\n        The function `find_peak` assigns lead SNPs based on their positions listed by p-value within a specified window size.\n\n        Args:\n            position (NDArray[np.float64]): positions of the SNPs sorted by p-value.\n            window_size (int): the distance in bp within which associations are clumped together around the lead snp.\n\n        Returns:\n            DenseVector: binary vector where 1 indicates a lead SNP and 0 indicates a non-lead SNP.\n\n        Examples:\n            &gt;&gt;&gt; from pyspark.ml import functions as fml\n            &gt;&gt;&gt; from pyspark.ml.linalg import DenseVector\n            &gt;&gt;&gt; WindowBasedClumping._prune_peak(np.array((3, 9, 8, 4, 6)), 2)\n            DenseVector([1.0, 1.0, 0.0, 0.0, 1.0])\n\n        \"\"\"\n        # Initializing the lead list with zeroes:\n        is_lead = np.zeros(len(position))\n\n        # List containing indices of leads:\n        lead_indices: list[int] = []\n\n        # Looping through all positions:\n        for index in range(len(position)):\n            # Looping through leads to find out if they are within a window:\n            for lead_index in lead_indices:\n                # If any of the leads within the window:\n                if abs(position[lead_index] - position[index]) &lt; window_size:\n                    # Skipping further checks:\n                    break\n            else:\n                # None of the leads were within the window:\n                lead_indices.append(index)\n                is_lead[index] = 1\n\n        return DenseVector(is_lead)\n\n    @staticmethod\n    def clump(\n        unclumped_associations: SummaryStatistics | StudyLocus,\n        distance: int = WindowBasedClumpingStepConfig().distance,\n    ) -&gt; StudyLocus:\n        \"\"\"Clump single point associations from summary statistics or study locus dataset based on window.\n\n        Args:\n            unclumped_associations (SummaryStatistics | StudyLocus): Input dataset to be used for clumping. Assumes that the input dataset is already filtered for significant variants.\n            distance (int): Distance in base pairs to be used for clumping. Defaults to 500_000.\n\n        Returns:\n            StudyLocus: clumped associations, where the clumped variants are flagged.\n        \"\"\"\n        # Quality check expression that flags variants that are not considered lead variant:\n        qc_check = f.col(\"semiIndices\")[f.col(\"pvRank\") - 1] &lt;= 0\n\n        # The quality control expression will depend on the input dataset, as the column might be already present:\n        qc_expression = (\n            # When the column is already present and the condition is met, the value is appended to the array, otherwise keep as is:\n            f.when(\n                qc_check,\n                f.array_union(\n                    f.col(\"qualityControls\"),\n                    f.array(f.lit(StudyLocusQualityCheck.WINDOW_CLUMPED.value)),\n                ),\n            ).otherwise(f.col(\"qualityControls\"))\n            if \"qualityControls\" in unclumped_associations.df.columns\n            # If column is not there yet, initialize it with the flag value, or an empty array:\n            else f.when(\n                qc_check, f.array(f.lit(StudyLocusQualityCheck.WINDOW_CLUMPED.value))\n            ).otherwise(f.array().cast(t.ArrayType(t.StringType())))\n        )\n\n        # Create window for locus clusters\n        # - variants where the distance between subsequent variants is below the defined threshold.\n        # - Variants are sorted by descending significance\n        cluster_window = Window.partitionBy(\n            \"studyId\", \"chromosome\", \"cluster_id\"\n        ).orderBy(f.col(\"pValueExponent\").asc(), f.col(\"pValueMantissa\").asc())\n\n        return StudyLocus(\n            _df=(\n                unclumped_associations.df\n                # Clustering variants for efficient windowing (complexity reduction):\n                .withColumn(\n                    \"cluster_id\",\n                    WindowBasedClumping._cluster_peaks(\n                        f.col(\"studyId\"),\n                        f.col(\"chromosome\"),\n                        f.col(\"position\"),\n                        distance,\n                    ),\n                )\n                # Within each cluster variants are ranked by significance:\n                .withColumn(\"pvRank\", f.row_number().over(cluster_window))\n                # Collect positions in cluster for the most significant variant (complexity reduction):\n                .withColumn(\n                    \"collectedPositions\",\n                    f.when(\n                        f.col(\"pvRank\") == 1,\n                        f.collect_list(f.col(\"position\")).over(\n                            cluster_window.rowsBetween(\n                                Window.currentRow, Window.unboundedFollowing\n                            )\n                        ),\n                    ).otherwise(f.array()),\n                )\n                # Collect top loci per cluster:\n                .withColumn(\n                    \"semiIndices\",\n                    f.when(\n                        f.size(f.col(\"collectedPositions\")) &gt; 0,\n                        fml.vector_to_array(\n                            f.udf(WindowBasedClumping._prune_peak, VectorUDT())(\n                                fml.array_to_vector(f.col(\"collectedPositions\")),\n                                f.lit(distance),\n                            )\n                        ),\n                    ),\n                )\n                # Propagating the result of the above calculation for all rows:\n                .withColumn(\n                    \"semiIndices\",\n                    f.when(\n                        f.col(\"semiIndices\").isNull(),\n                        f.first(f.col(\"semiIndices\"), ignorenulls=True).over(\n                            cluster_window\n                        ),\n                    ).otherwise(f.col(\"semiIndices\")),\n                )\n                # Adding study-locus id:\n                .withColumn(\n                    \"studyLocusId\",\n                    StudyLocus.assign_study_locus_id(\n                        [\"studyId\", \"variantId\"]\n                    ),\n                )\n                # Initialize QC column as array of strings:\n                .withColumn(\"qualityControls\", qc_expression)\n                .drop(\"pvRank\", \"collectedPositions\", \"semiIndices\", \"cluster_id\")\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/clumping/#gentropy.method.window_based_clumping.WindowBasedClumping.clump","title":"<code>clump(unclumped_associations: SummaryStatistics | StudyLocus, distance: int = WindowBasedClumpingStepConfig().distance) -&gt; StudyLocus</code>  <code>staticmethod</code>","text":"<p>Clump single point associations from summary statistics or study locus dataset based on window.</p> <p>Parameters:</p> Name Type Description Default <code>unclumped_associations</code> <code>SummaryStatistics | StudyLocus</code> <p>Input dataset to be used for clumping. Assumes that the input dataset is already filtered for significant variants.</p> required <code>distance</code> <code>int</code> <p>Distance in base pairs to be used for clumping. Defaults to 500_000.</p> <code>distance</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>clumped associations, where the clumped variants are flagged.</p> Source code in <code>src/gentropy/method/window_based_clumping.py</code> <pre><code>@staticmethod\ndef clump(\n    unclumped_associations: SummaryStatistics | StudyLocus,\n    distance: int = WindowBasedClumpingStepConfig().distance,\n) -&gt; StudyLocus:\n    \"\"\"Clump single point associations from summary statistics or study locus dataset based on window.\n\n    Args:\n        unclumped_associations (SummaryStatistics | StudyLocus): Input dataset to be used for clumping. Assumes that the input dataset is already filtered for significant variants.\n        distance (int): Distance in base pairs to be used for clumping. Defaults to 500_000.\n\n    Returns:\n        StudyLocus: clumped associations, where the clumped variants are flagged.\n    \"\"\"\n    # Quality check expression that flags variants that are not considered lead variant:\n    qc_check = f.col(\"semiIndices\")[f.col(\"pvRank\") - 1] &lt;= 0\n\n    # The quality control expression will depend on the input dataset, as the column might be already present:\n    qc_expression = (\n        # When the column is already present and the condition is met, the value is appended to the array, otherwise keep as is:\n        f.when(\n            qc_check,\n            f.array_union(\n                f.col(\"qualityControls\"),\n                f.array(f.lit(StudyLocusQualityCheck.WINDOW_CLUMPED.value)),\n            ),\n        ).otherwise(f.col(\"qualityControls\"))\n        if \"qualityControls\" in unclumped_associations.df.columns\n        # If column is not there yet, initialize it with the flag value, or an empty array:\n        else f.when(\n            qc_check, f.array(f.lit(StudyLocusQualityCheck.WINDOW_CLUMPED.value))\n        ).otherwise(f.array().cast(t.ArrayType(t.StringType())))\n    )\n\n    # Create window for locus clusters\n    # - variants where the distance between subsequent variants is below the defined threshold.\n    # - Variants are sorted by descending significance\n    cluster_window = Window.partitionBy(\n        \"studyId\", \"chromosome\", \"cluster_id\"\n    ).orderBy(f.col(\"pValueExponent\").asc(), f.col(\"pValueMantissa\").asc())\n\n    return StudyLocus(\n        _df=(\n            unclumped_associations.df\n            # Clustering variants for efficient windowing (complexity reduction):\n            .withColumn(\n                \"cluster_id\",\n                WindowBasedClumping._cluster_peaks(\n                    f.col(\"studyId\"),\n                    f.col(\"chromosome\"),\n                    f.col(\"position\"),\n                    distance,\n                ),\n            )\n            # Within each cluster variants are ranked by significance:\n            .withColumn(\"pvRank\", f.row_number().over(cluster_window))\n            # Collect positions in cluster for the most significant variant (complexity reduction):\n            .withColumn(\n                \"collectedPositions\",\n                f.when(\n                    f.col(\"pvRank\") == 1,\n                    f.collect_list(f.col(\"position\")).over(\n                        cluster_window.rowsBetween(\n                            Window.currentRow, Window.unboundedFollowing\n                        )\n                    ),\n                ).otherwise(f.array()),\n            )\n            # Collect top loci per cluster:\n            .withColumn(\n                \"semiIndices\",\n                f.when(\n                    f.size(f.col(\"collectedPositions\")) &gt; 0,\n                    fml.vector_to_array(\n                        f.udf(WindowBasedClumping._prune_peak, VectorUDT())(\n                            fml.array_to_vector(f.col(\"collectedPositions\")),\n                            f.lit(distance),\n                        )\n                    ),\n                ),\n            )\n            # Propagating the result of the above calculation for all rows:\n            .withColumn(\n                \"semiIndices\",\n                f.when(\n                    f.col(\"semiIndices\").isNull(),\n                    f.first(f.col(\"semiIndices\"), ignorenulls=True).over(\n                        cluster_window\n                    ),\n                ).otherwise(f.col(\"semiIndices\")),\n            )\n            # Adding study-locus id:\n            .withColumn(\n                \"studyLocusId\",\n                StudyLocus.assign_study_locus_id(\n                    [\"studyId\", \"variantId\"]\n                ),\n            )\n            # Initialize QC column as array of strings:\n            .withColumn(\"qualityControls\", qc_expression)\n            .drop(\"pvRank\", \"collectedPositions\", \"semiIndices\", \"cluster_id\")\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/clumping/#ld-based-clumping","title":"LD-based clumping:","text":""},{"location":"python_api/methods/clumping/#gentropy.method.clump.LDclumping","title":"<code>gentropy.method.clump.LDclumping</code>","text":"<p>LD clumping reports the most significant genetic associations in a region in terms of a smaller number of \u201cclumps\u201d of genetically linked SNPs.</p> Source code in <code>src/gentropy/method/clump.py</code> <pre><code>class LDclumping:\n    \"\"\"LD clumping reports the most significant genetic associations in a region in terms of a smaller number of \u201cclumps\u201d of genetically linked SNPs.\"\"\"\n\n    @staticmethod\n    def _is_lead_linked(\n        study_id: Column,\n        chromosome: Column,\n        variant_id: Column,\n        p_value_exponent: Column,\n        p_value_mantissa: Column,\n        ld_set: Column,\n    ) -&gt; Column:\n        \"\"\"Evaluates whether a lead variant is linked to a tag (with lowest p-value) in the same studyLocus dataset.\n\n        Args:\n            study_id (Column): studyId\n            chromosome (Column): chromosome\n            variant_id (Column): Lead variant id\n            p_value_exponent (Column): p-value exponent\n            p_value_mantissa (Column): p-value mantissa\n            ld_set (Column): Array of variants in LD with the lead variant\n\n        Returns:\n            Column: Boolean in which True indicates that the lead is linked to another tag in the same dataset.\n        \"\"\"\n        # Partitoning data by study and chromosome - this is the scope for looking for linked loci.\n        # Within the partition, we order the data by increasing p-value, and we collect the more significant lead variants in the window.\n        windowspec = (\n            Window.partitionBy(study_id, chromosome)\n            .orderBy(p_value_exponent.asc(), p_value_mantissa.asc())\n            .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n        )\n        more_significant_leads = f.collect_set(variant_id).over(windowspec)\n\n        # Collect all variants from the ld_set + adding the lead variant to the list to make sure that the lead is always in the list.\n        tags_in_studylocus = f.array_distinct(\n            f.array_union(\n                f.array(variant_id),\n                f.transform(ld_set, lambda x: x.getField(\"tagVariantId\")),\n            )\n        )\n\n        # If more than one tags of the ld_set can be found in the list of the more significant leads, the lead is linked.\n        # Study loci without variantId is considered as not linked.\n        # Also leads that were not found in the LD index is also considered as not linked.\n        return f.when(\n            variant_id.isNotNull(),\n            f.size(f.array_intersect(more_significant_leads, tags_in_studylocus)) &gt; 1,\n        ).otherwise(f.lit(False))\n\n    @classmethod\n    def clump(cls: type[LDclumping], associations: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Perform clumping on studyLocus dataset.\n\n        Args:\n            associations (StudyLocus): StudyLocus dataset\n\n        Returns:\n            StudyLocus: including flag and removing locus information for LD clumped loci.\n        \"\"\"\n        return associations.clump()\n</code></pre>"},{"location":"python_api/methods/clumping/#gentropy.method.clump.LDclumping.clump","title":"<code>clump(associations: StudyLocus) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Perform clumping on studyLocus dataset.</p> <p>Parameters:</p> Name Type Description Default <code>associations</code> <code>StudyLocus</code> <p>StudyLocus dataset</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>including flag and removing locus information for LD clumped loci.</p> Source code in <code>src/gentropy/method/clump.py</code> <pre><code>@classmethod\ndef clump(cls: type[LDclumping], associations: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Perform clumping on studyLocus dataset.\n\n    Args:\n        associations (StudyLocus): StudyLocus dataset\n\n    Returns:\n        StudyLocus: including flag and removing locus information for LD clumped loci.\n    \"\"\"\n    return associations.clump()\n</code></pre>"},{"location":"python_api/methods/clumping/#locus-breaker-clumping","title":"Locus-breaker clumping","text":""},{"location":"python_api/methods/clumping/#gentropy.method.locus_breaker_clumping.LocusBreakerClumping","title":"<code>gentropy.method.locus_breaker_clumping.LocusBreakerClumping</code>","text":"<p>Locus-breaker clumping method.</p> Source code in <code>src/gentropy/method/locus_breaker_clumping.py</code> <pre><code>class LocusBreakerClumping:\n    \"\"\"Locus-breaker clumping method.\"\"\"\n\n    @staticmethod\n    def locus_breaker(\n        summary_statistics: SummaryStatistics,\n        baseline_pvalue_cutoff: float,\n        distance_cutoff: int,\n        pvalue_cutoff: float,\n        flanking_distance: int,\n    ) -&gt; StudyLocus:\n        \"\"\"Identify GWAS associated loci based on the provided p-value and distance cutoff.\n\n        - The GWAS associated loci identified by this method have a varying width, and are separated by a distance greater than the provided distance cutoff.\n        - The distance is only calculted between single point associations that reach the baseline p-value cutoff.\n        - As the width of the selected genomic region dynamically depends on the loci, the resulting StudyLocus object will contain the locus start and end position.\n        - To ensure completeness, the locus is extended by a flanking distance in both ends.\n\n        Args:\n            summary_statistics (SummaryStatistics): Input summary statistics dataset.\n            baseline_pvalue_cutoff (float): baseline significance we consider for the locus.\n            distance_cutoff (int): minimum distance that separates two loci.\n            pvalue_cutoff (float): the minimum significance the locus should have.\n            flanking_distance (int): the distance to extend the locus in both directions.\n\n        Returns:\n            StudyLocus: clumped study loci with locus start and end positions + lead variant from the locus.\n        \"\"\"\n        # Extract columns from the summary statistics:\n        columns_sumstats_columns = summary_statistics.df.columns\n        # Convert pvalue_cutoff to neglog scale:\n        neglog_pv_cutoff = -np.log10(pvalue_cutoff)\n\n        # First window to calculate the distance between consecutive positions:\n        w1 = Window.partitionBy(\"studyId\", \"chromosome\").orderBy(\"position\")\n\n        # Second window to calculate the locus start and end:\n        w2 = (\n            Window.partitionBy(\"studyId\", \"chromosome\", \"locusStart\")\n            .orderBy(\"position\")\n            .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n        )\n\n        # Third window to rank the variants within the locus based on neglog p-value to find top loci:\n        w3 = Window.partitionBy(\n            \"studyId\", \"chromosome\", \"locusStart\", \"locusEnd\"\n        ).orderBy(f.col(\"negLogPValue\").desc())\n\n        return StudyLocus(\n            _df=(\n                # Applying the baseline p-value cutoff:\n                summary_statistics.pvalue_filter(baseline_pvalue_cutoff)\n                # Calculating the neglog p-value for easier sorting:\n                .df.withColumn(\n                    \"negLogPValue\",\n                    calculate_neglog_pvalue(\n                        f.col(\"pValueMantissa\"), f.col(\"pValueExponent\")\n                    ),\n                )\n                # Calculating the distance between consecutive positions, then identifying the locus start and end:\n                .withColumn(\"next_position\", f.lag(f.col(\"position\")).over(w1))\n                .withColumn(\"distance\", f.col(\"position\") - f.col(\"next_position\"))\n                .withColumn(\n                    \"locusStart\",\n                    f.when(\n                        (f.col(\"distance\") &gt; distance_cutoff)\n                        | f.col(\"distance\").isNull(),\n                        f.col(\"position\"),\n                    ),\n                )\n                .withColumn(\n                    \"locusStart\",\n                    f.when(\n                        f.last(f.col(\"locusStart\") - flanking_distance, True).over(\n                            w1.rowsBetween(-sys.maxsize, 0)\n                        )\n                        &gt; 0,\n                        f.last(f.col(\"locusStart\") - flanking_distance, True).over(\n                            w1.rowsBetween(-sys.maxsize, 0)\n                        ),\n                    ).otherwise(f.lit(0)),\n                )\n                .withColumn(\n                    \"locusEnd\", f.max(f.col(\"position\") + flanking_distance).over(w2)\n                )\n                .withColumn(\"rank\", f.rank().over(w3))\n                .filter(\n                    (f.col(\"rank\") == 1) &amp; (f.col(\"negLogPValue\") &gt; neglog_pv_cutoff)\n                )\n                .select(\n                    *columns_sumstats_columns,\n                    # To make sure that the type of locusStart and locusEnd follows schema of StudyLocus:\n                    f.col(\"locusStart\").cast(t.IntegerType()).alias(\"locusStart\"),\n                    f.col(\"locusEnd\").cast(t.IntegerType()).alias(\"locusEnd\"),\n                    f.lit(None)\n                    .cast(t.ArrayType(t.StringType()))\n                    .alias(\"qualityControls\"),\n                    StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n                )\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    @staticmethod\n    def process_locus_breaker_output(\n        lbc: StudyLocus,\n        wbc: StudyLocus,\n        large_loci_size: int,\n    ) -&gt; StudyLocus:\n        \"\"\"Process the locus breaker method result, and run window-based clumping on large loci.\n\n        Args:\n            lbc (StudyLocus): StudyLocus object from locus-breaker clumping.\n            wbc (StudyLocus): StudyLocus object from window-based clumping.\n            large_loci_size (int): the size to define large loci which should be broken with wbc.\n\n        Returns:\n            StudyLocus: clumped study loci with large loci broken by window-based clumping.\n        \"\"\"\n        large_loci_size = int(large_loci_size)\n        small_loci = lbc.filter(\n            (f.col(\"locusEnd\") - f.col(\"locusStart\")) &lt;= large_loci_size\n        )\n        large_loci = lbc.filter(\n            (f.col(\"locusEnd\") - f.col(\"locusStart\")) &gt; large_loci_size\n        )\n        large_loci_wbc = StudyLocus(\n            wbc.df.alias(\"wbc\")\n            .join(\n                large_loci.df.alias(\"ll\"),\n                (f.col(\"wbc.studyId\") == f.col(\"ll.studyId\"))\n                &amp; (f.col(\"wbc.chromosome\") == f.col(\"ll.chromosome\"))\n                &amp; (\n                    f.col(\"wbc.position\").between(\n                        f.col(\"ll.locusStart\"), f.col(\"ll.locusEnd\")\n                    )\n                ),\n                \"semi\",\n            )\n            .withColumns(\n                {\n                    \"locusStart\": f.col(\"position\") - large_loci_size // 2,\n                    \"locusEnd\": f.col(\"position\") + large_loci_size // 2,\n                }\n            ),\n            StudyLocus.get_schema(),\n        )\n        return StudyLocus(\n            large_loci_wbc.df.unionByName(small_loci.df),\n            StudyLocus.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/clumping/#gentropy.method.locus_breaker_clumping.LocusBreakerClumping.locus_breaker","title":"<code>locus_breaker(summary_statistics: SummaryStatistics, baseline_pvalue_cutoff: float, distance_cutoff: int, pvalue_cutoff: float, flanking_distance: int) -&gt; StudyLocus</code>  <code>staticmethod</code>","text":"<p>Identify GWAS associated loci based on the provided p-value and distance cutoff.</p> <ul> <li>The GWAS associated loci identified by this method have a varying width, and are separated by a distance greater than the provided distance cutoff.</li> <li>The distance is only calculted between single point associations that reach the baseline p-value cutoff.</li> <li>As the width of the selected genomic region dynamically depends on the loci, the resulting StudyLocus object will contain the locus start and end position.</li> <li>To ensure completeness, the locus is extended by a flanking distance in both ends.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>summary_statistics</code> <code>SummaryStatistics</code> <p>Input summary statistics dataset.</p> required <code>baseline_pvalue_cutoff</code> <code>float</code> <p>baseline significance we consider for the locus.</p> required <code>distance_cutoff</code> <code>int</code> <p>minimum distance that separates two loci.</p> required <code>pvalue_cutoff</code> <code>float</code> <p>the minimum significance the locus should have.</p> required <code>flanking_distance</code> <code>int</code> <p>the distance to extend the locus in both directions.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>clumped study loci with locus start and end positions + lead variant from the locus.</p> Source code in <code>src/gentropy/method/locus_breaker_clumping.py</code> <pre><code>@staticmethod\ndef locus_breaker(\n    summary_statistics: SummaryStatistics,\n    baseline_pvalue_cutoff: float,\n    distance_cutoff: int,\n    pvalue_cutoff: float,\n    flanking_distance: int,\n) -&gt; StudyLocus:\n    \"\"\"Identify GWAS associated loci based on the provided p-value and distance cutoff.\n\n    - The GWAS associated loci identified by this method have a varying width, and are separated by a distance greater than the provided distance cutoff.\n    - The distance is only calculted between single point associations that reach the baseline p-value cutoff.\n    - As the width of the selected genomic region dynamically depends on the loci, the resulting StudyLocus object will contain the locus start and end position.\n    - To ensure completeness, the locus is extended by a flanking distance in both ends.\n\n    Args:\n        summary_statistics (SummaryStatistics): Input summary statistics dataset.\n        baseline_pvalue_cutoff (float): baseline significance we consider for the locus.\n        distance_cutoff (int): minimum distance that separates two loci.\n        pvalue_cutoff (float): the minimum significance the locus should have.\n        flanking_distance (int): the distance to extend the locus in both directions.\n\n    Returns:\n        StudyLocus: clumped study loci with locus start and end positions + lead variant from the locus.\n    \"\"\"\n    # Extract columns from the summary statistics:\n    columns_sumstats_columns = summary_statistics.df.columns\n    # Convert pvalue_cutoff to neglog scale:\n    neglog_pv_cutoff = -np.log10(pvalue_cutoff)\n\n    # First window to calculate the distance between consecutive positions:\n    w1 = Window.partitionBy(\"studyId\", \"chromosome\").orderBy(\"position\")\n\n    # Second window to calculate the locus start and end:\n    w2 = (\n        Window.partitionBy(\"studyId\", \"chromosome\", \"locusStart\")\n        .orderBy(\"position\")\n        .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n    )\n\n    # Third window to rank the variants within the locus based on neglog p-value to find top loci:\n    w3 = Window.partitionBy(\n        \"studyId\", \"chromosome\", \"locusStart\", \"locusEnd\"\n    ).orderBy(f.col(\"negLogPValue\").desc())\n\n    return StudyLocus(\n        _df=(\n            # Applying the baseline p-value cutoff:\n            summary_statistics.pvalue_filter(baseline_pvalue_cutoff)\n            # Calculating the neglog p-value for easier sorting:\n            .df.withColumn(\n                \"negLogPValue\",\n                calculate_neglog_pvalue(\n                    f.col(\"pValueMantissa\"), f.col(\"pValueExponent\")\n                ),\n            )\n            # Calculating the distance between consecutive positions, then identifying the locus start and end:\n            .withColumn(\"next_position\", f.lag(f.col(\"position\")).over(w1))\n            .withColumn(\"distance\", f.col(\"position\") - f.col(\"next_position\"))\n            .withColumn(\n                \"locusStart\",\n                f.when(\n                    (f.col(\"distance\") &gt; distance_cutoff)\n                    | f.col(\"distance\").isNull(),\n                    f.col(\"position\"),\n                ),\n            )\n            .withColumn(\n                \"locusStart\",\n                f.when(\n                    f.last(f.col(\"locusStart\") - flanking_distance, True).over(\n                        w1.rowsBetween(-sys.maxsize, 0)\n                    )\n                    &gt; 0,\n                    f.last(f.col(\"locusStart\") - flanking_distance, True).over(\n                        w1.rowsBetween(-sys.maxsize, 0)\n                    ),\n                ).otherwise(f.lit(0)),\n            )\n            .withColumn(\n                \"locusEnd\", f.max(f.col(\"position\") + flanking_distance).over(w2)\n            )\n            .withColumn(\"rank\", f.rank().over(w3))\n            .filter(\n                (f.col(\"rank\") == 1) &amp; (f.col(\"negLogPValue\") &gt; neglog_pv_cutoff)\n            )\n            .select(\n                *columns_sumstats_columns,\n                # To make sure that the type of locusStart and locusEnd follows schema of StudyLocus:\n                f.col(\"locusStart\").cast(t.IntegerType()).alias(\"locusStart\"),\n                f.col(\"locusEnd\").cast(t.IntegerType()).alias(\"locusEnd\"),\n                f.lit(None)\n                .cast(t.ArrayType(t.StringType()))\n                .alias(\"qualityControls\"),\n                StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n            )\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/clumping/#gentropy.method.locus_breaker_clumping.LocusBreakerClumping.process_locus_breaker_output","title":"<code>process_locus_breaker_output(lbc: StudyLocus, wbc: StudyLocus, large_loci_size: int) -&gt; StudyLocus</code>  <code>staticmethod</code>","text":"<p>Process the locus breaker method result, and run window-based clumping on large loci.</p> <p>Parameters:</p> Name Type Description Default <code>lbc</code> <code>StudyLocus</code> <p>StudyLocus object from locus-breaker clumping.</p> required <code>wbc</code> <code>StudyLocus</code> <p>StudyLocus object from window-based clumping.</p> required <code>large_loci_size</code> <code>int</code> <p>the size to define large loci which should be broken with wbc.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>clumped study loci with large loci broken by window-based clumping.</p> Source code in <code>src/gentropy/method/locus_breaker_clumping.py</code> <pre><code>@staticmethod\ndef process_locus_breaker_output(\n    lbc: StudyLocus,\n    wbc: StudyLocus,\n    large_loci_size: int,\n) -&gt; StudyLocus:\n    \"\"\"Process the locus breaker method result, and run window-based clumping on large loci.\n\n    Args:\n        lbc (StudyLocus): StudyLocus object from locus-breaker clumping.\n        wbc (StudyLocus): StudyLocus object from window-based clumping.\n        large_loci_size (int): the size to define large loci which should be broken with wbc.\n\n    Returns:\n        StudyLocus: clumped study loci with large loci broken by window-based clumping.\n    \"\"\"\n    large_loci_size = int(large_loci_size)\n    small_loci = lbc.filter(\n        (f.col(\"locusEnd\") - f.col(\"locusStart\")) &lt;= large_loci_size\n    )\n    large_loci = lbc.filter(\n        (f.col(\"locusEnd\") - f.col(\"locusStart\")) &gt; large_loci_size\n    )\n    large_loci_wbc = StudyLocus(\n        wbc.df.alias(\"wbc\")\n        .join(\n            large_loci.df.alias(\"ll\"),\n            (f.col(\"wbc.studyId\") == f.col(\"ll.studyId\"))\n            &amp; (f.col(\"wbc.chromosome\") == f.col(\"ll.chromosome\"))\n            &amp; (\n                f.col(\"wbc.position\").between(\n                    f.col(\"ll.locusStart\"), f.col(\"ll.locusEnd\")\n                )\n            ),\n            \"semi\",\n        )\n        .withColumns(\n            {\n                \"locusStart\": f.col(\"position\") - large_loci_size // 2,\n                \"locusEnd\": f.col(\"position\") + large_loci_size // 2,\n            }\n        ),\n        StudyLocus.get_schema(),\n    )\n    return StudyLocus(\n        large_loci_wbc.df.unionByName(small_loci.df),\n        StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/coloc/","title":"Coloc","text":""},{"location":"python_api/methods/coloc/#gentropy.method.colocalisation.Coloc","title":"<code>gentropy.method.colocalisation.Coloc</code>","text":"<p>               Bases: <code>ColocalisationMethodInterface</code></p> <p>Calculate bayesian colocalisation based on overlapping signals from credible sets.</p> <p>Based on the R COLOC package, which uses the Bayes factors from the credible set to estimate the posterior probability of colocalisation. This method makes the simplifying assumption that only one single causal variant exists for any given trait in any genomic region.</p> Hypothesis Description H<sub>0</sub> no association with either trait in the region H<sub>1</sub> association with trait 1 only H<sub>2</sub> association with trait 2 only H<sub>3</sub> both traits are associated, but have different single causal variants H<sub>4</sub> both traits are associated and share the same single causal variant <p>Bayes factors required</p> <p>Coloc requires the availability of Bayes factors (BF) for each variant in the credible set (<code>logBF</code> column).</p> <p>Attributes:</p> Name Type Description <code>PSEUDOCOUNT</code> <code>float</code> <p>Pseudocount to avoid log(0). Defaults to 1e-10.</p> <code>OVERLAP_SIZE_CUTOFF</code> <code>int</code> <p>Minimum number of overlapping variants bfore filtering. Defaults to 5.</p> <code>POSTERIOR_CUTOFF</code> <code>float</code> <p>Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.5.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>class Coloc(ColocalisationMethodInterface):\n    \"\"\"Calculate bayesian colocalisation based on overlapping signals from credible sets.\n\n    Based on the [R COLOC package](https://github.com/chr1swallace/coloc/blob/main/R/claudia.R), which uses the Bayes factors from the credible set to estimate the posterior probability of colocalisation. This method makes the simplifying assumption that **only one single causal variant** exists for any given trait in any genomic region.\n\n    | Hypothesis    | Description                                                           |\n    | ------------- | --------------------------------------------------------------------- |\n    | H&lt;sub&gt;0&lt;/sub&gt; | no association with either trait in the region                        |\n    | H&lt;sub&gt;1&lt;/sub&gt; | association with trait 1 only                                         |\n    | H&lt;sub&gt;2&lt;/sub&gt; | association with trait 2 only                                         |\n    | H&lt;sub&gt;3&lt;/sub&gt; | both traits are associated, but have different single causal variants |\n    | H&lt;sub&gt;4&lt;/sub&gt; | both traits are associated and share the same single causal variant   |\n\n    !!! warning \"Bayes factors required\"\n\n        Coloc requires the availability of Bayes factors (BF) for each variant in the credible set (`logBF` column).\n\n    Attributes:\n        PSEUDOCOUNT (float): Pseudocount to avoid log(0). Defaults to 1e-10.\n        OVERLAP_SIZE_CUTOFF (int): Minimum number of overlapping variants bfore filtering. Defaults to 5.\n        POSTERIOR_CUTOFF (float): Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.5.\n    \"\"\"\n\n    METHOD_NAME: str = \"COLOC\"\n    METHOD_METRIC: str = \"h4\"\n    PSEUDOCOUNT: float = 1e-10\n    OVERLAP_SIZE_CUTOFF: int = 5\n    POSTERIOR_CUTOFF: float = 0.1\n\n    @staticmethod\n    def _get_posteriors(all_bfs: NDArray[np.float64]) -&gt; DenseVector:\n        \"\"\"Calculate posterior probabilities for each hypothesis.\n\n        Args:\n            all_bfs (NDArray[np.float64]): h0-h4 bayes factors\n\n        Returns:\n            DenseVector: Posterior\n\n        Example:\n            &gt;&gt;&gt; l = np.array([0.2, 0.1, 0.05, 0])\n            &gt;&gt;&gt; Coloc._get_posteriors(l)\n            DenseVector([0.279, 0.2524, 0.2401, 0.2284])\n        \"\"\"\n        diff = all_bfs - get_logsum(all_bfs)\n        bfs_posteriors = np.exp(diff)\n        return Vectors.dense(bfs_posteriors)\n\n    @classmethod\n    def colocalise(\n        cls: type[Coloc],\n        overlapping_signals: StudyLocusOverlap,\n        **kwargs: float,\n    ) -&gt; Colocalisation:\n        \"\"\"Calculate bayesian colocalisation based on overlapping signals.\n\n        Args:\n            overlapping_signals (StudyLocusOverlap): overlapping peaks\n            **kwargs (float): Additional parameters passed to the colocalise method.\n\n        Keyword Args:\n            priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4.\n            priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4.\n            priorc12 (float): Prior on variant being causal for traits 1 and 2. Defaults to 1e-5.\n\n        Returns:\n            Colocalisation: Colocalisation results\n\n        Raises:\n            TypeError: When passed incorrect prior argument types.\n        \"\"\"\n        # Ensure priors are always present, even if not passed\n        priorc1 = kwargs.get(\"priorc1\") or 1e-4\n        priorc2 = kwargs.get(\"priorc2\") or 1e-4\n        priorc12 = kwargs.get(\"priorc12\") or 1e-5\n        priors = [priorc1, priorc2, priorc12]\n        if any(not isinstance(prior, float) for prior in priors):\n            raise TypeError(\n                \"Passed incorrect type(s) for prior parameters. got %s\",\n                {type(p): p for p in priors},\n            )\n\n        # register udfs\n        logsum = f.udf(get_logsum, DoubleType())\n        posteriors = f.udf(Coloc._get_posteriors, VectorUDT())\n        return Colocalisation(\n            _df=(\n                overlapping_signals.df.withColumn(\n                    \"tagVariantSource\", get_tag_variant_source(f.col(\"statistics\"))\n                )\n                .select(\"*\", \"statistics.*\")\n                # Before summing log_BF columns nulls need to be filled with 0:\n                .fillna(\n                    0,\n                    subset=[\n                        \"left_logBF\",\n                        \"right_logBF\",\n                        \"left_posteriorProbability\",\n                        \"right_posteriorProbability\",\n                    ],\n                )\n                # Sum of log_BFs for each pair of signals\n                .withColumn(\n                    \"sum_log_bf\",\n                    f.col(\"left_logBF\") + f.col(\"right_logBF\"),\n                )\n                # Group by overlapping peak and generating dense vectors of log_BF:\n                .groupBy(\n                    \"chromosome\",\n                    \"leftStudyLocusId\",\n                    \"rightStudyLocusId\",\n                    \"rightStudyType\",\n                )\n                .agg(\n                    f.size(\n                        f.filter(\n                            f.collect_list(f.col(\"tagVariantSource\")),\n                            lambda x: x == \"both\",\n                        )\n                    )\n                    .cast(t.LongType())\n                    .alias(\"numberColocalisingVariants\"),\n                    fml.array_to_vector(f.collect_list(f.col(\"left_logBF\"))).alias(\n                        \"left_logBF\"\n                    ),\n                    fml.array_to_vector(f.collect_list(f.col(\"right_logBF\"))).alias(\n                        \"right_logBF\"\n                    ),\n                    fml.array_to_vector(\n                        f.collect_list(f.col(\"left_posteriorProbability\"))\n                    ).alias(\"left_posteriorProbability\"),\n                    fml.array_to_vector(\n                        f.collect_list(f.col(\"right_posteriorProbability\"))\n                    ).alias(\"right_posteriorProbability\"),\n                    fml.array_to_vector(f.collect_list(f.col(\"sum_log_bf\"))).alias(\n                        \"sum_log_bf\"\n                    ),\n                    f.collect_list(f.col(\"tagVariantSource\")).alias(\n                        \"tagVariantSourceList\"\n                    ),\n                )\n                .withColumn(\"logsum1\", logsum(f.col(\"left_logBF\")))\n                .withColumn(\"logsum2\", logsum(f.col(\"right_logBF\")))\n                .withColumn(\"logsum12\", logsum(f.col(\"sum_log_bf\")))\n                .drop(\"left_logBF\", \"right_logBF\", \"sum_log_bf\")\n                # Add priors\n                # priorc1 Prior on variant being causal for trait 1\n                .withColumn(\"priorc1\", f.lit(priorc1))\n                # priorc2 Prior on variant being causal for trait 2\n                .withColumn(\"priorc2\", f.lit(priorc2))\n                # priorc12 Prior on variant being causal for traits 1 and 2\n                .withColumn(\"priorc12\", f.lit(priorc12))\n                # h0-h2\n                .withColumn(\"lH0bf\", f.lit(0))\n                .withColumn(\"lH1bf\", f.log(f.col(\"priorc1\")) + f.col(\"logsum1\"))\n                .withColumn(\"lH2bf\", f.log(f.col(\"priorc2\")) + f.col(\"logsum2\"))\n                # h3\n                .withColumn(\"sumlogsum\", f.col(\"logsum1\") + f.col(\"logsum2\"))\n                .withColumn(\"max\", f.greatest(\"sumlogsum\", \"logsum12\"))\n                .withColumn(\n                    \"anySnpBothSidesHigh\",\n                    f.aggregate(\n                        f.transform(\n                            f.arrays_zip(\n                                fml.vector_to_array(f.col(\"left_posteriorProbability\")),\n                                fml.vector_to_array(\n                                    f.col(\"right_posteriorProbability\")\n                                ),\n                                f.col(\"tagVariantSourceList\"),\n                            ),\n                            # row[\"0\"] = left PP, row[\"1\"] = right PP, row[\"tagVariantSourceList\"]\n                            lambda row: f.when(\n                                (row[\"tagVariantSourceList\"] == \"both\")\n                                &amp; (row[\"0\"] &gt; Coloc.POSTERIOR_CUTOFF)\n                                &amp; (row[\"1\"] &gt; Coloc.POSTERIOR_CUTOFF),\n                                1.0,\n                            ).otherwise(0.0),\n                        ),\n                        f.lit(0.0),\n                        lambda acc, x: acc + x,\n                    )\n                    &gt; 0,  # True if sum of these 1.0's &gt; 0\n                )\n                .filter(\n                    (f.col(\"numberColocalisingVariants\") &gt; Coloc.OVERLAP_SIZE_CUTOFF)\n                    | (f.col(\"anySnpBothSidesHigh\"))\n                )\n                .withColumn(\n                    \"logdiff\",\n                    f.when(\n                        (f.col(\"sumlogsum\") == f.col(\"logsum12\")),\n                        Coloc.PSEUDOCOUNT,\n                    ).otherwise(\n                        f.col(\"max\")\n                        + f.log(\n                            f.exp(f.col(\"sumlogsum\") - f.col(\"max\"))\n                            - f.exp(f.col(\"logsum12\") - f.col(\"max\"))\n                        )\n                    ),\n                )\n                .withColumn(\n                    \"lH3bf\",\n                    f.log(f.col(\"priorc1\"))\n                    + f.log(f.col(\"priorc2\"))\n                    + f.col(\"logdiff\"),\n                )\n                .drop(\"right_logsum\", \"left_logsum\", \"sumlogsum\", \"max\", \"logdiff\")\n                # h4\n                .withColumn(\"lH4bf\", f.log(f.col(\"priorc12\")) + f.col(\"logsum12\"))\n                # cleaning\n                .drop(\n                    \"priorc1\", \"priorc2\", \"priorc12\", \"logsum1\", \"logsum2\", \"logsum12\"\n                )\n                # posteriors\n                .withColumn(\n                    \"allBF\",\n                    fml.array_to_vector(\n                        f.array(\n                            f.col(\"lH0bf\"),\n                            f.col(\"lH1bf\"),\n                            f.col(\"lH2bf\"),\n                            f.col(\"lH3bf\"),\n                            f.col(\"lH4bf\"),\n                        )\n                    ),\n                )\n                .withColumn(\n                    \"posteriors\", fml.vector_to_array(posteriors(f.col(\"allBF\")))\n                )\n                .withColumn(\"h0\", f.col(\"posteriors\").getItem(0))\n                .withColumn(\"h1\", f.col(\"posteriors\").getItem(1))\n                .withColumn(\"h2\", f.col(\"posteriors\").getItem(2))\n                .withColumn(\"h3\", f.col(\"posteriors\").getItem(3))\n                .withColumn(\"h4\", f.col(\"posteriors\").getItem(4))\n                # clean up\n                .drop(\n                    \"posteriors\",\n                    \"allBF\",\n                    \"lH0bf\",\n                    \"lH1bf\",\n                    \"lH2bf\",\n                    \"lH3bf\",\n                    \"lH4bf\",\n                    \"left_posteriorProbability\",\n                    \"right_posteriorProbability\",\n                    \"tagVariantSourceList\",\n                    \"anySnpBothSidesHigh\",\n                )\n                .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n                .join(\n                    overlapping_signals.calculate_beta_ratio(),\n                    on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                    how=\"left\",\n                )\n            ),\n            _schema=Colocalisation.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/coloc/#gentropy.method.colocalisation.Coloc.colocalise","title":"<code>colocalise(overlapping_signals: StudyLocusOverlap, **kwargs: float) -&gt; Colocalisation</code>  <code>classmethod</code>","text":"<p>Calculate bayesian colocalisation based on overlapping signals.</p> <p>Parameters:</p> Name Type Description Default <code>overlapping_signals</code> <code>StudyLocusOverlap</code> <p>overlapping peaks</p> required <code>**kwargs</code> <code>float</code> <p>Additional parameters passed to the colocalise method.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>priorc1</code> <code>float</code> <p>Prior on variant being causal for trait 1. Defaults to 1e-4.</p> <code>priorc2</code> <code>float</code> <p>Prior on variant being causal for trait 2. Defaults to 1e-4.</p> <code>priorc12</code> <code>float</code> <p>Prior on variant being causal for traits 1 and 2. Defaults to 1e-5.</p> <p>Returns:</p> Name Type Description <code>Colocalisation</code> <code>Colocalisation</code> <p>Colocalisation results</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>When passed incorrect prior argument types.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>@classmethod\ndef colocalise(\n    cls: type[Coloc],\n    overlapping_signals: StudyLocusOverlap,\n    **kwargs: float,\n) -&gt; Colocalisation:\n    \"\"\"Calculate bayesian colocalisation based on overlapping signals.\n\n    Args:\n        overlapping_signals (StudyLocusOverlap): overlapping peaks\n        **kwargs (float): Additional parameters passed to the colocalise method.\n\n    Keyword Args:\n        priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4.\n        priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4.\n        priorc12 (float): Prior on variant being causal for traits 1 and 2. Defaults to 1e-5.\n\n    Returns:\n        Colocalisation: Colocalisation results\n\n    Raises:\n        TypeError: When passed incorrect prior argument types.\n    \"\"\"\n    # Ensure priors are always present, even if not passed\n    priorc1 = kwargs.get(\"priorc1\") or 1e-4\n    priorc2 = kwargs.get(\"priorc2\") or 1e-4\n    priorc12 = kwargs.get(\"priorc12\") or 1e-5\n    priors = [priorc1, priorc2, priorc12]\n    if any(not isinstance(prior, float) for prior in priors):\n        raise TypeError(\n            \"Passed incorrect type(s) for prior parameters. got %s\",\n            {type(p): p for p in priors},\n        )\n\n    # register udfs\n    logsum = f.udf(get_logsum, DoubleType())\n    posteriors = f.udf(Coloc._get_posteriors, VectorUDT())\n    return Colocalisation(\n        _df=(\n            overlapping_signals.df.withColumn(\n                \"tagVariantSource\", get_tag_variant_source(f.col(\"statistics\"))\n            )\n            .select(\"*\", \"statistics.*\")\n            # Before summing log_BF columns nulls need to be filled with 0:\n            .fillna(\n                0,\n                subset=[\n                    \"left_logBF\",\n                    \"right_logBF\",\n                    \"left_posteriorProbability\",\n                    \"right_posteriorProbability\",\n                ],\n            )\n            # Sum of log_BFs for each pair of signals\n            .withColumn(\n                \"sum_log_bf\",\n                f.col(\"left_logBF\") + f.col(\"right_logBF\"),\n            )\n            # Group by overlapping peak and generating dense vectors of log_BF:\n            .groupBy(\n                \"chromosome\",\n                \"leftStudyLocusId\",\n                \"rightStudyLocusId\",\n                \"rightStudyType\",\n            )\n            .agg(\n                f.size(\n                    f.filter(\n                        f.collect_list(f.col(\"tagVariantSource\")),\n                        lambda x: x == \"both\",\n                    )\n                )\n                .cast(t.LongType())\n                .alias(\"numberColocalisingVariants\"),\n                fml.array_to_vector(f.collect_list(f.col(\"left_logBF\"))).alias(\n                    \"left_logBF\"\n                ),\n                fml.array_to_vector(f.collect_list(f.col(\"right_logBF\"))).alias(\n                    \"right_logBF\"\n                ),\n                fml.array_to_vector(\n                    f.collect_list(f.col(\"left_posteriorProbability\"))\n                ).alias(\"left_posteriorProbability\"),\n                fml.array_to_vector(\n                    f.collect_list(f.col(\"right_posteriorProbability\"))\n                ).alias(\"right_posteriorProbability\"),\n                fml.array_to_vector(f.collect_list(f.col(\"sum_log_bf\"))).alias(\n                    \"sum_log_bf\"\n                ),\n                f.collect_list(f.col(\"tagVariantSource\")).alias(\n                    \"tagVariantSourceList\"\n                ),\n            )\n            .withColumn(\"logsum1\", logsum(f.col(\"left_logBF\")))\n            .withColumn(\"logsum2\", logsum(f.col(\"right_logBF\")))\n            .withColumn(\"logsum12\", logsum(f.col(\"sum_log_bf\")))\n            .drop(\"left_logBF\", \"right_logBF\", \"sum_log_bf\")\n            # Add priors\n            # priorc1 Prior on variant being causal for trait 1\n            .withColumn(\"priorc1\", f.lit(priorc1))\n            # priorc2 Prior on variant being causal for trait 2\n            .withColumn(\"priorc2\", f.lit(priorc2))\n            # priorc12 Prior on variant being causal for traits 1 and 2\n            .withColumn(\"priorc12\", f.lit(priorc12))\n            # h0-h2\n            .withColumn(\"lH0bf\", f.lit(0))\n            .withColumn(\"lH1bf\", f.log(f.col(\"priorc1\")) + f.col(\"logsum1\"))\n            .withColumn(\"lH2bf\", f.log(f.col(\"priorc2\")) + f.col(\"logsum2\"))\n            # h3\n            .withColumn(\"sumlogsum\", f.col(\"logsum1\") + f.col(\"logsum2\"))\n            .withColumn(\"max\", f.greatest(\"sumlogsum\", \"logsum12\"))\n            .withColumn(\n                \"anySnpBothSidesHigh\",\n                f.aggregate(\n                    f.transform(\n                        f.arrays_zip(\n                            fml.vector_to_array(f.col(\"left_posteriorProbability\")),\n                            fml.vector_to_array(\n                                f.col(\"right_posteriorProbability\")\n                            ),\n                            f.col(\"tagVariantSourceList\"),\n                        ),\n                        # row[\"0\"] = left PP, row[\"1\"] = right PP, row[\"tagVariantSourceList\"]\n                        lambda row: f.when(\n                            (row[\"tagVariantSourceList\"] == \"both\")\n                            &amp; (row[\"0\"] &gt; Coloc.POSTERIOR_CUTOFF)\n                            &amp; (row[\"1\"] &gt; Coloc.POSTERIOR_CUTOFF),\n                            1.0,\n                        ).otherwise(0.0),\n                    ),\n                    f.lit(0.0),\n                    lambda acc, x: acc + x,\n                )\n                &gt; 0,  # True if sum of these 1.0's &gt; 0\n            )\n            .filter(\n                (f.col(\"numberColocalisingVariants\") &gt; Coloc.OVERLAP_SIZE_CUTOFF)\n                | (f.col(\"anySnpBothSidesHigh\"))\n            )\n            .withColumn(\n                \"logdiff\",\n                f.when(\n                    (f.col(\"sumlogsum\") == f.col(\"logsum12\")),\n                    Coloc.PSEUDOCOUNT,\n                ).otherwise(\n                    f.col(\"max\")\n                    + f.log(\n                        f.exp(f.col(\"sumlogsum\") - f.col(\"max\"))\n                        - f.exp(f.col(\"logsum12\") - f.col(\"max\"))\n                    )\n                ),\n            )\n            .withColumn(\n                \"lH3bf\",\n                f.log(f.col(\"priorc1\"))\n                + f.log(f.col(\"priorc2\"))\n                + f.col(\"logdiff\"),\n            )\n            .drop(\"right_logsum\", \"left_logsum\", \"sumlogsum\", \"max\", \"logdiff\")\n            # h4\n            .withColumn(\"lH4bf\", f.log(f.col(\"priorc12\")) + f.col(\"logsum12\"))\n            # cleaning\n            .drop(\n                \"priorc1\", \"priorc2\", \"priorc12\", \"logsum1\", \"logsum2\", \"logsum12\"\n            )\n            # posteriors\n            .withColumn(\n                \"allBF\",\n                fml.array_to_vector(\n                    f.array(\n                        f.col(\"lH0bf\"),\n                        f.col(\"lH1bf\"),\n                        f.col(\"lH2bf\"),\n                        f.col(\"lH3bf\"),\n                        f.col(\"lH4bf\"),\n                    )\n                ),\n            )\n            .withColumn(\n                \"posteriors\", fml.vector_to_array(posteriors(f.col(\"allBF\")))\n            )\n            .withColumn(\"h0\", f.col(\"posteriors\").getItem(0))\n            .withColumn(\"h1\", f.col(\"posteriors\").getItem(1))\n            .withColumn(\"h2\", f.col(\"posteriors\").getItem(2))\n            .withColumn(\"h3\", f.col(\"posteriors\").getItem(3))\n            .withColumn(\"h4\", f.col(\"posteriors\").getItem(4))\n            # clean up\n            .drop(\n                \"posteriors\",\n                \"allBF\",\n                \"lH0bf\",\n                \"lH1bf\",\n                \"lH2bf\",\n                \"lH3bf\",\n                \"lH4bf\",\n                \"left_posteriorProbability\",\n                \"right_posteriorProbability\",\n                \"tagVariantSourceList\",\n                \"anySnpBothSidesHigh\",\n            )\n            .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n            .join(\n                overlapping_signals.calculate_beta_ratio(),\n                on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                how=\"left\",\n            )\n        ),\n        _schema=Colocalisation.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/ecaviar/","title":"eCAVIAR","text":""},{"location":"python_api/methods/ecaviar/#gentropy.method.colocalisation.ECaviar","title":"<code>gentropy.method.colocalisation.ECaviar</code>","text":"<p>               Bases: <code>ColocalisationMethodInterface</code></p> <p>ECaviar-based colocalisation analysis.</p> <p>It extends CAVIAR\u00a0framework to explicitly estimate the posterior probability that the same variant is causal in 2 studies while accounting for the uncertainty of LD. eCAVIAR computes the colocalization posterior probability (CLPP) by utilizing the marginal posterior probabilities. This framework allows for multiple variants to be causal in a single locus.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>class ECaviar(ColocalisationMethodInterface):\n    \"\"\"ECaviar-based colocalisation analysis.\n\n    It extends [CAVIAR](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5142122/#bib18)\u00a0framework to explicitly estimate the posterior probability that the same variant is causal in 2 studies while accounting for the uncertainty of LD. eCAVIAR computes the colocalization posterior probability (**CLPP**) by utilizing the marginal posterior probabilities. This framework allows for **multiple variants to be causal** in a single locus.\n    \"\"\"\n\n    METHOD_NAME: str = \"eCAVIAR\"\n    METHOD_METRIC: str = \"clpp\"\n\n    @staticmethod\n    def _get_clpp(left_pp: Column, right_pp: Column) -&gt; Column:\n        \"\"\"Calculate the colocalisation posterior probability (CLPP).\n\n        If the fact that the same variant is found causal for two studies are independent events,\n        CLPP is defined as the product of posterior porbabilities that a variant is causal in both studies.\n\n        Args:\n            left_pp (Column): left posterior probability\n            right_pp (Column): right posterior probability\n\n        Returns:\n            Column: CLPP\n\n        Examples:\n            &gt;&gt;&gt; d = [{\"left_pp\": 0.5, \"right_pp\": 0.5}, {\"left_pp\": 0.25, \"right_pp\": 0.75}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d)\n            &gt;&gt;&gt; df.withColumn(\"clpp\", ECaviar._get_clpp(f.col(\"left_pp\"), f.col(\"right_pp\"))).show()\n            +-------+--------+------+\n            |left_pp|right_pp|  clpp|\n            +-------+--------+------+\n            |    0.5|     0.5|  0.25|\n            |   0.25|    0.75|0.1875|\n            +-------+--------+------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return left_pp * right_pp\n\n    @classmethod\n    def colocalise(\n        cls: type[ECaviar],\n        overlapping_signals: StudyLocusOverlap,\n        **kwargs: Any,\n    ) -&gt; Colocalisation:\n        \"\"\"Calculate bayesian colocalisation based on overlapping signals.\n\n        Args:\n            overlapping_signals (StudyLocusOverlap): overlapping signals.\n            **kwargs (Any): Additional parameters passed to the colocalise method.\n\n        Returns:\n            Colocalisation: colocalisation results based on eCAVIAR.\n        \"\"\"\n        return Colocalisation(\n            _df=(\n                overlapping_signals.df.withColumns(\n                    {\n                        \"clpp\": ECaviar._get_clpp(\n                            f.col(\"statistics.left_posteriorProbability\"),\n                            f.col(\"statistics.right_posteriorProbability\"),\n                        ),\n                        \"tagVariantSource\": get_tag_variant_source(f.col(\"statistics\")),\n                    }\n                )\n                .groupBy(\n                    \"leftStudyLocusId\",\n                    \"rightStudyLocusId\",\n                    \"rightStudyType\",\n                    \"chromosome\",\n                )\n                .agg(\n                    # Count the number of tag variants that can be found in both loci:\n                    f.size(\n                        f.filter(\n                            f.collect_list(f.col(\"tagVariantSource\")),\n                            lambda x: x == \"both\",\n                        )\n                    )\n                    .cast(t.LongType())\n                    .alias(\"numberColocalisingVariants\"),\n                    f.sum(f.col(\"clpp\")).alias(\"clpp\"),\n                )\n                .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n                .join(\n                    overlapping_signals.calculate_beta_ratio(),\n                    on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                    how=\"left\",\n                )\n            ),\n            _schema=Colocalisation.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/ecaviar/#gentropy.method.colocalisation.ECaviar.colocalise","title":"<code>colocalise(overlapping_signals: StudyLocusOverlap, **kwargs: Any) -&gt; Colocalisation</code>  <code>classmethod</code>","text":"<p>Calculate bayesian colocalisation based on overlapping signals.</p> <p>Parameters:</p> Name Type Description Default <code>overlapping_signals</code> <code>StudyLocusOverlap</code> <p>overlapping signals.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to the colocalise method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Colocalisation</code> <code>Colocalisation</code> <p>colocalisation results based on eCAVIAR.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>@classmethod\ndef colocalise(\n    cls: type[ECaviar],\n    overlapping_signals: StudyLocusOverlap,\n    **kwargs: Any,\n) -&gt; Colocalisation:\n    \"\"\"Calculate bayesian colocalisation based on overlapping signals.\n\n    Args:\n        overlapping_signals (StudyLocusOverlap): overlapping signals.\n        **kwargs (Any): Additional parameters passed to the colocalise method.\n\n    Returns:\n        Colocalisation: colocalisation results based on eCAVIAR.\n    \"\"\"\n    return Colocalisation(\n        _df=(\n            overlapping_signals.df.withColumns(\n                {\n                    \"clpp\": ECaviar._get_clpp(\n                        f.col(\"statistics.left_posteriorProbability\"),\n                        f.col(\"statistics.right_posteriorProbability\"),\n                    ),\n                    \"tagVariantSource\": get_tag_variant_source(f.col(\"statistics\")),\n                }\n            )\n            .groupBy(\n                \"leftStudyLocusId\",\n                \"rightStudyLocusId\",\n                \"rightStudyType\",\n                \"chromosome\",\n            )\n            .agg(\n                # Count the number of tag variants that can be found in both loci:\n                f.size(\n                    f.filter(\n                        f.collect_list(f.col(\"tagVariantSource\")),\n                        lambda x: x == \"both\",\n                    )\n                )\n                .cast(t.LongType())\n                .alias(\"numberColocalisingVariants\"),\n                f.sum(f.col(\"clpp\")).alias(\"clpp\"),\n            )\n            .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n            .join(\n                overlapping_signals.calculate_beta_ratio(),\n                on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                how=\"left\",\n            )\n        ),\n        _schema=Colocalisation.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/ld_annotator/","title":"LDAnnotator","text":""},{"location":"python_api/methods/ld_annotator/#gentropy.method.ld.LDAnnotator","title":"<code>gentropy.method.ld.LDAnnotator</code>","text":"<p>Class to annotate linkage disequilibrium (LD) operations from GnomAD.</p> Source code in <code>src/gentropy/method/ld.py</code> <pre><code>class LDAnnotator:\n    \"\"\"Class to annotate linkage disequilibrium (LD) operations from GnomAD.\"\"\"\n\n    @staticmethod\n    def _get_major_population(ordered_populations: Column) -&gt; Column:\n        \"\"\"Get major population based on an ldPopulationStructure array ordered by relativeSampleSize.\n\n        If there is a tie for the major population, nfe is selected if it is one of the major populations.\n        The first population in the array is selected if there is no tie for the major population, or there is a tie but nfe is not one of the major populations.\n\n        Args:\n            ordered_populations (Column): ldPopulationStructure array ordered by relativeSampleSize\n\n        Returns:\n            Column: major population\n        \"\"\"\n        major_population_size = ordered_populations[\"relativeSampleSize\"][0]\n        major_populations = f.filter(\n            ordered_populations,\n            lambda x: x[\"relativeSampleSize\"] == major_population_size,\n        )\n        # Check if nfe (Non-Finnish European) is one of the major populations\n        has_nfe = f.filter(major_populations, lambda x: x[\"ldPopulation\"] == \"nfe\")\n        return f.when(\n            (f.size(major_populations) &gt; 1) &amp; (f.size(has_nfe) == 1), f.lit(\"nfe\")\n        ).otherwise(ordered_populations[\"ldPopulation\"][0])\n\n    @staticmethod\n    def _calculate_r2_major(ld_set: Column, major_population: Column) -&gt; Column:\n        \"\"\"Calculate R2 using R of the major population in the study.\n\n        Args:\n            ld_set (Column): LD set\n            major_population (Column): Major population of the study\n\n        Returns:\n            Column: LD set with added 'r2Overall' field\n        \"\"\"\n        ld_set_with_major_pop = f.transform(\n            ld_set,\n            lambda x: f.struct(\n                x[\"tagVariantId\"].alias(\"tagVariantId\"),\n                f.filter(\n                    x[\"rValues\"], lambda y: y[\"population\"] == major_population\n                ).alias(\"rValues\"),\n            ),\n        )\n        return f.transform(\n            ld_set_with_major_pop,\n            lambda x: f.struct(\n                x[\"tagVariantId\"].alias(\"tagVariantId\"),\n                f.coalesce(f.pow(x[\"rValues\"][\"r\"][0], 2), f.lit(0.0)).alias(\n                    \"r2Overall\"\n                ),\n            ),\n        )\n\n    @staticmethod\n    def _qc_unresolved_ld(ld_set: Column, quality_controls: Column) -&gt; Column:\n        \"\"\"Flag associations with unresolved LD.\n\n        Args:\n            ld_set (Column): LD set\n            quality_controls (Column): Quality controls\n\n        Returns:\n            Column: Quality controls with added 'UNRESOLVED_LD' field\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            quality_controls,\n            ld_set.isNull(),\n            StudyLocusQualityCheck.UNRESOLVED_LD,\n        )\n\n    @staticmethod\n    def _rescue_lead_variant(ld_set: Column, variant_id: Column) -&gt; Column:\n        \"\"\"Rescue lead variant.\n\n        In cases in which no LD information is available but a lead variant is available, we include the lead as the only variant in the ldSet.\n\n        Args:\n            ld_set (Column): LD set\n            variant_id (Column): Variant ID\n\n        Returns:\n            Column: LD set with added 'tagVariantId' field\n        \"\"\"\n        return f.when(\n            ((ld_set.isNull() | (f.size(ld_set) == 0)) &amp; variant_id.isNotNull()),\n            f.array(\n                f.struct(\n                    variant_id.alias(\"tagVariantId\"),\n                    f.lit(1).alias(\"r2Overall\"),\n                )\n            ),\n        ).otherwise(ld_set)\n\n    @classmethod\n    def ld_annotate(\n        cls: type[LDAnnotator],\n        associations: StudyLocus,\n        studies: StudyIndex,\n        ld_index: LDIndex,\n        r2_threshold: float = 0.5,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotate linkage disequilibrium (LD) information to a set of studyLocus.\n\n        This function:\n            1. Annotates study locus with population structure information ordered by relativeSampleSize from the study index\n            2. Joins the LD index to the StudyLocus\n            3. Gets the major population from the population structure\n            4. Calculates R2 by using the R of the major ancestry\n            5. Flags associations with variants that are not found in the LD reference\n            6. Rescues lead variant when no LD information is available but lead variant is available\n\n        !!! note\n            Because the LD index has a pre-set threshold of R2 = 0.5, this is the minimum threshold for the LD information to be included in the ldSet.\n\n        Args:\n            associations (StudyLocus): Dataset to be LD annotated\n            studies (StudyIndex): Dataset with study information\n            ld_index (LDIndex): Dataset with LD information for every variant present in LD matrix\n            r2_threshold (float): R2 threshold to filter the LD set on. Default is 0.5.\n\n        Returns:\n            StudyLocus: including additional column with LD information.\n        \"\"\"\n        return StudyLocus(\n            _df=(\n                associations.df\n                # Drop ldSet column if already available\n                .select(*[col for col in associations.df.columns if col != \"ldSet\"])\n                # Annotate study locus with population structure ordered by relativeSampleSize from study index\n                .join(\n                    studies.df.select(\n                        \"studyId\",\n                        order_array_of_structs_by_field(\n                            \"ldPopulationStructure\", \"relativeSampleSize\"\n                        ).alias(\"ldPopulationStructure\"),\n                    ),\n                    on=\"studyId\",\n                    how=\"left\",\n                )\n                # Bring LD information from LD Index\n                .join(\n                    ld_index.df,\n                    on=[\"variantId\", \"chromosome\"],\n                    how=\"left\",\n                )\n                # Get major population from population structure if population structure available\n                .withColumn(\n                    \"majorPopulation\",\n                    f.when(\n                        f.col(\"ldPopulationStructure\").isNotNull(),\n                        cls._get_major_population(f.col(\"ldPopulationStructure\")),\n                    ),\n                )\n                # Calculate R2 using R of the major population\n                .withColumn(\n                    \"ldSet\",\n                    f.when(\n                        f.col(\"ldPopulationStructure\").isNotNull(),\n                        cls._calculate_r2_major(\n                            f.col(\"ldSet\"), f.col(\"majorPopulation\")\n                        ),\n                    ),\n                )\n                .drop(\"ldPopulationStructure\", \"majorPopulation\")\n                # Filter the LD set by the R2 threshold and set to null if no LD information passes the threshold\n                .withColumn(\n                    \"ldSet\",\n                    StudyLocus.filter_ld_set(f.col(\"ldSet\"), r2_threshold),\n                )\n                .withColumn(\"ldSet\", f.when(f.size(\"ldSet\") &gt; 0, f.col(\"ldSet\")))\n                # QC: Flag associations with variants that are not found in the LD reference\n                .withColumn(\n                    \"qualityControls\",\n                    cls._qc_unresolved_ld(f.col(\"ldSet\"), f.col(\"qualityControls\")),\n                )\n                # Add lead variant to empty ldSet when no LD information is available but lead variant is available\n                .withColumn(\n                    \"ldSet\",\n                    cls._rescue_lead_variant(f.col(\"ldSet\"), f.col(\"variantId\")),\n                )\n                # Ensure that the lead varaitn is always with r2==1\n                .withColumn(\n                    \"ldSet\",\n                    f.expr(\n                        \"\"\"\n                        transform(ldSet, x -&gt;\n                            IF(x.tagVariantId == variantId,\n                                named_struct('tagVariantId', x.tagVariantId, 'r2Overall', 1.0),\n                                x\n                            )\n                        )\n                        \"\"\"\n                    ),\n                )\n            ),\n            _schema=StudyLocus.get_schema(),\n        )._qc_no_population()\n</code></pre>"},{"location":"python_api/methods/ld_annotator/#gentropy.method.ld.LDAnnotator.ld_annotate","title":"<code>ld_annotate(associations: StudyLocus, studies: StudyIndex, ld_index: LDIndex, r2_threshold: float = 0.5) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Annotate linkage disequilibrium (LD) information to a set of studyLocus.</p> This function <ol> <li>Annotates study locus with population structure information ordered by relativeSampleSize from the study index</li> <li>Joins the LD index to the StudyLocus</li> <li>Gets the major population from the population structure</li> <li>Calculates R2 by using the R of the major ancestry</li> <li>Flags associations with variants that are not found in the LD reference</li> <li>Rescues lead variant when no LD information is available but lead variant is available</li> </ol> <p>Note</p> <p>Because the LD index has a pre-set threshold of R2 = 0.5, this is the minimum threshold for the LD information to be included in the ldSet.</p> <p>Parameters:</p> Name Type Description Default <code>associations</code> <code>StudyLocus</code> <p>Dataset to be LD annotated</p> required <code>studies</code> <code>StudyIndex</code> <p>Dataset with study information</p> required <code>ld_index</code> <code>LDIndex</code> <p>Dataset with LD information for every variant present in LD matrix</p> required <code>r2_threshold</code> <code>float</code> <p>R2 threshold to filter the LD set on. Default is 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>including additional column with LD information.</p> Source code in <code>src/gentropy/method/ld.py</code> <pre><code>@classmethod\ndef ld_annotate(\n    cls: type[LDAnnotator],\n    associations: StudyLocus,\n    studies: StudyIndex,\n    ld_index: LDIndex,\n    r2_threshold: float = 0.5,\n) -&gt; StudyLocus:\n    \"\"\"Annotate linkage disequilibrium (LD) information to a set of studyLocus.\n\n    This function:\n        1. Annotates study locus with population structure information ordered by relativeSampleSize from the study index\n        2. Joins the LD index to the StudyLocus\n        3. Gets the major population from the population structure\n        4. Calculates R2 by using the R of the major ancestry\n        5. Flags associations with variants that are not found in the LD reference\n        6. Rescues lead variant when no LD information is available but lead variant is available\n\n    !!! note\n        Because the LD index has a pre-set threshold of R2 = 0.5, this is the minimum threshold for the LD information to be included in the ldSet.\n\n    Args:\n        associations (StudyLocus): Dataset to be LD annotated\n        studies (StudyIndex): Dataset with study information\n        ld_index (LDIndex): Dataset with LD information for every variant present in LD matrix\n        r2_threshold (float): R2 threshold to filter the LD set on. Default is 0.5.\n\n    Returns:\n        StudyLocus: including additional column with LD information.\n    \"\"\"\n    return StudyLocus(\n        _df=(\n            associations.df\n            # Drop ldSet column if already available\n            .select(*[col for col in associations.df.columns if col != \"ldSet\"])\n            # Annotate study locus with population structure ordered by relativeSampleSize from study index\n            .join(\n                studies.df.select(\n                    \"studyId\",\n                    order_array_of_structs_by_field(\n                        \"ldPopulationStructure\", \"relativeSampleSize\"\n                    ).alias(\"ldPopulationStructure\"),\n                ),\n                on=\"studyId\",\n                how=\"left\",\n            )\n            # Bring LD information from LD Index\n            .join(\n                ld_index.df,\n                on=[\"variantId\", \"chromosome\"],\n                how=\"left\",\n            )\n            # Get major population from population structure if population structure available\n            .withColumn(\n                \"majorPopulation\",\n                f.when(\n                    f.col(\"ldPopulationStructure\").isNotNull(),\n                    cls._get_major_population(f.col(\"ldPopulationStructure\")),\n                ),\n            )\n            # Calculate R2 using R of the major population\n            .withColumn(\n                \"ldSet\",\n                f.when(\n                    f.col(\"ldPopulationStructure\").isNotNull(),\n                    cls._calculate_r2_major(\n                        f.col(\"ldSet\"), f.col(\"majorPopulation\")\n                    ),\n                ),\n            )\n            .drop(\"ldPopulationStructure\", \"majorPopulation\")\n            # Filter the LD set by the R2 threshold and set to null if no LD information passes the threshold\n            .withColumn(\n                \"ldSet\",\n                StudyLocus.filter_ld_set(f.col(\"ldSet\"), r2_threshold),\n            )\n            .withColumn(\"ldSet\", f.when(f.size(\"ldSet\") &gt; 0, f.col(\"ldSet\")))\n            # QC: Flag associations with variants that are not found in the LD reference\n            .withColumn(\n                \"qualityControls\",\n                cls._qc_unresolved_ld(f.col(\"ldSet\"), f.col(\"qualityControls\")),\n            )\n            # Add lead variant to empty ldSet when no LD information is available but lead variant is available\n            .withColumn(\n                \"ldSet\",\n                cls._rescue_lead_variant(f.col(\"ldSet\"), f.col(\"variantId\")),\n            )\n            # Ensure that the lead varaitn is always with r2==1\n            .withColumn(\n                \"ldSet\",\n                f.expr(\n                    \"\"\"\n                    transform(ldSet, x -&gt;\n                        IF(x.tagVariantId == variantId,\n                            named_struct('tagVariantId', x.tagVariantId, 'r2Overall', 1.0),\n                            x\n                        )\n                    )\n                    \"\"\"\n                ),\n            )\n        ),\n        _schema=StudyLocus.get_schema(),\n    )._qc_no_population()\n</code></pre>"},{"location":"python_api/methods/pics/","title":"PICS","text":"<p>PICS Overview:</p> <p>PICS is a fine-mapping method designed to identify the most likely causal SNPs associated with a trait or disease within a genomic region. It leverages both haplotype information and the observed association patterns from genome-wide association studies (GWAS).</p> <p>Please refer to the original publication for in-depth details: PICS Publication.</p> <p>We use PICS for both GWAS clumping results and GWAS curated studies.</p>"},{"location":"python_api/methods/pics/#gentropy.method.pics.PICS","title":"<code>gentropy.method.pics.PICS</code>","text":"<p>Probabilistic Identification of Causal SNPs (PICS), an algorithm estimating the probability that an individual variant is causal considering the haplotype structure and observed pattern of association at the genetic locus.</p> Source code in <code>src/gentropy/method/pics.py</code> <pre><code>class PICS:\n    \"\"\"Probabilistic Identification of Causal SNPs (PICS), an algorithm estimating the probability that an individual variant is causal considering the haplotype structure and observed pattern of association at the genetic locus.\"\"\"\n\n    # The fields for the picsed locus + ldSet tagVariantId is renamed to variantId:\n    PICSED_LOCUS_SCHEMA = t.ArrayType(\n        t.StructType(\n            [\n                t.StructField(\"variantId\", t.StringType(), True),\n                t.StructField(\"r2Overall\", t.DoubleType(), True),\n                t.StructField(\"posteriorProbability\", t.DoubleType(), True),\n                t.StructField(\"standardError\", t.DoubleType(), True),\n            ]\n        )\n    )\n\n    @staticmethod\n    def _pics_relative_posterior_probability(\n        neglog_p: float, pics_snp_mu: float, pics_snp_std: float\n    ) -&gt; float:\n        \"\"\"Compute the PICS posterior probability for a given SNP.\n\n        !!! info \"This probability needs to be scaled to take into account the probabilities of the other variants in the locus.\"\n\n        Args:\n            neglog_p (float): Negative log p-value of the lead variant\n            pics_snp_mu (float): Mean P value of the association between a SNP and a trait\n            pics_snp_std (float): Standard deviation for the P value of the association between a SNP and a trait\n\n        Returns:\n            float: Posterior probability of the association between a SNP and a trait\n\n        Examples:\n            &gt;&gt;&gt; rel_prob = PICS._pics_relative_posterior_probability(neglog_p=10.0, pics_snp_mu=1.0, pics_snp_std=10.0)\n            &gt;&gt;&gt; round(rel_prob, 3)\n            0.368\n        \"\"\"\n        return float(norm(pics_snp_mu, pics_snp_std).sf(neglog_p) * 2)\n\n    @staticmethod\n    def _pics_standard_deviation(neglog_p: float, r2: float, k: float) -&gt; float | None:\n        \"\"\"Compute the PICS standard deviation.\n\n        This distribution is obtained after a series of permutation tests described in the PICS method, and it is only\n        valid when the SNP is highly linked with the lead (r2 &gt; 0.5).\n\n        Args:\n            neglog_p (float): Negative log p-value of the lead variant\n            r2 (float): LD score between a given SNP and the lead variant\n            k (float): Empiric constant that can be adjusted to fit the curve, 6.4 recommended.\n\n        Returns:\n            float | None: Standard deviation for the P value of the association between a SNP and a trait\n\n        Examples:\n            &gt;&gt;&gt; PICS._pics_standard_deviation(neglog_p=1.0, r2=1.0, k=6.4)\n            0.0\n            &gt;&gt;&gt; round(PICS._pics_standard_deviation(neglog_p=10.0, r2=0.5, k=6.4), 3)\n            1.493\n            &gt;&gt;&gt; print(PICS._pics_standard_deviation(neglog_p=1.0, r2=0.0, k=6.4))\n            None\n        \"\"\"\n        return (\n            abs(((1 - (r2**0.5) ** k) ** 0.5) * (neglog_p**0.5) / 2)\n            if r2 &gt;= 0.5\n            else None\n        )\n\n    @staticmethod\n    def _pics_mu(neglog_p: float, r2: float) -&gt; float | None:\n        \"\"\"Compute the PICS mu that estimates the probability of association between a given SNP and the trait.\n\n        This distribution is obtained after a series of permutation tests described in the PICS method, and it is only\n        valid when the SNP is highly linked with the lead (r2 &gt; 0.5).\n\n        Args:\n            neglog_p (float): Negative log p-value of the lead variant\n            r2 (float): LD score between a given SNP and the lead variant\n\n        Returns:\n            float | None: Mean P value of the association between a SNP and a trait\n\n        Examples:\n            &gt;&gt;&gt; PICS._pics_mu(neglog_p=1.0, r2=1.0)\n            1.0\n            &gt;&gt;&gt; PICS._pics_mu(neglog_p=10.0, r2=0.5)\n            5.0\n            &gt;&gt;&gt; print(PICS._pics_mu(neglog_p=10.0, r2=0.3))\n            None\n        \"\"\"\n        return neglog_p * r2 if r2 &gt;= 0.5 else None\n\n    @staticmethod\n    def _finemap(\n        ld_set: list[Row], lead_neglog_p: float, k: float\n    ) -&gt; list[dict[str, Any]] | None:\n        \"\"\"Calculates the probability of a variant being causal in a study-locus context by applying the PICS method.\n\n        It is intended to be applied as an UDF in `PICS.finemap`, where each row is a StudyLocus association.\n        The function iterates over every SNP in the `ldSet` array, and it returns an updated locus with\n        its association signal and causality probability as of PICS.\n\n        Args:\n            ld_set (list[Row]): list of tagging variants after expanding the locus\n            lead_neglog_p (float): P value of the association signal between the lead variant and the study in the form of -log10.\n            k (float): Empiric constant that can be adjusted to fit the curve, 6.4 recommended.\n\n        Returns:\n            list[dict[str, Any]] | None: List of tagging variants with an estimation of the association signal and their posterior probability as of PICS.\n\n        Examples:\n            &gt;&gt;&gt; from pyspark.sql import Row\n            &gt;&gt;&gt; ld_set = [\n            ...     Row(variantId=\"var1\", r2Overall=0.8),\n            ...     Row(variantId=\"var2\", r2Overall=1),\n            ... ]\n            &gt;&gt;&gt; PICS._finemap(ld_set, lead_neglog_p=10.0, k=6.4)\n            [{'variantId': 'var1', 'r2Overall': 0.8, 'standardError': 0.07420896512708416, 'posteriorProbability': 0.07116959886882368}, {'variantId': 'var2', 'r2Overall': 1, 'standardError': 0.9977000638225533, 'posteriorProbability': 0.9288304011311763}]\n            &gt;&gt;&gt; empty_ld_set = []\n            &gt;&gt;&gt; PICS._finemap(empty_ld_set, lead_neglog_p=10.0, k=6.4)\n            []\n            &gt;&gt;&gt; ld_set_with_no_r2 = [\n            ...     Row(variantId=\"var1\", r2Overall=None),\n            ...     Row(variantId=\"var2\", r2Overall=None),\n            ... ]\n            &gt;&gt;&gt; PICS._finemap(ld_set_with_no_r2, lead_neglog_p=10.0, k=6.4)\n            []\n        \"\"\"\n        if ld_set is None:\n            return None\n        elif not ld_set:\n            return []\n        tmp_credible_set = []\n        new_credible_set = []\n        # First iteration: calculation of mu, standard deviation, and the relative posterior probability\n        for tag_struct in ld_set:\n            tag_dict = (\n                tag_struct.asDict()\n            )  # tag_struct is of type pyspark.Row, we'll represent it as a dict\n            if (\n                not tag_dict[\"r2Overall\"]\n                or tag_dict[\"r2Overall\"] &lt; 0.5\n                or not lead_neglog_p\n            ):\n                # If PICS cannot be calculated, we drop the variant from the credible set\n                continue\n\n            # Chaing chema:\n            if \"tagVariantId\" in tag_dict:\n                tag_dict[\"variantId\"] = tag_dict.pop(\"tagVariantId\")\n\n            pics_snp_mu = PICS._pics_mu(lead_neglog_p, tag_dict[\"r2Overall\"])\n            pics_snp_std = PICS._pics_standard_deviation(\n                lead_neglog_p, tag_dict[\"r2Overall\"], k\n            )\n            pics_snp_std = 0.001 if pics_snp_std == 0 else pics_snp_std\n            if pics_snp_mu is not None and pics_snp_std is not None:\n                posterior_probability = PICS._pics_relative_posterior_probability(\n                    lead_neglog_p, pics_snp_mu, pics_snp_std\n                )\n                tag_dict[\"standardError\"] = 10**-pics_snp_std\n                tag_dict[\"relativePosteriorProbability\"] = posterior_probability\n\n                tmp_credible_set.append(tag_dict)\n\n        # Second iteration: calculation of the sum of all the posteriors in each study-locus, so that we scale them between 0-1\n        total_posteriors = sum(\n            tag_dict.get(\"relativePosteriorProbability\", 0)\n            for tag_dict in tmp_credible_set\n        )\n\n        # Third iteration: calculation of the final posteriorProbability\n        for tag_dict in tmp_credible_set:\n            if total_posteriors != 0:\n                tag_dict[\"posteriorProbability\"] = float(\n                    tag_dict.get(\"relativePosteriorProbability\", 0) / total_posteriors\n                )\n            tag_dict.pop(\"relativePosteriorProbability\")\n            new_credible_set.append(tag_dict)\n        return new_credible_set\n\n    @classmethod\n    def finemap(\n        cls: type[PICS], associations: StudyLocus, k: float = 6.4\n    ) -&gt; StudyLocus:\n        \"\"\"Run PICS on a study locus.\n\n        !!! info \"Study locus needs to be LD annotated\"\n\n            The study locus needs to be LD annotated before PICS can be calculated.\n\n        Args:\n            associations (StudyLocus): Study locus to finemap using PICS\n            k (float): Empiric constant that can be adjusted to fit the curve, 6.4 recommended.\n\n        Returns:\n            StudyLocus: Study locus with PICS results\n        \"\"\"\n        # Finemapping method is an optional column:\n        finemapping_method_expression = (\n            f.lit(FinemappingMethod.PICS.value)\n            if \"finemappingMethod\" not in associations.df.columns\n            else f.coalesce(\n                f.col(\"finemappingMethod\"), f.lit(FinemappingMethod.PICS.value)\n            )\n        )\n\n        # Registering the UDF to be used in the pipeline:\n        finemap_udf = f.udf(\n            lambda ld_set, neglog_p: cls._finemap(ld_set, neglog_p, k),\n            cls.PICSED_LOCUS_SCHEMA,\n        )\n\n        return StudyLocus(\n            _df=(\n                associations.df\n                # Old locus column will be dropped if available\n                .select(*[col for col in associations.df.columns if col != \"locus\"])\n                # Estimate neglog_pvalue for the lead variant\n                .withColumn(\"neglog_pvalue\", associations.neglog_pvalue())\n                # New locus containing the PICS results\n                .withColumn(\n                    \"locus\",\n                    f.when(\n                        f.col(\"ldSet\").isNotNull(),\n                        finemap_udf(f.col(\"ldSet\"), f.col(\"neglog_pvalue\")),\n                    ),\n                )\n                # Updating single point statistics in the locus object for the lead variant:\n                .withColumn(\n                    \"locus\",\n                    f.transform(\n                        f.col(\"locus\"),\n                        lambda tag: f.when(\n                            f.col(\"variantId\") == tag[\"variantId\"],\n                            tag.withField(\"pValueMantissa\", f.col(\"pValueMantissa\"))\n                            .withField(\"pValueExponent\", f.col(\"pValueExponent\"))\n                            .withField(\"beta\", f.col(\"beta\")),\n                        ).otherwise(\n                            tag.withField(\n                                \"pValueMantissa\", f.lit(None).cast(t.FloatType())\n                            )\n                            .withField(\n                                \"pValueExponent\", f.lit(None).cast(t.IntegerType())\n                            )\n                            .withField(\"beta\", f.lit(None).cast(t.DoubleType()))\n                        ),\n                    ),\n                )\n                # Flagging all PICS loci with OUT_OF_SAMPLE_LD flag:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyLocus.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        f.lit(True),\n                        StudyLocusQualityCheck.OUT_OF_SAMPLE_LD,\n                    ),\n                )\n                .withColumn(\n                    \"finemappingMethod\",\n                    finemapping_method_expression,\n                )\n                .withColumn(\n                    \"studyLocusId\",\n                    StudyLocus.assign_study_locus_id(\n                        [\"studyId\", \"variantId\", \"finemappingMethod\"]\n                    ),\n                )\n                .drop(\"neglog_pvalue\")\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/pics/#gentropy.method.pics.PICS.finemap","title":"<code>finemap(associations: StudyLocus, k: float = 6.4) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Run PICS on a study locus.</p> <p>Study locus needs to be LD annotated</p> <p>The study locus needs to be LD annotated before PICS can be calculated.</p> <p>Parameters:</p> Name Type Description Default <code>associations</code> <code>StudyLocus</code> <p>Study locus to finemap using PICS</p> required <code>k</code> <code>float</code> <p>Empiric constant that can be adjusted to fit the curve, 6.4 recommended.</p> <code>6.4</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus with PICS results</p> Source code in <code>src/gentropy/method/pics.py</code> <pre><code>@classmethod\ndef finemap(\n    cls: type[PICS], associations: StudyLocus, k: float = 6.4\n) -&gt; StudyLocus:\n    \"\"\"Run PICS on a study locus.\n\n    !!! info \"Study locus needs to be LD annotated\"\n\n        The study locus needs to be LD annotated before PICS can be calculated.\n\n    Args:\n        associations (StudyLocus): Study locus to finemap using PICS\n        k (float): Empiric constant that can be adjusted to fit the curve, 6.4 recommended.\n\n    Returns:\n        StudyLocus: Study locus with PICS results\n    \"\"\"\n    # Finemapping method is an optional column:\n    finemapping_method_expression = (\n        f.lit(FinemappingMethod.PICS.value)\n        if \"finemappingMethod\" not in associations.df.columns\n        else f.coalesce(\n            f.col(\"finemappingMethod\"), f.lit(FinemappingMethod.PICS.value)\n        )\n    )\n\n    # Registering the UDF to be used in the pipeline:\n    finemap_udf = f.udf(\n        lambda ld_set, neglog_p: cls._finemap(ld_set, neglog_p, k),\n        cls.PICSED_LOCUS_SCHEMA,\n    )\n\n    return StudyLocus(\n        _df=(\n            associations.df\n            # Old locus column will be dropped if available\n            .select(*[col for col in associations.df.columns if col != \"locus\"])\n            # Estimate neglog_pvalue for the lead variant\n            .withColumn(\"neglog_pvalue\", associations.neglog_pvalue())\n            # New locus containing the PICS results\n            .withColumn(\n                \"locus\",\n                f.when(\n                    f.col(\"ldSet\").isNotNull(),\n                    finemap_udf(f.col(\"ldSet\"), f.col(\"neglog_pvalue\")),\n                ),\n            )\n            # Updating single point statistics in the locus object for the lead variant:\n            .withColumn(\n                \"locus\",\n                f.transform(\n                    f.col(\"locus\"),\n                    lambda tag: f.when(\n                        f.col(\"variantId\") == tag[\"variantId\"],\n                        tag.withField(\"pValueMantissa\", f.col(\"pValueMantissa\"))\n                        .withField(\"pValueExponent\", f.col(\"pValueExponent\"))\n                        .withField(\"beta\", f.col(\"beta\")),\n                    ).otherwise(\n                        tag.withField(\n                            \"pValueMantissa\", f.lit(None).cast(t.FloatType())\n                        )\n                        .withField(\n                            \"pValueExponent\", f.lit(None).cast(t.IntegerType())\n                        )\n                        .withField(\"beta\", f.lit(None).cast(t.DoubleType()))\n                    ),\n                ),\n            )\n            # Flagging all PICS loci with OUT_OF_SAMPLE_LD flag:\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.lit(True),\n                    StudyLocusQualityCheck.OUT_OF_SAMPLE_LD,\n                ),\n            )\n            .withColumn(\n                \"finemappingMethod\",\n                finemapping_method_expression,\n            )\n            .withColumn(\n                \"studyLocusId\",\n                StudyLocus.assign_study_locus_id(\n                    [\"studyId\", \"variantId\", \"finemappingMethod\"]\n                ),\n            )\n            .drop(\"neglog_pvalue\")\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/sumstat_imputation/","title":"Summary Statistics Imputation","text":"<p>Summary statistics imputation leverages linkage disequilibrium (LD) information to compute Z-scores of missing SNPs from neighbouring observed SNPs SNPs by taking advantage of the Linkage Disequilibrium.</p> <p>We implemented the basic model from RAISS (Robust and Accurate Imputation from Summary Statistics) package (see the original paper).</p> <p>The full repository for the RAISS package can be found here.</p> <p>The original model was suggested in 2014 by Bogdan Pasaniuc et al. here.</p> <p>It represents the following formula:</p> <p>E(zi|z_t) = M{i,t} \\cdot (M_{t,t})^{-1} \\cdot z_t</p> <p>Where:</p> <ul> <li> <p>E(z_i|z_t) represents the expected z-score of SNP 'i' given the observed z-scores at known SNP indexes 't'.</p> </li> <li> <p>M_{i,t} represents the LD (Linkage Disequilibrium) matrix between SNP 'i' and the known SNPs at indexes 't'.</p> </li> <li> <p>(M_{t,t})^{-1} represents the inverse of the LD matrix of the known SNPs at indexes 't'.</p> </li> <li> <p>z_t represents the vector of observed z-scores at the known SNP indexes 't'.</p> </li> </ul>"},{"location":"python_api/methods/sumstat_imputation/#gentropy.method.sumstat_imputation.SummaryStatisticsImputation","title":"<code>gentropy.method.sumstat_imputation.SummaryStatisticsImputation</code>","text":"<p>Implementation of RAISS summary statstics imputation model.</p> Source code in <code>src/gentropy/method/sumstat_imputation.py</code> <pre><code>class SummaryStatisticsImputation:\n    \"\"\"Implementation of RAISS summary statstics imputation model.\"\"\"\n\n    @staticmethod\n    def raiss_model(\n        z_scores_known: np.ndarray,\n        ld_matrix_known: np.ndarray,\n        ld_matrix_known_missing: np.ndarray,\n        lamb: float = 0.01,\n        rtol: float = 0.01,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Compute the imputation of the z-score using the RAISS model.\n\n        Args:\n            z_scores_known (np.ndarray): the vector of known Z scores\n            ld_matrix_known (np.ndarray) : the matrix of known LD correlations\n            ld_matrix_known_missing (np.ndarray): LD matrix of known SNPs with other unknown SNPs in large matrix (similar to `ld[unknowns, :][:,known]`)\n            lamb (float): size of the small value added to the diagonal of the covariance matrix before inversion. Defaults to 0.01.\n            rtol (float): threshold to filter eigenvectos by its eigenvalue. It makes an inversion biased but much more numerically robust. Default to 0.01.\n\n        Returns:\n            dict[str, Any]:\n                - var (np.ndarray): variance of the imputed SNPs\n                - mu (np.ndarray): the estimation of the zscore of the imputed SNPs\n                - ld_score (np.ndarray): the linkage disequilibrium score of the imputed SNPs\n                - condition_number (np.ndarray): the condition number of the correlation matrix\n                - correct_inversion (np.ndarray): a boolean array indicating if the inversion was successful\n                - imputation_r2 (np.ndarray): the R2 of the imputation\n        \"\"\"\n        sig_t_inv = SummaryStatisticsImputation._invert_sig_t(\n            ld_matrix_known, lamb, rtol\n        )\n        if sig_t_inv is None:\n            return {\n                \"var\": None,\n                \"mu\": None,\n                \"ld_score\": None,\n                \"condition_number\": None,\n                \"correct_inversion\": None,\n                \"imputation_r2\": None,\n            }\n        else:\n            condition_number = np.array(\n                [np.linalg.cond(ld_matrix_known)] * ld_matrix_known_missing.shape[0]\n            )\n            correct_inversion = np.array(\n                [\n                    SummaryStatisticsImputation._check_inversion(\n                        ld_matrix_known, sig_t_inv\n                    )\n                ]\n                * ld_matrix_known_missing.shape[0]\n            )\n\n            var, ld_score = SummaryStatisticsImputation._compute_var(\n                ld_matrix_known_missing, sig_t_inv, lamb\n            )\n\n            mu = SummaryStatisticsImputation._compute_mu(\n                ld_matrix_known_missing, sig_t_inv, z_scores_known\n            )\n            var_norm = SummaryStatisticsImputation._var_in_boundaries(var, lamb)\n\n            R2 = (1 + lamb) - var_norm\n\n            mu = mu / np.sqrt(R2)\n            return {\n                \"var\": var,\n                \"mu\": mu,\n                \"ld_score\": ld_score,\n                \"condition_number\": condition_number,\n                \"correct_inversion\": correct_inversion,\n                \"imputation_r2\": 1 - var,\n            }\n\n    @staticmethod\n    def _compute_mu(\n        sig_i_t: np.ndarray, sig_t_inv: np.ndarray, zt: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the estimation of z-score from neighborring snp.\n\n        Args:\n            sig_i_t (np.ndarray) : correlation matrix with line corresponding to unknown Snp (snp to impute) and column to known SNPs\n            sig_t_inv (np.ndarray): inverse of the correlation matrix of known matrix\n            zt (np.ndarray): Zscores of known snp\n        Returns:\n            np.ndarray: a vector of length i containing the estimate of zscore\n\n        \"\"\"\n        return np.dot(sig_i_t, np.dot(sig_t_inv, zt))\n\n    @staticmethod\n    def _compute_var(\n        sig_i_t: np.ndarray, sig_t_inv: np.ndarray, lamb: float\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Compute the expected variance of the imputed SNPs.\n\n        Args:\n            sig_i_t (np.ndarray) : correlation matrix with line corresponding to unknown Snp (snp to impute) and column to known SNPs\n            sig_t_inv (np.ndarray): inverse of the correlation matrix of known matrix\n            lamb (float): regularization term added to matrix\n\n        Returns:\n            tuple[np.ndarray, np.ndarray]: a tuple containing the variance and the ld score\n        \"\"\"\n        var = (1 + lamb) - np.einsum(\n            \"ij,jk,ki-&gt;i\", sig_i_t, sig_t_inv, sig_i_t.transpose()\n        )\n        ld_score = (sig_i_t**2).sum(1)\n\n        return var, ld_score\n\n    @staticmethod\n    def _check_inversion(sig_t: np.ndarray, sig_t_inv: np.ndarray) -&gt; bool:\n        \"\"\"Check if the inversion is correct.\n\n        Args:\n            sig_t (np.ndarray): the correlation matrix\n            sig_t_inv (np.ndarray): the inverse of the correlation matrix\n        Returns:\n            bool: True if the inversion is correct, False otherwise\n        \"\"\"\n        return np.allclose(sig_t, np.dot(sig_t, np.dot(sig_t_inv, sig_t)))\n\n    @staticmethod\n    def _var_in_boundaries(var: np.ndarray, lamb: float) -&gt; np.ndarray:\n        \"\"\"Forces the variance to be in the 0 to 1+lambda boundary. Theoritically we shouldn't have to do that.\n\n        Args:\n            var (np.ndarray): the variance of the imputed SNPs\n            lamb (float): regularization term added to the diagonal of the sig_t matrix\n\n        Returns:\n            np.ndarray: the variance of the imputed SNPs\n        \"\"\"\n        id_neg = np.where(var &lt; 0)\n        var[id_neg] = 0\n        id_inf = np.where(var &gt; (0.99999 + lamb))\n        var[id_inf] = 1\n\n        return var\n\n    @staticmethod\n    def _invert_sig_t(sig_t: np.ndarray, lamb: float, rtol: float) -&gt; np.ndarray:\n        \"\"\"Invert the correlation matrix. If the provided regularization values are not enough to stabilize the inversion process for the given matrix, the function calls itself recursively, increasing lamb and rtol by 10%.\n\n        Args:\n            sig_t (np.ndarray): the correlation matrix\n            lamb (float): regularization term added to the diagonal of the sig_t matrix\n            rtol (float): threshold to filter eigenvector with a eigenvalue under rtol make inversion biased but much more numerically robust\n\n        Returns:\n            np.ndarray: the inverse of the correlation matrix\n        \"\"\"\n        try:\n            np.fill_diagonal(sig_t, (1 + lamb))\n            sig_t_inv = scipy.linalg.pinv(sig_t, rtol=rtol, atol=0)\n            return sig_t_inv\n        except np.linalg.LinAlgError:\n            return SummaryStatisticsImputation._invert_sig_t(\n                sig_t, lamb * 1.1, rtol * 1.1\n            )\n</code></pre>"},{"location":"python_api/methods/sumstat_imputation/#gentropy.method.sumstat_imputation.SummaryStatisticsImputation.raiss_model","title":"<code>raiss_model(z_scores_known: np.ndarray, ld_matrix_known: np.ndarray, ld_matrix_known_missing: np.ndarray, lamb: float = 0.01, rtol: float = 0.01) -&gt; dict[str, Any]</code>  <code>staticmethod</code>","text":"<p>Compute the imputation of the z-score using the RAISS model.</p> <p>Parameters:</p> Name Type Description Default <code>z_scores_known</code> <code>ndarray</code> <p>the vector of known Z scores</p> required <code>ld_matrix_known</code> <code>np.ndarray) </code> <p>the matrix of known LD correlations</p> required <code>ld_matrix_known_missing</code> <code>ndarray</code> <p>LD matrix of known SNPs with other unknown SNPs in large matrix (similar to <code>ld[unknowns, :][:,known]</code>)</p> required <code>lamb</code> <code>float</code> <p>size of the small value added to the diagonal of the covariance matrix before inversion. Defaults to 0.01.</p> <code>0.01</code> <code>rtol</code> <code>float</code> <p>threshold to filter eigenvectos by its eigenvalue. It makes an inversion biased but much more numerically robust. Default to 0.01.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: - var (np.ndarray): variance of the imputed SNPs - mu (np.ndarray): the estimation of the zscore of the imputed SNPs - ld_score (np.ndarray): the linkage disequilibrium score of the imputed SNPs - condition_number (np.ndarray): the condition number of the correlation matrix - correct_inversion (np.ndarray): a boolean array indicating if the inversion was successful - imputation_r2 (np.ndarray): the R2 of the imputation</p> Source code in <code>src/gentropy/method/sumstat_imputation.py</code> <pre><code>@staticmethod\ndef raiss_model(\n    z_scores_known: np.ndarray,\n    ld_matrix_known: np.ndarray,\n    ld_matrix_known_missing: np.ndarray,\n    lamb: float = 0.01,\n    rtol: float = 0.01,\n) -&gt; dict[str, Any]:\n    \"\"\"Compute the imputation of the z-score using the RAISS model.\n\n    Args:\n        z_scores_known (np.ndarray): the vector of known Z scores\n        ld_matrix_known (np.ndarray) : the matrix of known LD correlations\n        ld_matrix_known_missing (np.ndarray): LD matrix of known SNPs with other unknown SNPs in large matrix (similar to `ld[unknowns, :][:,known]`)\n        lamb (float): size of the small value added to the diagonal of the covariance matrix before inversion. Defaults to 0.01.\n        rtol (float): threshold to filter eigenvectos by its eigenvalue. It makes an inversion biased but much more numerically robust. Default to 0.01.\n\n    Returns:\n        dict[str, Any]:\n            - var (np.ndarray): variance of the imputed SNPs\n            - mu (np.ndarray): the estimation of the zscore of the imputed SNPs\n            - ld_score (np.ndarray): the linkage disequilibrium score of the imputed SNPs\n            - condition_number (np.ndarray): the condition number of the correlation matrix\n            - correct_inversion (np.ndarray): a boolean array indicating if the inversion was successful\n            - imputation_r2 (np.ndarray): the R2 of the imputation\n    \"\"\"\n    sig_t_inv = SummaryStatisticsImputation._invert_sig_t(\n        ld_matrix_known, lamb, rtol\n    )\n    if sig_t_inv is None:\n        return {\n            \"var\": None,\n            \"mu\": None,\n            \"ld_score\": None,\n            \"condition_number\": None,\n            \"correct_inversion\": None,\n            \"imputation_r2\": None,\n        }\n    else:\n        condition_number = np.array(\n            [np.linalg.cond(ld_matrix_known)] * ld_matrix_known_missing.shape[0]\n        )\n        correct_inversion = np.array(\n            [\n                SummaryStatisticsImputation._check_inversion(\n                    ld_matrix_known, sig_t_inv\n                )\n            ]\n            * ld_matrix_known_missing.shape[0]\n        )\n\n        var, ld_score = SummaryStatisticsImputation._compute_var(\n            ld_matrix_known_missing, sig_t_inv, lamb\n        )\n\n        mu = SummaryStatisticsImputation._compute_mu(\n            ld_matrix_known_missing, sig_t_inv, z_scores_known\n        )\n        var_norm = SummaryStatisticsImputation._var_in_boundaries(var, lamb)\n\n        R2 = (1 + lamb) - var_norm\n\n        mu = mu / np.sqrt(R2)\n        return {\n            \"var\": var,\n            \"mu\": mu,\n            \"ld_score\": ld_score,\n            \"condition_number\": condition_number,\n            \"correct_inversion\": correct_inversion,\n            \"imputation_r2\": 1 - var,\n        }\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/","title":"QC of GWAS Summary Statistics","text":"<p>This class consists of several general quality control checks for GWAS with full summary statistics. There are several checks included:</p> <ol> <li> <p>Genomic control lambda (median of the distribution of Chi2 statistics divided by expected for Chi2 with df=1). Lambda should be reasonably close to 1. Ideally not bigger than 2.</p> </li> <li> <p>P-Z check: the linear regression between log10 of reported p-values and log10 of p-values inferred from betas and standard errors. Intercept of the regression should be close to 0, slope close to 1.</p> </li> <li> <p>Mean beta check: mean of beta. Should be close to 0.</p> </li> <li> <p>The N_eff check: It estimates the ratio between effective sample size and the expected one and checks its distribution. It is possible to conduct only if the effective allele frequency is provided in the study. The median ratio is always close to 1, standard error should be close to 0.</p> </li> <li> <p>Number of SNPs and number of significant SNPs.</p> </li> </ol>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.SummaryStatisticsQC","title":"<code>gentropy.method.sumstat_quality_controls.SummaryStatisticsQC</code>","text":"<p>Summary statistics QC methods.</p> <p>This module contains methods for quality control of GWAS summary statistics. The list of methods includes:</p> <pre><code>- sumstat_qc_beta_check: This is the mean beta check. The mean beta should be close to 0.\n\n- sumstat_qc_pz_check: This is the PZ check. It runs a linear regression between reported p-values and p-values inferred from z-scores.\n\n- sumstat_n_eff_check: This is the effective sample size check. It estimates the ratio between the effective sample size and the expected one and checks its distribution.\n\n- gc_lambda_check: This is the genomic control lambda check.\n\n- number_of_snps: This function calculates the number of SNPs and the number of SNPs with a p-value less than 5e-8.\n</code></pre> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>class SummaryStatisticsQC:\n    \"\"\"Summary statistics QC methods.\n\n    This module contains methods for quality control of GWAS summary statistics.\n    The list of methods includes:\n\n        - sumstat_qc_beta_check: This is the mean beta check. The mean beta should be close to 0.\n\n        - sumstat_qc_pz_check: This is the PZ check. It runs a linear regression between reported p-values and p-values inferred from z-scores.\n\n        - sumstat_n_eff_check: This is the effective sample size check. It estimates the ratio between the effective sample size and the expected one and checks its distribution.\n\n        - gc_lambda_check: This is the genomic control lambda check.\n\n        - number_of_snps: This function calculates the number of SNPs and the number of SNPs with a p-value less than 5e-8.\n    \"\"\"\n\n    @staticmethod\n    def sumstat_qc_beta_check(\n        gwas_for_qc: SummaryStatistics,\n    ) -&gt; DataFrame:\n        \"\"\"The mean beta check for QC of GWAS summary statstics.\n\n        Args:\n            gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n\n        Returns:\n            DataFrame: PySpark DataFrame with the mean beta for each study.\n        \"\"\"\n        gwas_df = gwas_for_qc._df\n        qc_c = gwas_df.groupBy(\"studyId\").agg(\n            f.mean(\"beta\").alias(\"mean_beta\"),\n        )\n        return qc_c\n\n    @staticmethod\n    def _calculate_logpval(z2: float) -&gt; float:\n        \"\"\"Calculate negative log10-pval from Z-score.\n\n        Args:\n            z2 (float): Z-score squared.\n\n        Returns:\n            float: log10-pval.\n\n        Examples:\n            &gt;&gt;&gt; SummaryStatisticsQC._calculate_logpval(1.0)\n            0.49851554582799334\n        \"\"\"\n        logpval = -np.log10(sc.stats.chi2.sf((z2), 1))\n        return float(logpval)\n\n    @staticmethod\n    def sumstat_qc_pz_check(\n        gwas_for_qc: SummaryStatistics,\n    ) -&gt; DataFrame:\n        \"\"\"The PZ check for QC of GWAS summary statstics. It runs linear regression between reported p-values and p-values infered from z-scores.\n\n        Args:\n            gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n\n        Returns:\n            DataFrame: PySpark DataFrame with the results of the linear regression for each study.\n        \"\"\"\n        gwas_df = gwas_for_qc._df\n\n        calculate_logpval_udf = f.udf(\n            SummaryStatisticsQC._calculate_logpval, t.DoubleType()\n        )\n\n        qc_c = (\n            gwas_df.withColumn(\"Z2\", (f.col(\"beta\") / f.col(\"standardError\")) ** 2)\n            .filter(f.col(\"Z2\") &lt;= 100)\n            .withColumn(\"new_logpval\", calculate_logpval_udf(f.col(\"Z2\")))\n            .withColumn(\"log_mantissa\", log10(\"pValueMantissa\"))\n            .withColumn(\n                \"diffpval\",\n                -f.col(\"log_mantissa\") - f.col(\"pValueExponent\") - f.col(\"new_logpval\"),\n            )\n            .groupBy(\"studyId\")\n            .agg(\n                f.mean(\"diffpval\").alias(\"mean_diff_pz\"),\n                f.stddev(\"diffpval\").alias(\"se_diff_pz\"),\n            )\n            .select(\"studyId\", \"mean_diff_pz\", \"se_diff_pz\")\n        )\n\n        return qc_c\n\n    @staticmethod\n    def sumstat_n_eff_check(\n        gwas_for_qc: SummaryStatistics,\n        n_total: int = 100_000,\n        limit: int = 10_000_000,\n        min_count: int = 100,\n    ) -&gt; DataFrame:\n        \"\"\"The effective sample size check for QC of GWAS summary statstics.\n\n        It estiamtes the ratio between effective sample size and the expected one and checks it's distribution.\n        It is possible to conduct only if the effective allele frequency is provided in the study.\n        The median rartio is always close to 1, but standard error could be inflated.\n\n        Args:\n            gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n            n_total (int): The reported sample size of the study. The QC metrics is robust toward the sample size.\n            limit (int): The limit for the number of variants to be used for the estimation.\n            min_count (int): The minimum number of variants to be used for the estimation.\n\n        Returns:\n            DataFrame: PySpark DataFrame with the effective sample size ratio for each study.\n        \"\"\"\n        gwas_df = gwas_for_qc._df\n\n        gwas_df = gwas_df.dropna(subset=[\"effectAlleleFrequencyFromSource\"])\n\n        counts_df = gwas_df.groupBy(\"studyId\").count()\n\n        # Join the original DataFrame with the counts DataFrame\n        df_with_counts = gwas_df.join(counts_df, on=\"studyId\")\n\n        # Filter the DataFrame to keep only the groups with count greater than or equal to min_count\n        filtered_df = df_with_counts.filter(f.col(\"count\") &gt;= min_count).drop(\"count\")\n\n        window = Window.partitionBy(\"studyId\").orderBy(\"studyId\")\n        gwas_df = (\n            filtered_df.withColumn(\"row_num\", row_number().over(window))\n            .filter(f.col(\"row_num\") &lt;= limit)\n            .drop(\"row_num\")\n        )\n\n        gwas_df = gwas_df.withColumn(\n            \"var_af\",\n            2\n            * (\n                f.col(\"effectAlleleFrequencyFromSource\")\n                * (1 - f.col(\"effectAlleleFrequencyFromSource\"))\n            ),\n        ).withColumn(\n            \"pheno_var\",\n            ((f.col(\"standardError\") ** 2) * n_total * f.col(\"var_af\"))\n            + ((f.col(\"beta\") ** 2) * f.col(\"var_af\")),\n        )\n\n        window = Window.partitionBy(\"studyId\").orderBy(\"studyId\")\n\n        # Calculate the median of 'pheno_var' for each 'studyId' and add it as a new column\n        gwas_df = gwas_df.withColumn(\n            \"pheno_median\", expr(\"percentile_approx(pheno_var, 0.5)\").over(window)\n        )\n\n        gwas_df = gwas_df.withColumn(\n            \"N_hat_ratio\",\n            (\n                (f.col(\"pheno_median\") - ((f.col(\"beta\") ** 2) * f.col(\"var_af\")))\n                / ((f.col(\"standardError\") ** 2) * f.col(\"var_af\") * n_total)\n            ),\n        )\n\n        qc_c = (\n            gwas_df.groupBy(\"studyId\")\n            .agg(\n                f.stddev(\"N_hat_ratio\").alias(\"se_N\"),\n            )\n            .select(\"studyId\", \"se_N\")\n        )\n\n        return qc_c\n\n    @staticmethod\n    def gc_lambda_check(\n        gwas_for_qc: SummaryStatistics,\n    ) -&gt; DataFrame:\n        \"\"\"The genomic control lambda check for QC of GWAS summary statstics.\n\n        Args:\n            gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n\n        Returns:\n            DataFrame: PySpark DataFrame with the genomic control lambda for each study.\n        \"\"\"\n        gwas_df = gwas_for_qc._df\n\n        qc_c = (\n            gwas_df.select(\"studyId\", \"beta\", \"standardError\")\n            .withColumn(\"Z2\", (f.col(\"beta\") / f.col(\"standardError\")) ** 2)\n            .groupBy(\"studyId\")\n            .agg(f.expr(\"percentile_approx(Z2, 0.5)\").alias(\"gc_lambda\"))\n            .withColumn(\"gc_lambda\", f.col(\"gc_lambda\") / chi2.ppf(0.5, df=1))\n            .select(\"studyId\", \"gc_lambda\")\n        )\n\n        return qc_c\n\n    @staticmethod\n    def number_of_snps(\n        gwas_for_qc: SummaryStatistics, pval_threshold: float = 5e-8\n    ) -&gt; DataFrame:\n        \"\"\"The function caluates number of SNPs and number of SNPs with p-value less than 5e-8.\n\n        Args:\n            gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n            pval_threshold (float): The threshold for the p-value.\n\n        Returns:\n            DataFrame: PySpark DataFrame with the number of SNPs and number of SNPs with p-value less than threshold.\n        \"\"\"\n        gwas_df = gwas_for_qc._df\n\n        snp_counts = gwas_df.groupBy(\"studyId\").agg(\n            f.count(\"*\").alias(\"n_variants\"),\n            f.sum(\n                (\n                    f.log10(f.col(\"pValueMantissa\")) + f.col(\"pValueExponent\")\n                    &lt;= np.log10(pval_threshold)\n                ).cast(\"int\")\n            ).alias(\"n_variants_sig\"),\n        )\n\n        return snp_counts\n\n    @staticmethod\n    def get_quality_control_metrics(\n        gwas: SummaryStatistics,\n        pval_threshold: float = 1e-8,\n    ) -&gt; DataFrame:\n        \"\"\"The function calculates the quality control metrics for the summary statistics.\n\n        Args:\n            gwas (SummaryStatistics): The instance of the SummaryStatistics class.\n            pval_threshold (float): The threshold for the p-value.\n\n        Returns:\n            DataFrame: PySpark DataFrame with the quality control metrics for the summary statistics.\n        \"\"\"\n        qc1 = SummaryStatisticsQC.sumstat_qc_beta_check(gwas_for_qc=gwas)\n        qc2 = SummaryStatisticsQC.sumstat_qc_pz_check(gwas_for_qc=gwas)\n        qc4 = SummaryStatisticsQC.gc_lambda_check(gwas_for_qc=gwas)\n        qc5 = SummaryStatisticsQC.number_of_snps(\n            gwas_for_qc=gwas, pval_threshold=pval_threshold\n        )\n        df = (\n            qc1.join(qc2, on=\"studyId\", how=\"outer\")\n            .join(qc4, on=\"studyId\", how=\"outer\")\n            .join(qc5, on=\"studyId\", how=\"outer\")\n        )\n\n        return df\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.SummaryStatisticsQC.gc_lambda_check","title":"<code>gc_lambda_check(gwas_for_qc: SummaryStatistics) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>The genomic control lambda check for QC of GWAS summary statstics.</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>SummaryStatistics</code> <p>The instance of the SummaryStatistics class.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the genomic control lambda for each study.</p> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>@staticmethod\ndef gc_lambda_check(\n    gwas_for_qc: SummaryStatistics,\n) -&gt; DataFrame:\n    \"\"\"The genomic control lambda check for QC of GWAS summary statstics.\n\n    Args:\n        gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the genomic control lambda for each study.\n    \"\"\"\n    gwas_df = gwas_for_qc._df\n\n    qc_c = (\n        gwas_df.select(\"studyId\", \"beta\", \"standardError\")\n        .withColumn(\"Z2\", (f.col(\"beta\") / f.col(\"standardError\")) ** 2)\n        .groupBy(\"studyId\")\n        .agg(f.expr(\"percentile_approx(Z2, 0.5)\").alias(\"gc_lambda\"))\n        .withColumn(\"gc_lambda\", f.col(\"gc_lambda\") / chi2.ppf(0.5, df=1))\n        .select(\"studyId\", \"gc_lambda\")\n    )\n\n    return qc_c\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.SummaryStatisticsQC.get_quality_control_metrics","title":"<code>get_quality_control_metrics(gwas: SummaryStatistics, pval_threshold: float = 1e-08) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>The function calculates the quality control metrics for the summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>gwas</code> <code>SummaryStatistics</code> <p>The instance of the SummaryStatistics class.</p> required <code>pval_threshold</code> <code>float</code> <p>The threshold for the p-value.</p> <code>1e-08</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the quality control metrics for the summary statistics.</p> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>@staticmethod\ndef get_quality_control_metrics(\n    gwas: SummaryStatistics,\n    pval_threshold: float = 1e-8,\n) -&gt; DataFrame:\n    \"\"\"The function calculates the quality control metrics for the summary statistics.\n\n    Args:\n        gwas (SummaryStatistics): The instance of the SummaryStatistics class.\n        pval_threshold (float): The threshold for the p-value.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the quality control metrics for the summary statistics.\n    \"\"\"\n    qc1 = SummaryStatisticsQC.sumstat_qc_beta_check(gwas_for_qc=gwas)\n    qc2 = SummaryStatisticsQC.sumstat_qc_pz_check(gwas_for_qc=gwas)\n    qc4 = SummaryStatisticsQC.gc_lambda_check(gwas_for_qc=gwas)\n    qc5 = SummaryStatisticsQC.number_of_snps(\n        gwas_for_qc=gwas, pval_threshold=pval_threshold\n    )\n    df = (\n        qc1.join(qc2, on=\"studyId\", how=\"outer\")\n        .join(qc4, on=\"studyId\", how=\"outer\")\n        .join(qc5, on=\"studyId\", how=\"outer\")\n    )\n\n    return df\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.SummaryStatisticsQC.number_of_snps","title":"<code>number_of_snps(gwas_for_qc: SummaryStatistics, pval_threshold: float = 5e-08) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>The function caluates number of SNPs and number of SNPs with p-value less than 5e-8.</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>SummaryStatistics</code> <p>The instance of the SummaryStatistics class.</p> required <code>pval_threshold</code> <code>float</code> <p>The threshold for the p-value.</p> <code>5e-08</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the number of SNPs and number of SNPs with p-value less than threshold.</p> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>@staticmethod\ndef number_of_snps(\n    gwas_for_qc: SummaryStatistics, pval_threshold: float = 5e-8\n) -&gt; DataFrame:\n    \"\"\"The function caluates number of SNPs and number of SNPs with p-value less than 5e-8.\n\n    Args:\n        gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n        pval_threshold (float): The threshold for the p-value.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the number of SNPs and number of SNPs with p-value less than threshold.\n    \"\"\"\n    gwas_df = gwas_for_qc._df\n\n    snp_counts = gwas_df.groupBy(\"studyId\").agg(\n        f.count(\"*\").alias(\"n_variants\"),\n        f.sum(\n            (\n                f.log10(f.col(\"pValueMantissa\")) + f.col(\"pValueExponent\")\n                &lt;= np.log10(pval_threshold)\n            ).cast(\"int\")\n        ).alias(\"n_variants_sig\"),\n    )\n\n    return snp_counts\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.SummaryStatisticsQC.sumstat_n_eff_check","title":"<code>sumstat_n_eff_check(gwas_for_qc: SummaryStatistics, n_total: int = 100000, limit: int = 10000000, min_count: int = 100) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>The effective sample size check for QC of GWAS summary statstics.</p> <p>It estiamtes the ratio between effective sample size and the expected one and checks it's distribution. It is possible to conduct only if the effective allele frequency is provided in the study. The median rartio is always close to 1, but standard error could be inflated.</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>SummaryStatistics</code> <p>The instance of the SummaryStatistics class.</p> required <code>n_total</code> <code>int</code> <p>The reported sample size of the study. The QC metrics is robust toward the sample size.</p> <code>100000</code> <code>limit</code> <code>int</code> <p>The limit for the number of variants to be used for the estimation.</p> <code>10000000</code> <code>min_count</code> <code>int</code> <p>The minimum number of variants to be used for the estimation.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the effective sample size ratio for each study.</p> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>@staticmethod\ndef sumstat_n_eff_check(\n    gwas_for_qc: SummaryStatistics,\n    n_total: int = 100_000,\n    limit: int = 10_000_000,\n    min_count: int = 100,\n) -&gt; DataFrame:\n    \"\"\"The effective sample size check for QC of GWAS summary statstics.\n\n    It estiamtes the ratio between effective sample size and the expected one and checks it's distribution.\n    It is possible to conduct only if the effective allele frequency is provided in the study.\n    The median rartio is always close to 1, but standard error could be inflated.\n\n    Args:\n        gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n        n_total (int): The reported sample size of the study. The QC metrics is robust toward the sample size.\n        limit (int): The limit for the number of variants to be used for the estimation.\n        min_count (int): The minimum number of variants to be used for the estimation.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the effective sample size ratio for each study.\n    \"\"\"\n    gwas_df = gwas_for_qc._df\n\n    gwas_df = gwas_df.dropna(subset=[\"effectAlleleFrequencyFromSource\"])\n\n    counts_df = gwas_df.groupBy(\"studyId\").count()\n\n    # Join the original DataFrame with the counts DataFrame\n    df_with_counts = gwas_df.join(counts_df, on=\"studyId\")\n\n    # Filter the DataFrame to keep only the groups with count greater than or equal to min_count\n    filtered_df = df_with_counts.filter(f.col(\"count\") &gt;= min_count).drop(\"count\")\n\n    window = Window.partitionBy(\"studyId\").orderBy(\"studyId\")\n    gwas_df = (\n        filtered_df.withColumn(\"row_num\", row_number().over(window))\n        .filter(f.col(\"row_num\") &lt;= limit)\n        .drop(\"row_num\")\n    )\n\n    gwas_df = gwas_df.withColumn(\n        \"var_af\",\n        2\n        * (\n            f.col(\"effectAlleleFrequencyFromSource\")\n            * (1 - f.col(\"effectAlleleFrequencyFromSource\"))\n        ),\n    ).withColumn(\n        \"pheno_var\",\n        ((f.col(\"standardError\") ** 2) * n_total * f.col(\"var_af\"))\n        + ((f.col(\"beta\") ** 2) * f.col(\"var_af\")),\n    )\n\n    window = Window.partitionBy(\"studyId\").orderBy(\"studyId\")\n\n    # Calculate the median of 'pheno_var' for each 'studyId' and add it as a new column\n    gwas_df = gwas_df.withColumn(\n        \"pheno_median\", expr(\"percentile_approx(pheno_var, 0.5)\").over(window)\n    )\n\n    gwas_df = gwas_df.withColumn(\n        \"N_hat_ratio\",\n        (\n            (f.col(\"pheno_median\") - ((f.col(\"beta\") ** 2) * f.col(\"var_af\")))\n            / ((f.col(\"standardError\") ** 2) * f.col(\"var_af\") * n_total)\n        ),\n    )\n\n    qc_c = (\n        gwas_df.groupBy(\"studyId\")\n        .agg(\n            f.stddev(\"N_hat_ratio\").alias(\"se_N\"),\n        )\n        .select(\"studyId\", \"se_N\")\n    )\n\n    return qc_c\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.SummaryStatisticsQC.sumstat_qc_beta_check","title":"<code>sumstat_qc_beta_check(gwas_for_qc: SummaryStatistics) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>The mean beta check for QC of GWAS summary statstics.</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>SummaryStatistics</code> <p>The instance of the SummaryStatistics class.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the mean beta for each study.</p> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>@staticmethod\ndef sumstat_qc_beta_check(\n    gwas_for_qc: SummaryStatistics,\n) -&gt; DataFrame:\n    \"\"\"The mean beta check for QC of GWAS summary statstics.\n\n    Args:\n        gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the mean beta for each study.\n    \"\"\"\n    gwas_df = gwas_for_qc._df\n    qc_c = gwas_df.groupBy(\"studyId\").agg(\n        f.mean(\"beta\").alias(\"mean_beta\"),\n    )\n    return qc_c\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.SummaryStatisticsQC.sumstat_qc_pz_check","title":"<code>sumstat_qc_pz_check(gwas_for_qc: SummaryStatistics) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>The PZ check for QC of GWAS summary statstics. It runs linear regression between reported p-values and p-values infered from z-scores.</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>SummaryStatistics</code> <p>The instance of the SummaryStatistics class.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the results of the linear regression for each study.</p> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>@staticmethod\ndef sumstat_qc_pz_check(\n    gwas_for_qc: SummaryStatistics,\n) -&gt; DataFrame:\n    \"\"\"The PZ check for QC of GWAS summary statstics. It runs linear regression between reported p-values and p-values infered from z-scores.\n\n    Args:\n        gwas_for_qc (SummaryStatistics): The instance of the SummaryStatistics class.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the results of the linear regression for each study.\n    \"\"\"\n    gwas_df = gwas_for_qc._df\n\n    calculate_logpval_udf = f.udf(\n        SummaryStatisticsQC._calculate_logpval, t.DoubleType()\n    )\n\n    qc_c = (\n        gwas_df.withColumn(\"Z2\", (f.col(\"beta\") / f.col(\"standardError\")) ** 2)\n        .filter(f.col(\"Z2\") &lt;= 100)\n        .withColumn(\"new_logpval\", calculate_logpval_udf(f.col(\"Z2\")))\n        .withColumn(\"log_mantissa\", log10(\"pValueMantissa\"))\n        .withColumn(\n            \"diffpval\",\n            -f.col(\"log_mantissa\") - f.col(\"pValueExponent\") - f.col(\"new_logpval\"),\n        )\n        .groupBy(\"studyId\")\n        .agg(\n            f.mean(\"diffpval\").alias(\"mean_diff_pz\"),\n            f.stddev(\"diffpval\").alias(\"se_diff_pz\"),\n        )\n        .select(\"studyId\", \"mean_diff_pz\", \"se_diff_pz\")\n    )\n\n    return qc_c\n</code></pre>"},{"location":"python_api/methods/susie_inf/","title":"SuSiE-inf - Fine-mapping with infinitesimal effects v1.1","text":"<p>This is an implementation of the SuSiE-inf method found here: https://github.com/FinucaneLab/fine-mapping-inf https://www.nature.com/articles/s41588-023-01597-3</p> <p>This fine-mapping approach has two approaches for updating estimates of the variance components - Method of Moments and Maximum Likelihood Estimator ('MoM' / 'MLE') The function takes an array of Z-scores and a numpy array matrix of variant LD to perform finemapping.</p>"},{"location":"python_api/methods/susie_inf/#gentropy.method.susie_inf.SUSIE_inf","title":"<code>gentropy.method.susie_inf.SUSIE_inf</code>  <code>dataclass</code>","text":"<p>SuSiE fine-mapping of a study locus from fine-mapping-inf package.</p> <p>Note: code copied from fine-mapping-inf package as a placeholder https://github.com/FinucaneLab/fine-mapping-inf</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if missing LD or if unsupported variance estimation</p> Source code in <code>src/gentropy/method/susie_inf.py</code> <pre><code>@dataclass\nclass SUSIE_inf:\n    \"\"\"SuSiE fine-mapping of a study locus from fine-mapping-inf package.\n\n    Note: code copied from fine-mapping-inf package as a placeholder\n    https://github.com/FinucaneLab/fine-mapping-inf\n\n    Raises:\n        RuntimeError: if missing LD or if unsupported variance estimation\n    \"\"\"\n\n    @staticmethod\n    def susie_inf(  # noqa: C901\n        z: np.ndarray,\n        meansq: float = 1,\n        n: int = 100000,\n        L: int = 10,\n        LD: np.ndarray | None = None,\n        V: np.ndarray | None = None,\n        Dsq: np.ndarray | None = None,\n        est_ssq: bool = True,\n        ssq: np.ndarray | None = None,\n        ssq_range: tuple[float, float] = (0, 1),\n        pi0: np.ndarray | None = None,\n        est_sigmasq: bool = True,\n        est_tausq: bool = False,\n        sigmasq: float = 1,\n        tausq: float = 0,\n        sigmasq_range: tuple[float, float] | None = None,\n        tausq_range: tuple[float, float] | None = None,\n        PIP: np.ndarray | None = None,\n        mu: np.ndarray | None = None,\n        method: str = \"moments\",\n        maxiter: int = 100,\n        PIP_tol: float = 0.001,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Susie with random effects.\n\n        Args:\n            z (np.ndarray): vector of z-scores (equal to X'y/sqrt(n))\n            meansq (float): average squared magnitude of y (equal to ||y||^2/n)\n            n (int): sample size\n            L (int): number of modeled causal effects\n            LD (np.ndarray | None): LD matrix (equal to X'X/n)\n            V (np.ndarray | None): precomputed p x p matrix of eigenvectors of X'X\n            Dsq (np.ndarray | None): precomputed length-p vector of eigenvalues of X'X\n            est_ssq (bool): estimate prior effect size variances s^2 using MLE\n            ssq (np.ndarray | None): length-L initialization s^2 for each effect\n            ssq_range (tuple[float, float]): lower and upper bounds for each s^2, if estimated\n            pi0 (np.ndarray | None): length-p vector of prior causal probability for each SNP; must sum to 1\n            est_sigmasq (bool): estimate variance sigma^2\n            est_tausq (bool): estimate both variances sigma^2 and tau^2\n            sigmasq (float): initial value for sigma^2\n            tausq (float): initial value for tau^2\n            sigmasq_range (tuple[float, float] | None): lower and upper bounds for sigma^2, if estimated using MLE\n            tausq_range (tuple[float, float] | None): lower and upper bounds for tau^2, if estimated using MLE\n            PIP (np.ndarray | None): p x L initializations of PIPs\n            mu (np.ndarray | None): p x L initializations of mu\n            method (str): one of {'moments','MLE'}\n            maxiter (int): maximum number of SuSiE iterations\n            PIP_tol (float): convergence threshold for PIP difference between iterations\n\n        Returns:\n            dict[str, Any]: Dictionary with keys:\n                PIP -- p x L matrix of PIPs, individually for each effect\n                mu -- p x L matrix of posterior means conditional on causal\n                omega -- p x L matrix of posterior precisions conditional on causal\n                lbf_variable -- p x L matrix of log-Bayes-factors, for each effect\n                ssq -- length-L array of final effect size variances s^2\n                sigmasq -- final value of sigma^2\n                tausq -- final value of tau^2\n                alpha -- length-p array of posterior means of infinitesimal effects\n                lbf -- length-p array of log-Bayes-factors for each CS\n\n        Raises:\n            RuntimeError: if missing LD or if unsupported variance estimation method\n        \"\"\"\n        p = len(z)\n        # Precompute V,D^2 in the SVD X=UDV', and V'X'y and y'y\n        if (V is None or Dsq is None) and LD is None:\n            raise RuntimeError(\"Missing LD\")\n        elif V is None or Dsq is None:\n            eigvals, V = scipy.linalg.eigh(LD)\n            Dsq = np.maximum(n * eigvals, 0)\n        else:\n            Dsq = np.maximum(Dsq, 0)\n        Xty = np.sqrt(n) * z\n        VtXty = V.T.dot(Xty)\n        yty = n * meansq\n        # Initialize diagonal variances, diag(X' Omega X), X' Omega y\n        var = tausq * Dsq + sigmasq\n        diagXtOmegaX = np.sum(V**2 * (Dsq / var), axis=1)\n        XtOmegay = V.dot(VtXty / var)\n        # Initialize s_l^2, PIP_j, mu_j, omega_j\n        if ssq is None:\n            ssq = np.ones(L) * 0.2\n        if PIP is None:\n            PIP = np.ones((p, L)) / p\n        if mu is None:\n            mu = np.zeros((p, L))\n        lbf_variable = np.zeros((p, L))\n        omega = diagXtOmegaX[:, np.newaxis] + 1 / ssq\n        # Initialize prior causal probabilities\n        if pi0 is None:\n            logpi0 = np.ones(p) * np.log(1.0 / p)\n        else:\n            logpi0 = -np.ones(p) * np.inf\n            inds = np.nonzero(pi0 &gt; 0)[0]\n            logpi0[inds] = np.log(pi0[inds])\n\n        ####### Main SuSiE iteration loop ######\n        def f(x: float) -&gt; float:\n            \"\"\"Negative ELBO as function of x = sigma_e^2.\n\n            Args:\n                x (float): sigma_e^2\n\n            Returns:\n                float: negative ELBO as function of x = sigma_e^2\n            \"\"\"\n            return -scipy.special.logsumexp(\n                -0.5 * np.log(1 + x * diagXtOmegaX)\n                + x * XtOmegar**2 / (2 * (1 + x * diagXtOmegaX))\n                + logpi0\n            )\n\n        for it in range(maxiter):\n            PIP_prev = PIP.copy()\n            # Single effect regression for each effect l = 1,...,L\n            for _l in range(L):\n                # Compute X' Omega r_l for residual r_l\n                b = np.sum(mu * PIP, axis=1) - mu[:, _l] * PIP[:, _l]\n                XtOmegaXb = V.dot(V.T.dot(b) * Dsq / var)\n                XtOmegar = XtOmegay - XtOmegaXb\n                if est_ssq:\n                    # Update prior variance ssq[l]\n                    res = minimize_scalar(f, bounds=ssq_range, method=\"bounded\")\n                    if res.success:\n                        ssq[_l] = res.x\n                # Update omega, mu, and PIP\n                omega[:, _l] = diagXtOmegaX + 1 / ssq[_l]\n                mu[:, _l] = XtOmegar / omega[:, _l]\n                lbf_variable[:, _l] = XtOmegar**2 / (2 * omega[:, _l]) - 0.5 * np.log(\n                    omega[:, _l] * ssq[_l]\n                )\n                logPIP = lbf_variable[:, _l] + logpi0\n                PIP[:, _l] = np.exp(logPIP - scipy.special.logsumexp(logPIP))\n            # Update variance components\n            if est_sigmasq or est_tausq:\n                if method == \"moments\":\n                    (sigmasq, tausq) = SUSIE_inf._MoM(\n                        PIP,\n                        mu,\n                        omega,\n                        sigmasq,\n                        tausq,\n                        n,\n                        V,\n                        Dsq,\n                        VtXty,\n                        Xty,\n                        yty,\n                        est_sigmasq,\n                        est_tausq,\n                    )\n                elif method == \"MLE\":\n                    (sigmasq, tausq) = SUSIE_inf._MLE(\n                        PIP,\n                        mu,\n                        omega,\n                        sigmasq,\n                        tausq,\n                        n,\n                        V,\n                        Dsq,\n                        VtXty,\n                        yty,\n                        est_sigmasq,\n                        est_tausq,\n                        it,\n                        sigmasq_range,\n                        tausq_range,\n                    )\n                else:\n                    raise RuntimeError(\"Unsupported variance estimation method\")\n                # Update X' Omega X, X' Omega y\n                var = tausq * Dsq + sigmasq\n                diagXtOmegaX = np.sum(V**2 * (Dsq / var), axis=1)\n                XtOmegay = V.dot(VtXty / var)\n            # Determine convergence from PIP differences\n            PIP_diff = np.max(np.abs(PIP_prev - PIP))\n            if PIP_diff &lt; PIP_tol:\n                break\n        # Compute posterior means of b and alpha\n        b = np.sum(mu * PIP, axis=1)\n        XtOmegaXb = V.dot(V.T.dot(b) * Dsq / var)\n        XtOmegar = XtOmegay - XtOmegaXb\n        alpha = tausq * XtOmegar\n\n        priors = np.log(np.repeat(1 / p, p))\n        lbf_cs = np.apply_along_axis(\n            lambda x: logsumexp(x + priors), axis=0, arr=lbf_variable\n        )\n        return {\n            \"PIP\": PIP,\n            \"mu\": mu,\n            \"omega\": omega,\n            \"lbf_variable\": lbf_variable,\n            \"ssq\": ssq,\n            \"sigmasq\": sigmasq,\n            \"tausq\": tausq,\n            \"alpha\": alpha,\n            \"lbf\": lbf_cs,\n        }\n\n    @staticmethod\n    def _MoM(\n        PIP: np.ndarray,\n        mu: np.ndarray,\n        omega: np.ndarray,\n        sigmasq: float,\n        tausq: float,\n        n: int,\n        V: np.ndarray,\n        Dsq: np.ndarray,\n        VtXty: np.ndarray,\n        Xty: np.ndarray,\n        yty: float,\n        est_sigmasq: bool,\n        est_tausq: bool,\n    ) -&gt; tuple[float, float]:\n        \"\"\"Subroutine to estimate sigma^2, tau^2 using method-of-moments.\n\n        Args:\n            PIP (np.ndarray): p x L matrix of PIPs\n            mu (np.ndarray): p x L matrix of posterior means conditional on causal\n            omega (np.ndarray): p x L matrix of posterior precisions conditional on causal\n            sigmasq (float): initial value for sigma^2\n            tausq (float): initial value for tau^2\n            n (int): sample size\n            V (np.ndarray): precomputed p x p matrix of eigenvectors of X'X\n            Dsq (np.ndarray): precomputed length-p vector of eigenvalues of X'X\n            VtXty (np.ndarray): precomputed length-p vector V'X'y\n            Xty (np.ndarray): precomputed length-p vector X'y\n            yty (float): precomputed y'y\n            est_sigmasq (bool): estimate variance sigma^2\n            est_tausq (bool): estimate both variances sigma^2 and tau^2\n\n        Returns:\n            tuple[float, float]: (sigmasq,tausq) tuple of updated variances\n        \"\"\"\n        (p, L) = mu.shape\n        # Compute A\n        A = np.array([[n, sum(Dsq)], [0, sum(Dsq**2)]])\n        A[1, 0] = A[0, 1]\n        # Compute diag(V'MV)\n        b = np.sum(mu * PIP, axis=1)\n        Vtb = V.T.dot(b)\n        diagVtMV = Vtb**2\n        tmpD = np.zeros(p)\n        for _l in range(L):\n            bl = mu[:, _l] * PIP[:, _l]\n            Vtbl = V.T.dot(bl)\n            diagVtMV -= Vtbl**2\n            tmpD += PIP[:, _l] * (mu[:, _l] ** 2 + 1 / omega[:, _l])\n        diagVtMV += np.sum((V.T) ** 2 * tmpD, axis=1)\n        # Compute x\n        x = np.zeros(2)\n        x[0] = yty - 2 * sum(b * Xty) + sum(Dsq * diagVtMV)\n        x[1] = sum(Xty**2) - 2 * sum(Vtb * VtXty * Dsq) + sum(Dsq**2 * diagVtMV)\n        if est_tausq:\n            sol = scipy.linalg.solve(A, x)\n            if sol[0] &gt; 0 and sol[1] &gt; 0:\n                (sigmasq, tausq) = sol\n            else:\n                (sigmasq, tausq) = (x[0] / n, 0)\n        elif est_sigmasq:\n            sigmasq = (x[0] - A[0, 1] * tausq) / n\n        return sigmasq, tausq\n\n    @staticmethod\n    def _MLE(\n        PIP: np.ndarray,\n        mu: np.ndarray,\n        omega: np.ndarray,\n        sigmasq: float,\n        tausq: float,\n        n: int,\n        V: np.ndarray,\n        Dsq: np.ndarray,\n        VtXty: np.ndarray,\n        yty: float,\n        est_sigmasq: bool,\n        est_tausq: bool,\n        it: int,\n        sigmasq_range: tuple[float, float] | None = None,\n        tausq_range: tuple[float, float] | None = None,\n    ) -&gt; tuple[float, float]:\n        \"\"\"Subroutine to estimate sigma^2, tau^2 using MLE.\n\n        Args:\n            PIP (np.ndarray): p x L matrix of PIPs\n            mu (np.ndarray): p x L matrix of posterior means conditional on causal\n            omega (np.ndarray): p x L matrix of posterior precisions conditional on causal\n            sigmasq (float): initial value for sigma^2\n            tausq (float): initial value for tau^2\n            n (int): sample size\n            V (np.ndarray): precomputed p x p matrix of eigenvectors of X'X\n            Dsq (np.ndarray): precomputed length-p vector of eigenvalues of X'X\n            VtXty (np.ndarray): precomputed length-p vector V'X'y\n            yty (float): precomputed y'y\n            est_sigmasq (bool): estimate variance sigma^2\n            est_tausq (bool): estimate both variances sigma^2 and tau^2\n            it (int): iteration number\n            sigmasq_range (tuple[float, float] | None): lower and upper bounds for sigma^2, if estimated using MLE\n            tausq_range (tuple[float, float] | None): lower and upper bounds for tau^2, if estimated using MLE\n\n        Returns:\n            tuple[float, float]: (sigmasq,tausq) tuple of updated variances\n        \"\"\"\n        (p, L) = mu.shape\n        if sigmasq_range is None:\n            sigmasq_range = (0.2 * yty / n, 1.2 * yty / n)\n        if tausq_range is None:\n            tausq_range = (1e-12, 1.2 * yty / (n * p))\n        # Compute diag(V'MV)\n        b = np.sum(mu * PIP, axis=1)\n        Vtb = V.T.dot(b)\n        diagVtMV = Vtb**2\n        tmpD = np.zeros(p)\n        for _l in range(L):\n            bl = mu[:, _l] * PIP[:, _l]\n            Vtbl = V.T.dot(bl)\n            diagVtMV -= Vtbl**2\n            tmpD += PIP[:, _l] * (mu[:, _l] ** 2 + 1 / omega[:, _l])\n        diagVtMV += np.sum((V.T) ** 2 * tmpD, axis=1)\n\n        # negative ELBO as function of x = (sigma_e^2,sigma_g^2)\n        def f(x: tuple[float, float]) -&gt; float:\n            \"\"\"Negative ELBO as function of x = (sigma_e^2,sigma_g^2).\n\n            Args:\n                x (tuple[float, float]): (sigma_e^2,sigma_g^2)\n\n            Returns:\n                float: negative ELBO as function of x = (sigma_e^2,sigma_g^2)\n            \"\"\"\n            return (\n                0.5 * (n - p) * np.log(x[0])\n                + 0.5 / x[0] * yty\n                + np.sum(\n                    0.5 * np.log(x[1] * Dsq + x[0])\n                    - 0.5 * x[1] / x[0] * VtXty**2 / (x[1] * Dsq + x[0])\n                    - Vtb * VtXty / (x[1] * Dsq + x[0])\n                    + 0.5 * Dsq / (x[1] * Dsq + x[0]) * diagVtMV\n                )\n            )\n\n        if est_tausq:\n            res = minimize(\n                f,\n                (sigmasq, tausq),\n                method=\"L-BFGS-B\",\n                bounds=(sigmasq_range, tausq_range),\n            )\n            if res.success:\n                sigmasq, tausq = res.x\n        elif est_sigmasq:\n\n            def g(x: float) -&gt; float:\n                \"\"\"Negative ELBO as function of x = sigma_e^2.\n\n                Args:\n                    x (float): sigma_e^2\n\n                Returns:\n                    float: negative ELBO as function of x = sigma_e^2\n                \"\"\"\n                return f((x, tausq))\n\n            res = minimize(g, sigmasq, method=\"L-BFGS-B\", bounds=(sigmasq_range,))\n            if res.success:\n                sigmasq = res.x\n        return sigmasq, tausq\n\n    @staticmethod\n    def cred_inf(\n        PIP: np.ndarray,\n        n: int = 100_000,\n        coverage: float = 0.99,\n        purity: float = 0.5,\n        LD: np.ndarray | None = None,\n        V: np.ndarray | None = None,\n        Dsq: np.ndarray | None = None,\n        dedup: bool = True,\n    ) -&gt; list[Any]:\n        \"\"\"Compute credible sets from single-effect PIPs.\n\n        Args:\n            PIP (np.ndarray): p x L matrix of PIPs\n            n (int): sample size\n            coverage (float): coverage of credible sets\n            purity (float): purity of credible sets\n            LD (np.ndarray | None): LD matrix (equal to X'X/n)\n            V (np.ndarray | None): precomputed p x p matrix of eigenvectors of X'X\n            Dsq (np.ndarray | None): precomputed length-p vector of eigenvalues of X'X\n            dedup (bool): whether to deduplicate credible sets\n\n        Returns:\n            list[Any]: list of L lists of SNP indices in each credible set\n\n        Raises:\n            RuntimeError: if missing inputs for purity filtering\n            ValueError: if either LD or V, Dsq are None\n        \"\"\"\n        if (V is None or Dsq is None or n is None) and LD is None:\n            raise RuntimeError(\"Missing inputs for purity filtering\")\n        # Compute credible sets\n        cred = []\n        for i in range(PIP.shape[1]):\n            sortinds = np.argsort(PIP[:, i])[::-1]\n            ind = min(np.nonzero(np.cumsum(PIP[sortinds, i]) &gt;= coverage)[0])\n            credset = sortinds[: (ind + 1)]\n            # Filter by purity\n            if len(credset) == 1:\n                cred.append(list(credset))\n                continue\n            if len(credset) &lt; 100:\n                rows = credset\n            else:\n                np.random.seed(123)\n                rows = np.random.choice(credset, size=100, replace=False)\n            if LD is not None:\n                LDloc = LD[np.ix_(rows, rows)]\n            elif V is not None and Dsq is not None:\n                LDloc = (V[rows, :] * Dsq).dot(V[rows, :].T) / n\n            else:\n                raise ValueError(\"Both LD and V, Dsq cannot be None\")\n            if np.min(np.abs(LDloc)) &gt; purity:\n                cred.append(sorted(credset))\n        if dedup:\n            cred = list(\n                map(\n                    list,\n                    sorted(set(map(tuple, cred)), key=list(map(tuple, cred)).index),\n                )\n            )\n        return cred\n\n    @staticmethod\n    def credible_set_qc(\n        cred_sets: StudyLocus,\n        p_value_threshold: float = 1e-5,\n        purity_min_r2: float = 0.01,\n        clump: bool = False,\n        ld_index: LDIndex | None = None,\n        study_index: StudyIndex | None = None,\n        ld_min_r2: float | None = 0.8,\n    ) -&gt; StudyLocus:\n        \"\"\"Filter credible sets by lead P-value and min-R2 purity, and performs LD clumping.\n\n        In case of duplicated loci, the filtering retains the loci wth the highest credibleSetlog10BF.\n\n\n        Args:\n            cred_sets (StudyLocus): StudyLocus object with credible sets to filter/clump\n            p_value_threshold (float): p-value threshold for filtering credible sets, default is 1e-5\n            purity_min_r2 (float): min-R2 purity threshold for filtering credible sets, default is 0.01\n            clump (bool): Whether to clump the credible sets by LD, default is False\n            ld_index (LDIndex | None): LDIndex object\n            study_index (StudyIndex | None): StudyIndex object\n            ld_min_r2 (float | None): LD R2 threshold for clumping, default is 0.8\n\n        Returns:\n            StudyLocus: Credible sets which pass filters and LD clumping.\n\n        Raises:\n            AssertionError: When running in clump mode, but no study study_index or ld_index or ld_min_r2 were provided.\n        \"\"\"\n        cred_sets.df = (\n            cred_sets.df.withColumn(\n                \"pValue\", f.col(\"pValueMantissa\") * f.pow(10, f.col(\"pValueExponent\"))\n            )\n            .filter(f.col(\"pValue\") &lt;= p_value_threshold)\n            .filter(f.col(\"purityMinR2\") &gt;= purity_min_r2)\n            .drop(\"pValue\")\n            .withColumn(\n                \"rn\",\n                f.row_number().over(\n                    Window.partitionBy(\"studyLocusId\").orderBy(\n                        f.desc(\"credibleSetLog10BF\")\n                    )\n                ),\n            )\n            .filter(f.col(\"rn\") == 1)\n            .drop(\"rn\")\n        )\n        if clump:\n            assert study_index, \"Running in clump mode, which requires study_index.\"\n            assert ld_index, \"Running in clump mode, which requires ld_index.\"\n            assert ld_min_r2, \"Running in clump mode, which requires ld_min_r2 value.\"\n            cred_sets = (\n                cred_sets.annotate_ld(study_index, ld_index, ld_min_r2)\n                .clump()\n                .filter(\n                    ~f.array_contains(\n                        f.col(\"qualityControls\"),\n                        StudyLocusQualityCheck.LD_CLUMPED.value,\n                    )\n                )\n            )\n\n        return cred_sets\n</code></pre>"},{"location":"python_api/methods/susie_inf/#gentropy.method.susie_inf.SUSIE_inf.cred_inf","title":"<code>cred_inf(PIP: np.ndarray, n: int = 100000, coverage: float = 0.99, purity: float = 0.5, LD: np.ndarray | None = None, V: np.ndarray | None = None, Dsq: np.ndarray | None = None, dedup: bool = True) -&gt; list[Any]</code>  <code>staticmethod</code>","text":"<p>Compute credible sets from single-effect PIPs.</p> <p>Parameters:</p> Name Type Description Default <code>PIP</code> <code>ndarray</code> <p>p x L matrix of PIPs</p> required <code>n</code> <code>int</code> <p>sample size</p> <code>100000</code> <code>coverage</code> <code>float</code> <p>coverage of credible sets</p> <code>0.99</code> <code>purity</code> <code>float</code> <p>purity of credible sets</p> <code>0.5</code> <code>LD</code> <code>ndarray | None</code> <p>LD matrix (equal to X'X/n)</p> <code>None</code> <code>V</code> <code>ndarray | None</code> <p>precomputed p x p matrix of eigenvectors of X'X</p> <code>None</code> <code>Dsq</code> <code>ndarray | None</code> <p>precomputed length-p vector of eigenvalues of X'X</p> <code>None</code> <code>dedup</code> <code>bool</code> <p>whether to deduplicate credible sets</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: list of L lists of SNP indices in each credible set</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if missing inputs for purity filtering</p> <code>ValueError</code> <p>if either LD or V, Dsq are None</p> Source code in <code>src/gentropy/method/susie_inf.py</code> <pre><code>@staticmethod\ndef cred_inf(\n    PIP: np.ndarray,\n    n: int = 100_000,\n    coverage: float = 0.99,\n    purity: float = 0.5,\n    LD: np.ndarray | None = None,\n    V: np.ndarray | None = None,\n    Dsq: np.ndarray | None = None,\n    dedup: bool = True,\n) -&gt; list[Any]:\n    \"\"\"Compute credible sets from single-effect PIPs.\n\n    Args:\n        PIP (np.ndarray): p x L matrix of PIPs\n        n (int): sample size\n        coverage (float): coverage of credible sets\n        purity (float): purity of credible sets\n        LD (np.ndarray | None): LD matrix (equal to X'X/n)\n        V (np.ndarray | None): precomputed p x p matrix of eigenvectors of X'X\n        Dsq (np.ndarray | None): precomputed length-p vector of eigenvalues of X'X\n        dedup (bool): whether to deduplicate credible sets\n\n    Returns:\n        list[Any]: list of L lists of SNP indices in each credible set\n\n    Raises:\n        RuntimeError: if missing inputs for purity filtering\n        ValueError: if either LD or V, Dsq are None\n    \"\"\"\n    if (V is None or Dsq is None or n is None) and LD is None:\n        raise RuntimeError(\"Missing inputs for purity filtering\")\n    # Compute credible sets\n    cred = []\n    for i in range(PIP.shape[1]):\n        sortinds = np.argsort(PIP[:, i])[::-1]\n        ind = min(np.nonzero(np.cumsum(PIP[sortinds, i]) &gt;= coverage)[0])\n        credset = sortinds[: (ind + 1)]\n        # Filter by purity\n        if len(credset) == 1:\n            cred.append(list(credset))\n            continue\n        if len(credset) &lt; 100:\n            rows = credset\n        else:\n            np.random.seed(123)\n            rows = np.random.choice(credset, size=100, replace=False)\n        if LD is not None:\n            LDloc = LD[np.ix_(rows, rows)]\n        elif V is not None and Dsq is not None:\n            LDloc = (V[rows, :] * Dsq).dot(V[rows, :].T) / n\n        else:\n            raise ValueError(\"Both LD and V, Dsq cannot be None\")\n        if np.min(np.abs(LDloc)) &gt; purity:\n            cred.append(sorted(credset))\n    if dedup:\n        cred = list(\n            map(\n                list,\n                sorted(set(map(tuple, cred)), key=list(map(tuple, cred)).index),\n            )\n        )\n    return cred\n</code></pre>"},{"location":"python_api/methods/susie_inf/#gentropy.method.susie_inf.SUSIE_inf.credible_set_qc","title":"<code>credible_set_qc(cred_sets: StudyLocus, p_value_threshold: float = 1e-05, purity_min_r2: float = 0.01, clump: bool = False, ld_index: LDIndex | None = None, study_index: StudyIndex | None = None, ld_min_r2: float | None = 0.8) -&gt; StudyLocus</code>  <code>staticmethod</code>","text":"<p>Filter credible sets by lead P-value and min-R2 purity, and performs LD clumping.</p> <p>In case of duplicated loci, the filtering retains the loci wth the highest credibleSetlog10BF.</p> <p>Parameters:</p> Name Type Description Default <code>cred_sets</code> <code>StudyLocus</code> <p>StudyLocus object with credible sets to filter/clump</p> required <code>p_value_threshold</code> <code>float</code> <p>p-value threshold for filtering credible sets, default is 1e-5</p> <code>1e-05</code> <code>purity_min_r2</code> <code>float</code> <p>min-R2 purity threshold for filtering credible sets, default is 0.01</p> <code>0.01</code> <code>clump</code> <code>bool</code> <p>Whether to clump the credible sets by LD, default is False</p> <code>False</code> <code>ld_index</code> <code>LDIndex | None</code> <p>LDIndex object</p> <code>None</code> <code>study_index</code> <code>StudyIndex | None</code> <p>StudyIndex object</p> <code>None</code> <code>ld_min_r2</code> <code>float | None</code> <p>LD R2 threshold for clumping, default is 0.8</p> <code>0.8</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Credible sets which pass filters and LD clumping.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>When running in clump mode, but no study study_index or ld_index or ld_min_r2 were provided.</p> Source code in <code>src/gentropy/method/susie_inf.py</code> <pre><code>@staticmethod\ndef credible_set_qc(\n    cred_sets: StudyLocus,\n    p_value_threshold: float = 1e-5,\n    purity_min_r2: float = 0.01,\n    clump: bool = False,\n    ld_index: LDIndex | None = None,\n    study_index: StudyIndex | None = None,\n    ld_min_r2: float | None = 0.8,\n) -&gt; StudyLocus:\n    \"\"\"Filter credible sets by lead P-value and min-R2 purity, and performs LD clumping.\n\n    In case of duplicated loci, the filtering retains the loci wth the highest credibleSetlog10BF.\n\n\n    Args:\n        cred_sets (StudyLocus): StudyLocus object with credible sets to filter/clump\n        p_value_threshold (float): p-value threshold for filtering credible sets, default is 1e-5\n        purity_min_r2 (float): min-R2 purity threshold for filtering credible sets, default is 0.01\n        clump (bool): Whether to clump the credible sets by LD, default is False\n        ld_index (LDIndex | None): LDIndex object\n        study_index (StudyIndex | None): StudyIndex object\n        ld_min_r2 (float | None): LD R2 threshold for clumping, default is 0.8\n\n    Returns:\n        StudyLocus: Credible sets which pass filters and LD clumping.\n\n    Raises:\n        AssertionError: When running in clump mode, but no study study_index or ld_index or ld_min_r2 were provided.\n    \"\"\"\n    cred_sets.df = (\n        cred_sets.df.withColumn(\n            \"pValue\", f.col(\"pValueMantissa\") * f.pow(10, f.col(\"pValueExponent\"))\n        )\n        .filter(f.col(\"pValue\") &lt;= p_value_threshold)\n        .filter(f.col(\"purityMinR2\") &gt;= purity_min_r2)\n        .drop(\"pValue\")\n        .withColumn(\n            \"rn\",\n            f.row_number().over(\n                Window.partitionBy(\"studyLocusId\").orderBy(\n                    f.desc(\"credibleSetLog10BF\")\n                )\n            ),\n        )\n        .filter(f.col(\"rn\") == 1)\n        .drop(\"rn\")\n    )\n    if clump:\n        assert study_index, \"Running in clump mode, which requires study_index.\"\n        assert ld_index, \"Running in clump mode, which requires ld_index.\"\n        assert ld_min_r2, \"Running in clump mode, which requires ld_min_r2 value.\"\n        cred_sets = (\n            cred_sets.annotate_ld(study_index, ld_index, ld_min_r2)\n            .clump()\n            .filter(\n                ~f.array_contains(\n                    f.col(\"qualityControls\"),\n                    StudyLocusQualityCheck.LD_CLUMPED.value,\n                )\n            )\n        )\n\n    return cred_sets\n</code></pre>"},{"location":"python_api/methods/susie_inf/#gentropy.method.susie_inf.SUSIE_inf.susie_inf","title":"<code>susie_inf(z: np.ndarray, meansq: float = 1, n: int = 100000, L: int = 10, LD: np.ndarray | None = None, V: np.ndarray | None = None, Dsq: np.ndarray | None = None, est_ssq: bool = True, ssq: np.ndarray | None = None, ssq_range: tuple[float, float] = (0, 1), pi0: np.ndarray | None = None, est_sigmasq: bool = True, est_tausq: bool = False, sigmasq: float = 1, tausq: float = 0, sigmasq_range: tuple[float, float] | None = None, tausq_range: tuple[float, float] | None = None, PIP: np.ndarray | None = None, mu: np.ndarray | None = None, method: str = 'moments', maxiter: int = 100, PIP_tol: float = 0.001) -&gt; dict[str, Any]</code>  <code>staticmethod</code>","text":"<p>Susie with random effects.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>ndarray</code> <p>vector of z-scores (equal to X'y/sqrt(n))</p> required <code>meansq</code> <code>float</code> <p>average squared magnitude of y (equal to ||y||^2/n)</p> <code>1</code> <code>n</code> <code>int</code> <p>sample size</p> <code>100000</code> <code>L</code> <code>int</code> <p>number of modeled causal effects</p> <code>10</code> <code>LD</code> <code>ndarray | None</code> <p>LD matrix (equal to X'X/n)</p> <code>None</code> <code>V</code> <code>ndarray | None</code> <p>precomputed p x p matrix of eigenvectors of X'X</p> <code>None</code> <code>Dsq</code> <code>ndarray | None</code> <p>precomputed length-p vector of eigenvalues of X'X</p> <code>None</code> <code>est_ssq</code> <code>bool</code> <p>estimate prior effect size variances s^2 using MLE</p> <code>True</code> <code>ssq</code> <code>ndarray | None</code> <p>length-L initialization s^2 for each effect</p> <code>None</code> <code>ssq_range</code> <code>tuple[float, float]</code> <p>lower and upper bounds for each s^2, if estimated</p> <code>(0, 1)</code> <code>pi0</code> <code>ndarray | None</code> <p>length-p vector of prior causal probability for each SNP; must sum to 1</p> <code>None</code> <code>est_sigmasq</code> <code>bool</code> <p>estimate variance sigma^2</p> <code>True</code> <code>est_tausq</code> <code>bool</code> <p>estimate both variances sigma^2 and tau^2</p> <code>False</code> <code>sigmasq</code> <code>float</code> <p>initial value for sigma^2</p> <code>1</code> <code>tausq</code> <code>float</code> <p>initial value for tau^2</p> <code>0</code> <code>sigmasq_range</code> <code>tuple[float, float] | None</code> <p>lower and upper bounds for sigma^2, if estimated using MLE</p> <code>None</code> <code>tausq_range</code> <code>tuple[float, float] | None</code> <p>lower and upper bounds for tau^2, if estimated using MLE</p> <code>None</code> <code>PIP</code> <code>ndarray | None</code> <p>p x L initializations of PIPs</p> <code>None</code> <code>mu</code> <code>ndarray | None</code> <p>p x L initializations of mu</p> <code>None</code> <code>method</code> <code>str</code> <p>one of {'moments','MLE'}</p> <code>'moments'</code> <code>maxiter</code> <code>int</code> <p>maximum number of SuSiE iterations</p> <code>100</code> <code>PIP_tol</code> <code>float</code> <p>convergence threshold for PIP difference between iterations</p> <code>0.001</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary with keys: PIP -- p x L matrix of PIPs, individually for each effect mu -- p x L matrix of posterior means conditional on causal omega -- p x L matrix of posterior precisions conditional on causal lbf_variable -- p x L matrix of log-Bayes-factors, for each effect ssq -- length-L array of final effect size variances s^2 sigmasq -- final value of sigma^2 tausq -- final value of tau^2 alpha -- length-p array of posterior means of infinitesimal effects lbf -- length-p array of log-Bayes-factors for each CS</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if missing LD or if unsupported variance estimation method</p> Source code in <code>src/gentropy/method/susie_inf.py</code> <pre><code>@staticmethod\ndef susie_inf(  # noqa: C901\n    z: np.ndarray,\n    meansq: float = 1,\n    n: int = 100000,\n    L: int = 10,\n    LD: np.ndarray | None = None,\n    V: np.ndarray | None = None,\n    Dsq: np.ndarray | None = None,\n    est_ssq: bool = True,\n    ssq: np.ndarray | None = None,\n    ssq_range: tuple[float, float] = (0, 1),\n    pi0: np.ndarray | None = None,\n    est_sigmasq: bool = True,\n    est_tausq: bool = False,\n    sigmasq: float = 1,\n    tausq: float = 0,\n    sigmasq_range: tuple[float, float] | None = None,\n    tausq_range: tuple[float, float] | None = None,\n    PIP: np.ndarray | None = None,\n    mu: np.ndarray | None = None,\n    method: str = \"moments\",\n    maxiter: int = 100,\n    PIP_tol: float = 0.001,\n) -&gt; dict[str, Any]:\n    \"\"\"Susie with random effects.\n\n    Args:\n        z (np.ndarray): vector of z-scores (equal to X'y/sqrt(n))\n        meansq (float): average squared magnitude of y (equal to ||y||^2/n)\n        n (int): sample size\n        L (int): number of modeled causal effects\n        LD (np.ndarray | None): LD matrix (equal to X'X/n)\n        V (np.ndarray | None): precomputed p x p matrix of eigenvectors of X'X\n        Dsq (np.ndarray | None): precomputed length-p vector of eigenvalues of X'X\n        est_ssq (bool): estimate prior effect size variances s^2 using MLE\n        ssq (np.ndarray | None): length-L initialization s^2 for each effect\n        ssq_range (tuple[float, float]): lower and upper bounds for each s^2, if estimated\n        pi0 (np.ndarray | None): length-p vector of prior causal probability for each SNP; must sum to 1\n        est_sigmasq (bool): estimate variance sigma^2\n        est_tausq (bool): estimate both variances sigma^2 and tau^2\n        sigmasq (float): initial value for sigma^2\n        tausq (float): initial value for tau^2\n        sigmasq_range (tuple[float, float] | None): lower and upper bounds for sigma^2, if estimated using MLE\n        tausq_range (tuple[float, float] | None): lower and upper bounds for tau^2, if estimated using MLE\n        PIP (np.ndarray | None): p x L initializations of PIPs\n        mu (np.ndarray | None): p x L initializations of mu\n        method (str): one of {'moments','MLE'}\n        maxiter (int): maximum number of SuSiE iterations\n        PIP_tol (float): convergence threshold for PIP difference between iterations\n\n    Returns:\n        dict[str, Any]: Dictionary with keys:\n            PIP -- p x L matrix of PIPs, individually for each effect\n            mu -- p x L matrix of posterior means conditional on causal\n            omega -- p x L matrix of posterior precisions conditional on causal\n            lbf_variable -- p x L matrix of log-Bayes-factors, for each effect\n            ssq -- length-L array of final effect size variances s^2\n            sigmasq -- final value of sigma^2\n            tausq -- final value of tau^2\n            alpha -- length-p array of posterior means of infinitesimal effects\n            lbf -- length-p array of log-Bayes-factors for each CS\n\n    Raises:\n        RuntimeError: if missing LD or if unsupported variance estimation method\n    \"\"\"\n    p = len(z)\n    # Precompute V,D^2 in the SVD X=UDV', and V'X'y and y'y\n    if (V is None or Dsq is None) and LD is None:\n        raise RuntimeError(\"Missing LD\")\n    elif V is None or Dsq is None:\n        eigvals, V = scipy.linalg.eigh(LD)\n        Dsq = np.maximum(n * eigvals, 0)\n    else:\n        Dsq = np.maximum(Dsq, 0)\n    Xty = np.sqrt(n) * z\n    VtXty = V.T.dot(Xty)\n    yty = n * meansq\n    # Initialize diagonal variances, diag(X' Omega X), X' Omega y\n    var = tausq * Dsq + sigmasq\n    diagXtOmegaX = np.sum(V**2 * (Dsq / var), axis=1)\n    XtOmegay = V.dot(VtXty / var)\n    # Initialize s_l^2, PIP_j, mu_j, omega_j\n    if ssq is None:\n        ssq = np.ones(L) * 0.2\n    if PIP is None:\n        PIP = np.ones((p, L)) / p\n    if mu is None:\n        mu = np.zeros((p, L))\n    lbf_variable = np.zeros((p, L))\n    omega = diagXtOmegaX[:, np.newaxis] + 1 / ssq\n    # Initialize prior causal probabilities\n    if pi0 is None:\n        logpi0 = np.ones(p) * np.log(1.0 / p)\n    else:\n        logpi0 = -np.ones(p) * np.inf\n        inds = np.nonzero(pi0 &gt; 0)[0]\n        logpi0[inds] = np.log(pi0[inds])\n\n    ####### Main SuSiE iteration loop ######\n    def f(x: float) -&gt; float:\n        \"\"\"Negative ELBO as function of x = sigma_e^2.\n\n        Args:\n            x (float): sigma_e^2\n\n        Returns:\n            float: negative ELBO as function of x = sigma_e^2\n        \"\"\"\n        return -scipy.special.logsumexp(\n            -0.5 * np.log(1 + x * diagXtOmegaX)\n            + x * XtOmegar**2 / (2 * (1 + x * diagXtOmegaX))\n            + logpi0\n        )\n\n    for it in range(maxiter):\n        PIP_prev = PIP.copy()\n        # Single effect regression for each effect l = 1,...,L\n        for _l in range(L):\n            # Compute X' Omega r_l for residual r_l\n            b = np.sum(mu * PIP, axis=1) - mu[:, _l] * PIP[:, _l]\n            XtOmegaXb = V.dot(V.T.dot(b) * Dsq / var)\n            XtOmegar = XtOmegay - XtOmegaXb\n            if est_ssq:\n                # Update prior variance ssq[l]\n                res = minimize_scalar(f, bounds=ssq_range, method=\"bounded\")\n                if res.success:\n                    ssq[_l] = res.x\n            # Update omega, mu, and PIP\n            omega[:, _l] = diagXtOmegaX + 1 / ssq[_l]\n            mu[:, _l] = XtOmegar / omega[:, _l]\n            lbf_variable[:, _l] = XtOmegar**2 / (2 * omega[:, _l]) - 0.5 * np.log(\n                omega[:, _l] * ssq[_l]\n            )\n            logPIP = lbf_variable[:, _l] + logpi0\n            PIP[:, _l] = np.exp(logPIP - scipy.special.logsumexp(logPIP))\n        # Update variance components\n        if est_sigmasq or est_tausq:\n            if method == \"moments\":\n                (sigmasq, tausq) = SUSIE_inf._MoM(\n                    PIP,\n                    mu,\n                    omega,\n                    sigmasq,\n                    tausq,\n                    n,\n                    V,\n                    Dsq,\n                    VtXty,\n                    Xty,\n                    yty,\n                    est_sigmasq,\n                    est_tausq,\n                )\n            elif method == \"MLE\":\n                (sigmasq, tausq) = SUSIE_inf._MLE(\n                    PIP,\n                    mu,\n                    omega,\n                    sigmasq,\n                    tausq,\n                    n,\n                    V,\n                    Dsq,\n                    VtXty,\n                    yty,\n                    est_sigmasq,\n                    est_tausq,\n                    it,\n                    sigmasq_range,\n                    tausq_range,\n                )\n            else:\n                raise RuntimeError(\"Unsupported variance estimation method\")\n            # Update X' Omega X, X' Omega y\n            var = tausq * Dsq + sigmasq\n            diagXtOmegaX = np.sum(V**2 * (Dsq / var), axis=1)\n            XtOmegay = V.dot(VtXty / var)\n        # Determine convergence from PIP differences\n        PIP_diff = np.max(np.abs(PIP_prev - PIP))\n        if PIP_diff &lt; PIP_tol:\n            break\n    # Compute posterior means of b and alpha\n    b = np.sum(mu * PIP, axis=1)\n    XtOmegaXb = V.dot(V.T.dot(b) * Dsq / var)\n    XtOmegar = XtOmegay - XtOmegaXb\n    alpha = tausq * XtOmegar\n\n    priors = np.log(np.repeat(1 / p, p))\n    lbf_cs = np.apply_along_axis(\n        lambda x: logsumexp(x + priors), axis=0, arr=lbf_variable\n    )\n    return {\n        \"PIP\": PIP,\n        \"mu\": mu,\n        \"omega\": omega,\n        \"lbf_variable\": lbf_variable,\n        \"ssq\": ssq,\n        \"sigmasq\": sigmasq,\n        \"tausq\": tausq,\n        \"alpha\": alpha,\n        \"lbf\": lbf_cs,\n    }\n</code></pre>"},{"location":"python_api/methods/l2g/_l2g/","title":"Locus to Gene (L2G) model","text":"<p>The \u201clocus-to-gene\u201d (L2G) model derives features to prioritize likely causal genes at each GWAS locus based on genetic and functional genomics features. The main categories of predictive features are:</p> <ul> <li>Distance: (from credible set variants to gene)</li> <li>Molecular QTL Colocalization</li> <li>Chromatin Interaction: (e.g., promoter-capture Hi-C)</li> <li>Variant Pathogenicity: (from VEP)</li> </ul> <p>Some of the predictive features weight variant-to-gene (or genomic region-to-gene) evidence based on the posterior probability that the variant is causal, determined through fine-mapping of the GWAS association.</p> <p>For a more detailed description of how each feature is computed, see the L2G Feature documentation.</p> <p>Details of the L2G model are provided in our Nature Genetics publication (ref - Nature Genetics Publication):</p> <ul> <li>Title: An open approach to systematically prioritize causal variants and genes at all published human GWAS trait-associated loci.</li> <li>Authors: Mountjoy, E., Schmidt, E.M., Carmona, M. et al.</li> <li>Journal: Nat Genet 53, 1527\u20131533 (2021).</li> <li>DOI: 10.1038/s41588-021-00945-5</li> </ul>"},{"location":"python_api/methods/l2g/feature_factory/","title":"L2G Feature Factory","text":""},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.FeatureFactory","title":"<code>gentropy.method.l2g.feature_factory.FeatureFactory</code>","text":"<p>Factory class for creating features.</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>class FeatureFactory:\n    \"\"\"Factory class for creating features.\"\"\"\n\n    feature_mapper: Mapping[str, type[L2GFeature]] = {\n        \"distanceSentinelTss\": DistanceSentinelTssFeature,\n        \"distanceSentinelTssNeighbourhood\": DistanceSentinelTssNeighbourhoodFeature,\n        \"distanceSentinelFootprint\": DistanceSentinelFootprintFeature,\n        \"distanceSentinelFootprintNeighbourhood\": DistanceSentinelFootprintNeighbourhoodFeature,\n        \"distanceTssMean\": DistanceTssMeanFeature,\n        \"distanceTssMeanNeighbourhood\": DistanceTssMeanNeighbourhoodFeature,\n        \"distanceFootprintMean\": DistanceFootprintMeanFeature,\n        \"distanceFootprintMeanNeighbourhood\": DistanceFootprintMeanNeighbourhoodFeature,\n        \"eQtlColocClppMaximum\": EQtlColocClppMaximumFeature,\n        \"eQtlColocClppMaximumNeighbourhood\": EQtlColocClppMaximumNeighbourhoodFeature,\n        \"pQtlColocClppMaximum\": PQtlColocClppMaximumFeature,\n        \"pQtlColocClppMaximumNeighbourhood\": PQtlColocClppMaximumNeighbourhoodFeature,\n        \"sQtlColocClppMaximum\": SQtlColocClppMaximumFeature,\n        \"sQtlColocClppMaximumNeighbourhood\": SQtlColocClppMaximumNeighbourhoodFeature,\n        \"eQtlColocH4Maximum\": EQtlColocH4MaximumFeature,\n        \"eQtlColocH4MaximumNeighbourhood\": EQtlColocH4MaximumNeighbourhoodFeature,\n        \"pQtlColocH4Maximum\": PQtlColocH4MaximumFeature,\n        \"pQtlColocH4MaximumNeighbourhood\": PQtlColocH4MaximumNeighbourhoodFeature,\n        \"sQtlColocH4Maximum\": SQtlColocH4MaximumFeature,\n        \"sQtlColocH4MaximumNeighbourhood\": SQtlColocH4MaximumNeighbourhoodFeature,\n        \"vepMean\": VepMeanFeature,\n        \"vepMeanNeighbourhood\": VepMeanNeighbourhoodFeature,\n        \"vepMaximum\": VepMaximumFeature,\n        \"vepMaximumNeighbourhood\": VepMaximumNeighbourhoodFeature,\n        \"geneCount500kb\": GeneCountFeature,\n        \"proteinGeneCount500kb\": ProteinGeneCountFeature,\n        \"isProteinCoding\": ProteinCodingFeature,\n        \"credibleSetConfidence\": CredibleSetConfidenceFeature,\n    }\n\n    def __init__(\n        self: FeatureFactory,\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        features_list: list[str],\n    ) -&gt; None:\n        \"\"\"Initializes the factory.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            features_list (list[str]): list of features to compute.\n        \"\"\"\n        self.study_loci_to_annotate = study_loci_to_annotate\n        self.features_list = features_list\n\n    def generate_features(\n        self: FeatureFactory,\n        features_input_loader: L2GFeatureInputLoader,\n    ) -&gt; list[L2GFeature]:\n        \"\"\"Generates a feature matrix by reading an object with instructions on how to create the features.\n\n        Args:\n            features_input_loader (L2GFeatureInputLoader): object with required features dependencies.\n\n        Returns:\n            list[L2GFeature]: list of computed features.\n\n        Raises:\n            ValueError: If feature not found.\n        \"\"\"\n        computed_features = []\n        for feature in self.features_list:\n            if feature in self.feature_mapper:\n                computed_features.append(\n                    self.compute_feature(feature, features_input_loader)\n                )\n            else:\n                raise ValueError(f\"Feature {feature} not found.\")\n        return computed_features\n\n    def compute_feature(\n        self: FeatureFactory,\n        feature_name: str,\n        features_input_loader: L2GFeatureInputLoader,\n    ) -&gt; L2GFeature:\n        \"\"\"Instantiates feature class.\n\n        Args:\n            feature_name (str): name of the feature\n            features_input_loader (L2GFeatureInputLoader): Object that contais features input.\n\n        Returns:\n            L2GFeature: instantiated feature object\n        \"\"\"\n        # Extract feature class and dependency type\n        feature_cls = self.feature_mapper[feature_name]\n        feature_dependency_type = feature_cls.feature_dependency_type\n        return feature_cls.compute(\n            study_loci_to_annotate=self.study_loci_to_annotate,\n            feature_dependency=features_input_loader.get_dependency_by_type(\n                feature_dependency_type\n            ),\n        )\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.FeatureFactory.__init__","title":"<code>__init__(study_loci_to_annotate: StudyLocus | L2GGoldStandard, features_list: list[str]) -&gt; None</code>","text":"<p>Initializes the factory.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>features_list</code> <code>list[str]</code> <p>list of features to compute.</p> required Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def __init__(\n    self: FeatureFactory,\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    features_list: list[str],\n) -&gt; None:\n    \"\"\"Initializes the factory.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        features_list (list[str]): list of features to compute.\n    \"\"\"\n    self.study_loci_to_annotate = study_loci_to_annotate\n    self.features_list = features_list\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.FeatureFactory.compute_feature","title":"<code>compute_feature(feature_name: str, features_input_loader: L2GFeatureInputLoader) -&gt; L2GFeature</code>","text":"<p>Instantiates feature class.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>name of the feature</p> required <code>features_input_loader</code> <code>L2GFeatureInputLoader</code> <p>Object that contais features input.</p> required <p>Returns:</p> Name Type Description <code>L2GFeature</code> <code>L2GFeature</code> <p>instantiated feature object</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def compute_feature(\n    self: FeatureFactory,\n    feature_name: str,\n    features_input_loader: L2GFeatureInputLoader,\n) -&gt; L2GFeature:\n    \"\"\"Instantiates feature class.\n\n    Args:\n        feature_name (str): name of the feature\n        features_input_loader (L2GFeatureInputLoader): Object that contais features input.\n\n    Returns:\n        L2GFeature: instantiated feature object\n    \"\"\"\n    # Extract feature class and dependency type\n    feature_cls = self.feature_mapper[feature_name]\n    feature_dependency_type = feature_cls.feature_dependency_type\n    return feature_cls.compute(\n        study_loci_to_annotate=self.study_loci_to_annotate,\n        feature_dependency=features_input_loader.get_dependency_by_type(\n            feature_dependency_type\n        ),\n    )\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.FeatureFactory.generate_features","title":"<code>generate_features(features_input_loader: L2GFeatureInputLoader) -&gt; list[L2GFeature]</code>","text":"<p>Generates a feature matrix by reading an object with instructions on how to create the features.</p> <p>Parameters:</p> Name Type Description Default <code>features_input_loader</code> <code>L2GFeatureInputLoader</code> <p>object with required features dependencies.</p> required <p>Returns:</p> Type Description <code>list[L2GFeature]</code> <p>list[L2GFeature]: list of computed features.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If feature not found.</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def generate_features(\n    self: FeatureFactory,\n    features_input_loader: L2GFeatureInputLoader,\n) -&gt; list[L2GFeature]:\n    \"\"\"Generates a feature matrix by reading an object with instructions on how to create the features.\n\n    Args:\n        features_input_loader (L2GFeatureInputLoader): object with required features dependencies.\n\n    Returns:\n        list[L2GFeature]: list of computed features.\n\n    Raises:\n        ValueError: If feature not found.\n    \"\"\"\n    computed_features = []\n    for feature in self.features_list:\n        if feature in self.feature_mapper:\n            computed_features.append(\n                self.compute_feature(feature, features_input_loader)\n            )\n        else:\n            raise ValueError(f\"Feature {feature} not found.\")\n    return computed_features\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.L2GFeatureInputLoader","title":"<code>gentropy.method.l2g.feature_factory.L2GFeatureInputLoader</code>","text":"<p>Loads all input datasets required for the L2GFeature dataset.</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>class L2GFeatureInputLoader:\n    \"\"\"Loads all input datasets required for the L2GFeature dataset.\"\"\"\n\n    def __init__(\n        self,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initializes L2GFeatureInputLoader with the provided inputs and returns loaded dependencies as a dictionary.\n\n        Args:\n            **kwargs (Any): keyword arguments with the name of the dependency and the dependency itself.\n        \"\"\"\n        self.input_dependencies = {k: v for k, v in kwargs.items() if v is not None}\n\n    def get_dependency_by_type(\n        self, dependency_type: list[Any] | Any\n    ) -&gt; dict[str, Any]:\n        \"\"\"Returns the dependency that matches the provided type.\n\n        Args:\n            dependency_type (list[Any] | Any): type(s) of the dependency to return.\n\n        Returns:\n            dict[str, Any]: dictionary of dependenci(es) that match the provided type(s).\n        \"\"\"\n        if not isinstance(dependency_type, list):\n            dependency_type = [dependency_type]\n        return {\n            k: v\n            for k, v in self.input_dependencies.items()\n            if isinstance(v, tuple(dependency_type))\n        }\n\n    def __iter__(self) -&gt; Iterator[tuple[str, Any]]:\n        \"\"\"Make the class iterable, returning an iterator over key-value pairs.\n\n        Returns:\n            Iterator[tuple[str, Any]]: iterator over the dictionary's key-value pairs.\n        \"\"\"\n        return iter(self.input_dependencies.items())\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the input dependencies.\n\n        Useful for understanding the loader content without having to print the object attribute.\n\n        Returns:\n            str: string representation of the input dependencies.\n        \"\"\"\n        return repr(self.input_dependencies)\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.L2GFeatureInputLoader.__init__","title":"<code>__init__(**kwargs: Any) -&gt; None</code>","text":"<p>Initializes L2GFeatureInputLoader with the provided inputs and returns loaded dependencies as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments with the name of the dependency and the dependency itself.</p> <code>{}</code> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def __init__(\n    self,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initializes L2GFeatureInputLoader with the provided inputs and returns loaded dependencies as a dictionary.\n\n    Args:\n        **kwargs (Any): keyword arguments with the name of the dependency and the dependency itself.\n    \"\"\"\n    self.input_dependencies = {k: v for k, v in kwargs.items() if v is not None}\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.L2GFeatureInputLoader.get_dependency_by_type","title":"<code>get_dependency_by_type(dependency_type: list[Any] | Any) -&gt; dict[str, Any]</code>","text":"<p>Returns the dependency that matches the provided type.</p> <p>Parameters:</p> Name Type Description Default <code>dependency_type</code> <code>list[Any] | Any</code> <p>type(s) of the dependency to return.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: dictionary of dependenci(es) that match the provided type(s).</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def get_dependency_by_type(\n    self, dependency_type: list[Any] | Any\n) -&gt; dict[str, Any]:\n    \"\"\"Returns the dependency that matches the provided type.\n\n    Args:\n        dependency_type (list[Any] | Any): type(s) of the dependency to return.\n\n    Returns:\n        dict[str, Any]: dictionary of dependenci(es) that match the provided type(s).\n    \"\"\"\n    if not isinstance(dependency_type, list):\n        dependency_type = [dependency_type]\n    return {\n        k: v\n        for k, v in self.input_dependencies.items()\n        if isinstance(v, tuple(dependency_type))\n    }\n</code></pre>"},{"location":"python_api/methods/l2g/model/","title":"L2G Model","text":""},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel","title":"<code>gentropy.method.l2g.model.LocusToGeneModel</code>  <code>dataclass</code>","text":"<p>Wrapper for the Locus to Gene classifier.</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>@dataclass\nclass LocusToGeneModel:\n    \"\"\"Wrapper for the Locus to Gene classifier.\"\"\"\n\n    model: Any = GradientBoostingClassifier(random_state=42)\n    features_list: list[str] = field(default_factory=list)\n    hyperparameters: dict[str, Any] = field(\n        default_factory=lambda: {\n            \"n_estimators\": 100,\n            \"max_depth\": 10,\n            \"ccp_alpha\": 0,\n            \"learning_rate\": 0.1,\n            \"min_samples_leaf\": 5,\n            \"min_samples_split\": 5,\n            \"subsample\": 1,\n        }\n    )\n    training_data: L2GFeatureMatrix | None = None\n    label_encoder: dict[str, int] = field(\n        default_factory=lambda: {\n            \"negative\": 0,\n            \"positive\": 1,\n        }\n    )\n\n    def __post_init__(self: LocusToGeneModel) -&gt; None:\n        \"\"\"Post-initialisation to fit the estimator with the provided params.\"\"\"\n        self.model.set_params(**self.hyperparameters_dict)\n\n    @classmethod\n    def load_from_disk(\n        cls: type[LocusToGeneModel],\n        session: Session,\n        path: str,\n        model_name: str = \"classifier.skops\",\n        **kwargs: Any,\n    ) -&gt; LocusToGeneModel:\n        \"\"\"Load a fitted model from disk.\n\n        Args:\n            session (Session): Session object that loads the training data\n            path (str): Path to the directory containing model and metadata\n            model_name (str): Name of the persisted model to load. Defaults to \"classifier.skops\".\n            **kwargs(Any): Keyword arguments to pass to the constructor\n\n        Returns:\n            LocusToGeneModel: L2G model loaded from disk\n\n        Raises:\n            ValueError: If the model has not been fitted yet\n        \"\"\"\n        model_path = (Path(path) / model_name).as_posix()\n        if model_path.startswith(\"gs://\"):\n            path = model_path.removeprefix(\"gs://\")\n            bucket_name = path.split(\"/\")[0]\n            blob_name = \"/\".join(path.split(\"/\")[1:])\n            from google.cloud import storage\n\n            client = storage.Client()\n            bucket = storage.Bucket(client=client, name=bucket_name)\n            blob = storage.Blob(name=blob_name, bucket=bucket)\n            data = blob.download_as_string(client=client)\n            loaded_model = sio.loads(data, trusted=sio.get_untrusted_types(data=data))\n        else:\n            loaded_model = sio.load(\n                model_path, trusted=sio.get_untrusted_types(file=model_path)\n            )\n            try:\n                # Try loading the training data if it is in the model directory\n                training_data = L2GFeatureMatrix(\n                    _df=session.spark.createDataFrame(\n                        # Parquet is read with Pandas to easily read local files\n                        pd.read_parquet(\n                            (Path(path) / \"training_data.parquet\").as_posix()\n                        )\n                    ),\n                    features_list=kwargs.get(\"features_list\"),\n                )\n            except Exception as e:\n                logging.error(\"Training data set to none. Error: %s\", e)\n                training_data = None\n\n        if not loaded_model._is_fitted():\n            raise ValueError(\"Model has not been fitted yet.\")\n        return cls(model=loaded_model, training_data=training_data, **kwargs)\n\n    @classmethod\n    def load_from_hub(\n        cls: type[LocusToGeneModel],\n        session: Session,\n        hf_model_id: str,\n        hf_model_version: str | None = None,\n        hf_token: str | None = None,\n    ) -&gt; LocusToGeneModel:\n        \"\"\"Load a model from the Hugging Face Hub. This will download the model from the hub and load it from disk.\n\n        Args:\n            session (Session): Session object to load the training data\n            hf_model_id (str): Model ID on the Hugging Face Hub\n            hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n            hf_token (str | None): Hugging Face Hub token to download the model (only required if private)\n\n        Returns:\n            LocusToGeneModel: L2G model loaded from the Hugging Face Hub\n        \"\"\"\n\n        def get_features_list_from_metadata() -&gt; list[str]:\n            \"\"\"Get the features list (in the right order) from the metadata JSON file downloaded from the Hub.\n\n            Returns:\n                list[str]: Features list\n            \"\"\"\n            import json\n\n            model_config_path = str(Path(local_path) / \"config.json\")\n            with open(model_config_path) as f:\n                model_config = json.load(f)\n            return [\n                column\n                for column in model_config[\"sklearn\"][\"columns\"]\n                if column\n                not in [\n                    \"studyLocusId\",\n                    \"geneId\",\n                    \"traitFromSourceMappedId\",\n                    \"goldStandardSet\",\n                ]\n            ]\n\n        local_path = hf_model_id\n        hub_utils.download(\n            repo_id=hf_model_id,\n            dst=local_path,\n            token=hf_token,\n            revision=hf_model_version,\n        )\n        features_list = get_features_list_from_metadata()\n        return cls.load_from_disk(\n            session,\n            local_path,\n            features_list=features_list,\n        )\n\n    @property\n    def hyperparameters_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return hyperparameters as a dictionary.\n\n        Returns:\n            dict[str, Any]: Hyperparameters\n\n        Raises:\n            ValueError: If hyperparameters have not been set\n        \"\"\"\n        if not self.hyperparameters:\n            raise ValueError(\"Hyperparameters have not been set.\")\n        elif isinstance(self.hyperparameters, dict):\n            return self.hyperparameters\n        return self.hyperparameters.default_factory()\n\n    def predict(\n        self: LocusToGeneModel,\n        feature_matrix: L2GFeatureMatrix,\n        session: Session,\n    ) -&gt; L2GPrediction:\n        \"\"\"Apply the model to a given feature matrix dataframe. The feature matrix needs to be preprocessed first.\n\n        Args:\n            feature_matrix (L2GFeatureMatrix): Feature matrix to apply the model to.\n            session (Session): Session object to convert data to Spark\n\n        Returns:\n            L2GPrediction: Dataset containing credible sets and their L2G scores\n        \"\"\"\n        from gentropy.dataset.l2g_prediction import L2GPrediction\n\n        pd_dataframe.iteritems = pd_dataframe.items\n\n        feature_matrix_pdf = feature_matrix._df.toPandas()\n        # L2G score is the probability the classifier assigns to the positive class (the second element in the probability array)\n        feature_matrix_pdf[\"score\"] = self.model.predict_proba(\n            # We drop the fixed columns to only pass the feature values to the classifier\n            feature_matrix_pdf.drop(feature_matrix.fixed_cols, axis=1)\n            .apply(pd_to_numeric)\n            .values\n        )[:, 1]\n        output_cols = [field.name for field in L2GPrediction.get_schema().fields]\n        return L2GPrediction(\n            _df=session.spark.createDataFrame(feature_matrix_pdf.filter(output_cols)),\n            _schema=L2GPrediction.get_schema(),\n            model=self,\n        )\n\n    def save(self: LocusToGeneModel, path: str) -&gt; None:\n        \"\"\"Saves fitted model to disk using the skops persistence format.\n\n        Args:\n            path (str): Path to save the persisted model. Should end with .skops\n\n        Raises:\n            ValueError: If the model has not been fitted yet or if the path does not end with .skops\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model has not been fitted yet.\")\n        if not path.endswith(\".skops\"):\n            raise ValueError(\"Path should end with .skops\")\n        if path.startswith(\"gs://\"):\n            local_path = path.split(\"/\")[-1]\n            sio.dump(self.model, local_path)\n            copy_to_gcs(local_path, path)\n        else:\n            # create directory if path does not exist\n            Path(path).parent.mkdir(parents=True, exist_ok=True)\n            sio.dump(self.model, path)\n\n    @staticmethod\n    def load_feature_matrix_from_wandb(wandb_run_name: str) -&gt; pd.DataFrame:\n        \"\"\"Loads dataset of feature matrix used during a wandb run.\n\n        Args:\n            wandb_run_name (str): Name of the wandb run to load the feature matrix from\n\n        Returns:\n            pd.DataFrame: Feature matrix used during the wandb run\n        \"\"\"\n        with open(wandb_run_name) as f:\n            raw_data = json.load(f)\n\n        data = raw_data[\"data\"]\n        columns = raw_data[\"columns\"]\n        return pd.DataFrame(data, columns=columns)\n\n    def _create_hugging_face_model_card(\n        self: LocusToGeneModel,\n        local_repo: str,\n    ) -&gt; None:\n        \"\"\"Create a model card to document the model in the hub. The model card is saved in the local repo before pushing it to the hub.\n\n        Args:\n            local_repo (str): Path to the folder where the README file will be saved to be pushed to the Hugging Face Hub\n        \"\"\"\n        from skops import card\n\n        # Define card metadata\n        description = \"\"\"The locus-to-gene (L2G) model derives features to prioritise likely causal genes at each GWAS locus based on genetic and functional genomics features. The main categories of predictive features are:\n\n        - Distance: (from credible set variants to gene)\n        - Molecular QTL Colocalization\n        - Variant Pathogenicity: (from VEP)\n\n        More information at: https://opentargets.github.io/gentropy/python_api/methods/l2g/_l2g/\n        \"\"\"\n        how_to = \"\"\"To use the model, you can load it using the `LocusToGeneModel.load_from_hub` method. This will return a `LocusToGeneModel` object that can be used to make predictions on a feature matrix.\n        The model can then be used to make predictions using the `predict` method.\n\n        More information can be found at: https://opentargets.github.io/gentropy/python_api/methods/l2g/model/\n        \"\"\"\n        model_card = card.Card(\n            self.model,\n            metadata=card.metadata_from_config(Path(local_repo)),\n        )\n        model_card.add(\n            **{\n                \"Model description\": description,\n                \"Model description/Training Procedure\": \"Gradient Boosting Classifier\",\n                \"How to Get Started with the Model\": how_to,\n                \"Model Card Authors\": \"Open Targets\",\n                \"License\": \"MIT\",\n                \"Citation\": \"https://doi.org/10.1038/s41588-021-00945-5\",\n            }\n        )\n        model_card.delete(\"Model description/Training Procedure/Model Plot\")\n        model_card.delete(\"Model description/Evaluation Results\")\n        model_card.delete(\"Model Card Authors\")\n        model_card.delete(\"Model Card Contact\")\n        model_card.save(Path(local_repo) / \"README.md\")\n\n    def export_to_hugging_face_hub(\n        self: LocusToGeneModel,\n        model_path: str,\n        hf_hub_token: str,\n        data: pd_dataframe,\n        commit_message: str,\n        repo_id: str = \"opentargets/locus_to_gene\",\n        local_repo: str = \"locus_to_gene\",\n    ) -&gt; None:\n        \"\"\"Share the model and training dataset on Hugging Face Hub.\n\n        Args:\n            model_path (str): The path to the L2G model file.\n            hf_hub_token (str): Hugging Face Hub token\n            data (pd_dataframe): Data used to train the model. This is used to have an example input for the model and to store the column order.\n            commit_message (str): Commit message for the push\n            repo_id (str): The Hugging Face Hub repo id where the model will be stored.\n            local_repo (str): Path to the folder where the contents of the model repo + the documentation are located. This is used to push the model to the Hugging Face Hub.\n\n        Raises:\n            RuntimeError: If the push to the Hugging Face Hub fails\n        \"\"\"\n        from sklearn import __version__ as sklearn_version\n\n        try:\n            hub_utils.init(\n                model=model_path,\n                requirements=[f\"scikit-learn={sklearn_version}\"],\n                dst=local_repo,\n                task=\"tabular-classification\",\n                data=data,\n            )\n            self._create_hugging_face_model_card(local_repo)\n            data.to_parquet(f\"{local_repo}/training_data.parquet\")\n            hub_utils.push(\n                repo_id=repo_id,\n                source=local_repo,\n                token=hf_hub_token,\n                commit_message=commit_message,\n                create_remote=True,\n            )\n        except Exception as e:\n            # remove the local repo if the push fails\n            if Path(local_repo).exists():\n                for p in Path(local_repo).glob(\"*\"):\n                    p.unlink()\n                Path(local_repo).rmdir()\n            raise RuntimeError from e\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.hyperparameters_dict","title":"<code>hyperparameters_dict: dict[str, Any]</code>  <code>property</code>","text":"<p>Return hyperparameters as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Hyperparameters</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If hyperparameters have not been set</p>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.export_to_hugging_face_hub","title":"<code>export_to_hugging_face_hub(model_path: str, hf_hub_token: str, data: pd_dataframe, commit_message: str, repo_id: str = 'opentargets/locus_to_gene', local_repo: str = 'locus_to_gene') -&gt; None</code>","text":"<p>Share the model and training dataset on Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the L2G model file.</p> required <code>hf_hub_token</code> <code>str</code> <p>Hugging Face Hub token</p> required <code>data</code> <code>DataFrame</code> <p>Data used to train the model. This is used to have an example input for the model and to store the column order.</p> required <code>commit_message</code> <code>str</code> <p>Commit message for the push</p> required <code>repo_id</code> <code>str</code> <p>The Hugging Face Hub repo id where the model will be stored.</p> <code>'opentargets/locus_to_gene'</code> <code>local_repo</code> <code>str</code> <p>Path to the folder where the contents of the model repo + the documentation are located. This is used to push the model to the Hugging Face Hub.</p> <code>'locus_to_gene'</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the push to the Hugging Face Hub fails</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>def export_to_hugging_face_hub(\n    self: LocusToGeneModel,\n    model_path: str,\n    hf_hub_token: str,\n    data: pd_dataframe,\n    commit_message: str,\n    repo_id: str = \"opentargets/locus_to_gene\",\n    local_repo: str = \"locus_to_gene\",\n) -&gt; None:\n    \"\"\"Share the model and training dataset on Hugging Face Hub.\n\n    Args:\n        model_path (str): The path to the L2G model file.\n        hf_hub_token (str): Hugging Face Hub token\n        data (pd_dataframe): Data used to train the model. This is used to have an example input for the model and to store the column order.\n        commit_message (str): Commit message for the push\n        repo_id (str): The Hugging Face Hub repo id where the model will be stored.\n        local_repo (str): Path to the folder where the contents of the model repo + the documentation are located. This is used to push the model to the Hugging Face Hub.\n\n    Raises:\n        RuntimeError: If the push to the Hugging Face Hub fails\n    \"\"\"\n    from sklearn import __version__ as sklearn_version\n\n    try:\n        hub_utils.init(\n            model=model_path,\n            requirements=[f\"scikit-learn={sklearn_version}\"],\n            dst=local_repo,\n            task=\"tabular-classification\",\n            data=data,\n        )\n        self._create_hugging_face_model_card(local_repo)\n        data.to_parquet(f\"{local_repo}/training_data.parquet\")\n        hub_utils.push(\n            repo_id=repo_id,\n            source=local_repo,\n            token=hf_hub_token,\n            commit_message=commit_message,\n            create_remote=True,\n        )\n    except Exception as e:\n        # remove the local repo if the push fails\n        if Path(local_repo).exists():\n            for p in Path(local_repo).glob(\"*\"):\n                p.unlink()\n            Path(local_repo).rmdir()\n        raise RuntimeError from e\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.load_feature_matrix_from_wandb","title":"<code>load_feature_matrix_from_wandb(wandb_run_name: str) -&gt; pd.DataFrame</code>  <code>staticmethod</code>","text":"<p>Loads dataset of feature matrix used during a wandb run.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_run_name</code> <code>str</code> <p>Name of the wandb run to load the feature matrix from</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Feature matrix used during the wandb run</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>@staticmethod\ndef load_feature_matrix_from_wandb(wandb_run_name: str) -&gt; pd.DataFrame:\n    \"\"\"Loads dataset of feature matrix used during a wandb run.\n\n    Args:\n        wandb_run_name (str): Name of the wandb run to load the feature matrix from\n\n    Returns:\n        pd.DataFrame: Feature matrix used during the wandb run\n    \"\"\"\n    with open(wandb_run_name) as f:\n        raw_data = json.load(f)\n\n    data = raw_data[\"data\"]\n    columns = raw_data[\"columns\"]\n    return pd.DataFrame(data, columns=columns)\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.load_from_disk","title":"<code>load_from_disk(session: Session, path: str, model_name: str = 'classifier.skops', **kwargs: Any) -&gt; LocusToGeneModel</code>  <code>classmethod</code>","text":"<p>Load a fitted model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that loads the training data</p> required <code>path</code> <code>str</code> <p>Path to the directory containing model and metadata</p> required <code>model_name</code> <code>str</code> <p>Name of the persisted model to load. Defaults to \"classifier.skops\".</p> <code>'classifier.skops'</code> <code>**kwargs(Any)</code> <p>Keyword arguments to pass to the constructor</p> required <p>Returns:</p> Name Type Description <code>LocusToGeneModel</code> <code>LocusToGeneModel</code> <p>L2G model loaded from disk</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been fitted yet</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>@classmethod\ndef load_from_disk(\n    cls: type[LocusToGeneModel],\n    session: Session,\n    path: str,\n    model_name: str = \"classifier.skops\",\n    **kwargs: Any,\n) -&gt; LocusToGeneModel:\n    \"\"\"Load a fitted model from disk.\n\n    Args:\n        session (Session): Session object that loads the training data\n        path (str): Path to the directory containing model and metadata\n        model_name (str): Name of the persisted model to load. Defaults to \"classifier.skops\".\n        **kwargs(Any): Keyword arguments to pass to the constructor\n\n    Returns:\n        LocusToGeneModel: L2G model loaded from disk\n\n    Raises:\n        ValueError: If the model has not been fitted yet\n    \"\"\"\n    model_path = (Path(path) / model_name).as_posix()\n    if model_path.startswith(\"gs://\"):\n        path = model_path.removeprefix(\"gs://\")\n        bucket_name = path.split(\"/\")[0]\n        blob_name = \"/\".join(path.split(\"/\")[1:])\n        from google.cloud import storage\n\n        client = storage.Client()\n        bucket = storage.Bucket(client=client, name=bucket_name)\n        blob = storage.Blob(name=blob_name, bucket=bucket)\n        data = blob.download_as_string(client=client)\n        loaded_model = sio.loads(data, trusted=sio.get_untrusted_types(data=data))\n    else:\n        loaded_model = sio.load(\n            model_path, trusted=sio.get_untrusted_types(file=model_path)\n        )\n        try:\n            # Try loading the training data if it is in the model directory\n            training_data = L2GFeatureMatrix(\n                _df=session.spark.createDataFrame(\n                    # Parquet is read with Pandas to easily read local files\n                    pd.read_parquet(\n                        (Path(path) / \"training_data.parquet\").as_posix()\n                    )\n                ),\n                features_list=kwargs.get(\"features_list\"),\n            )\n        except Exception as e:\n            logging.error(\"Training data set to none. Error: %s\", e)\n            training_data = None\n\n    if not loaded_model._is_fitted():\n        raise ValueError(\"Model has not been fitted yet.\")\n    return cls(model=loaded_model, training_data=training_data, **kwargs)\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.load_from_hub","title":"<code>load_from_hub(session: Session, hf_model_id: str, hf_model_version: str | None = None, hf_token: str | None = None) -&gt; LocusToGeneModel</code>  <code>classmethod</code>","text":"<p>Load a model from the Hugging Face Hub. This will download the model from the hub and load it from disk.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object to load the training data</p> required <code>hf_model_id</code> <code>str</code> <p>Model ID on the Hugging Face Hub</p> required <code>hf_model_version</code> <code>str | None</code> <p>Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.</p> <code>None</code> <code>hf_token</code> <code>str | None</code> <p>Hugging Face Hub token to download the model (only required if private)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LocusToGeneModel</code> <code>LocusToGeneModel</code> <p>L2G model loaded from the Hugging Face Hub</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>@classmethod\ndef load_from_hub(\n    cls: type[LocusToGeneModel],\n    session: Session,\n    hf_model_id: str,\n    hf_model_version: str | None = None,\n    hf_token: str | None = None,\n) -&gt; LocusToGeneModel:\n    \"\"\"Load a model from the Hugging Face Hub. This will download the model from the hub and load it from disk.\n\n    Args:\n        session (Session): Session object to load the training data\n        hf_model_id (str): Model ID on the Hugging Face Hub\n        hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n        hf_token (str | None): Hugging Face Hub token to download the model (only required if private)\n\n    Returns:\n        LocusToGeneModel: L2G model loaded from the Hugging Face Hub\n    \"\"\"\n\n    def get_features_list_from_metadata() -&gt; list[str]:\n        \"\"\"Get the features list (in the right order) from the metadata JSON file downloaded from the Hub.\n\n        Returns:\n            list[str]: Features list\n        \"\"\"\n        import json\n\n        model_config_path = str(Path(local_path) / \"config.json\")\n        with open(model_config_path) as f:\n            model_config = json.load(f)\n        return [\n            column\n            for column in model_config[\"sklearn\"][\"columns\"]\n            if column\n            not in [\n                \"studyLocusId\",\n                \"geneId\",\n                \"traitFromSourceMappedId\",\n                \"goldStandardSet\",\n            ]\n        ]\n\n    local_path = hf_model_id\n    hub_utils.download(\n        repo_id=hf_model_id,\n        dst=local_path,\n        token=hf_token,\n        revision=hf_model_version,\n    )\n    features_list = get_features_list_from_metadata()\n    return cls.load_from_disk(\n        session,\n        local_path,\n        features_list=features_list,\n    )\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.predict","title":"<code>predict(feature_matrix: L2GFeatureMatrix, session: Session) -&gt; L2GPrediction</code>","text":"<p>Apply the model to a given feature matrix dataframe. The feature matrix needs to be preprocessed first.</p> <p>Parameters:</p> Name Type Description Default <code>feature_matrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix to apply the model to.</p> required <code>session</code> <code>Session</code> <p>Session object to convert data to Spark</p> required <p>Returns:</p> Name Type Description <code>L2GPrediction</code> <code>L2GPrediction</code> <p>Dataset containing credible sets and their L2G scores</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>def predict(\n    self: LocusToGeneModel,\n    feature_matrix: L2GFeatureMatrix,\n    session: Session,\n) -&gt; L2GPrediction:\n    \"\"\"Apply the model to a given feature matrix dataframe. The feature matrix needs to be preprocessed first.\n\n    Args:\n        feature_matrix (L2GFeatureMatrix): Feature matrix to apply the model to.\n        session (Session): Session object to convert data to Spark\n\n    Returns:\n        L2GPrediction: Dataset containing credible sets and their L2G scores\n    \"\"\"\n    from gentropy.dataset.l2g_prediction import L2GPrediction\n\n    pd_dataframe.iteritems = pd_dataframe.items\n\n    feature_matrix_pdf = feature_matrix._df.toPandas()\n    # L2G score is the probability the classifier assigns to the positive class (the second element in the probability array)\n    feature_matrix_pdf[\"score\"] = self.model.predict_proba(\n        # We drop the fixed columns to only pass the feature values to the classifier\n        feature_matrix_pdf.drop(feature_matrix.fixed_cols, axis=1)\n        .apply(pd_to_numeric)\n        .values\n    )[:, 1]\n    output_cols = [field.name for field in L2GPrediction.get_schema().fields]\n    return L2GPrediction(\n        _df=session.spark.createDataFrame(feature_matrix_pdf.filter(output_cols)),\n        _schema=L2GPrediction.get_schema(),\n        model=self,\n    )\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.save","title":"<code>save(path: str) -&gt; None</code>","text":"<p>Saves fitted model to disk using the skops persistence format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the persisted model. Should end with .skops</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been fitted yet or if the path does not end with .skops</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>def save(self: LocusToGeneModel, path: str) -&gt; None:\n    \"\"\"Saves fitted model to disk using the skops persistence format.\n\n    Args:\n        path (str): Path to save the persisted model. Should end with .skops\n\n    Raises:\n        ValueError: If the model has not been fitted yet or if the path does not end with .skops\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n    if not path.endswith(\".skops\"):\n        raise ValueError(\"Path should end with .skops\")\n    if path.startswith(\"gs://\"):\n        local_path = path.split(\"/\")[-1]\n        sio.dump(self.model, local_path)\n        copy_to_gcs(local_path, path)\n    else:\n        # create directory if path does not exist\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n        sio.dump(self.model, path)\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/","title":"L2G Trainer","text":""},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer","title":"<code>gentropy.method.l2g.trainer.LocusToGeneTrainer</code>  <code>dataclass</code>","text":"<p>Modelling of what is the most likely causal gene associated with a given locus.</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>@dataclass\nclass LocusToGeneTrainer:\n    \"\"\"Modelling of what is the most likely causal gene associated with a given locus.\"\"\"\n\n    model: LocusToGeneModel\n    feature_matrix: L2GFeatureMatrix\n\n    # Initialise vars\n    features_list: list[str] | None = None\n    label_col: str = \"goldStandardSet\"\n    x_train: np.ndarray | None = None\n    y_train: np.ndarray | None = None\n    x_test: np.ndarray | None = None\n    y_test: np.ndarray | None = None\n    groups_train: np.ndarray | None = None\n    run: Run | None = None\n    wandb_l2g_project_name: str = \"gentropy-locus-to-gene\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Set default features_list to feature_matrix's features_list if not provided.\"\"\"\n        self.features_list = (\n            self.feature_matrix.features_list\n            if self.features_list is None\n            else self.features_list\n        )\n\n    def fit(\n        self: LocusToGeneTrainer,\n    ) -&gt; LocusToGeneModel:\n        \"\"\"Fit the pipeline to the feature matrix dataframe.\n\n        Returns:\n            LocusToGeneModel: Fitted model\n\n        Raises:\n            ValueError: Train data not set, nothing to fit.\n            AssertionError: When x_train_size or y_train_size are not zero.\n        \"\"\"\n        if (\n            self.x_train is not None\n            and self.y_train is not None\n            and self.features_list is not None\n        ):\n            assert (\n                self.x_train.size != 0 and self.y_train.size != 0\n            ), \"Train data not set, nothing to fit.\"\n            fitted_model = self.model.model.fit(X=self.x_train, y=self.y_train)\n            self.model = LocusToGeneModel(\n                model=fitted_model,\n                hyperparameters=fitted_model.get_params(),\n                training_data=self.feature_matrix,\n                features_list=self.features_list,\n            )\n            return self.model\n        raise ValueError(\"Train data not set, nothing to fit.\")\n\n    def _get_shap_explanation(\n        self: LocusToGeneTrainer,\n        model: LocusToGeneModel,\n    ) -&gt; Explanation:\n        \"\"\"Get the SHAP values for the given model and data. We sample the full X matrix (without the labels) to interpret their shap values.\n\n        Args:\n            model (LocusToGeneModel): Model to explain.\n\n        Returns:\n                Explanation: SHAP values for the given model and data.\n\n        Raises:\n            ValueError: Train data not set, cannot get SHAP values.\n            Exception: (ExplanationError) When the additivity check fails.\n        \"\"\"\n        if self.x_train is not None and self.x_test is not None:\n            training_data = pd.DataFrame(\n                np.vstack((self.x_train, self.x_test)),\n                columns=self.features_list,\n            )\n            explainer = shap.TreeExplainer(\n                model.model,\n                data=training_data,\n                feature_perturbation=\"interventional\",\n                model_output=\"probability\",\n            )\n            try:\n                return explainer(training_data.sample(n=1_000))\n            except Exception as e:\n                if \"Additivity check failed in TreeExplainer\" in repr(e):\n                    return explainer(\n                        training_data.sample(n=1_000), check_additivity=False\n                    )\n                else:\n                    raise\n\n        raise ValueError(\"Train data not set.\")\n\n    def log_plot_image_to_wandb(\n        self: LocusToGeneTrainer, title: str, plot: Axes\n    ) -&gt; None:\n        \"\"\"Accepts a plot object, and saves the fig to PNG to then log it in W&amp;B.\n\n        Args:\n            title (str): Title of the plot.\n            plot (Axes): Shap plot to log.\n\n        Raises:\n            ValueError: Run not set, cannot log to W&amp;B.\n        \"\"\"\n        if self.run is None:\n            raise ValueError(\"Run not set, cannot log to W&amp;B.\")\n        if not plot:\n            # Scatter plot returns none, so we need to handle this case\n            plt.savefig(\"tmp.png\", bbox_inches=\"tight\")\n        else:\n            plot.figure.savefig(\"tmp.png\", bbox_inches=\"tight\")\n        self.run.log({title: Image(\"tmp.png\")})\n        plt.close()\n        os.remove(\"tmp.png\")\n\n    def log_to_wandb(\n        self: LocusToGeneTrainer,\n        wandb_run_name: str,\n    ) -&gt; None:\n        \"\"\"Log evaluation results and feature importance to W&amp;B to compare between different L2G runs.\n\n        Dashboard is available at https://wandb.ai/open-targets/gentropy-locus-to-gene?nw=nwuseropentargets\n        Credentials to access W&amp;B are available at the OT central login sheet.\n\n        Args:\n            wandb_run_name (str): Name of the W&amp;B run\n\n        Raises:\n            RuntimeError: If dependencies are not available.\n            AssertionError: When x_train_size or y_train_size are not zero.\n        \"\"\"\n        if (\n            self.x_train is None\n            or self.x_test is None\n            or self.y_train is None\n            or self.y_test is None\n            or self.features_list is None\n        ):\n            raise RuntimeError(\"Train data not set, we cannot log to W&amp;B.\")\n        assert (\n            self.x_train.size != 0 and self.y_train.size != 0\n        ), \"Train data not set, nothing to evaluate.\"\n        fitted_classifier = self.model.model\n        y_predicted = fitted_classifier.predict(self.x_test)\n        y_probas = fitted_classifier.predict_proba(self.x_test)\n        self.run = wandb_init(\n            project=self.wandb_l2g_project_name,\n            name=wandb_run_name,\n            config=fitted_classifier.get_params(),\n        )\n        # Track classification plots\n        plot_classifier(\n            self.model.model,\n            self.x_train,\n            self.x_test,\n            self.y_train,\n            self.y_test,\n            y_predicted,\n            y_probas,\n            labels=list(self.model.label_encoder.values()),\n            model_name=\"L2G-classifier\",\n            feature_names=self.features_list,\n            is_binary=True,\n        )\n        # Track evaluation metrics\n        self.run.log(\n            {\n                \"areaUnderROC\": roc_auc_score(\n                    self.y_test, y_probas[:, 1], average=\"weighted\"\n                )\n            }\n        )\n        self.run.log({\"accuracy\": accuracy_score(self.y_test, y_predicted)})\n        self.run.log(\n            {\n                \"weightedPrecision\": precision_score(\n                    self.y_test, y_predicted, average=\"weighted\"\n                )\n            }\n        )\n        self.run.log(\n            {\n                \"averagePrecision\": average_precision_score(\n                    self.y_test, y_predicted, average=\"weighted\"\n                )\n            }\n        )\n        self.run.log(\n            {\n                \"weightedRecall\": recall_score(\n                    self.y_test, y_predicted, average=\"weighted\"\n                )\n            }\n        )\n        self.run.log({\"f1\": f1_score(self.y_test, y_predicted, average=\"weighted\")})\n        # Track gold standards and their features\n        self.run.log(\n            {\"featureMatrix\": Table(dataframe=self.feature_matrix._df.toPandas())}\n        )\n        # Log feature missingness\n        self.run.log(\n            {\n                \"missingnessRates\": self.feature_matrix.calculate_feature_missingness_rate()\n            }\n        )\n        # Plot marginal contribution of each feature\n        explanation = self._get_shap_explanation(self.model)\n        self.log_plot_image_to_wandb(\n            \"Feature Contribution\",\n            shap.plots.bar(\n                explanation, max_display=len(self.features_list), show=False\n            ),\n        )\n        self.log_plot_image_to_wandb(\n            \"Beeswarm Plot\",\n            shap.plots.beeswarm(\n                explanation, max_display=len(self.features_list), show=False\n            ),\n        )\n        # Plot correlation between feature values and their importance\n        for feature in self.features_list:\n            self.log_plot_image_to_wandb(\n                f\"Effect of {feature} on the predictions\",\n                shap.plots.scatter(\n                    explanation[:, feature],\n                    show=False,\n                ),\n            )\n        wandb_termlog(\"Logged Shapley contributions.\")\n        self.run.finish()\n\n    def train(\n        self: LocusToGeneTrainer,\n        wandb_run_name: str,\n        cross_validate: bool = True,\n        n_splits: int = 5,\n        hyperparameter_grid: dict[str, Any] | None = None,\n    ) -&gt; LocusToGeneModel:\n        \"\"\"Train the Locus to Gene model.\n\n        If cross_validation is set to True, we implement the following strategy:\n            1. Create held-out test set\n            2. Perform cross-validation on training set\n            3. Train final model on full training set\n            4. Evaluate once on test set\n\n        Args:\n            wandb_run_name (str): Name of the W&amp;B run. Unless this is provided, the model will not be logged to W&amp;B.\n            cross_validate (bool): Whether to run cross-validation. Defaults to True.\n            n_splits(int): Number of folds the data is splitted in. The model is trained and evaluated `k - 1` times. Defaults to 5.\n            hyperparameter_grid (dict[str, Any] | None): Hyperparameter grid to sweep over. Defaults to None.\n\n        Returns:\n            LocusToGeneModel: Fitted model\n        \"\"\"\n        data_df = self.feature_matrix._df.toPandas()\n        # enforce that data_df is a Pandas DataFrame\n\n        # Encode labels in `goldStandardSet` to a numeric value\n        data_df[self.label_col] = data_df[self.label_col].map(self.model.label_encoder)\n\n        X = data_df[self.features_list].apply(pd.to_numeric).values\n        y = data_df[self.label_col].apply(pd.to_numeric).values\n        gene_trait_groups = (\n            data_df[\"traitFromSourceMappedId\"].astype(str)\n            + \"_\"\n            + data_df[\"geneId\"].astype(str)\n        )  # Group identifier has to be a single string\n\n        # Create hold-out test set separating EFO/Gene pairs between train/test\n        train_test_split = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n        for train_idx, test_idx in train_test_split.split(X, y, gene_trait_groups):\n            self.x_train, self.x_test = X[train_idx], X[test_idx]\n            self.y_train, self.y_test = y[train_idx], y[test_idx]\n            self.groups_train = gene_trait_groups[train_idx]\n\n        # Cross-validation\n        if cross_validate:\n            self.cross_validate(\n                wandb_run_name=f\"{wandb_run_name}-cv\",\n                parameter_grid=hyperparameter_grid,\n                n_splits=n_splits,\n            )\n\n        # Train final model on full training set\n        self.fit()\n\n        # Evaluate once on hold out test set\n        self.log_to_wandb(\n            wandb_run_name=f\"{wandb_run_name}-holdout\",\n        )\n\n        return self.model\n\n    def cross_validate(\n        self: LocusToGeneTrainer,\n        wandb_run_name: str,\n        parameter_grid: dict[str, Any] | None = None,\n        n_splits: int = 5,\n    ) -&gt; None:\n        \"\"\"Log results of cross validation and hyperparameter tuning with W&amp;B Sweeps. Metrics for every combination of hyperparameters will be logged to W&amp;B for comparison.\n\n        Args:\n            wandb_run_name (str): Name of the W&amp;B run\n            parameter_grid (dict[str, Any] | None): Dictionary containing the hyperparameters to sweep over. The keys are the hyperparameter names, and the values are dictionaries containing the values to sweep over.\n            n_splits (int): Number of folds the data is splitted in. The model is trained and evaluated `k - 1` times. Defaults to 5.\n        \"\"\"\n\n        def cross_validate_single_fold(\n            fold_index: int,\n            sweep_id: str,\n            sweep_run_name: str,\n            config: dict[str, Any],\n        ) -&gt; None:\n            \"\"\"Run cross-validation for a single fold.\n\n            Args:\n                fold_index (int): Index of the fold to run\n                sweep_id (str): ID of the sweep\n                sweep_run_name (str): Name of the sweep run\n                config (dict[str, Any]): Configuration from the sweep\n\n            Raises:\n                ValueError: If training data is not set\n            \"\"\"\n            reset_wandb_env()\n            train_idx, val_idx = cv_splits[fold_index]\n\n            if (\n                self.x_train is None\n                or self.y_train is None\n                or self.groups_train is None\n            ):\n                raise ValueError(\"Training data not set\")\n\n            # Initialize a new run for this fold\n            os.environ[\"WANDB_SWEEP_ID\"] = sweep_id\n            run = wandb_init(\n                project=self.wandb_l2g_project_name,\n                name=sweep_run_name,\n                config=config,\n                group=sweep_run_name,\n                job_type=\"fold\",\n                reinit=True,\n            )\n\n            x_fold_train, x_fold_val = (\n                self.x_train[train_idx],\n                self.x_train[val_idx],\n            )\n            y_fold_train, y_fold_val = (\n                self.y_train[train_idx],\n                self.y_train[val_idx],\n            )\n\n            fold_model = clone(self.model.model)\n            fold_model.set_params(**config)\n            fold_model.fit(x_fold_train, y_fold_train)\n            y_pred_proba = fold_model.predict_proba(x_fold_val)[:, 1]\n            y_pred = (y_pred_proba &gt;= 0.5).astype(int)\n\n            # Log metrics\n            metrics = {\n                \"weightedPrecision\": precision_score(y_fold_val, y_pred),\n                \"averagePrecision\": average_precision_score(y_fold_val, y_pred_proba),\n                \"areaUnderROC\": roc_auc_score(y_fold_val, y_pred_proba),\n                \"accuracy\": accuracy_score(y_fold_val, y_pred),\n                \"weightedRecall\": recall_score(y_fold_val, y_pred, average=\"weighted\"),\n                \"f1\": f1_score(y_fold_val, y_pred, average=\"weighted\"),\n            }\n\n            run.log(metrics)\n            wandb_termlog(f\"Logged metrics for fold {fold_index + 1}.\")\n            run.finish()\n\n        # If no grid is provided, use default ones set in the model\n        parameter_grid = parameter_grid or {\n            param: {\"values\": [value]}\n            for param, value in self.model.hyperparameters.items()\n        }\n        sweep_config = {\n            \"method\": \"grid\",\n            \"name\": wandb_run_name,  # Add name to sweep config\n            \"metric\": {\"name\": \"areaUnderROC\", \"goal\": \"maximize\"},\n            \"parameters\": parameter_grid,\n        }\n        sweep_id = wandb_sweep(sweep_config, project=self.wandb_l2g_project_name)\n\n        gkf = GroupKFold(n_splits=n_splits)\n        cv_splits = list(gkf.split(self.x_train, self.y_train, self.groups_train))\n\n        def run_all_folds() -&gt; None:\n            \"\"\"Run cross-validation for all folds within a sweep.\"\"\"\n            # Initialize the sweep run and get metadata\n            sweep_run = wandb_init(name=wandb_run_name)\n            sweep_id = sweep_run.sweep_id or \"unknown\"\n            sweep_url = sweep_run.get_sweep_url()\n            project_url = sweep_run.get_project_url()\n            sweep_group_url = f\"{project_url}/groups/{sweep_id}\"\n            sweep_run.notes = sweep_group_url\n            sweep_run.save()\n            config = dict(sweep_run.config)\n\n            # Reset wandb setup to ensure clean state\n            _setup(_reset=True)\n\n            # Run all folds\n            for fold_index in range(len(cv_splits)):\n                cross_validate_single_fold(\n                    fold_index=fold_index,\n                    sweep_id=sweep_id,\n                    sweep_run_name=f\"{wandb_run_name}-fold{fold_index + 1}\",\n                    config=config,\n                )\n\n            wandb_termlog(f\"Sweep URL: {sweep_url}\")\n            wandb_termlog(f\"Sweep Group URL: {sweep_group_url}\")\n\n        wandb_agent(sweep_id, run_all_folds)\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.cross_validate","title":"<code>cross_validate(wandb_run_name: str, parameter_grid: dict[str, Any] | None = None, n_splits: int = 5) -&gt; None</code>","text":"<p>Log results of cross validation and hyperparameter tuning with W&amp;B Sweeps. Metrics for every combination of hyperparameters will be logged to W&amp;B for comparison.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_run_name</code> <code>str</code> <p>Name of the W&amp;B run</p> required <code>parameter_grid</code> <code>dict[str, Any] | None</code> <p>Dictionary containing the hyperparameters to sweep over. The keys are the hyperparameter names, and the values are dictionaries containing the values to sweep over.</p> <code>None</code> <code>n_splits</code> <code>int</code> <p>Number of folds the data is splitted in. The model is trained and evaluated <code>k - 1</code> times. Defaults to 5.</p> <code>5</code> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def cross_validate(\n    self: LocusToGeneTrainer,\n    wandb_run_name: str,\n    parameter_grid: dict[str, Any] | None = None,\n    n_splits: int = 5,\n) -&gt; None:\n    \"\"\"Log results of cross validation and hyperparameter tuning with W&amp;B Sweeps. Metrics for every combination of hyperparameters will be logged to W&amp;B for comparison.\n\n    Args:\n        wandb_run_name (str): Name of the W&amp;B run\n        parameter_grid (dict[str, Any] | None): Dictionary containing the hyperparameters to sweep over. The keys are the hyperparameter names, and the values are dictionaries containing the values to sweep over.\n        n_splits (int): Number of folds the data is splitted in. The model is trained and evaluated `k - 1` times. Defaults to 5.\n    \"\"\"\n\n    def cross_validate_single_fold(\n        fold_index: int,\n        sweep_id: str,\n        sweep_run_name: str,\n        config: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Run cross-validation for a single fold.\n\n        Args:\n            fold_index (int): Index of the fold to run\n            sweep_id (str): ID of the sweep\n            sweep_run_name (str): Name of the sweep run\n            config (dict[str, Any]): Configuration from the sweep\n\n        Raises:\n            ValueError: If training data is not set\n        \"\"\"\n        reset_wandb_env()\n        train_idx, val_idx = cv_splits[fold_index]\n\n        if (\n            self.x_train is None\n            or self.y_train is None\n            or self.groups_train is None\n        ):\n            raise ValueError(\"Training data not set\")\n\n        # Initialize a new run for this fold\n        os.environ[\"WANDB_SWEEP_ID\"] = sweep_id\n        run = wandb_init(\n            project=self.wandb_l2g_project_name,\n            name=sweep_run_name,\n            config=config,\n            group=sweep_run_name,\n            job_type=\"fold\",\n            reinit=True,\n        )\n\n        x_fold_train, x_fold_val = (\n            self.x_train[train_idx],\n            self.x_train[val_idx],\n        )\n        y_fold_train, y_fold_val = (\n            self.y_train[train_idx],\n            self.y_train[val_idx],\n        )\n\n        fold_model = clone(self.model.model)\n        fold_model.set_params(**config)\n        fold_model.fit(x_fold_train, y_fold_train)\n        y_pred_proba = fold_model.predict_proba(x_fold_val)[:, 1]\n        y_pred = (y_pred_proba &gt;= 0.5).astype(int)\n\n        # Log metrics\n        metrics = {\n            \"weightedPrecision\": precision_score(y_fold_val, y_pred),\n            \"averagePrecision\": average_precision_score(y_fold_val, y_pred_proba),\n            \"areaUnderROC\": roc_auc_score(y_fold_val, y_pred_proba),\n            \"accuracy\": accuracy_score(y_fold_val, y_pred),\n            \"weightedRecall\": recall_score(y_fold_val, y_pred, average=\"weighted\"),\n            \"f1\": f1_score(y_fold_val, y_pred, average=\"weighted\"),\n        }\n\n        run.log(metrics)\n        wandb_termlog(f\"Logged metrics for fold {fold_index + 1}.\")\n        run.finish()\n\n    # If no grid is provided, use default ones set in the model\n    parameter_grid = parameter_grid or {\n        param: {\"values\": [value]}\n        for param, value in self.model.hyperparameters.items()\n    }\n    sweep_config = {\n        \"method\": \"grid\",\n        \"name\": wandb_run_name,  # Add name to sweep config\n        \"metric\": {\"name\": \"areaUnderROC\", \"goal\": \"maximize\"},\n        \"parameters\": parameter_grid,\n    }\n    sweep_id = wandb_sweep(sweep_config, project=self.wandb_l2g_project_name)\n\n    gkf = GroupKFold(n_splits=n_splits)\n    cv_splits = list(gkf.split(self.x_train, self.y_train, self.groups_train))\n\n    def run_all_folds() -&gt; None:\n        \"\"\"Run cross-validation for all folds within a sweep.\"\"\"\n        # Initialize the sweep run and get metadata\n        sweep_run = wandb_init(name=wandb_run_name)\n        sweep_id = sweep_run.sweep_id or \"unknown\"\n        sweep_url = sweep_run.get_sweep_url()\n        project_url = sweep_run.get_project_url()\n        sweep_group_url = f\"{project_url}/groups/{sweep_id}\"\n        sweep_run.notes = sweep_group_url\n        sweep_run.save()\n        config = dict(sweep_run.config)\n\n        # Reset wandb setup to ensure clean state\n        _setup(_reset=True)\n\n        # Run all folds\n        for fold_index in range(len(cv_splits)):\n            cross_validate_single_fold(\n                fold_index=fold_index,\n                sweep_id=sweep_id,\n                sweep_run_name=f\"{wandb_run_name}-fold{fold_index + 1}\",\n                config=config,\n            )\n\n        wandb_termlog(f\"Sweep URL: {sweep_url}\")\n        wandb_termlog(f\"Sweep Group URL: {sweep_group_url}\")\n\n    wandb_agent(sweep_id, run_all_folds)\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.fit","title":"<code>fit() -&gt; LocusToGeneModel</code>","text":"<p>Fit the pipeline to the feature matrix dataframe.</p> <p>Returns:</p> Name Type Description <code>LocusToGeneModel</code> <code>LocusToGeneModel</code> <p>Fitted model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Train data not set, nothing to fit.</p> <code>AssertionError</code> <p>When x_train_size or y_train_size are not zero.</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def fit(\n    self: LocusToGeneTrainer,\n) -&gt; LocusToGeneModel:\n    \"\"\"Fit the pipeline to the feature matrix dataframe.\n\n    Returns:\n        LocusToGeneModel: Fitted model\n\n    Raises:\n        ValueError: Train data not set, nothing to fit.\n        AssertionError: When x_train_size or y_train_size are not zero.\n    \"\"\"\n    if (\n        self.x_train is not None\n        and self.y_train is not None\n        and self.features_list is not None\n    ):\n        assert (\n            self.x_train.size != 0 and self.y_train.size != 0\n        ), \"Train data not set, nothing to fit.\"\n        fitted_model = self.model.model.fit(X=self.x_train, y=self.y_train)\n        self.model = LocusToGeneModel(\n            model=fitted_model,\n            hyperparameters=fitted_model.get_params(),\n            training_data=self.feature_matrix,\n            features_list=self.features_list,\n        )\n        return self.model\n    raise ValueError(\"Train data not set, nothing to fit.\")\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.log_plot_image_to_wandb","title":"<code>log_plot_image_to_wandb(title: str, plot: Axes) -&gt; None</code>","text":"<p>Accepts a plot object, and saves the fig to PNG to then log it in W&amp;B.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the plot.</p> required <code>plot</code> <code>Axes</code> <p>Shap plot to log.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Run not set, cannot log to W&amp;B.</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def log_plot_image_to_wandb(\n    self: LocusToGeneTrainer, title: str, plot: Axes\n) -&gt; None:\n    \"\"\"Accepts a plot object, and saves the fig to PNG to then log it in W&amp;B.\n\n    Args:\n        title (str): Title of the plot.\n        plot (Axes): Shap plot to log.\n\n    Raises:\n        ValueError: Run not set, cannot log to W&amp;B.\n    \"\"\"\n    if self.run is None:\n        raise ValueError(\"Run not set, cannot log to W&amp;B.\")\n    if not plot:\n        # Scatter plot returns none, so we need to handle this case\n        plt.savefig(\"tmp.png\", bbox_inches=\"tight\")\n    else:\n        plot.figure.savefig(\"tmp.png\", bbox_inches=\"tight\")\n    self.run.log({title: Image(\"tmp.png\")})\n    plt.close()\n    os.remove(\"tmp.png\")\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.log_to_wandb","title":"<code>log_to_wandb(wandb_run_name: str) -&gt; None</code>","text":"<p>Log evaluation results and feature importance to W&amp;B to compare between different L2G runs.</p> <p>Dashboard is available at https://wandb.ai/open-targets/gentropy-locus-to-gene?nw=nwuseropentargets Credentials to access W&amp;B are available at the OT central login sheet.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_run_name</code> <code>str</code> <p>Name of the W&amp;B run</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If dependencies are not available.</p> <code>AssertionError</code> <p>When x_train_size or y_train_size are not zero.</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def log_to_wandb(\n    self: LocusToGeneTrainer,\n    wandb_run_name: str,\n) -&gt; None:\n    \"\"\"Log evaluation results and feature importance to W&amp;B to compare between different L2G runs.\n\n    Dashboard is available at https://wandb.ai/open-targets/gentropy-locus-to-gene?nw=nwuseropentargets\n    Credentials to access W&amp;B are available at the OT central login sheet.\n\n    Args:\n        wandb_run_name (str): Name of the W&amp;B run\n\n    Raises:\n        RuntimeError: If dependencies are not available.\n        AssertionError: When x_train_size or y_train_size are not zero.\n    \"\"\"\n    if (\n        self.x_train is None\n        or self.x_test is None\n        or self.y_train is None\n        or self.y_test is None\n        or self.features_list is None\n    ):\n        raise RuntimeError(\"Train data not set, we cannot log to W&amp;B.\")\n    assert (\n        self.x_train.size != 0 and self.y_train.size != 0\n    ), \"Train data not set, nothing to evaluate.\"\n    fitted_classifier = self.model.model\n    y_predicted = fitted_classifier.predict(self.x_test)\n    y_probas = fitted_classifier.predict_proba(self.x_test)\n    self.run = wandb_init(\n        project=self.wandb_l2g_project_name,\n        name=wandb_run_name,\n        config=fitted_classifier.get_params(),\n    )\n    # Track classification plots\n    plot_classifier(\n        self.model.model,\n        self.x_train,\n        self.x_test,\n        self.y_train,\n        self.y_test,\n        y_predicted,\n        y_probas,\n        labels=list(self.model.label_encoder.values()),\n        model_name=\"L2G-classifier\",\n        feature_names=self.features_list,\n        is_binary=True,\n    )\n    # Track evaluation metrics\n    self.run.log(\n        {\n            \"areaUnderROC\": roc_auc_score(\n                self.y_test, y_probas[:, 1], average=\"weighted\"\n            )\n        }\n    )\n    self.run.log({\"accuracy\": accuracy_score(self.y_test, y_predicted)})\n    self.run.log(\n        {\n            \"weightedPrecision\": precision_score(\n                self.y_test, y_predicted, average=\"weighted\"\n            )\n        }\n    )\n    self.run.log(\n        {\n            \"averagePrecision\": average_precision_score(\n                self.y_test, y_predicted, average=\"weighted\"\n            )\n        }\n    )\n    self.run.log(\n        {\n            \"weightedRecall\": recall_score(\n                self.y_test, y_predicted, average=\"weighted\"\n            )\n        }\n    )\n    self.run.log({\"f1\": f1_score(self.y_test, y_predicted, average=\"weighted\")})\n    # Track gold standards and their features\n    self.run.log(\n        {\"featureMatrix\": Table(dataframe=self.feature_matrix._df.toPandas())}\n    )\n    # Log feature missingness\n    self.run.log(\n        {\n            \"missingnessRates\": self.feature_matrix.calculate_feature_missingness_rate()\n        }\n    )\n    # Plot marginal contribution of each feature\n    explanation = self._get_shap_explanation(self.model)\n    self.log_plot_image_to_wandb(\n        \"Feature Contribution\",\n        shap.plots.bar(\n            explanation, max_display=len(self.features_list), show=False\n        ),\n    )\n    self.log_plot_image_to_wandb(\n        \"Beeswarm Plot\",\n        shap.plots.beeswarm(\n            explanation, max_display=len(self.features_list), show=False\n        ),\n    )\n    # Plot correlation between feature values and their importance\n    for feature in self.features_list:\n        self.log_plot_image_to_wandb(\n            f\"Effect of {feature} on the predictions\",\n            shap.plots.scatter(\n                explanation[:, feature],\n                show=False,\n            ),\n        )\n    wandb_termlog(\"Logged Shapley contributions.\")\n    self.run.finish()\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.train","title":"<code>train(wandb_run_name: str, cross_validate: bool = True, n_splits: int = 5, hyperparameter_grid: dict[str, Any] | None = None) -&gt; LocusToGeneModel</code>","text":"<p>Train the Locus to Gene model.</p> <p>If cross_validation is set to True, we implement the following strategy:     1. Create held-out test set     2. Perform cross-validation on training set     3. Train final model on full training set     4. Evaluate once on test set</p> <p>Parameters:</p> Name Type Description Default <code>wandb_run_name</code> <code>str</code> <p>Name of the W&amp;B run. Unless this is provided, the model will not be logged to W&amp;B.</p> required <code>cross_validate</code> <code>bool</code> <p>Whether to run cross-validation. Defaults to True.</p> <code>True</code> <code>n_splits(int)</code> <p>Number of folds the data is splitted in. The model is trained and evaluated <code>k - 1</code> times. Defaults to 5.</p> required <code>hyperparameter_grid</code> <code>dict[str, Any] | None</code> <p>Hyperparameter grid to sweep over. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LocusToGeneModel</code> <code>LocusToGeneModel</code> <p>Fitted model</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def train(\n    self: LocusToGeneTrainer,\n    wandb_run_name: str,\n    cross_validate: bool = True,\n    n_splits: int = 5,\n    hyperparameter_grid: dict[str, Any] | None = None,\n) -&gt; LocusToGeneModel:\n    \"\"\"Train the Locus to Gene model.\n\n    If cross_validation is set to True, we implement the following strategy:\n        1. Create held-out test set\n        2. Perform cross-validation on training set\n        3. Train final model on full training set\n        4. Evaluate once on test set\n\n    Args:\n        wandb_run_name (str): Name of the W&amp;B run. Unless this is provided, the model will not be logged to W&amp;B.\n        cross_validate (bool): Whether to run cross-validation. Defaults to True.\n        n_splits(int): Number of folds the data is splitted in. The model is trained and evaluated `k - 1` times. Defaults to 5.\n        hyperparameter_grid (dict[str, Any] | None): Hyperparameter grid to sweep over. Defaults to None.\n\n    Returns:\n        LocusToGeneModel: Fitted model\n    \"\"\"\n    data_df = self.feature_matrix._df.toPandas()\n    # enforce that data_df is a Pandas DataFrame\n\n    # Encode labels in `goldStandardSet` to a numeric value\n    data_df[self.label_col] = data_df[self.label_col].map(self.model.label_encoder)\n\n    X = data_df[self.features_list].apply(pd.to_numeric).values\n    y = data_df[self.label_col].apply(pd.to_numeric).values\n    gene_trait_groups = (\n        data_df[\"traitFromSourceMappedId\"].astype(str)\n        + \"_\"\n        + data_df[\"geneId\"].astype(str)\n    )  # Group identifier has to be a single string\n\n    # Create hold-out test set separating EFO/Gene pairs between train/test\n    train_test_split = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n    for train_idx, test_idx in train_test_split.split(X, y, gene_trait_groups):\n        self.x_train, self.x_test = X[train_idx], X[test_idx]\n        self.y_train, self.y_test = y[train_idx], y[test_idx]\n        self.groups_train = gene_trait_groups[train_idx]\n\n    # Cross-validation\n    if cross_validate:\n        self.cross_validate(\n            wandb_run_name=f\"{wandb_run_name}-cv\",\n            parameter_grid=hyperparameter_grid,\n            n_splits=n_splits,\n        )\n\n    # Train final model on full training set\n    self.fit()\n\n    # Evaluate once on hold out test set\n    self.log_to_wandb(\n        wandb_run_name=f\"{wandb_run_name}-holdout\",\n    )\n\n    return self.model\n</code></pre>"},{"location":"python_api/steps/_steps/","title":"Step","text":"<p>This section provides description for the <code>Step</code> class. Each <code>Step</code> uses its own set of Methods and Datasets and implements the logic necessary to read a set of inputs, perform the transformation and write the outputs. All steps are available through the command line interface when running the <code>gentropy</code> command.</p>"},{"location":"python_api/steps/biosample_index_step/","title":"biosample_index","text":""},{"location":"python_api/steps/biosample_index_step/#gentropy.biosample_index.BiosampleIndexStep","title":"<code>gentropy.biosample_index.BiosampleIndexStep</code>","text":"<p>Biosample index step.</p> <p>This step generates a Biosample index dataset from the various ontology sources. Currently Cell Ontology and Uberon are supported.</p> Source code in <code>src/gentropy/biosample_index.py</code> <pre><code>class BiosampleIndexStep:\n    \"\"\"Biosample index step.\n\n    This step generates a Biosample index dataset from the various ontology sources. Currently Cell Ontology and Uberon are supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        cell_ontology_input_path: str,\n        uberon_input_path: str,\n        efo_input_path: str,\n        biosample_index_path: str,\n    ) -&gt; None:\n        \"\"\"Run Biosample index generation step.\n\n        Args:\n            session (Session): Session object.\n            cell_ontology_input_path (str): Input cell ontology dataset path.\n            uberon_input_path (str): Input uberon dataset path.\n            efo_input_path (str): Input efo dataset path.\n            biosample_index_path (str): Output biosample index dataset path.\n        \"\"\"\n        cell_ontology_index = extract_ontology_from_json(\n            cell_ontology_input_path, session.spark\n        )\n        uberon_index = extract_ontology_from_json(uberon_input_path, session.spark)\n        efo_index = extract_ontology_from_json(\n            efo_input_path, session.spark\n        ).retain_rows_with_ancestor_id([\"CL_0000000\"])\n\n        biosample_index = cell_ontology_index.merge_indices([uberon_index, efo_index])\n\n        biosample_index.df.coalesce(session.output_partitions).write.mode(\n            session.write_mode\n        ).parquet(biosample_index_path)\n</code></pre>"},{"location":"python_api/steps/biosample_index_step/#gentropy.biosample_index.BiosampleIndexStep.__init__","title":"<code>__init__(session: Session, cell_ontology_input_path: str, uberon_input_path: str, efo_input_path: str, biosample_index_path: str) -&gt; None</code>","text":"<p>Run Biosample index generation step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>cell_ontology_input_path</code> <code>str</code> <p>Input cell ontology dataset path.</p> required <code>uberon_input_path</code> <code>str</code> <p>Input uberon dataset path.</p> required <code>efo_input_path</code> <code>str</code> <p>Input efo dataset path.</p> required <code>biosample_index_path</code> <code>str</code> <p>Output biosample index dataset path.</p> required Source code in <code>src/gentropy/biosample_index.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    cell_ontology_input_path: str,\n    uberon_input_path: str,\n    efo_input_path: str,\n    biosample_index_path: str,\n) -&gt; None:\n    \"\"\"Run Biosample index generation step.\n\n    Args:\n        session (Session): Session object.\n        cell_ontology_input_path (str): Input cell ontology dataset path.\n        uberon_input_path (str): Input uberon dataset path.\n        efo_input_path (str): Input efo dataset path.\n        biosample_index_path (str): Output biosample index dataset path.\n    \"\"\"\n    cell_ontology_index = extract_ontology_from_json(\n        cell_ontology_input_path, session.spark\n    )\n    uberon_index = extract_ontology_from_json(uberon_input_path, session.spark)\n    efo_index = extract_ontology_from_json(\n        efo_input_path, session.spark\n    ).retain_rows_with_ancestor_id([\"CL_0000000\"])\n\n    biosample_index = cell_ontology_index.merge_indices([uberon_index, efo_index])\n\n    biosample_index.df.coalesce(session.output_partitions).write.mode(\n        session.write_mode\n    ).parquet(biosample_index_path)\n</code></pre>"},{"location":"python_api/steps/colocalisation/","title":"colocalisation","text":""},{"location":"python_api/steps/colocalisation/#gentropy.colocalisation.ColocalisationStep","title":"<code>gentropy.colocalisation.ColocalisationStep</code>","text":"<p>Colocalisation step.</p> <p>This workflow runs colocalisation analyses that assess the degree to which independent signals of the association share the same causal variant in a region of the genome, typically limited by linkage disequilibrium (LD).</p> Source code in <code>src/gentropy/colocalisation.py</code> <pre><code>class ColocalisationStep:\n    \"\"\"Colocalisation step.\n\n    This workflow runs colocalisation analyses that assess the degree to which independent signals of the association share the same causal variant in a region of the genome, typically limited by linkage disequilibrium (LD).\n    \"\"\"\n\n    __coloc_methods__ = {\n        method.METHOD_NAME.lower(): method\n        for method in ColocalisationMethodInterface.__subclasses__()\n    }\n\n    def __init__(\n        self,\n        session: Session,\n        credible_set_path: str,\n        coloc_path: str,\n        colocalisation_method: str,\n        colocalisation_method_params: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"Run Colocalisation step.\n\n        This step allows for running two colocalisation methods: ecaviar and coloc.\n\n        Args:\n            session (Session): Session object.\n            credible_set_path (str): Input credible sets path.\n            coloc_path (str): Output Colocalisation path.\n            colocalisation_method (str): Colocalisation method.\n            colocalisation_method_params (dict[str, Any] | None): Keyword arguments passed to the colocalise method of Colocalisation class. Defaults to None\n\n        Keyword Args:\n            priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4. For coloc method only.\n            priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4. For coloc method only.\n            priorc12 (float): Prior on variant being causal for both traits. Defaults to 1e-5. For coloc method only.\n        \"\"\"\n        colocalisation_method = colocalisation_method.lower()\n        colocalisation_class = self._get_colocalisation_class(colocalisation_method)\n\n        # Extract\n        credible_set = StudyLocus.from_parquet(\n            session, credible_set_path, recusiveFileLookup=True\n        )\n        if colocalisation_method == Coloc.METHOD_NAME.lower():\n            credible_set = credible_set.filter(\n                col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n                )\n            )\n\n        # Transform\n        overlaps = credible_set.find_overlaps()\n\n        # Make a partial caller to ensure that colocalisation_method_params are added to the call only when dict is not empty\n        coloc = colocalisation_class.colocalise\n        if colocalisation_method_params:\n            coloc = partial(coloc, **colocalisation_method_params)\n        colocalisation_results = coloc(overlaps)\n        # Load\n        colocalisation_results.df.coalesce(session.output_partitions).write.mode(\n            session.write_mode\n        ).parquet(f\"{coloc_path}/{colocalisation_method.lower()}\")\n\n    @classmethod\n    def _get_colocalisation_class(\n        cls, method: str\n    ) -&gt; type[ColocalisationMethodInterface]:\n        \"\"\"Get colocalisation class.\n\n        Args:\n            method (str): Colocalisation method.\n\n        Returns:\n            type[ColocalisationMethodInterface]: Class that implements the ColocalisationMethodInterface.\n\n        Raises:\n            ValueError: if method not available.\n\n        Examples:\n            &gt;&gt;&gt; ColocalisationStep._get_colocalisation_class(\"ECaviar\")\n            &lt;class 'gentropy.method.colocalisation.ECaviar'&gt;\n        \"\"\"\n        method = method.lower()\n        if method not in cls.__coloc_methods__:\n            raise ValueError(f\"Colocalisation method {method} not available.\")\n        return cls.__coloc_methods__[method]\n</code></pre>"},{"location":"python_api/steps/colocalisation/#gentropy.colocalisation.ColocalisationStep.__init__","title":"<code>__init__(session: Session, credible_set_path: str, coloc_path: str, colocalisation_method: str, colocalisation_method_params: dict[str, Any] | None = None) -&gt; None</code>","text":"<p>Run Colocalisation step.</p> <p>This step allows for running two colocalisation methods: ecaviar and coloc.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>credible_set_path</code> <code>str</code> <p>Input credible sets path.</p> required <code>coloc_path</code> <code>str</code> <p>Output Colocalisation path.</p> required <code>colocalisation_method</code> <code>str</code> <p>Colocalisation method.</p> required <code>colocalisation_method_params</code> <code>dict[str, Any] | None</code> <p>Keyword arguments passed to the colocalise method of Colocalisation class. Defaults to None</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>priorc1</code> <code>float</code> <p>Prior on variant being causal for trait 1. Defaults to 1e-4. For coloc method only.</p> <code>priorc2</code> <code>float</code> <p>Prior on variant being causal for trait 2. Defaults to 1e-4. For coloc method only.</p> <code>priorc12</code> <code>float</code> <p>Prior on variant being causal for both traits. Defaults to 1e-5. For coloc method only.</p> Source code in <code>src/gentropy/colocalisation.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    credible_set_path: str,\n    coloc_path: str,\n    colocalisation_method: str,\n    colocalisation_method_params: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Run Colocalisation step.\n\n    This step allows for running two colocalisation methods: ecaviar and coloc.\n\n    Args:\n        session (Session): Session object.\n        credible_set_path (str): Input credible sets path.\n        coloc_path (str): Output Colocalisation path.\n        colocalisation_method (str): Colocalisation method.\n        colocalisation_method_params (dict[str, Any] | None): Keyword arguments passed to the colocalise method of Colocalisation class. Defaults to None\n\n    Keyword Args:\n        priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4. For coloc method only.\n        priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4. For coloc method only.\n        priorc12 (float): Prior on variant being causal for both traits. Defaults to 1e-5. For coloc method only.\n    \"\"\"\n    colocalisation_method = colocalisation_method.lower()\n    colocalisation_class = self._get_colocalisation_class(colocalisation_method)\n\n    # Extract\n    credible_set = StudyLocus.from_parquet(\n        session, credible_set_path, recusiveFileLookup=True\n    )\n    if colocalisation_method == Coloc.METHOD_NAME.lower():\n        credible_set = credible_set.filter(\n            col(\"finemappingMethod\").isin(\n                FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n            )\n        )\n\n    # Transform\n    overlaps = credible_set.find_overlaps()\n\n    # Make a partial caller to ensure that colocalisation_method_params are added to the call only when dict is not empty\n    coloc = colocalisation_class.colocalise\n    if colocalisation_method_params:\n        coloc = partial(coloc, **colocalisation_method_params)\n    colocalisation_results = coloc(overlaps)\n    # Load\n    colocalisation_results.df.coalesce(session.output_partitions).write.mode(\n        session.write_mode\n    ).parquet(f\"{coloc_path}/{colocalisation_method.lower()}\")\n</code></pre>"},{"location":"python_api/steps/credible_set_qc_step/","title":"credible_set_qc","text":""},{"location":"python_api/steps/credible_set_qc_step/#gentropy.credible_set_qc.CredibleSetQCStep","title":"<code>gentropy.credible_set_qc.CredibleSetQCStep</code>","text":"<p>Credible set quality control step for fine mapped StudyLoci.</p> Source code in <code>src/gentropy/credible_set_qc.py</code> <pre><code>class CredibleSetQCStep:\n    \"\"\"Credible set quality control step for fine mapped StudyLoci.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        credible_sets_path: str,\n        output_path: str,\n        p_value_threshold: float,\n        purity_min_r2: float,\n        clump: bool,\n        ld_index_path: str | None,\n        study_index_path: str | None,\n        ld_min_r2: float | None,\n        n_partitions: int | None,\n    ) -&gt; None:\n        \"\"\"Run credible set quality control step.\n\n        Check defaults used by steps in hydra configuration `gentropy.config.CredibleSetQCStepConfig`\n\n        Due to the large number of partitions at the input credible_set_path after finemapping, the\n        best strategy it is to repartition and save the dataset after deduplication.\n\n        The `clump` mode will perform additional LD based clumping on the input credible sets.\n        Enabling `clump` mode requires providing `ld_index_path`, `study_index_path` and `ld_min_r2`.\n\n        Args:\n            session (Session): Session object.\n            credible_sets_path (str): Path to credible sets file.\n            output_path (str): Path to write the output file.\n            p_value_threshold (float): P-value threshold for credible set quality control.\n            purity_min_r2 (float): Minimum R2 for purity estimation.\n            clump (bool): Whether to clump the credible sets by LD.\n            ld_index_path (str | None): Path to LD index file.\n            study_index_path (str | None): Path to study index file.\n            ld_min_r2 (float | None): Minimum R2 for LD estimation.\n            n_partitions (int | None): Number of partitions to coalesce the dataset after reading. Defaults to 200\n        \"\"\"\n        n_partitions = n_partitions or 200\n\n        ld_index = (\n            LDIndex.from_parquet(session, ld_index_path) if ld_index_path else None\n        )\n        study_index = (\n            StudyIndex.from_parquet(session, study_index_path)\n            if study_index_path\n            else None\n        )\n\n        cred_sets = StudyLocus.from_parquet(\n            session, credible_sets_path, recursiveFileLookup=True\n        ).coalesce(n_partitions)\n\n        cred_sets_clean = SUSIE_inf.credible_set_qc(\n            cred_sets,\n            p_value_threshold,\n            purity_min_r2,\n            clump,\n            ld_index,\n            study_index,\n            ld_min_r2,\n        )\n        # ensure the saved object is still a valid StudyLocus\n        StudyLocus(\n            _df=cred_sets_clean.df, _schema=StudyLocus.get_schema()\n        ).df.write.mode(session.write_mode).parquet(output_path)\n</code></pre>"},{"location":"python_api/steps/credible_set_qc_step/#gentropy.credible_set_qc.CredibleSetQCStep.__init__","title":"<code>__init__(session: Session, credible_sets_path: str, output_path: str, p_value_threshold: float, purity_min_r2: float, clump: bool, ld_index_path: str | None, study_index_path: str | None, ld_min_r2: float | None, n_partitions: int | None) -&gt; None</code>","text":"<p>Run credible set quality control step.</p> <p>Check defaults used by steps in hydra configuration <code>gentropy.config.CredibleSetQCStepConfig</code></p> <p>Due to the large number of partitions at the input credible_set_path after finemapping, the best strategy it is to repartition and save the dataset after deduplication.</p> <p>The <code>clump</code> mode will perform additional LD based clumping on the input credible sets. Enabling <code>clump</code> mode requires providing <code>ld_index_path</code>, <code>study_index_path</code> and <code>ld_min_r2</code>.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>credible_sets_path</code> <code>str</code> <p>Path to credible sets file.</p> required <code>output_path</code> <code>str</code> <p>Path to write the output file.</p> required <code>p_value_threshold</code> <code>float</code> <p>P-value threshold for credible set quality control.</p> required <code>purity_min_r2</code> <code>float</code> <p>Minimum R2 for purity estimation.</p> required <code>clump</code> <code>bool</code> <p>Whether to clump the credible sets by LD.</p> required <code>ld_index_path</code> <code>str | None</code> <p>Path to LD index file.</p> required <code>study_index_path</code> <code>str | None</code> <p>Path to study index file.</p> required <code>ld_min_r2</code> <code>float | None</code> <p>Minimum R2 for LD estimation.</p> required <code>n_partitions</code> <code>int | None</code> <p>Number of partitions to coalesce the dataset after reading. Defaults to 200</p> required Source code in <code>src/gentropy/credible_set_qc.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    credible_sets_path: str,\n    output_path: str,\n    p_value_threshold: float,\n    purity_min_r2: float,\n    clump: bool,\n    ld_index_path: str | None,\n    study_index_path: str | None,\n    ld_min_r2: float | None,\n    n_partitions: int | None,\n) -&gt; None:\n    \"\"\"Run credible set quality control step.\n\n    Check defaults used by steps in hydra configuration `gentropy.config.CredibleSetQCStepConfig`\n\n    Due to the large number of partitions at the input credible_set_path after finemapping, the\n    best strategy it is to repartition and save the dataset after deduplication.\n\n    The `clump` mode will perform additional LD based clumping on the input credible sets.\n    Enabling `clump` mode requires providing `ld_index_path`, `study_index_path` and `ld_min_r2`.\n\n    Args:\n        session (Session): Session object.\n        credible_sets_path (str): Path to credible sets file.\n        output_path (str): Path to write the output file.\n        p_value_threshold (float): P-value threshold for credible set quality control.\n        purity_min_r2 (float): Minimum R2 for purity estimation.\n        clump (bool): Whether to clump the credible sets by LD.\n        ld_index_path (str | None): Path to LD index file.\n        study_index_path (str | None): Path to study index file.\n        ld_min_r2 (float | None): Minimum R2 for LD estimation.\n        n_partitions (int | None): Number of partitions to coalesce the dataset after reading. Defaults to 200\n    \"\"\"\n    n_partitions = n_partitions or 200\n\n    ld_index = (\n        LDIndex.from_parquet(session, ld_index_path) if ld_index_path else None\n    )\n    study_index = (\n        StudyIndex.from_parquet(session, study_index_path)\n        if study_index_path\n        else None\n    )\n\n    cred_sets = StudyLocus.from_parquet(\n        session, credible_sets_path, recursiveFileLookup=True\n    ).coalesce(n_partitions)\n\n    cred_sets_clean = SUSIE_inf.credible_set_qc(\n        cred_sets,\n        p_value_threshold,\n        purity_min_r2,\n        clump,\n        ld_index,\n        study_index,\n        ld_min_r2,\n    )\n    # ensure the saved object is still a valid StudyLocus\n    StudyLocus(\n        _df=cred_sets_clean.df, _schema=StudyLocus.get_schema()\n    ).df.write.mode(session.write_mode).parquet(output_path)\n</code></pre>"},{"location":"python_api/steps/credible_set_qc_step/#gentropy.config.CredibleSetQCStepConfig","title":"<code>gentropy.config.CredibleSetQCStepConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>StepConfig</code></p> <p>Credible set quality control step configuration.</p> Source code in <code>src/gentropy/config.py</code> <pre><code>@dataclass\nclass CredibleSetQCStepConfig(StepConfig):\n    \"\"\"Credible set quality control step configuration.\"\"\"\n\n    credible_sets_path: str = MISSING\n    output_path: str = MISSING\n    p_value_threshold: float = 1e-5\n    purity_min_r2: float = 0.01\n    clump: bool = False\n    ld_index_path: str | None = None\n    study_index_path: str | None = None\n    ld_min_r2: float | None = 0.8\n    n_partitions: int | None = 200\n    _target_: str = \"gentropy.credible_set_qc.CredibleSetQCStep\"\n</code></pre>"},{"location":"python_api/steps/eqtl_catalogue/","title":"eQTL Catalogue","text":""},{"location":"python_api/steps/eqtl_catalogue/#gentropy.eqtl_catalogue.EqtlCatalogueStep","title":"<code>gentropy.eqtl_catalogue.EqtlCatalogueStep</code>","text":"<p>eQTL Catalogue ingestion step.</p> <p>From SuSIE fine mapping results (available at their FTP ), we extract credible sets and study index datasets from gene expression QTL studies.</p> Source code in <code>src/gentropy/eqtl_catalogue.py</code> <pre><code>class EqtlCatalogueStep:\n    \"\"\"eQTL Catalogue ingestion step.\n\n    From SuSIE fine mapping results (available at [their FTP](https://ftp.ebi.ac.uk/pub/databases/spot/eQTL/susie/) ), we extract credible sets and study index datasets from gene expression QTL studies.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        mqtl_quantification_methods_blacklist: list[str],\n        eqtl_catalogue_paths_imported: str,\n        eqtl_catalogue_study_index_out: str,\n        eqtl_catalogue_credible_sets_out: str,\n        eqtl_lead_pvalue_threshold: float = EqtlCatalogueConfig().eqtl_lead_pvalue_threshold,\n    ) -&gt; None:\n        \"\"\"Run eQTL Catalogue ingestion step.\n\n        Args:\n            session (Session): Session object.\n            mqtl_quantification_methods_blacklist (list[str]): Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv\n            eqtl_catalogue_paths_imported (str): Input eQTL Catalogue fine mapping results path.\n            eqtl_catalogue_study_index_out (str): Output eQTL Catalogue study index path.\n            eqtl_catalogue_credible_sets_out (str): Output eQTL Catalogue credible sets path.\n            eqtl_lead_pvalue_threshold (float, optional): Lead p-value threshold. Defaults to EqtlCatalogueConfig().eqtl_lead_pvalue_threshold.\n        \"\"\"\n        # Extract\n        studies_metadata = EqtlCatalogueStudyIndex.read_studies_from_source(\n            session, list(mqtl_quantification_methods_blacklist)\n        )\n\n        # Load raw data only for the studies we are interested in ingestion. This makes the proces much lighter.\n        studies_to_ingest = EqtlCatalogueStudyIndex.get_studies_of_interest(\n            studies_metadata\n        )\n        credible_sets_df = EqtlCatalogueFinemapping.read_credible_set_from_source(\n            session,\n            credible_set_path=[\n                f\"{eqtl_catalogue_paths_imported}/{qtd_id}.credible_sets.tsv\"\n                for qtd_id in studies_to_ingest\n            ],\n        )\n        lbf_df = EqtlCatalogueFinemapping.read_lbf_from_source(\n            session,\n            lbf_path=[\n                f\"{eqtl_catalogue_paths_imported}/{qtd_id}.lbf_variable.txt\"\n                for qtd_id in studies_to_ingest\n            ],\n        )\n\n        # Transform\n        processed_susie_df = EqtlCatalogueFinemapping.parse_susie_results(\n            credible_sets_df, lbf_df, studies_metadata\n        )\n\n        (\n            EqtlCatalogueStudyIndex.from_susie_results(processed_susie_df)\n            # Writing the output:\n            .df.write.mode(session.write_mode)\n            .parquet(eqtl_catalogue_study_index_out)\n        )\n\n        (\n            EqtlCatalogueFinemapping.from_susie_results(processed_susie_df)\n            # Flagging sub-significnat loci:\n            .validate_lead_pvalue(pvalue_cutoff=eqtl_lead_pvalue_threshold)\n            # Writing the output:\n            .df.write.mode(session.write_mode)\n            .parquet(eqtl_catalogue_credible_sets_out)\n        )\n</code></pre>"},{"location":"python_api/steps/eqtl_catalogue/#gentropy.eqtl_catalogue.EqtlCatalogueStep.__init__","title":"<code>__init__(session: Session, mqtl_quantification_methods_blacklist: list[str], eqtl_catalogue_paths_imported: str, eqtl_catalogue_study_index_out: str, eqtl_catalogue_credible_sets_out: str, eqtl_lead_pvalue_threshold: float = EqtlCatalogueConfig().eqtl_lead_pvalue_threshold) -&gt; None</code>","text":"<p>Run eQTL Catalogue ingestion step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>mqtl_quantification_methods_blacklist</code> <code>list[str]</code> <p>Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv</p> required <code>eqtl_catalogue_paths_imported</code> <code>str</code> <p>Input eQTL Catalogue fine mapping results path.</p> required <code>eqtl_catalogue_study_index_out</code> <code>str</code> <p>Output eQTL Catalogue study index path.</p> required <code>eqtl_catalogue_credible_sets_out</code> <code>str</code> <p>Output eQTL Catalogue credible sets path.</p> required <code>eqtl_lead_pvalue_threshold</code> <code>float</code> <p>Lead p-value threshold. Defaults to EqtlCatalogueConfig().eqtl_lead_pvalue_threshold.</p> <code>eqtl_lead_pvalue_threshold</code> Source code in <code>src/gentropy/eqtl_catalogue.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    mqtl_quantification_methods_blacklist: list[str],\n    eqtl_catalogue_paths_imported: str,\n    eqtl_catalogue_study_index_out: str,\n    eqtl_catalogue_credible_sets_out: str,\n    eqtl_lead_pvalue_threshold: float = EqtlCatalogueConfig().eqtl_lead_pvalue_threshold,\n) -&gt; None:\n    \"\"\"Run eQTL Catalogue ingestion step.\n\n    Args:\n        session (Session): Session object.\n        mqtl_quantification_methods_blacklist (list[str]): Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv\n        eqtl_catalogue_paths_imported (str): Input eQTL Catalogue fine mapping results path.\n        eqtl_catalogue_study_index_out (str): Output eQTL Catalogue study index path.\n        eqtl_catalogue_credible_sets_out (str): Output eQTL Catalogue credible sets path.\n        eqtl_lead_pvalue_threshold (float, optional): Lead p-value threshold. Defaults to EqtlCatalogueConfig().eqtl_lead_pvalue_threshold.\n    \"\"\"\n    # Extract\n    studies_metadata = EqtlCatalogueStudyIndex.read_studies_from_source(\n        session, list(mqtl_quantification_methods_blacklist)\n    )\n\n    # Load raw data only for the studies we are interested in ingestion. This makes the proces much lighter.\n    studies_to_ingest = EqtlCatalogueStudyIndex.get_studies_of_interest(\n        studies_metadata\n    )\n    credible_sets_df = EqtlCatalogueFinemapping.read_credible_set_from_source(\n        session,\n        credible_set_path=[\n            f\"{eqtl_catalogue_paths_imported}/{qtd_id}.credible_sets.tsv\"\n            for qtd_id in studies_to_ingest\n        ],\n    )\n    lbf_df = EqtlCatalogueFinemapping.read_lbf_from_source(\n        session,\n        lbf_path=[\n            f\"{eqtl_catalogue_paths_imported}/{qtd_id}.lbf_variable.txt\"\n            for qtd_id in studies_to_ingest\n        ],\n    )\n\n    # Transform\n    processed_susie_df = EqtlCatalogueFinemapping.parse_susie_results(\n        credible_sets_df, lbf_df, studies_metadata\n    )\n\n    (\n        EqtlCatalogueStudyIndex.from_susie_results(processed_susie_df)\n        # Writing the output:\n        .df.write.mode(session.write_mode)\n        .parquet(eqtl_catalogue_study_index_out)\n    )\n\n    (\n        EqtlCatalogueFinemapping.from_susie_results(processed_susie_df)\n        # Flagging sub-significnat loci:\n        .validate_lead_pvalue(pvalue_cutoff=eqtl_lead_pvalue_threshold)\n        # Writing the output:\n        .df.write.mode(session.write_mode)\n        .parquet(eqtl_catalogue_credible_sets_out)\n    )\n</code></pre>"},{"location":"python_api/steps/finngen_studies/","title":"finngen_studies","text":""},{"location":"python_api/steps/finngen_studies/#gentropy.finngen_studies.FinnGenStudiesStep","title":"<code>gentropy.finngen_studies.FinnGenStudiesStep</code>","text":"<p>FinnGen study index generation step.</p> Source code in <code>src/gentropy/finngen_studies.py</code> <pre><code>class FinnGenStudiesStep:\n    \"\"\"FinnGen study index generation step.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        finngen_study_index_out: str,\n        finngen_phenotype_table_url: str = FinngenStudiesConfig().finngen_phenotype_table_url,\n        finngen_release_prefix: str = FinngenStudiesConfig().finngen_release_prefix,\n        finngen_summary_stats_url_prefix: str = FinngenStudiesConfig().finngen_summary_stats_url_prefix,\n        finngen_summary_stats_url_suffix: str = FinngenStudiesConfig().finngen_summary_stats_url_suffix,\n        efo_curation_mapping_url: str = FinngenStudiesConfig().efo_curation_mapping_url,\n        sample_size: int = FinngenStudiesConfig().sample_size,\n    ) -&gt; None:\n        \"\"\"Run FinnGen study index generation step.\n\n        Args:\n            session (Session): Session object.\n            finngen_study_index_out (str): Output FinnGen study index path.\n            finngen_phenotype_table_url (str): URL to the FinnGen phenotype table.\n            finngen_release_prefix (str): FinnGen release prefix.\n            finngen_summary_stats_url_prefix (str): FinnGen summary stats URL prefix.\n            finngen_summary_stats_url_suffix (str): FinnGen summary stats URL suffix.\n            efo_curation_mapping_url (str): URL to the EFO curation mapping file\n            sample_size (int): Number of individuals that participated in sample collection, derived from finngen release metadata.\n        \"\"\"\n        _match = FinnGenStudyIndex.validate_release_prefix(finngen_release_prefix)\n        release_prefix = _match[\"prefix\"]\n        release = _match[\"release\"]\n\n        efo_curation_df = FinnGenStudyIndex.read_efo_curation(\n            session.spark,\n            efo_curation_mapping_url,\n        )\n        study_index = FinnGenStudyIndex.from_source(\n            session.spark,\n            finngen_phenotype_table_url,\n            release_prefix,\n            finngen_summary_stats_url_prefix,\n            finngen_summary_stats_url_suffix,\n            sample_size,\n        )\n        study_index_with_efo = FinnGenStudyIndex.join_efo_mapping(\n            study_index,\n            efo_curation_df,\n            release,\n        )\n        study_index_with_efo.df.write.mode(session.write_mode).parquet(\n            finngen_study_index_out\n        )\n</code></pre>"},{"location":"python_api/steps/finngen_studies/#gentropy.finngen_studies.FinnGenStudiesStep.__init__","title":"<code>__init__(session: Session, finngen_study_index_out: str, finngen_phenotype_table_url: str = FinngenStudiesConfig().finngen_phenotype_table_url, finngen_release_prefix: str = FinngenStudiesConfig().finngen_release_prefix, finngen_summary_stats_url_prefix: str = FinngenStudiesConfig().finngen_summary_stats_url_prefix, finngen_summary_stats_url_suffix: str = FinngenStudiesConfig().finngen_summary_stats_url_suffix, efo_curation_mapping_url: str = FinngenStudiesConfig().efo_curation_mapping_url, sample_size: int = FinngenStudiesConfig().sample_size) -&gt; None</code>","text":"<p>Run FinnGen study index generation step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>finngen_study_index_out</code> <code>str</code> <p>Output FinnGen study index path.</p> required <code>finngen_phenotype_table_url</code> <code>str</code> <p>URL to the FinnGen phenotype table.</p> <code>finngen_phenotype_table_url</code> <code>finngen_release_prefix</code> <code>str</code> <p>FinnGen release prefix.</p> <code>finngen_release_prefix</code> <code>finngen_summary_stats_url_prefix</code> <code>str</code> <p>FinnGen summary stats URL prefix.</p> <code>finngen_summary_stats_url_prefix</code> <code>finngen_summary_stats_url_suffix</code> <code>str</code> <p>FinnGen summary stats URL suffix.</p> <code>finngen_summary_stats_url_suffix</code> <code>efo_curation_mapping_url</code> <code>str</code> <p>URL to the EFO curation mapping file</p> <code>efo_curation_mapping_url</code> <code>sample_size</code> <code>int</code> <p>Number of individuals that participated in sample collection, derived from finngen release metadata.</p> <code>sample_size</code> Source code in <code>src/gentropy/finngen_studies.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    finngen_study_index_out: str,\n    finngen_phenotype_table_url: str = FinngenStudiesConfig().finngen_phenotype_table_url,\n    finngen_release_prefix: str = FinngenStudiesConfig().finngen_release_prefix,\n    finngen_summary_stats_url_prefix: str = FinngenStudiesConfig().finngen_summary_stats_url_prefix,\n    finngen_summary_stats_url_suffix: str = FinngenStudiesConfig().finngen_summary_stats_url_suffix,\n    efo_curation_mapping_url: str = FinngenStudiesConfig().efo_curation_mapping_url,\n    sample_size: int = FinngenStudiesConfig().sample_size,\n) -&gt; None:\n    \"\"\"Run FinnGen study index generation step.\n\n    Args:\n        session (Session): Session object.\n        finngen_study_index_out (str): Output FinnGen study index path.\n        finngen_phenotype_table_url (str): URL to the FinnGen phenotype table.\n        finngen_release_prefix (str): FinnGen release prefix.\n        finngen_summary_stats_url_prefix (str): FinnGen summary stats URL prefix.\n        finngen_summary_stats_url_suffix (str): FinnGen summary stats URL suffix.\n        efo_curation_mapping_url (str): URL to the EFO curation mapping file\n        sample_size (int): Number of individuals that participated in sample collection, derived from finngen release metadata.\n    \"\"\"\n    _match = FinnGenStudyIndex.validate_release_prefix(finngen_release_prefix)\n    release_prefix = _match[\"prefix\"]\n    release = _match[\"release\"]\n\n    efo_curation_df = FinnGenStudyIndex.read_efo_curation(\n        session.spark,\n        efo_curation_mapping_url,\n    )\n    study_index = FinnGenStudyIndex.from_source(\n        session.spark,\n        finngen_phenotype_table_url,\n        release_prefix,\n        finngen_summary_stats_url_prefix,\n        finngen_summary_stats_url_suffix,\n        sample_size,\n    )\n    study_index_with_efo = FinnGenStudyIndex.join_efo_mapping(\n        study_index,\n        efo_curation_df,\n        release,\n    )\n    study_index_with_efo.df.write.mode(session.write_mode).parquet(\n        finngen_study_index_out\n    )\n</code></pre>"},{"location":"python_api/steps/finngen_sumstat_preprocess/","title":"finngen_sumstat_preprocess","text":""},{"location":"python_api/steps/finngen_sumstat_preprocess/#gentropy.finngen_sumstat_preprocess.FinnGenSumstatPreprocessStep","title":"<code>gentropy.finngen_sumstat_preprocess.FinnGenSumstatPreprocessStep</code>","text":"<p>FinnGen sumstats preprocessing.</p> Source code in <code>src/gentropy/finngen_sumstat_preprocess.py</code> <pre><code>class FinnGenSumstatPreprocessStep:\n    \"\"\"FinnGen sumstats preprocessing.\"\"\"\n\n    def __init__(\n        self, session: Session, raw_sumstats_path: str, out_sumstats_path: str\n    ) -&gt; None:\n        \"\"\"Run FinnGen summary stats preprocessing step.\n\n        Args:\n            session (Session): Session object.\n            raw_sumstats_path (str): Input raw summary stats path.\n            out_sumstats_path (str): Output summary stats path.\n        \"\"\"\n        # Process summary stats.\n        (\n            FinnGenSummaryStats.from_source(session.spark, raw_file=raw_sumstats_path)\n            .df.write.mode(session.write_mode)\n            .parquet(out_sumstats_path)\n        )\n</code></pre>"},{"location":"python_api/steps/finngen_sumstat_preprocess/#gentropy.finngen_sumstat_preprocess.FinnGenSumstatPreprocessStep.__init__","title":"<code>__init__(session: Session, raw_sumstats_path: str, out_sumstats_path: str) -&gt; None</code>","text":"<p>Run FinnGen summary stats preprocessing step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>raw_sumstats_path</code> <code>str</code> <p>Input raw summary stats path.</p> required <code>out_sumstats_path</code> <code>str</code> <p>Output summary stats path.</p> required Source code in <code>src/gentropy/finngen_sumstat_preprocess.py</code> <pre><code>def __init__(\n    self, session: Session, raw_sumstats_path: str, out_sumstats_path: str\n) -&gt; None:\n    \"\"\"Run FinnGen summary stats preprocessing step.\n\n    Args:\n        session (Session): Session object.\n        raw_sumstats_path (str): Input raw summary stats path.\n        out_sumstats_path (str): Output summary stats path.\n    \"\"\"\n    # Process summary stats.\n    (\n        FinnGenSummaryStats.from_source(session.spark, raw_file=raw_sumstats_path)\n        .df.write.mode(session.write_mode)\n        .parquet(out_sumstats_path)\n    )\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_curation/","title":"gwas_catalog_study_curation","text":""},{"location":"python_api/steps/gwas_catalog_curation/#gentropy.gwas_catalog_study_curation.GWASCatalogStudyCurationStep","title":"<code>gentropy.gwas_catalog_study_curation.GWASCatalogStudyCurationStep</code>","text":"<p>Annotate GWAS Catalog studies with additional curation and create a curation backlog.</p> Source code in <code>src/gentropy/gwas_catalog_study_curation.py</code> <pre><code>class GWASCatalogStudyCurationStep:\n    \"\"\"Annotate GWAS Catalog studies with additional curation and create a curation backlog.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        catalog_study_files: list[str],\n        catalog_ancestry_files: list[str],\n        gwas_catalog_study_curation_out: str,\n        gwas_catalog_study_curation_file: str | None,\n    ) -&gt; None:\n        \"\"\"Run step to annotate and create backlog.\n\n        Args:\n            session (Session): Session object.\n            catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n            catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n            gwas_catalog_study_curation_out (str): Path for the updated curation table.\n            gwas_catalog_study_curation_file (str | None): Path to the original curation table. Optinal\n\n        Raises:\n            ValueError: If the curation file is provided but not a CSV file or URL.\n        \"\"\"\n        catalog_studies = session.spark.read.csv(\n            list(catalog_study_files), sep=\"\\t\", header=True\n        )\n        ancestry_lut = session.spark.read.csv(\n            list(catalog_ancestry_files), sep=\"\\t\", header=True\n        )\n\n        if gwas_catalog_study_curation_file:\n            if gwas_catalog_study_curation_file.endswith(\".csv\"):\n                gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_csv(\n                    session, gwas_catalog_study_curation_file\n                )\n            elif gwas_catalog_study_curation_file.startswith(\"http\"):\n                gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_url(\n                    session, gwas_catalog_study_curation_file\n                )\n            else:\n                raise ValueError(\n                    \"Only CSV files or URLs are accepted as curation file.\"\n                )\n\n        # Process GWAS Catalog studies and get list of studies for curation:\n        (\n            StudyIndexGWASCatalogParser.from_source(catalog_studies, ancestry_lut)\n            # Adding existing curation:\n            .annotate_from_study_curation(gwas_catalog_study_curation)\n            # Extract new studies for curation:\n            .extract_studies_for_curation(gwas_catalog_study_curation)\n            # Save table:\n            .toPandas()\n            .to_csv(gwas_catalog_study_curation_out, sep=\"\\t\", index=False)\n        )\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_curation/#gentropy.gwas_catalog_study_curation.GWASCatalogStudyCurationStep.__init__","title":"<code>__init__(session: Session, catalog_study_files: list[str], catalog_ancestry_files: list[str], gwas_catalog_study_curation_out: str, gwas_catalog_study_curation_file: str | None) -&gt; None</code>","text":"<p>Run step to annotate and create backlog.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>catalog_study_files</code> <code>list[str]</code> <p>List of raw GWAS catalog studies file.</p> required <code>catalog_ancestry_files</code> <code>list[str]</code> <p>List of raw ancestry annotations files from GWAS Catalog.</p> required <code>gwas_catalog_study_curation_out</code> <code>str</code> <p>Path for the updated curation table.</p> required <code>gwas_catalog_study_curation_file</code> <code>str | None</code> <p>Path to the original curation table. Optinal</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the curation file is provided but not a CSV file or URL.</p> Source code in <code>src/gentropy/gwas_catalog_study_curation.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    catalog_study_files: list[str],\n    catalog_ancestry_files: list[str],\n    gwas_catalog_study_curation_out: str,\n    gwas_catalog_study_curation_file: str | None,\n) -&gt; None:\n    \"\"\"Run step to annotate and create backlog.\n\n    Args:\n        session (Session): Session object.\n        catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n        catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n        gwas_catalog_study_curation_out (str): Path for the updated curation table.\n        gwas_catalog_study_curation_file (str | None): Path to the original curation table. Optinal\n\n    Raises:\n        ValueError: If the curation file is provided but not a CSV file or URL.\n    \"\"\"\n    catalog_studies = session.spark.read.csv(\n        list(catalog_study_files), sep=\"\\t\", header=True\n    )\n    ancestry_lut = session.spark.read.csv(\n        list(catalog_ancestry_files), sep=\"\\t\", header=True\n    )\n\n    if gwas_catalog_study_curation_file:\n        if gwas_catalog_study_curation_file.endswith(\".csv\"):\n            gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_csv(\n                session, gwas_catalog_study_curation_file\n            )\n        elif gwas_catalog_study_curation_file.startswith(\"http\"):\n            gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_url(\n                session, gwas_catalog_study_curation_file\n            )\n        else:\n            raise ValueError(\n                \"Only CSV files or URLs are accepted as curation file.\"\n            )\n\n    # Process GWAS Catalog studies and get list of studies for curation:\n    (\n        StudyIndexGWASCatalogParser.from_source(catalog_studies, ancestry_lut)\n        # Adding existing curation:\n        .annotate_from_study_curation(gwas_catalog_study_curation)\n        # Extract new studies for curation:\n        .extract_studies_for_curation(gwas_catalog_study_curation)\n        # Save table:\n        .toPandas()\n        .to_csv(gwas_catalog_study_curation_out, sep=\"\\t\", index=False)\n    )\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_study_index/","title":"gwas_catalog_study_inclusion","text":""},{"location":"python_api/steps/gwas_catalog_study_index/#gentropy.gwas_catalog_study_index.GWASCatalogStudyIndexGenerationStep","title":"<code>gentropy.gwas_catalog_study_index.GWASCatalogStudyIndexGenerationStep</code>","text":"<p>GWAS Catalog study index generation.</p> <p>This step generates a study index from the GWAS Catalog studies and ancestry files. It can also add additional curation information and summary statistics QC information when available.</p> <p>''' warning This step does not generate study index for gwas catalog top hits.</p> <p>This step provides several optional arguments to add additional information to the study index:</p> <ul> <li>gwas_catalog_study_curation_file: csv file or URL containing the curation table. If provided it annotates the study index with the additional curation information performed by the Open Targets team.</li> <li>sumstats_qc_path: Path to the summary statistics QC table. If provided it annotates the study index with the summary statistics QC information in the <code>sumStatQCValues</code> columns (e.g. <code>n_variants</code>, <code>n_variants_sig</code> etc.).</li> </ul> Source code in <code>src/gentropy/gwas_catalog_study_index.py</code> <pre><code>class GWASCatalogStudyIndexGenerationStep:\n    \"\"\"GWAS Catalog study index generation.\n\n    This step generates a study index from the GWAS Catalog studies and ancestry files. It can also add additional curation information and summary statistics QC information when available.\n\n    ''' warning\n    This step does not generate study index for gwas catalog top hits.\n\n    This step provides several optional arguments to add additional information to the study index:\n\n    - gwas_catalog_study_curation_file: csv file or URL containing the curation table. If provided it annotates the study index with the additional curation information performed by the Open Targets team.\n    - sumstats_qc_path: Path to the summary statistics QC table. If provided it annotates the study index with the summary statistics QC information in the `sumStatQCValues` columns (e.g. `n_variants`, `n_variants_sig` etc.).\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        catalog_study_files: list[str],\n        catalog_ancestry_files: list[str],\n        study_index_path: str,\n        gwas_catalog_study_curation_file: str | None = None,\n        sumstats_qc_path: str | None = None,\n    ) -&gt; None:\n        \"\"\"Run step.\n\n        Args:\n            session (Session): Session objecct.\n            catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n            catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n            study_index_path (str): Output GWAS catalog studies path.\n            gwas_catalog_study_curation_file (str | None): csv file or URL containing the curation table. Optional.\n            sumstats_qc_path (str | None): Path to the summary statistics QC table. Optional.\n\n        Raises:\n            ValueError: If the curation file is provided but not a CSV file or URL.\n        \"\"\"\n        # Core Study Index Generation:\n        study_index = StudyIndexGWASCatalogParser.from_source(\n            session.spark.read.csv(list(catalog_study_files), sep=\"\\t\", header=True),\n            session.spark.read.csv(list(catalog_ancestry_files), sep=\"\\t\", header=True),\n        )\n\n        # Annotate with curation if provided:\n        if gwas_catalog_study_curation_file:\n            if gwas_catalog_study_curation_file.endswith(\n                \".tsv\"\n            ) | gwas_catalog_study_curation_file.endswith(\".tsv\"):\n                gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_csv(\n                    session, gwas_catalog_study_curation_file\n                )\n            elif gwas_catalog_study_curation_file.startswith(\"http\"):\n                gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_url(\n                    session, gwas_catalog_study_curation_file\n                )\n            else:\n                raise ValueError(\n                    \"Only CSV/TSV files or URLs are accepted as curation file.\"\n                )\n            study_index = study_index.annotate_from_study_curation(\n                gwas_catalog_study_curation\n            )\n\n        # Annotate with sumstats QC if provided:\n        if sumstats_qc_path:\n            schema = StructType(\n                [\n                    StructField(\"studyId\", StringType(), True),\n                    StructField(\"mean_beta\", DoubleType(), True),\n                    StructField(\"mean_diff_pz\", DoubleType(), True),\n                    StructField(\"se_diff_pz\", DoubleType(), True),\n                    StructField(\"gc_lambda\", DoubleType(), True),\n                    StructField(\"n_variants\", LongType(), True),\n                    StructField(\"n_variants_sig\", LongType(), True),\n                ]\n            )\n            sumstats_qc = session.spark.read.schema(schema).parquet(\n                sumstats_qc_path, recursiveFileLookup=True\n            )\n            study_index_with_qc = study_index.annotate_sumstats_qc(sumstats_qc)\n\n            # Write the study\n            study_index_with_qc.df.write.mode(session.write_mode).parquet(\n                study_index_path\n            )\n        else:\n            study_index.df.write.mode(session.write_mode).parquet(study_index_path)\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_study_index/#gentropy.gwas_catalog_study_index.GWASCatalogStudyIndexGenerationStep.__init__","title":"<code>__init__(session: Session, catalog_study_files: list[str], catalog_ancestry_files: list[str], study_index_path: str, gwas_catalog_study_curation_file: str | None = None, sumstats_qc_path: str | None = None) -&gt; None</code>","text":"<p>Run step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session objecct.</p> required <code>catalog_study_files</code> <code>list[str]</code> <p>List of raw GWAS catalog studies file.</p> required <code>catalog_ancestry_files</code> <code>list[str]</code> <p>List of raw ancestry annotations files from GWAS Catalog.</p> required <code>study_index_path</code> <code>str</code> <p>Output GWAS catalog studies path.</p> required <code>gwas_catalog_study_curation_file</code> <code>str | None</code> <p>csv file or URL containing the curation table. Optional.</p> <code>None</code> <code>sumstats_qc_path</code> <code>str | None</code> <p>Path to the summary statistics QC table. Optional.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the curation file is provided but not a CSV file or URL.</p> Source code in <code>src/gentropy/gwas_catalog_study_index.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    catalog_study_files: list[str],\n    catalog_ancestry_files: list[str],\n    study_index_path: str,\n    gwas_catalog_study_curation_file: str | None = None,\n    sumstats_qc_path: str | None = None,\n) -&gt; None:\n    \"\"\"Run step.\n\n    Args:\n        session (Session): Session objecct.\n        catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n        catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n        study_index_path (str): Output GWAS catalog studies path.\n        gwas_catalog_study_curation_file (str | None): csv file or URL containing the curation table. Optional.\n        sumstats_qc_path (str | None): Path to the summary statistics QC table. Optional.\n\n    Raises:\n        ValueError: If the curation file is provided but not a CSV file or URL.\n    \"\"\"\n    # Core Study Index Generation:\n    study_index = StudyIndexGWASCatalogParser.from_source(\n        session.spark.read.csv(list(catalog_study_files), sep=\"\\t\", header=True),\n        session.spark.read.csv(list(catalog_ancestry_files), sep=\"\\t\", header=True),\n    )\n\n    # Annotate with curation if provided:\n    if gwas_catalog_study_curation_file:\n        if gwas_catalog_study_curation_file.endswith(\n            \".tsv\"\n        ) | gwas_catalog_study_curation_file.endswith(\".tsv\"):\n            gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_csv(\n                session, gwas_catalog_study_curation_file\n            )\n        elif gwas_catalog_study_curation_file.startswith(\"http\"):\n            gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_url(\n                session, gwas_catalog_study_curation_file\n            )\n        else:\n            raise ValueError(\n                \"Only CSV/TSV files or URLs are accepted as curation file.\"\n            )\n        study_index = study_index.annotate_from_study_curation(\n            gwas_catalog_study_curation\n        )\n\n    # Annotate with sumstats QC if provided:\n    if sumstats_qc_path:\n        schema = StructType(\n            [\n                StructField(\"studyId\", StringType(), True),\n                StructField(\"mean_beta\", DoubleType(), True),\n                StructField(\"mean_diff_pz\", DoubleType(), True),\n                StructField(\"se_diff_pz\", DoubleType(), True),\n                StructField(\"gc_lambda\", DoubleType(), True),\n                StructField(\"n_variants\", LongType(), True),\n                StructField(\"n_variants_sig\", LongType(), True),\n            ]\n        )\n        sumstats_qc = session.spark.read.schema(schema).parquet(\n            sumstats_qc_path, recursiveFileLookup=True\n        )\n        study_index_with_qc = study_index.annotate_sumstats_qc(sumstats_qc)\n\n        # Write the study\n        study_index_with_qc.df.write.mode(session.write_mode).parquet(\n            study_index_path\n        )\n    else:\n        study_index.df.write.mode(session.write_mode).parquet(study_index_path)\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_sumstat_preprocess/","title":"gwas_catalog_sumstat_preprocess","text":""},{"location":"python_api/steps/gwas_catalog_sumstat_preprocess/#gentropy.gwas_catalog_sumstat_preprocess.GWASCatalogSumstatsPreprocessStep","title":"<code>gentropy.gwas_catalog_sumstat_preprocess.GWASCatalogSumstatsPreprocessStep</code>","text":"<p>Step to preprocess GWAS Catalog harmonised summary stats.</p> <p>It additionally performs sanity filter of GWAS before saving it.</p> Source code in <code>src/gentropy/gwas_catalog_sumstat_preprocess.py</code> <pre><code>class GWASCatalogSumstatsPreprocessStep:\n    \"\"\"Step to preprocess GWAS Catalog harmonised summary stats.\n\n    It additionally performs sanity filter of GWAS before saving it.\n    \"\"\"\n\n    def __init__(\n        self, session: Session, raw_sumstats_path: str, out_sumstats_path: str\n    ) -&gt; None:\n        \"\"\"Run step to preprocess GWAS Catalog harmonised summary stats and produce SummaryStatistics dataset.\n\n        Args:\n            session (Session): Session object.\n            raw_sumstats_path (str): Input GWAS Catalog harmonised summary stats path.\n            out_sumstats_path (str): Output SummaryStatistics dataset path.\n        \"\"\"\n        # Processing dataset:\n        GWASCatalogSummaryStatistics.from_gwas_harmonized_summary_stats(\n            session.spark, raw_sumstats_path\n        ).sanity_filter().df.write.mode(session.write_mode).parquet(out_sumstats_path)\n        session.logger.info(\"Processing dataset successfully completed.\")\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_sumstat_preprocess/#gentropy.gwas_catalog_sumstat_preprocess.GWASCatalogSumstatsPreprocessStep.__init__","title":"<code>__init__(session: Session, raw_sumstats_path: str, out_sumstats_path: str) -&gt; None</code>","text":"<p>Run step to preprocess GWAS Catalog harmonised summary stats and produce SummaryStatistics dataset.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>raw_sumstats_path</code> <code>str</code> <p>Input GWAS Catalog harmonised summary stats path.</p> required <code>out_sumstats_path</code> <code>str</code> <p>Output SummaryStatistics dataset path.</p> required Source code in <code>src/gentropy/gwas_catalog_sumstat_preprocess.py</code> <pre><code>def __init__(\n    self, session: Session, raw_sumstats_path: str, out_sumstats_path: str\n) -&gt; None:\n    \"\"\"Run step to preprocess GWAS Catalog harmonised summary stats and produce SummaryStatistics dataset.\n\n    Args:\n        session (Session): Session object.\n        raw_sumstats_path (str): Input GWAS Catalog harmonised summary stats path.\n        out_sumstats_path (str): Output SummaryStatistics dataset path.\n    \"\"\"\n    # Processing dataset:\n    GWASCatalogSummaryStatistics.from_gwas_harmonized_summary_stats(\n        session.spark, raw_sumstats_path\n    ).sanity_filter().df.write.mode(session.write_mode).parquet(out_sumstats_path)\n    session.logger.info(\"Processing dataset successfully completed.\")\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_top_hits/","title":"GWAS Catalog Top Hits Ingestion Step","text":""},{"location":"python_api/steps/gwas_catalog_top_hits/#gentropy.gwas_catalog_top_hits.GWASCatalogTopHitIngestionStep","title":"<code>gentropy.gwas_catalog_top_hits.GWASCatalogTopHitIngestionStep</code>","text":"<p>GWAS Catalog ingestion step to extract GWASCatalog top hits.</p> Source code in <code>src/gentropy/gwas_catalog_top_hits.py</code> <pre><code>class GWASCatalogTopHitIngestionStep:\n    \"\"\"GWAS Catalog ingestion step to extract GWASCatalog top hits.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        catalog_study_files: list[str],\n        catalog_ancestry_files: list[str],\n        catalog_associations_file: str,\n        variant_annotation_path: str,\n        catalog_studies_out: str,\n        catalog_associations_out: str,\n        distance: int = WindowBasedClumpingStepConfig().distance,\n    ) -&gt; None:\n        \"\"\"Run step.\n\n        Args:\n            session (Session): Session object.\n            catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n            catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n            catalog_associations_file (str): Raw GWAS catalog associations file.\n            variant_annotation_path (str): Path to GnomAD variants.\n            catalog_studies_out (str): Output GWAS catalog studies path.\n            catalog_associations_out (str): Output GWAS catalog associations path.\n            distance (int): Distance, within which tagging variants are collected around the semi-index.\n        \"\"\"\n        # Extract\n        gnomad_variants = VariantIndex.from_parquet(session, variant_annotation_path)\n        catalog_studies = session.spark.read.csv(\n            list(catalog_study_files), sep=\"\\t\", header=True\n        )\n        ancestry_lut = session.spark.read.csv(\n            list(catalog_ancestry_files), sep=\"\\t\", header=True\n        )\n        catalog_associations = session.spark.read.csv(\n            catalog_associations_file, sep=\"\\t\", header=True\n        ).persist()\n\n        # Transform\n        study_index, study_locus = GWASCatalogStudySplitter.split(\n            StudyIndexGWASCatalogParser.from_source(catalog_studies, ancestry_lut),\n            GWASCatalogCuratedAssociationsParser.from_source(\n                catalog_associations, gnomad_variants\n            ),\n        )\n        # Load\n        (\n            study_index\n            # Flag all studies without sumstats\n            .add_no_sumstats_flag()\n            # Save dataset:\n            .df.write.mode(session.write_mode)\n            .parquet(catalog_studies_out)\n        )\n\n        (\n            study_locus.window_based_clumping(distance)\n            .df.write.mode(session.write_mode)\n            .parquet(catalog_associations_out)\n        )\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_top_hits/#gentropy.gwas_catalog_top_hits.GWASCatalogTopHitIngestionStep.__init__","title":"<code>__init__(session: Session, catalog_study_files: list[str], catalog_ancestry_files: list[str], catalog_associations_file: str, variant_annotation_path: str, catalog_studies_out: str, catalog_associations_out: str, distance: int = WindowBasedClumpingStepConfig().distance) -&gt; None</code>","text":"<p>Run step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>catalog_study_files</code> <code>list[str]</code> <p>List of raw GWAS catalog studies file.</p> required <code>catalog_ancestry_files</code> <code>list[str]</code> <p>List of raw ancestry annotations files from GWAS Catalog.</p> required <code>catalog_associations_file</code> <code>str</code> <p>Raw GWAS catalog associations file.</p> required <code>variant_annotation_path</code> <code>str</code> <p>Path to GnomAD variants.</p> required <code>catalog_studies_out</code> <code>str</code> <p>Output GWAS catalog studies path.</p> required <code>catalog_associations_out</code> <code>str</code> <p>Output GWAS catalog associations path.</p> required <code>distance</code> <code>int</code> <p>Distance, within which tagging variants are collected around the semi-index.</p> <code>distance</code> Source code in <code>src/gentropy/gwas_catalog_top_hits.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    catalog_study_files: list[str],\n    catalog_ancestry_files: list[str],\n    catalog_associations_file: str,\n    variant_annotation_path: str,\n    catalog_studies_out: str,\n    catalog_associations_out: str,\n    distance: int = WindowBasedClumpingStepConfig().distance,\n) -&gt; None:\n    \"\"\"Run step.\n\n    Args:\n        session (Session): Session object.\n        catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n        catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n        catalog_associations_file (str): Raw GWAS catalog associations file.\n        variant_annotation_path (str): Path to GnomAD variants.\n        catalog_studies_out (str): Output GWAS catalog studies path.\n        catalog_associations_out (str): Output GWAS catalog associations path.\n        distance (int): Distance, within which tagging variants are collected around the semi-index.\n    \"\"\"\n    # Extract\n    gnomad_variants = VariantIndex.from_parquet(session, variant_annotation_path)\n    catalog_studies = session.spark.read.csv(\n        list(catalog_study_files), sep=\"\\t\", header=True\n    )\n    ancestry_lut = session.spark.read.csv(\n        list(catalog_ancestry_files), sep=\"\\t\", header=True\n    )\n    catalog_associations = session.spark.read.csv(\n        catalog_associations_file, sep=\"\\t\", header=True\n    ).persist()\n\n    # Transform\n    study_index, study_locus = GWASCatalogStudySplitter.split(\n        StudyIndexGWASCatalogParser.from_source(catalog_studies, ancestry_lut),\n        GWASCatalogCuratedAssociationsParser.from_source(\n            catalog_associations, gnomad_variants\n        ),\n    )\n    # Load\n    (\n        study_index\n        # Flag all studies without sumstats\n        .add_no_sumstats_flag()\n        # Save dataset:\n        .df.write.mode(session.write_mode)\n        .parquet(catalog_studies_out)\n    )\n\n    (\n        study_locus.window_based_clumping(distance)\n        .df.write.mode(session.write_mode)\n        .parquet(catalog_associations_out)\n    )\n</code></pre>"},{"location":"python_api/steps/l2g/","title":"Locus to Gene (L2G)","text":""},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneFeatureMatrixStep","title":"<code>gentropy.l2g.LocusToGeneFeatureMatrixStep</code>","text":"<p>Annotate credible set with functional genomics features.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>class LocusToGeneFeatureMatrixStep:\n    \"\"\"Annotate credible set with functional genomics features.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        *,\n        features_list: list[str],\n        credible_set_path: str,\n        variant_index_path: str | None = None,\n        colocalisation_path: str | None = None,\n        study_index_path: str | None = None,\n        target_index_path: str | None = None,\n        feature_matrix_path: str,\n    ) -&gt; None:\n        \"\"\"Initialise the step and run the logic based on mode.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            features_list (list[str]): List of features to use for the model\n            credible_set_path (str): Path to the credible set dataset necessary to build the feature matrix\n            variant_index_path (str | None): Path to the variant index dataset\n            colocalisation_path (str | None): Path to the colocalisation dataset\n            study_index_path (str | None): Path to the study index dataset\n            target_index_path (str | None): Path to the target index dataset\n            feature_matrix_path (str): Path to the L2G feature matrix output dataset\n        \"\"\"\n        credible_set = StudyLocus.from_parquet(\n            session, credible_set_path, recursiveFileLookup=True\n        )\n        studies = (\n            StudyIndex.from_parquet(session, study_index_path, recursiveFileLookup=True)\n            if study_index_path\n            else None\n        )\n        variant_index = (\n            VariantIndex.from_parquet(session, variant_index_path)\n            if variant_index_path\n            else None\n        )\n        coloc = (\n            Colocalisation.from_parquet(\n                session, colocalisation_path, recursiveFileLookup=True\n            )\n            if colocalisation_path\n            else None\n        )\n        target_index = (\n            TargetIndex.from_parquet(\n                session, target_index_path, recursiveFileLookup=True\n            )\n            if target_index_path\n            else None\n        )\n        features_input_loader = L2GFeatureInputLoader(\n            variant_index=variant_index,\n            colocalisation=coloc,\n            study_index=studies,\n            study_locus=credible_set,\n            target_index=target_index,\n        )\n\n        fm = credible_set.filter(f.col(\"studyType\") == \"gwas\").build_feature_matrix(\n            features_list, features_input_loader\n        )\n        fm._df.coalesce(session.output_partitions).write.mode(\n            session.write_mode\n        ).parquet(feature_matrix_path)\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneFeatureMatrixStep.__init__","title":"<code>__init__(session: Session, *, features_list: list[str], credible_set_path: str, variant_index_path: str | None = None, colocalisation_path: str | None = None, study_index_path: str | None = None, target_index_path: str | None = None, feature_matrix_path: str) -&gt; None</code>","text":"<p>Initialise the step and run the logic based on mode.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>features_list</code> <code>list[str]</code> <p>List of features to use for the model</p> required <code>credible_set_path</code> <code>str</code> <p>Path to the credible set dataset necessary to build the feature matrix</p> required <code>variant_index_path</code> <code>str | None</code> <p>Path to the variant index dataset</p> <code>None</code> <code>colocalisation_path</code> <code>str | None</code> <p>Path to the colocalisation dataset</p> <code>None</code> <code>study_index_path</code> <code>str | None</code> <p>Path to the study index dataset</p> <code>None</code> <code>target_index_path</code> <code>str | None</code> <p>Path to the target index dataset</p> <code>None</code> <code>feature_matrix_path</code> <code>str</code> <p>Path to the L2G feature matrix output dataset</p> required Source code in <code>src/gentropy/l2g.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    *,\n    features_list: list[str],\n    credible_set_path: str,\n    variant_index_path: str | None = None,\n    colocalisation_path: str | None = None,\n    study_index_path: str | None = None,\n    target_index_path: str | None = None,\n    feature_matrix_path: str,\n) -&gt; None:\n    \"\"\"Initialise the step and run the logic based on mode.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        features_list (list[str]): List of features to use for the model\n        credible_set_path (str): Path to the credible set dataset necessary to build the feature matrix\n        variant_index_path (str | None): Path to the variant index dataset\n        colocalisation_path (str | None): Path to the colocalisation dataset\n        study_index_path (str | None): Path to the study index dataset\n        target_index_path (str | None): Path to the target index dataset\n        feature_matrix_path (str): Path to the L2G feature matrix output dataset\n    \"\"\"\n    credible_set = StudyLocus.from_parquet(\n        session, credible_set_path, recursiveFileLookup=True\n    )\n    studies = (\n        StudyIndex.from_parquet(session, study_index_path, recursiveFileLookup=True)\n        if study_index_path\n        else None\n    )\n    variant_index = (\n        VariantIndex.from_parquet(session, variant_index_path)\n        if variant_index_path\n        else None\n    )\n    coloc = (\n        Colocalisation.from_parquet(\n            session, colocalisation_path, recursiveFileLookup=True\n        )\n        if colocalisation_path\n        else None\n    )\n    target_index = (\n        TargetIndex.from_parquet(\n            session, target_index_path, recursiveFileLookup=True\n        )\n        if target_index_path\n        else None\n    )\n    features_input_loader = L2GFeatureInputLoader(\n        variant_index=variant_index,\n        colocalisation=coloc,\n        study_index=studies,\n        study_locus=credible_set,\n        target_index=target_index,\n    )\n\n    fm = credible_set.filter(f.col(\"studyType\") == \"gwas\").build_feature_matrix(\n        features_list, features_input_loader\n    )\n    fm._df.coalesce(session.output_partitions).write.mode(\n        session.write_mode\n    ).parquet(feature_matrix_path)\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep","title":"<code>gentropy.l2g.LocusToGeneStep</code>","text":"<p>Locus to gene step.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>class LocusToGeneStep:\n    \"\"\"Locus to gene step.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        *,\n        run_mode: str,\n        hyperparameters: dict[str, Any],\n        download_from_hub: bool,\n        cross_validate: bool,\n        wandb_run_name: str,\n        credible_set_path: str,\n        feature_matrix_path: str,\n        model_path: str | None = None,\n        features_list: list[str] | None = None,\n        gold_standard_curation_path: str | None = None,\n        variant_index_path: str | None = None,\n        gene_interactions_path: str | None = None,\n        predictions_path: str | None = None,\n        l2g_threshold: float | None = None,\n        hf_hub_repo_id: str | None = None,\n        hf_model_commit_message: str | None = \"chore: update model\",\n        hf_model_version: str | None = None,\n        explain_predictions: bool | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the step and run the logic based on mode.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            run_mode (str): Run mode, either 'train' or 'predict'\n            hyperparameters (dict[str, Any]): Hyperparameters for the model\n            download_from_hub (bool): Whether to download the model from Hugging Face Hub\n            cross_validate (bool): Whether to run cross validation (5-fold by default) to train the model.\n            wandb_run_name (str): Name of the run to track model training in Weights and Biases\n            credible_set_path (str): Path to the credible set dataset necessary to build the feature matrix\n            feature_matrix_path (str): Path to the L2G feature matrix input dataset\n            model_path (str | None): Path to the model. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).\n            features_list (list[str] | None): List of features to use to train the model\n            gold_standard_curation_path (str | None): Path to the gold standard curation file\n            variant_index_path (str | None): Path to the variant index\n            gene_interactions_path (str | None): Path to the gene interactions dataset\n            predictions_path (str | None): Path to the L2G predictions output dataset\n            l2g_threshold (float | None): An optional threshold for the L2G score to filter predictions. A threshold of 0.05 is recommended.\n            hf_hub_repo_id (str | None): Hugging Face Hub repository ID. If provided, the model will be uploaded to Hugging Face.\n            hf_model_commit_message (str | None): Commit message when we upload the model to the Hugging Face Hub\n            hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n            explain_predictions (bool | None): Whether to extract SHAP importances for the L2G predictions. This is computationally expensive.\n\n        Raises:\n            ValueError: If run_mode is not 'train' or 'predict'\n        \"\"\"\n        if run_mode not in [\"train\", \"predict\"]:\n            raise ValueError(\n                f\"run_mode must be one of 'train' or 'predict', got {run_mode}\"\n            )\n\n        self.session = session\n        self.run_mode = run_mode\n        self.predictions_path = predictions_path\n        self.features_list = list(features_list) if features_list else None\n        self.hyperparameters = dict(hyperparameters)\n        self.wandb_run_name = wandb_run_name\n        self.cross_validate = cross_validate\n        self.hf_hub_repo_id = hf_hub_repo_id\n        self.download_from_hub = download_from_hub\n        self.hf_model_commit_message = hf_model_commit_message\n        self.l2g_threshold = l2g_threshold or 0.0\n        self.gold_standard_curation_path = gold_standard_curation_path\n        self.gene_interactions_path = gene_interactions_path\n        self.variant_index_path = variant_index_path\n        self.model_path = (\n            hf_hub_repo_id\n            if not model_path and download_from_hub and hf_hub_repo_id\n            else model_path\n        )\n        self.hf_model_version = hf_model_version\n        self.explain_predictions = explain_predictions\n\n        # Load common inputs\n        self.credible_set = StudyLocus.from_parquet(\n            session, credible_set_path, recursiveFileLookup=True\n        )\n        self.feature_matrix = L2GFeatureMatrix(\n            _df=session.load_data(feature_matrix_path),\n        )\n\n        if run_mode == \"predict\":\n            self.run_predict()\n        elif run_mode == \"train\":\n            self.gold_standard = self.prepare_gold_standard()\n            self.run_train()\n\n    def prepare_gold_standard(self) -&gt; L2GGoldStandard:\n        \"\"\"Prepare the gold standard for training.\n\n        Returns:\n            L2GGoldStandard: training dataset.\n\n        Raises:\n            ValueError: When gold standard path, is not provided, or when\n                parsing OTG gold standard but missing interactions and variant index paths.\n            TypeError: When gold standard is not OTG gold standard nor L2GGoldStandard.\n\n        \"\"\"\n        if self.gold_standard_curation_path is None:\n            raise ValueError(\"Gold Standard is required for model training.\")\n        # Read the gold standard either from json or parquet, default to parquet if can not infer the format from extension.\n        ext = self.gold_standard_curation_path.split(\".\")[-1]\n        ext = \"parquet\" if ext not in [\"parquet\", \"json\"] else ext\n        gold_standard = self.session.load_data(self.gold_standard_curation_path, ext)\n        schema_issues = compare_struct_schemas(\n            gold_standard.schema, L2GGoldStandard.get_schema()\n        )\n        # Parse the gold standard depending on the input schema\n        match schema_issues:\n            case {**extra} if not extra:\n                # Schema is the same as L2GGoldStandard - load the GS\n                # NOTE: match to empty dict will be non-selective\n                # see https://stackoverflow.com/questions/75389166/how-to-match-an-empty-dictionary\n                logging.info(\"Successfully parsed gold standard.\")\n                return L2GGoldStandard(\n                    _df=gold_standard,\n                    _schema=L2GGoldStandard.get_schema(),\n                )\n            case {\n                \"missing_mandatory_columns\": [\n                    \"studyLocusId\",\n                    \"variantId\",\n                    \"studyId\",\n                    \"geneId\",\n                    \"goldStandardSet\",\n                ],\n                \"unexpected_columns\": [\n                    \"association_info\",\n                    \"gold_standard_info\",\n                    \"metadata\",\n                    \"sentinel_variant\",\n                    \"trait_info\",\n                ],\n            }:\n                # There are schema mismatches, this would mean that we have\n                logging.info(\"Detected OTG Gold Standard. Attempting to parse it.\")\n                otg_curation = gold_standard\n                if self.gene_interactions_path is None:\n                    raise ValueError(\"Interactions are required for parsing curation.\")\n                if self.variant_index_path is None:\n                    raise ValueError(\"Variant Index are required for parsing curation.\")\n\n                interactions = self.session.load_data(\n                    self.gene_interactions_path, \"parquet\"\n                )\n                variant_index = VariantIndex.from_parquet(\n                    self.session, self.variant_index_path\n                )\n                study_locus_overlap = StudyLocus(\n                    _df=self.credible_set.df.join(\n                        otg_curation.select(\n                            f.concat_ws(\n                                \"_\",\n                                f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n                                f.col(\"sentinel_variant.locus_GRCh38.position\"),\n                                f.col(\"sentinel_variant.alleles.reference\"),\n                                f.col(\"sentinel_variant.alleles.alternative\"),\n                            ).alias(\"variantId\"),\n                            f.col(\"association_info.otg_id\").alias(\"studyId\"),\n                        ),\n                        on=[\n                            \"studyId\",\n                            \"variantId\",\n                        ],\n                        how=\"inner\",\n                    ),\n                    _schema=StudyLocus.get_schema(),\n                ).find_overlaps()\n\n                return L2GGoldStandard.from_otg_curation(\n                    gold_standard_curation=otg_curation,\n                    variant_index=variant_index,\n                    study_locus_overlap=study_locus_overlap,\n                    interactions=interactions,\n                )\n            case _:\n                raise TypeError(\"Incorrect gold standard dataset provided.\")\n\n    def run_predict(self) -&gt; None:\n        \"\"\"Run the prediction step.\n\n        Raises:\n            ValueError: If predictions_path is not provided for prediction mode\n        \"\"\"\n        if not self.predictions_path:\n            raise ValueError(\"predictions_path must be provided for prediction mode\")\n        predictions = (\n            L2GPrediction.from_credible_set(\n                self.session,\n                self.credible_set,\n                self.feature_matrix,\n                model_path=self.model_path,\n                features_list=self.features_list,\n                hf_token=access_gcp_secret(\"hfhub-key\", \"open-targets-genetics-dev\"),\n                hf_model_version=self.hf_model_version,\n                download_from_hub=self.download_from_hub,\n            )\n            .filter(f.col(\"score\") &gt;= self.l2g_threshold)\n            .add_features(\n                self.feature_matrix,\n            )\n        )\n        if self.explain_predictions:\n            predictions = predictions.explain()\n        predictions.df.coalesce(self.session.output_partitions).write.mode(\n            self.session.write_mode\n        ).parquet(self.predictions_path)\n        self.session.logger.info(\"L2G predictions saved successfully.\")\n\n    def run_train(self) -&gt; None:\n        \"\"\"Run the training step.\n\n        Raises:\n            ValueError: If features list is not provided for model training.\n        \"\"\"\n        if self.features_list is None:\n            raise ValueError(\"Features list is required for model training.\")\n        # Initialize access to weights and biases\n        wandb_key = access_gcp_secret(\"wandb-key\", \"open-targets-genetics-dev\")\n        wandb_login(key=wandb_key)\n\n        # Instantiate classifier and train model\n        l2g_model = LocusToGeneModel(\n            model=GradientBoostingClassifier(random_state=42, loss=\"log_loss\"),\n            hyperparameters=self.hyperparameters,\n            features_list=self.features_list,\n        )\n\n        # Calculate the gold standard features\n        feature_matrix = self._annotate_gold_standards_w_feature_matrix()\n\n        # Run the training\n        trained_model = LocusToGeneTrainer(\n            model=l2g_model, feature_matrix=feature_matrix\n        ).train(self.wandb_run_name, cross_validate=self.cross_validate)\n\n        # Export the model\n        if trained_model.training_data and trained_model.model and self.model_path:\n            trained_model.save(self.model_path)\n            if self.hf_hub_repo_id and self.hf_model_commit_message:\n                hf_hub_token = access_gcp_secret(\n                    \"hfhub-key\", \"open-targets-genetics-dev\"\n                )\n                trained_model.export_to_hugging_face_hub(\n                    # we upload the model saved in the filesystem\n                    self.model_path.split(\"/\")[-1],\n                    hf_hub_token,\n                    data=trained_model.training_data._df.toPandas(),\n                    repo_id=self.hf_hub_repo_id,\n                    commit_message=self.hf_model_commit_message,\n                )\n\n    def _annotate_gold_standards_w_feature_matrix(self) -&gt; L2GFeatureMatrix:\n        \"\"\"Generate the feature matrix of annotated gold standards.\n\n        Returns:\n            L2GFeatureMatrix: Feature matrix with gold standards annotated with features.\n        \"\"\"\n        return (\n            self.gold_standard.build_feature_matrix(\n                self.feature_matrix, self.credible_set\n            )\n            .select_features(self.features_list)\n            .persist()\n        )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep.__init__","title":"<code>__init__(session: Session, *, run_mode: str, hyperparameters: dict[str, Any], download_from_hub: bool, cross_validate: bool, wandb_run_name: str, credible_set_path: str, feature_matrix_path: str, model_path: str | None = None, features_list: list[str] | None = None, gold_standard_curation_path: str | None = None, variant_index_path: str | None = None, gene_interactions_path: str | None = None, predictions_path: str | None = None, l2g_threshold: float | None = None, hf_hub_repo_id: str | None = None, hf_model_commit_message: str | None = 'chore: update model', hf_model_version: str | None = None, explain_predictions: bool | None = None) -&gt; None</code>","text":"<p>Initialise the step and run the logic based on mode.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>run_mode</code> <code>str</code> <p>Run mode, either 'train' or 'predict'</p> required <code>hyperparameters</code> <code>dict[str, Any]</code> <p>Hyperparameters for the model</p> required <code>download_from_hub</code> <code>bool</code> <p>Whether to download the model from Hugging Face Hub</p> required <code>cross_validate</code> <code>bool</code> <p>Whether to run cross validation (5-fold by default) to train the model.</p> required <code>wandb_run_name</code> <code>str</code> <p>Name of the run to track model training in Weights and Biases</p> required <code>credible_set_path</code> <code>str</code> <p>Path to the credible set dataset necessary to build the feature matrix</p> required <code>feature_matrix_path</code> <code>str</code> <p>Path to the L2G feature matrix input dataset</p> required <code>model_path</code> <code>str | None</code> <p>Path to the model. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).</p> <code>None</code> <code>features_list</code> <code>list[str] | None</code> <p>List of features to use to train the model</p> <code>None</code> <code>gold_standard_curation_path</code> <code>str | None</code> <p>Path to the gold standard curation file</p> <code>None</code> <code>variant_index_path</code> <code>str | None</code> <p>Path to the variant index</p> <code>None</code> <code>gene_interactions_path</code> <code>str | None</code> <p>Path to the gene interactions dataset</p> <code>None</code> <code>predictions_path</code> <code>str | None</code> <p>Path to the L2G predictions output dataset</p> <code>None</code> <code>l2g_threshold</code> <code>float | None</code> <p>An optional threshold for the L2G score to filter predictions. A threshold of 0.05 is recommended.</p> <code>None</code> <code>hf_hub_repo_id</code> <code>str | None</code> <p>Hugging Face Hub repository ID. If provided, the model will be uploaded to Hugging Face.</p> <code>None</code> <code>hf_model_commit_message</code> <code>str | None</code> <p>Commit message when we upload the model to the Hugging Face Hub</p> <code>'chore: update model'</code> <code>hf_model_version</code> <code>str | None</code> <p>Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.</p> <code>None</code> <code>explain_predictions</code> <code>bool | None</code> <p>Whether to extract SHAP importances for the L2G predictions. This is computationally expensive.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run_mode is not 'train' or 'predict'</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    *,\n    run_mode: str,\n    hyperparameters: dict[str, Any],\n    download_from_hub: bool,\n    cross_validate: bool,\n    wandb_run_name: str,\n    credible_set_path: str,\n    feature_matrix_path: str,\n    model_path: str | None = None,\n    features_list: list[str] | None = None,\n    gold_standard_curation_path: str | None = None,\n    variant_index_path: str | None = None,\n    gene_interactions_path: str | None = None,\n    predictions_path: str | None = None,\n    l2g_threshold: float | None = None,\n    hf_hub_repo_id: str | None = None,\n    hf_model_commit_message: str | None = \"chore: update model\",\n    hf_model_version: str | None = None,\n    explain_predictions: bool | None = None,\n) -&gt; None:\n    \"\"\"Initialise the step and run the logic based on mode.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        run_mode (str): Run mode, either 'train' or 'predict'\n        hyperparameters (dict[str, Any]): Hyperparameters for the model\n        download_from_hub (bool): Whether to download the model from Hugging Face Hub\n        cross_validate (bool): Whether to run cross validation (5-fold by default) to train the model.\n        wandb_run_name (str): Name of the run to track model training in Weights and Biases\n        credible_set_path (str): Path to the credible set dataset necessary to build the feature matrix\n        feature_matrix_path (str): Path to the L2G feature matrix input dataset\n        model_path (str | None): Path to the model. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).\n        features_list (list[str] | None): List of features to use to train the model\n        gold_standard_curation_path (str | None): Path to the gold standard curation file\n        variant_index_path (str | None): Path to the variant index\n        gene_interactions_path (str | None): Path to the gene interactions dataset\n        predictions_path (str | None): Path to the L2G predictions output dataset\n        l2g_threshold (float | None): An optional threshold for the L2G score to filter predictions. A threshold of 0.05 is recommended.\n        hf_hub_repo_id (str | None): Hugging Face Hub repository ID. If provided, the model will be uploaded to Hugging Face.\n        hf_model_commit_message (str | None): Commit message when we upload the model to the Hugging Face Hub\n        hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n        explain_predictions (bool | None): Whether to extract SHAP importances for the L2G predictions. This is computationally expensive.\n\n    Raises:\n        ValueError: If run_mode is not 'train' or 'predict'\n    \"\"\"\n    if run_mode not in [\"train\", \"predict\"]:\n        raise ValueError(\n            f\"run_mode must be one of 'train' or 'predict', got {run_mode}\"\n        )\n\n    self.session = session\n    self.run_mode = run_mode\n    self.predictions_path = predictions_path\n    self.features_list = list(features_list) if features_list else None\n    self.hyperparameters = dict(hyperparameters)\n    self.wandb_run_name = wandb_run_name\n    self.cross_validate = cross_validate\n    self.hf_hub_repo_id = hf_hub_repo_id\n    self.download_from_hub = download_from_hub\n    self.hf_model_commit_message = hf_model_commit_message\n    self.l2g_threshold = l2g_threshold or 0.0\n    self.gold_standard_curation_path = gold_standard_curation_path\n    self.gene_interactions_path = gene_interactions_path\n    self.variant_index_path = variant_index_path\n    self.model_path = (\n        hf_hub_repo_id\n        if not model_path and download_from_hub and hf_hub_repo_id\n        else model_path\n    )\n    self.hf_model_version = hf_model_version\n    self.explain_predictions = explain_predictions\n\n    # Load common inputs\n    self.credible_set = StudyLocus.from_parquet(\n        session, credible_set_path, recursiveFileLookup=True\n    )\n    self.feature_matrix = L2GFeatureMatrix(\n        _df=session.load_data(feature_matrix_path),\n    )\n\n    if run_mode == \"predict\":\n        self.run_predict()\n    elif run_mode == \"train\":\n        self.gold_standard = self.prepare_gold_standard()\n        self.run_train()\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep.prepare_gold_standard","title":"<code>prepare_gold_standard() -&gt; L2GGoldStandard</code>","text":"<p>Prepare the gold standard for training.</p> <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>training dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When gold standard path, is not provided, or when parsing OTG gold standard but missing interactions and variant index paths.</p> <code>TypeError</code> <p>When gold standard is not OTG gold standard nor L2GGoldStandard.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>def prepare_gold_standard(self) -&gt; L2GGoldStandard:\n    \"\"\"Prepare the gold standard for training.\n\n    Returns:\n        L2GGoldStandard: training dataset.\n\n    Raises:\n        ValueError: When gold standard path, is not provided, or when\n            parsing OTG gold standard but missing interactions and variant index paths.\n        TypeError: When gold standard is not OTG gold standard nor L2GGoldStandard.\n\n    \"\"\"\n    if self.gold_standard_curation_path is None:\n        raise ValueError(\"Gold Standard is required for model training.\")\n    # Read the gold standard either from json or parquet, default to parquet if can not infer the format from extension.\n    ext = self.gold_standard_curation_path.split(\".\")[-1]\n    ext = \"parquet\" if ext not in [\"parquet\", \"json\"] else ext\n    gold_standard = self.session.load_data(self.gold_standard_curation_path, ext)\n    schema_issues = compare_struct_schemas(\n        gold_standard.schema, L2GGoldStandard.get_schema()\n    )\n    # Parse the gold standard depending on the input schema\n    match schema_issues:\n        case {**extra} if not extra:\n            # Schema is the same as L2GGoldStandard - load the GS\n            # NOTE: match to empty dict will be non-selective\n            # see https://stackoverflow.com/questions/75389166/how-to-match-an-empty-dictionary\n            logging.info(\"Successfully parsed gold standard.\")\n            return L2GGoldStandard(\n                _df=gold_standard,\n                _schema=L2GGoldStandard.get_schema(),\n            )\n        case {\n            \"missing_mandatory_columns\": [\n                \"studyLocusId\",\n                \"variantId\",\n                \"studyId\",\n                \"geneId\",\n                \"goldStandardSet\",\n            ],\n            \"unexpected_columns\": [\n                \"association_info\",\n                \"gold_standard_info\",\n                \"metadata\",\n                \"sentinel_variant\",\n                \"trait_info\",\n            ],\n        }:\n            # There are schema mismatches, this would mean that we have\n            logging.info(\"Detected OTG Gold Standard. Attempting to parse it.\")\n            otg_curation = gold_standard\n            if self.gene_interactions_path is None:\n                raise ValueError(\"Interactions are required for parsing curation.\")\n            if self.variant_index_path is None:\n                raise ValueError(\"Variant Index are required for parsing curation.\")\n\n            interactions = self.session.load_data(\n                self.gene_interactions_path, \"parquet\"\n            )\n            variant_index = VariantIndex.from_parquet(\n                self.session, self.variant_index_path\n            )\n            study_locus_overlap = StudyLocus(\n                _df=self.credible_set.df.join(\n                    otg_curation.select(\n                        f.concat_ws(\n                            \"_\",\n                            f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n                            f.col(\"sentinel_variant.locus_GRCh38.position\"),\n                            f.col(\"sentinel_variant.alleles.reference\"),\n                            f.col(\"sentinel_variant.alleles.alternative\"),\n                        ).alias(\"variantId\"),\n                        f.col(\"association_info.otg_id\").alias(\"studyId\"),\n                    ),\n                    on=[\n                        \"studyId\",\n                        \"variantId\",\n                    ],\n                    how=\"inner\",\n                ),\n                _schema=StudyLocus.get_schema(),\n            ).find_overlaps()\n\n            return L2GGoldStandard.from_otg_curation(\n                gold_standard_curation=otg_curation,\n                variant_index=variant_index,\n                study_locus_overlap=study_locus_overlap,\n                interactions=interactions,\n            )\n        case _:\n            raise TypeError(\"Incorrect gold standard dataset provided.\")\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep.run_predict","title":"<code>run_predict() -&gt; None</code>","text":"<p>Run the prediction step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If predictions_path is not provided for prediction mode</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>def run_predict(self) -&gt; None:\n    \"\"\"Run the prediction step.\n\n    Raises:\n        ValueError: If predictions_path is not provided for prediction mode\n    \"\"\"\n    if not self.predictions_path:\n        raise ValueError(\"predictions_path must be provided for prediction mode\")\n    predictions = (\n        L2GPrediction.from_credible_set(\n            self.session,\n            self.credible_set,\n            self.feature_matrix,\n            model_path=self.model_path,\n            features_list=self.features_list,\n            hf_token=access_gcp_secret(\"hfhub-key\", \"open-targets-genetics-dev\"),\n            hf_model_version=self.hf_model_version,\n            download_from_hub=self.download_from_hub,\n        )\n        .filter(f.col(\"score\") &gt;= self.l2g_threshold)\n        .add_features(\n            self.feature_matrix,\n        )\n    )\n    if self.explain_predictions:\n        predictions = predictions.explain()\n    predictions.df.coalesce(self.session.output_partitions).write.mode(\n        self.session.write_mode\n    ).parquet(self.predictions_path)\n    self.session.logger.info(\"L2G predictions saved successfully.\")\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep.run_train","title":"<code>run_train() -&gt; None</code>","text":"<p>Run the training step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If features list is not provided for model training.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>def run_train(self) -&gt; None:\n    \"\"\"Run the training step.\n\n    Raises:\n        ValueError: If features list is not provided for model training.\n    \"\"\"\n    if self.features_list is None:\n        raise ValueError(\"Features list is required for model training.\")\n    # Initialize access to weights and biases\n    wandb_key = access_gcp_secret(\"wandb-key\", \"open-targets-genetics-dev\")\n    wandb_login(key=wandb_key)\n\n    # Instantiate classifier and train model\n    l2g_model = LocusToGeneModel(\n        model=GradientBoostingClassifier(random_state=42, loss=\"log_loss\"),\n        hyperparameters=self.hyperparameters,\n        features_list=self.features_list,\n    )\n\n    # Calculate the gold standard features\n    feature_matrix = self._annotate_gold_standards_w_feature_matrix()\n\n    # Run the training\n    trained_model = LocusToGeneTrainer(\n        model=l2g_model, feature_matrix=feature_matrix\n    ).train(self.wandb_run_name, cross_validate=self.cross_validate)\n\n    # Export the model\n    if trained_model.training_data and trained_model.model and self.model_path:\n        trained_model.save(self.model_path)\n        if self.hf_hub_repo_id and self.hf_model_commit_message:\n            hf_hub_token = access_gcp_secret(\n                \"hfhub-key\", \"open-targets-genetics-dev\"\n            )\n            trained_model.export_to_hugging_face_hub(\n                # we upload the model saved in the filesystem\n                self.model_path.split(\"/\")[-1],\n                hf_hub_token,\n                data=trained_model.training_data._df.toPandas(),\n                repo_id=self.hf_hub_repo_id,\n                commit_message=self.hf_model_commit_message,\n            )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneEvidenceStep","title":"<code>gentropy.l2g.LocusToGeneEvidenceStep</code>","text":"<p>Locus to gene evidence step.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>class LocusToGeneEvidenceStep:\n    \"\"\"Locus to gene evidence step.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        locus_to_gene_predictions_path: str,\n        credible_set_path: str,\n        study_index_path: str,\n        evidence_output_path: str,\n        locus_to_gene_threshold: float,\n    ) -&gt; None:\n        \"\"\"Initialise the step and generate disease/target evidence.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            locus_to_gene_predictions_path (str): Path to the L2G predictions dataset\n            credible_set_path (str): Path to the credible set dataset\n            study_index_path (str): Path to the study index dataset\n            evidence_output_path (str): Path to the L2G evidence output dataset. The output format is ndjson gzipped.\n            locus_to_gene_threshold (float, optional): Threshold to consider a gene as a target. Defaults to 0.05.\n        \"\"\"\n        # Reading the predictions\n        locus_to_gene_prediction = L2GPrediction.from_parquet(\n            session, locus_to_gene_predictions_path\n        )\n        # Reading the credible set\n        credible_sets = StudyLocus.from_parquet(session, credible_set_path)\n\n        # Reading the study index\n        study_index = StudyIndex.from_parquet(session, study_index_path)\n\n        # Generate evidence and save file:\n        (\n            locus_to_gene_prediction.to_disease_target_evidence(\n                credible_sets, study_index, locus_to_gene_threshold\n            )\n            .coalesce(session.output_partitions)\n            .write.mode(session.write_mode)\n            .option(\"compression\", \"gzip\")\n            .json(evidence_output_path)\n        )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneEvidenceStep.__init__","title":"<code>__init__(session: Session, locus_to_gene_predictions_path: str, credible_set_path: str, study_index_path: str, evidence_output_path: str, locus_to_gene_threshold: float) -&gt; None</code>","text":"<p>Initialise the step and generate disease/target evidence.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>locus_to_gene_predictions_path</code> <code>str</code> <p>Path to the L2G predictions dataset</p> required <code>credible_set_path</code> <code>str</code> <p>Path to the credible set dataset</p> required <code>study_index_path</code> <code>str</code> <p>Path to the study index dataset</p> required <code>evidence_output_path</code> <code>str</code> <p>Path to the L2G evidence output dataset. The output format is ndjson gzipped.</p> required <code>locus_to_gene_threshold</code> <code>float</code> <p>Threshold to consider a gene as a target. Defaults to 0.05.</p> required Source code in <code>src/gentropy/l2g.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    locus_to_gene_predictions_path: str,\n    credible_set_path: str,\n    study_index_path: str,\n    evidence_output_path: str,\n    locus_to_gene_threshold: float,\n) -&gt; None:\n    \"\"\"Initialise the step and generate disease/target evidence.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        locus_to_gene_predictions_path (str): Path to the L2G predictions dataset\n        credible_set_path (str): Path to the credible set dataset\n        study_index_path (str): Path to the study index dataset\n        evidence_output_path (str): Path to the L2G evidence output dataset. The output format is ndjson gzipped.\n        locus_to_gene_threshold (float, optional): Threshold to consider a gene as a target. Defaults to 0.05.\n    \"\"\"\n    # Reading the predictions\n    locus_to_gene_prediction = L2GPrediction.from_parquet(\n        session, locus_to_gene_predictions_path\n    )\n    # Reading the credible set\n    credible_sets = StudyLocus.from_parquet(session, credible_set_path)\n\n    # Reading the study index\n    study_index = StudyIndex.from_parquet(session, study_index_path)\n\n    # Generate evidence and save file:\n    (\n        locus_to_gene_prediction.to_disease_target_evidence(\n            credible_sets, study_index, locus_to_gene_threshold\n        )\n        .coalesce(session.output_partitions)\n        .write.mode(session.write_mode)\n        .option(\"compression\", \"gzip\")\n        .json(evidence_output_path)\n    )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneAssociationsStep","title":"<code>gentropy.l2g.LocusToGeneAssociationsStep</code>","text":"<p>Locus to gene associations step.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>class LocusToGeneAssociationsStep:\n    \"\"\"Locus to gene associations step.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        evidence_input_path: str,\n        disease_index_path: str,\n        direct_associations_output_path: str,\n        indirect_associations_output_path: str,\n    ) -&gt; None:\n        \"\"\"Create direct and indirect association datasets.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            evidence_input_path (str): Path to the L2G evidence input dataset\n            disease_index_path (str): Path to disease index file\n            direct_associations_output_path (str): Path to the direct associations output dataset\n            indirect_associations_output_path (str): Path to the indirect associations output dataset\n        \"\"\"\n        # Read in the disease index\n        disease_index = session.spark.read.parquet(disease_index_path).select(\n            f.col(\"id\").alias(\"diseaseId\"),\n            f.explode(\"ancestors\").alias(\"ancestorDiseaseId\"),\n        )\n\n        # Read in the L2G evidence\n        disease_target_evidence = session.spark.read.json(evidence_input_path).select(\n            f.col(\"targetFromSourceId\").alias(\"targetId\"),\n            f.col(\"diseaseFromSourceMappedId\").alias(\"diseaseId\"),\n            f.col(\"resourceScore\"),\n        )\n\n        # Generate direct assocations and save file\n        (\n            disease_target_evidence.groupBy(\"targetId\", \"diseaseId\")\n            .agg(f.collect_set(\"resourceScore\").alias(\"scores\"))\n            .select(\n                \"targetId\",\n                \"diseaseId\",\n                calculate_harmonic_sum(f.col(\"scores\")).alias(\"harmonicSum\"),\n            )\n            .write.mode(session.write_mode)\n            .parquet(direct_associations_output_path)\n        )\n\n        # Generate indirect assocations and save file\n        (\n            disease_target_evidence.join(disease_index, on=\"diseaseId\", how=\"inner\")\n            .groupBy(\"targetId\", \"ancestorDiseaseId\")\n            .agg(f.collect_set(\"resourceScore\").alias(\"scores\"))\n            .select(\n                \"targetId\",\n                \"ancestorDiseaseId\",\n                calculate_harmonic_sum(f.col(\"scores\")).alias(\"harmonicSum\"),\n            )\n            .write.mode(session.write_mode)\n            .parquet(indirect_associations_output_path)\n        )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneAssociationsStep.__init__","title":"<code>__init__(session: Session, evidence_input_path: str, disease_index_path: str, direct_associations_output_path: str, indirect_associations_output_path: str) -&gt; None</code>","text":"<p>Create direct and indirect association datasets.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>evidence_input_path</code> <code>str</code> <p>Path to the L2G evidence input dataset</p> required <code>disease_index_path</code> <code>str</code> <p>Path to disease index file</p> required <code>direct_associations_output_path</code> <code>str</code> <p>Path to the direct associations output dataset</p> required <code>indirect_associations_output_path</code> <code>str</code> <p>Path to the indirect associations output dataset</p> required Source code in <code>src/gentropy/l2g.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    evidence_input_path: str,\n    disease_index_path: str,\n    direct_associations_output_path: str,\n    indirect_associations_output_path: str,\n) -&gt; None:\n    \"\"\"Create direct and indirect association datasets.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        evidence_input_path (str): Path to the L2G evidence input dataset\n        disease_index_path (str): Path to disease index file\n        direct_associations_output_path (str): Path to the direct associations output dataset\n        indirect_associations_output_path (str): Path to the indirect associations output dataset\n    \"\"\"\n    # Read in the disease index\n    disease_index = session.spark.read.parquet(disease_index_path).select(\n        f.col(\"id\").alias(\"diseaseId\"),\n        f.explode(\"ancestors\").alias(\"ancestorDiseaseId\"),\n    )\n\n    # Read in the L2G evidence\n    disease_target_evidence = session.spark.read.json(evidence_input_path).select(\n        f.col(\"targetFromSourceId\").alias(\"targetId\"),\n        f.col(\"diseaseFromSourceMappedId\").alias(\"diseaseId\"),\n        f.col(\"resourceScore\"),\n    )\n\n    # Generate direct assocations and save file\n    (\n        disease_target_evidence.groupBy(\"targetId\", \"diseaseId\")\n        .agg(f.collect_set(\"resourceScore\").alias(\"scores\"))\n        .select(\n            \"targetId\",\n            \"diseaseId\",\n            calculate_harmonic_sum(f.col(\"scores\")).alias(\"harmonicSum\"),\n        )\n        .write.mode(session.write_mode)\n        .parquet(direct_associations_output_path)\n    )\n\n    # Generate indirect assocations and save file\n    (\n        disease_target_evidence.join(disease_index, on=\"diseaseId\", how=\"inner\")\n        .groupBy(\"targetId\", \"ancestorDiseaseId\")\n        .agg(f.collect_set(\"resourceScore\").alias(\"scores\"))\n        .select(\n            \"targetId\",\n            \"ancestorDiseaseId\",\n            calculate_harmonic_sum(f.col(\"scores\")).alias(\"harmonicSum\"),\n        )\n        .write.mode(session.write_mode)\n        .parquet(indirect_associations_output_path)\n    )\n</code></pre>"},{"location":"python_api/steps/ld_clump/","title":"ld_based_clumping","text":""},{"location":"python_api/steps/ld_clump/#gentropy.ld_based_clumping.LDBasedClumpingStep","title":"<code>gentropy.ld_based_clumping.LDBasedClumpingStep</code>","text":"<p>Step to perform LD-based clumping on study locus dataset.</p> <p>As a first step, study locus is enriched with population specific linked-variants. That's why the study index and the ld index is required for this step. Study loci are flaggged in the resulting dataset, which can be explained by a more significant association from the same study.</p> Source code in <code>src/gentropy/ld_based_clumping.py</code> <pre><code>class LDBasedClumpingStep:\n    \"\"\"Step to perform LD-based clumping on study locus dataset.\n\n    As a first step, study locus is enriched with population specific linked-variants.\n    That's why the study index and the ld index is required for this step. Study loci are flaggged\n    in the resulting dataset, which can be explained by a more significant association\n    from the same study.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        study_locus_input_path: str,\n        study_index_path: str,\n        ld_index_path: str,\n        clumped_study_locus_output_path: str,\n    ) -&gt; None:\n        \"\"\"Run LD-based clumping step.\n\n        Args:\n            session (Session): Session object.\n            study_locus_input_path (str): Path to the input study locus.\n            study_index_path (str): Path to the study index.\n            ld_index_path (str): Path to the LD index.\n            clumped_study_locus_output_path (str): path of the resulting, clumped study-locus dataset.\n        \"\"\"\n        study_locus = StudyLocus.from_parquet(session, study_locus_input_path)\n        ld_index = LDIndex.from_parquet(session, ld_index_path)\n        study_index = StudyIndex.from_parquet(session, study_index_path)\n\n        (\n            study_locus\n            # Annotating study locus with LD information:\n            .annotate_ld(study_index, ld_index)\n            .clump()\n            # Save result:\n            .df.write.mode(session.write_mode)\n            .parquet(clumped_study_locus_output_path)\n        )\n</code></pre>"},{"location":"python_api/steps/ld_clump/#gentropy.ld_based_clumping.LDBasedClumpingStep.__init__","title":"<code>__init__(session: Session, study_locus_input_path: str, study_index_path: str, ld_index_path: str, clumped_study_locus_output_path: str) -&gt; None</code>","text":"<p>Run LD-based clumping step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>study_locus_input_path</code> <code>str</code> <p>Path to the input study locus.</p> required <code>study_index_path</code> <code>str</code> <p>Path to the study index.</p> required <code>ld_index_path</code> <code>str</code> <p>Path to the LD index.</p> required <code>clumped_study_locus_output_path</code> <code>str</code> <p>path of the resulting, clumped study-locus dataset.</p> required Source code in <code>src/gentropy/ld_based_clumping.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    study_locus_input_path: str,\n    study_index_path: str,\n    ld_index_path: str,\n    clumped_study_locus_output_path: str,\n) -&gt; None:\n    \"\"\"Run LD-based clumping step.\n\n    Args:\n        session (Session): Session object.\n        study_locus_input_path (str): Path to the input study locus.\n        study_index_path (str): Path to the study index.\n        ld_index_path (str): Path to the LD index.\n        clumped_study_locus_output_path (str): path of the resulting, clumped study-locus dataset.\n    \"\"\"\n    study_locus = StudyLocus.from_parquet(session, study_locus_input_path)\n    ld_index = LDIndex.from_parquet(session, ld_index_path)\n    study_index = StudyIndex.from_parquet(session, study_index_path)\n\n    (\n        study_locus\n        # Annotating study locus with LD information:\n        .annotate_ld(study_index, ld_index)\n        .clump()\n        # Save result:\n        .df.write.mode(session.write_mode)\n        .parquet(clumped_study_locus_output_path)\n    )\n</code></pre>"},{"location":"python_api/steps/ld_index/","title":"GnomAD Linkage data ingestion","text":""},{"location":"python_api/steps/ld_index/#gentropy.gnomad_ingestion.LDIndexStep","title":"<code>gentropy.gnomad_ingestion.LDIndexStep</code>","text":"<p>LD index step.</p> <p>This step is resource intensive</p> <p>Suggested params: high memory machine, 5TB of boot disk, no SSDs.</p> Source code in <code>src/gentropy/gnomad_ingestion.py</code> <pre><code>class LDIndexStep:\n    \"\"\"LD index step.\n\n    !!! warning \"This step is resource intensive\"\n\n        Suggested params: high memory machine, 5TB of boot disk, no SSDs.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        ld_index_out: str,\n        min_r2: float = LDIndexConfig().min_r2,\n        ld_matrix_template: str = LDIndexConfig().ld_matrix_template,\n        ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template,\n        ld_populations: list[LD_Population |\n                             str] = LDIndexConfig().ld_populations,\n        liftover_ht_path: str = LDIndexConfig().liftover_ht_path,\n        grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path,\n    ) -&gt; None:\n        \"\"\"Run step.\n\n        Args:\n            session (Session): Session object.\n            ld_index_out (str): Output LD index path. (required)\n            min_r2 (float): Minimum r2 to consider when considering variants within a window.\n            ld_matrix_template (str): Input path to the gnomAD ld file with placeholder for population\n            ld_index_raw_template (str): Input path to the raw gnomAD LD indices file with placeholder for population string\n            ld_populations (list[LD_Population | str]): Population names derived from the ld file paths\n            liftover_ht_path (str): Path to the liftover ht file\n            grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates.\n\n        Default values are provided in LDIndexConfig.\n        \"\"\"\n        (\n            GnomADLDMatrix(\n                ld_matrix_template=ld_matrix_template,\n                ld_index_raw_template=ld_index_raw_template,\n                grch37_to_grch38_chain_path=grch37_to_grch38_chain_path,\n                ld_populations=ld_populations,\n                liftover_ht_path=liftover_ht_path,\n            )\n            .as_ld_index(min_r2)\n            .df.write.partitionBy(\"chromosome\")\n            .mode(session.write_mode)\n            .parquet(ld_index_out)\n        )\n        session.logger.info(ld_index_out)\n</code></pre>"},{"location":"python_api/steps/ld_index/#gentropy.gnomad_ingestion.LDIndexStep.__init__","title":"<code>__init__(session: Session, ld_index_out: str, min_r2: float = LDIndexConfig().min_r2, ld_matrix_template: str = LDIndexConfig().ld_matrix_template, ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template, ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations, liftover_ht_path: str = LDIndexConfig().liftover_ht_path, grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path) -&gt; None</code>","text":"<p>Run step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>ld_index_out</code> <code>str</code> <p>Output LD index path. (required)</p> required <code>min_r2</code> <code>float</code> <p>Minimum r2 to consider when considering variants within a window.</p> <code>min_r2</code> <code>ld_matrix_template</code> <code>str</code> <p>Input path to the gnomAD ld file with placeholder for population</p> <code>ld_matrix_template</code> <code>ld_index_raw_template</code> <code>str</code> <p>Input path to the raw gnomAD LD indices file with placeholder for population string</p> <code>ld_index_raw_template</code> <code>ld_populations</code> <code>list[LD_Population | str]</code> <p>Population names derived from the ld file paths</p> <code>ld_populations</code> <code>liftover_ht_path</code> <code>str</code> <p>Path to the liftover ht file</p> <code>liftover_ht_path</code> <code>grch37_to_grch38_chain_path</code> <code>str</code> <p>Path to the chain file used to lift over the coordinates.</p> <code>grch37_to_grch38_chain_path</code> <p>Default values are provided in LDIndexConfig.</p> Source code in <code>src/gentropy/gnomad_ingestion.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    ld_index_out: str,\n    min_r2: float = LDIndexConfig().min_r2,\n    ld_matrix_template: str = LDIndexConfig().ld_matrix_template,\n    ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template,\n    ld_populations: list[LD_Population |\n                         str] = LDIndexConfig().ld_populations,\n    liftover_ht_path: str = LDIndexConfig().liftover_ht_path,\n    grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path,\n) -&gt; None:\n    \"\"\"Run step.\n\n    Args:\n        session (Session): Session object.\n        ld_index_out (str): Output LD index path. (required)\n        min_r2 (float): Minimum r2 to consider when considering variants within a window.\n        ld_matrix_template (str): Input path to the gnomAD ld file with placeholder for population\n        ld_index_raw_template (str): Input path to the raw gnomAD LD indices file with placeholder for population string\n        ld_populations (list[LD_Population | str]): Population names derived from the ld file paths\n        liftover_ht_path (str): Path to the liftover ht file\n        grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates.\n\n    Default values are provided in LDIndexConfig.\n    \"\"\"\n    (\n        GnomADLDMatrix(\n            ld_matrix_template=ld_matrix_template,\n            ld_index_raw_template=ld_index_raw_template,\n            grch37_to_grch38_chain_path=grch37_to_grch38_chain_path,\n            ld_populations=ld_populations,\n            liftover_ht_path=liftover_ht_path,\n        )\n        .as_ld_index(min_r2)\n        .df.write.partitionBy(\"chromosome\")\n        .mode(session.write_mode)\n        .parquet(ld_index_out)\n    )\n    session.logger.info(ld_index_out)\n</code></pre>"},{"location":"python_api/steps/locus_breaker_clumping/","title":"locus_breaker_clumping","text":""},{"location":"python_api/steps/locus_breaker_clumping/#gentropy.locus_breaker_clumping.LocusBreakerClumpingStep","title":"<code>gentropy.locus_breaker_clumping.LocusBreakerClumpingStep</code>","text":"<p>Step to perform locus-breaker clumping on a study.</p> Source code in <code>src/gentropy/locus_breaker_clumping.py</code> <pre><code>class LocusBreakerClumpingStep:\n    \"\"\"Step to perform locus-breaker clumping on a study.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        summary_statistics_input_path: str,\n        clumped_study_locus_output_path: str,\n        lbc_baseline_pvalue: float,\n        lbc_distance_cutoff: int,\n        lbc_pvalue_threshold: float,\n        lbc_flanking_distance: int,\n        large_loci_size: int,\n        wbc_clump_distance: int,\n        wbc_pvalue_threshold: float,\n        collect_locus: bool = False,\n        remove_mhc: bool = True,\n    ) -&gt; None:\n        \"\"\"Run locus-breaker clumping step.\n\n        This step will perform locus-breaker clumping on the full set of summary statistics.\n        StudyLocus larger than the large_loci_size, by distance, will be further clumped with window-based\n        clumping.\n\n        Args:\n            session (Session): Session object.\n            summary_statistics_input_path (str): Path to the input study locus.\n            clumped_study_locus_output_path (str): path of the resulting, clumped study-locus dataset.\n            lbc_baseline_pvalue (float): Baseline p-value for locus breaker clumping.\n            lbc_distance_cutoff (int): Distance cutoff for locus breaker clumping.\n            lbc_pvalue_threshold (float): P-value threshold for locus breaker clumping.\n            lbc_flanking_distance (int): Flanking distance for locus breaker clumping.\n            large_loci_size (int): Threshold distance to define large loci for window-based clumping.\n            wbc_clump_distance (int): Clump distance for window breaker clumping.\n            wbc_pvalue_threshold (float): P-value threshold for window breaker clumping.\n            collect_locus (bool, optional): Whether to collect locus. Defaults to False.\n            remove_mhc (bool, optional): If true will use exclude_region() to remove the MHC region.\n        \"\"\"\n        sum_stats = SummaryStatistics.from_parquet(\n            session,\n            summary_statistics_input_path,\n        )\n        lbc = sum_stats.locus_breaker_clumping(\n            lbc_baseline_pvalue,\n            lbc_distance_cutoff,\n            lbc_pvalue_threshold,\n            lbc_flanking_distance,\n        )\n        wbc = sum_stats.window_based_clumping(wbc_clump_distance, wbc_pvalue_threshold)\n\n        clumped_result = LocusBreakerClumping.process_locus_breaker_output(\n            lbc,\n            wbc,\n            large_loci_size,\n        )\n        if remove_mhc:\n            clumped_result = clumped_result.exclude_region(\n                GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC),\n                exclude_overlap=True,\n            )\n\n        if collect_locus:\n            clumped_result = clumped_result.annotate_locus_statistics_boundaries(\n                sum_stats\n            )\n        clumped_result.df.write.partitionBy(\"studyLocusId\").mode(\n            session.write_mode\n        ).parquet(clumped_study_locus_output_path)\n</code></pre>"},{"location":"python_api/steps/locus_breaker_clumping/#gentropy.locus_breaker_clumping.LocusBreakerClumpingStep.__init__","title":"<code>__init__(session: Session, summary_statistics_input_path: str, clumped_study_locus_output_path: str, lbc_baseline_pvalue: float, lbc_distance_cutoff: int, lbc_pvalue_threshold: float, lbc_flanking_distance: int, large_loci_size: int, wbc_clump_distance: int, wbc_pvalue_threshold: float, collect_locus: bool = False, remove_mhc: bool = True) -&gt; None</code>","text":"<p>Run locus-breaker clumping step.</p> <p>This step will perform locus-breaker clumping on the full set of summary statistics. StudyLocus larger than the large_loci_size, by distance, will be further clumped with window-based clumping.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>summary_statistics_input_path</code> <code>str</code> <p>Path to the input study locus.</p> required <code>clumped_study_locus_output_path</code> <code>str</code> <p>path of the resulting, clumped study-locus dataset.</p> required <code>lbc_baseline_pvalue</code> <code>float</code> <p>Baseline p-value for locus breaker clumping.</p> required <code>lbc_distance_cutoff</code> <code>int</code> <p>Distance cutoff for locus breaker clumping.</p> required <code>lbc_pvalue_threshold</code> <code>float</code> <p>P-value threshold for locus breaker clumping.</p> required <code>lbc_flanking_distance</code> <code>int</code> <p>Flanking distance for locus breaker clumping.</p> required <code>large_loci_size</code> <code>int</code> <p>Threshold distance to define large loci for window-based clumping.</p> required <code>wbc_clump_distance</code> <code>int</code> <p>Clump distance for window breaker clumping.</p> required <code>wbc_pvalue_threshold</code> <code>float</code> <p>P-value threshold for window breaker clumping.</p> required <code>collect_locus</code> <code>bool</code> <p>Whether to collect locus. Defaults to False.</p> <code>False</code> <code>remove_mhc</code> <code>bool</code> <p>If true will use exclude_region() to remove the MHC region.</p> <code>True</code> Source code in <code>src/gentropy/locus_breaker_clumping.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    summary_statistics_input_path: str,\n    clumped_study_locus_output_path: str,\n    lbc_baseline_pvalue: float,\n    lbc_distance_cutoff: int,\n    lbc_pvalue_threshold: float,\n    lbc_flanking_distance: int,\n    large_loci_size: int,\n    wbc_clump_distance: int,\n    wbc_pvalue_threshold: float,\n    collect_locus: bool = False,\n    remove_mhc: bool = True,\n) -&gt; None:\n    \"\"\"Run locus-breaker clumping step.\n\n    This step will perform locus-breaker clumping on the full set of summary statistics.\n    StudyLocus larger than the large_loci_size, by distance, will be further clumped with window-based\n    clumping.\n\n    Args:\n        session (Session): Session object.\n        summary_statistics_input_path (str): Path to the input study locus.\n        clumped_study_locus_output_path (str): path of the resulting, clumped study-locus dataset.\n        lbc_baseline_pvalue (float): Baseline p-value for locus breaker clumping.\n        lbc_distance_cutoff (int): Distance cutoff for locus breaker clumping.\n        lbc_pvalue_threshold (float): P-value threshold for locus breaker clumping.\n        lbc_flanking_distance (int): Flanking distance for locus breaker clumping.\n        large_loci_size (int): Threshold distance to define large loci for window-based clumping.\n        wbc_clump_distance (int): Clump distance for window breaker clumping.\n        wbc_pvalue_threshold (float): P-value threshold for window breaker clumping.\n        collect_locus (bool, optional): Whether to collect locus. Defaults to False.\n        remove_mhc (bool, optional): If true will use exclude_region() to remove the MHC region.\n    \"\"\"\n    sum_stats = SummaryStatistics.from_parquet(\n        session,\n        summary_statistics_input_path,\n    )\n    lbc = sum_stats.locus_breaker_clumping(\n        lbc_baseline_pvalue,\n        lbc_distance_cutoff,\n        lbc_pvalue_threshold,\n        lbc_flanking_distance,\n    )\n    wbc = sum_stats.window_based_clumping(wbc_clump_distance, wbc_pvalue_threshold)\n\n    clumped_result = LocusBreakerClumping.process_locus_breaker_output(\n        lbc,\n        wbc,\n        large_loci_size,\n    )\n    if remove_mhc:\n        clumped_result = clumped_result.exclude_region(\n            GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC),\n            exclude_overlap=True,\n        )\n\n    if collect_locus:\n        clumped_result = clumped_result.annotate_locus_statistics_boundaries(\n            sum_stats\n        )\n    clumped_result.df.write.partitionBy(\"studyLocusId\").mode(\n        session.write_mode\n    ).parquet(clumped_study_locus_output_path)\n</code></pre>"},{"location":"python_api/steps/pics/","title":"pics","text":""},{"location":"python_api/steps/pics/#gentropy.pics.PICSStep","title":"<code>gentropy.pics.PICSStep</code>","text":"<p>PICS finemapping of LD-annotated StudyLocus.</p> Source code in <code>src/gentropy/pics.py</code> <pre><code>class PICSStep:\n    \"\"\"PICS finemapping of LD-annotated StudyLocus.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        study_locus_ld_annotated_in: str,\n        picsed_study_locus_out: str,\n    ) -&gt; None:\n        \"\"\"Run PICS on LD annotated study-locus.\n\n        Args:\n            session (Session): Session object.\n            study_locus_ld_annotated_in (str): Input LD annotated study-locus path.\n            picsed_study_locus_out (str): Output PICSed study-locus path.\n        \"\"\"\n        # Extract\n        study_locus_ld_annotated = StudyLocus.from_parquet(\n            session, study_locus_ld_annotated_in\n        )\n        # PICS\n        (\n            PICS.finemap(study_locus_ld_annotated)\n            .filter_credible_set(credible_interval=CredibleInterval.IS99)\n            # Flagging sub-significnat loci:\n            .validate_lead_pvalue(\n                pvalue_cutoff=WindowBasedClumpingStepConfig().gwas_significance\n            )\n            # Writing the output:\n            .df.write.mode(session.write_mode)\n            .parquet(picsed_study_locus_out)\n        )\n</code></pre>"},{"location":"python_api/steps/pics/#gentropy.pics.PICSStep.__init__","title":"<code>__init__(session: Session, study_locus_ld_annotated_in: str, picsed_study_locus_out: str) -&gt; None</code>","text":"<p>Run PICS on LD annotated study-locus.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>study_locus_ld_annotated_in</code> <code>str</code> <p>Input LD annotated study-locus path.</p> required <code>picsed_study_locus_out</code> <code>str</code> <p>Output PICSed study-locus path.</p> required Source code in <code>src/gentropy/pics.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    study_locus_ld_annotated_in: str,\n    picsed_study_locus_out: str,\n) -&gt; None:\n    \"\"\"Run PICS on LD annotated study-locus.\n\n    Args:\n        session (Session): Session object.\n        study_locus_ld_annotated_in (str): Input LD annotated study-locus path.\n        picsed_study_locus_out (str): Output PICSed study-locus path.\n    \"\"\"\n    # Extract\n    study_locus_ld_annotated = StudyLocus.from_parquet(\n        session, study_locus_ld_annotated_in\n    )\n    # PICS\n    (\n        PICS.finemap(study_locus_ld_annotated)\n        .filter_credible_set(credible_interval=CredibleInterval.IS99)\n        # Flagging sub-significnat loci:\n        .validate_lead_pvalue(\n            pvalue_cutoff=WindowBasedClumpingStepConfig().gwas_significance\n        )\n        # Writing the output:\n        .df.write.mode(session.write_mode)\n        .parquet(picsed_study_locus_out)\n    )\n</code></pre>"},{"location":"python_api/steps/study_locus_validation/","title":"Study-Locus Validation","text":""},{"location":"python_api/steps/study_locus_validation/#gentropy.study_locus_validation.StudyLocusValidationStep","title":"<code>gentropy.study_locus_validation.StudyLocusValidationStep</code>","text":"<p>Study index validation step.</p> <p>This step reads and outputs a study index dataset with flagged studies when target of disease validation fails.</p> Source code in <code>src/gentropy/study_locus_validation.py</code> <pre><code>class StudyLocusValidationStep:\n    \"\"\"Study index validation step.\n\n    This step reads and outputs a study index dataset with flagged studies\n    when target of disease validation fails.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        study_locus_path: list[str],\n        study_index_path: str,\n        target_index_path: str,\n        valid_study_locus_path: str,\n        invalid_study_locus_path: str,\n        trans_qtl_threshold: int,\n        invalid_qc_reasons: list[str] = [],\n    ) -&gt; None:\n        \"\"\"Initialize step.\n\n        Args:\n            session (Session): Session object.\n            study_locus_path (list[str]): Path to study locus dataset.\n            study_index_path (str): Path to study index file.\n            target_index_path (str): path to the target index.\n            valid_study_locus_path (str): Path to write the valid records.\n            invalid_study_locus_path (str): Path to write the output file.\n            trans_qtl_threshold (int): genomic distance above which a QTL is considered trans.\n            invalid_qc_reasons (list[str]): List of invalid quality check reason names from `StudyLocusQualityCheck` (e.g. ['SUBSIGNIFICANT_FLAG']).\n        \"\"\"\n        # Reading datasets:\n        study_index = StudyIndex.from_parquet(session, study_index_path)\n        target_index = TargetIndex.from_parquet(session, target_index_path)\n\n        # Running validation then writing output:\n        study_locus_with_qc = (\n            StudyLocus.from_parquet(session, list(study_locus_path))\n            # Add flag for MHC region\n            .qc_MHC_region()\n            .validate_chromosome_label()  # Flagging credible sets with unsupported chromosomes\n            .validate_study(study_index)  # Flagging studies not in study index\n            .annotate_study_type(study_index)  # Add study type to study locus\n            .qc_redundant_top_hits_from_PICS()  # Flagging top hits from studies with PICS summary statistics\n            .qc_explained_by_SuSiE()  # Flagging credible sets in regions explained by SuSiE\n            # Annotates credible intervals and filter to only keep 95% credible sets\n            .filter_credible_set(credible_interval=CredibleInterval.IS95)\n            # Flagging credible sets with PIP &gt; 1 or PIP &lt; 0.95\n            .qc_abnormal_pips(\n                sum_pips_lower_threshold=0.95, sum_pips_upper_threshold=1.0001\n            )\n            # Annotate credible set confidence:\n            .assign_confidence()\n            # Flagging trans qtls:\n            .flag_trans_qtls(study_index, target_index, trans_qtl_threshold)\n        ).persist()  # we will need this for 2 types of outputs\n\n        # Valid study locus partitioned to simplify the finding of overlaps\n        study_locus_with_qc.valid_rows(invalid_qc_reasons).df.repartitionByRange(\n            session.output_partitions, \"chromosome\", \"position\"\n        ).sortWithinPartitions(\"chromosome\", \"position\").write.mode(\n            session.write_mode\n        ).parquet(valid_study_locus_path)\n\n        # Invalid study locus\n        study_locus_with_qc.valid_rows(invalid_qc_reasons, invalid=True).df.coalesce(\n            session.output_partitions\n        ).write.mode(session.write_mode).parquet(invalid_study_locus_path)\n</code></pre>"},{"location":"python_api/steps/study_locus_validation/#gentropy.study_locus_validation.StudyLocusValidationStep.__init__","title":"<code>__init__(session: Session, study_locus_path: list[str], study_index_path: str, target_index_path: str, valid_study_locus_path: str, invalid_study_locus_path: str, trans_qtl_threshold: int, invalid_qc_reasons: list[str] = []) -&gt; None</code>","text":"<p>Initialize step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>study_locus_path</code> <code>list[str]</code> <p>Path to study locus dataset.</p> required <code>study_index_path</code> <code>str</code> <p>Path to study index file.</p> required <code>target_index_path</code> <code>str</code> <p>path to the target index.</p> required <code>valid_study_locus_path</code> <code>str</code> <p>Path to write the valid records.</p> required <code>invalid_study_locus_path</code> <code>str</code> <p>Path to write the output file.</p> required <code>trans_qtl_threshold</code> <code>int</code> <p>genomic distance above which a QTL is considered trans.</p> required <code>invalid_qc_reasons</code> <code>list[str]</code> <p>List of invalid quality check reason names from <code>StudyLocusQualityCheck</code> (e.g. ['SUBSIGNIFICANT_FLAG']).</p> <code>[]</code> Source code in <code>src/gentropy/study_locus_validation.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    study_locus_path: list[str],\n    study_index_path: str,\n    target_index_path: str,\n    valid_study_locus_path: str,\n    invalid_study_locus_path: str,\n    trans_qtl_threshold: int,\n    invalid_qc_reasons: list[str] = [],\n) -&gt; None:\n    \"\"\"Initialize step.\n\n    Args:\n        session (Session): Session object.\n        study_locus_path (list[str]): Path to study locus dataset.\n        study_index_path (str): Path to study index file.\n        target_index_path (str): path to the target index.\n        valid_study_locus_path (str): Path to write the valid records.\n        invalid_study_locus_path (str): Path to write the output file.\n        trans_qtl_threshold (int): genomic distance above which a QTL is considered trans.\n        invalid_qc_reasons (list[str]): List of invalid quality check reason names from `StudyLocusQualityCheck` (e.g. ['SUBSIGNIFICANT_FLAG']).\n    \"\"\"\n    # Reading datasets:\n    study_index = StudyIndex.from_parquet(session, study_index_path)\n    target_index = TargetIndex.from_parquet(session, target_index_path)\n\n    # Running validation then writing output:\n    study_locus_with_qc = (\n        StudyLocus.from_parquet(session, list(study_locus_path))\n        # Add flag for MHC region\n        .qc_MHC_region()\n        .validate_chromosome_label()  # Flagging credible sets with unsupported chromosomes\n        .validate_study(study_index)  # Flagging studies not in study index\n        .annotate_study_type(study_index)  # Add study type to study locus\n        .qc_redundant_top_hits_from_PICS()  # Flagging top hits from studies with PICS summary statistics\n        .qc_explained_by_SuSiE()  # Flagging credible sets in regions explained by SuSiE\n        # Annotates credible intervals and filter to only keep 95% credible sets\n        .filter_credible_set(credible_interval=CredibleInterval.IS95)\n        # Flagging credible sets with PIP &gt; 1 or PIP &lt; 0.95\n        .qc_abnormal_pips(\n            sum_pips_lower_threshold=0.95, sum_pips_upper_threshold=1.0001\n        )\n        # Annotate credible set confidence:\n        .assign_confidence()\n        # Flagging trans qtls:\n        .flag_trans_qtls(study_index, target_index, trans_qtl_threshold)\n    ).persist()  # we will need this for 2 types of outputs\n\n    # Valid study locus partitioned to simplify the finding of overlaps\n    study_locus_with_qc.valid_rows(invalid_qc_reasons).df.repartitionByRange(\n        session.output_partitions, \"chromosome\", \"position\"\n    ).sortWithinPartitions(\"chromosome\", \"position\").write.mode(\n        session.write_mode\n    ).parquet(valid_study_locus_path)\n\n    # Invalid study locus\n    study_locus_with_qc.valid_rows(invalid_qc_reasons, invalid=True).df.coalesce(\n        session.output_partitions\n    ).write.mode(session.write_mode).parquet(invalid_study_locus_path)\n</code></pre>"},{"location":"python_api/steps/study_validation/","title":"Study Validation","text":""},{"location":"python_api/steps/study_validation/#gentropy.study_validation.StudyValidationStep","title":"<code>gentropy.study_validation.StudyValidationStep</code>","text":"<p>Study index validation step.</p> <p>This step reads and outputs a study index dataset with flagged studies when target of disease validation fails.</p> Source code in <code>src/gentropy/study_validation.py</code> <pre><code>class StudyValidationStep:\n    \"\"\"Study index validation step.\n\n    This step reads and outputs a study index dataset with flagged studies\n    when target of disease validation fails.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        study_index_path: list[str],\n        target_index_path: str,\n        disease_index_path: str,\n        biosample_index_path: str,\n        valid_study_index_path: str,\n        invalid_study_index_path: str,\n        invalid_qc_reasons: list[str] = [],\n    ) -&gt; None:\n        \"\"\"Initialize step.\n\n        Args:\n            session (Session): Session object.\n            study_index_path (list[str]): Path to study index file.\n            target_index_path (str): Path to target index file.\n            disease_index_path (str): Path to disease index file.\n            biosample_index_path (str): Path to biosample index file.\n            valid_study_index_path (str): Path to write the valid records.\n            invalid_study_index_path (str): Path to write the output file.\n            invalid_qc_reasons (list[str]): List of invalid quality check reason names from `StudyQualityCheck` (e.g. ['DUPLICATED_STUDY']).\n        \"\"\"\n        # Reading datasets:\n        target_index = TargetIndex.from_parquet(session, target_index_path)\n        biosample_index = BiosampleIndex.from_parquet(session, biosample_index_path)\n        # Reading disease index and pre-process.\n        # This logic does not belong anywhere, but gentorpy has no disease dataset yet.\n        disease_index = (\n            session.spark.read.parquet(disease_index_path)\n            .select(\n                f.col(\"id\").alias(\"diseaseId\"),\n                f.explode_outer(\n                    f.when(\n                        f.col(\"obsoleteTerms\").isNotNull(),\n                        f.array_union(f.array(\"id\"), f.col(\"obsoleteTerms\")),\n                    )\n                ).alias(\"efo\"),\n            )\n            .withColumn(\"efo\", f.coalesce(f.col(\"efo\"), f.col(\"diseaseId\")))\n        )\n        study_index = StudyIndex.from_parquet(session, list(study_index_path))\n\n        # Running validation:\n        study_index_with_qc = (\n            study_index.deconvolute_studies()  # Deconvolute studies where the same study is ingested from multiple sources\n            .validate_study_type()  # Flagging non-supported study types\n            .validate_target(target_index)  # Flagging QTL studies with invalid targets\n            .validate_disease(disease_index)  # Flagging invalid EFOs\n            .validate_biosample(\n                biosample_index\n            )  # Flagging QTL studies with invalid biosamples\n        ).persist()  # we will need this for 2 types of outputs\n\n        study_index_with_qc.valid_rows(invalid_qc_reasons, invalid=True).df.coalesce(\n            session.output_partitions\n        ).write.mode(session.write_mode).parquet(invalid_study_index_path)\n\n        study_index_with_qc.valid_rows(invalid_qc_reasons).df.coalesce(\n            session.output_partitions\n        ).write.mode(session.write_mode).parquet(valid_study_index_path)\n</code></pre>"},{"location":"python_api/steps/study_validation/#gentropy.study_validation.StudyValidationStep.__init__","title":"<code>__init__(session: Session, study_index_path: list[str], target_index_path: str, disease_index_path: str, biosample_index_path: str, valid_study_index_path: str, invalid_study_index_path: str, invalid_qc_reasons: list[str] = []) -&gt; None</code>","text":"<p>Initialize step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>study_index_path</code> <code>list[str]</code> <p>Path to study index file.</p> required <code>target_index_path</code> <code>str</code> <p>Path to target index file.</p> required <code>disease_index_path</code> <code>str</code> <p>Path to disease index file.</p> required <code>biosample_index_path</code> <code>str</code> <p>Path to biosample index file.</p> required <code>valid_study_index_path</code> <code>str</code> <p>Path to write the valid records.</p> required <code>invalid_study_index_path</code> <code>str</code> <p>Path to write the output file.</p> required <code>invalid_qc_reasons</code> <code>list[str]</code> <p>List of invalid quality check reason names from <code>StudyQualityCheck</code> (e.g. ['DUPLICATED_STUDY']).</p> <code>[]</code> Source code in <code>src/gentropy/study_validation.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    study_index_path: list[str],\n    target_index_path: str,\n    disease_index_path: str,\n    biosample_index_path: str,\n    valid_study_index_path: str,\n    invalid_study_index_path: str,\n    invalid_qc_reasons: list[str] = [],\n) -&gt; None:\n    \"\"\"Initialize step.\n\n    Args:\n        session (Session): Session object.\n        study_index_path (list[str]): Path to study index file.\n        target_index_path (str): Path to target index file.\n        disease_index_path (str): Path to disease index file.\n        biosample_index_path (str): Path to biosample index file.\n        valid_study_index_path (str): Path to write the valid records.\n        invalid_study_index_path (str): Path to write the output file.\n        invalid_qc_reasons (list[str]): List of invalid quality check reason names from `StudyQualityCheck` (e.g. ['DUPLICATED_STUDY']).\n    \"\"\"\n    # Reading datasets:\n    target_index = TargetIndex.from_parquet(session, target_index_path)\n    biosample_index = BiosampleIndex.from_parquet(session, biosample_index_path)\n    # Reading disease index and pre-process.\n    # This logic does not belong anywhere, but gentorpy has no disease dataset yet.\n    disease_index = (\n        session.spark.read.parquet(disease_index_path)\n        .select(\n            f.col(\"id\").alias(\"diseaseId\"),\n            f.explode_outer(\n                f.when(\n                    f.col(\"obsoleteTerms\").isNotNull(),\n                    f.array_union(f.array(\"id\"), f.col(\"obsoleteTerms\")),\n                )\n            ).alias(\"efo\"),\n        )\n        .withColumn(\"efo\", f.coalesce(f.col(\"efo\"), f.col(\"diseaseId\")))\n    )\n    study_index = StudyIndex.from_parquet(session, list(study_index_path))\n\n    # Running validation:\n    study_index_with_qc = (\n        study_index.deconvolute_studies()  # Deconvolute studies where the same study is ingested from multiple sources\n        .validate_study_type()  # Flagging non-supported study types\n        .validate_target(target_index)  # Flagging QTL studies with invalid targets\n        .validate_disease(disease_index)  # Flagging invalid EFOs\n        .validate_biosample(\n            biosample_index\n        )  # Flagging QTL studies with invalid biosamples\n    ).persist()  # we will need this for 2 types of outputs\n\n    study_index_with_qc.valid_rows(invalid_qc_reasons, invalid=True).df.coalesce(\n        session.output_partitions\n    ).write.mode(session.write_mode).parquet(invalid_study_index_path)\n\n    study_index_with_qc.valid_rows(invalid_qc_reasons).df.coalesce(\n        session.output_partitions\n    ).write.mode(session.write_mode).parquet(valid_study_index_path)\n</code></pre>"},{"location":"python_api/steps/summary_statistics_qc/","title":"summary_statistics_qc","text":""},{"location":"python_api/steps/summary_statistics_qc/#gentropy.sumstat_qc_step.SummaryStatisticsQCStep","title":"<code>gentropy.sumstat_qc_step.SummaryStatisticsQCStep</code>","text":"<p>Step to run GWAS QC.</p> Source code in <code>src/gentropy/sumstat_qc_step.py</code> <pre><code>class SummaryStatisticsQCStep:\n    \"\"\"Step to run GWAS QC.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        gwas_path: str,\n        output_path: str,\n        pval_threshold: float = 1e-8,\n    ) -&gt; None:\n        \"\"\"Calculating quality control metrics on the provided GWAS study.\n\n        Args:\n            session (Session): Spark session\n            gwas_path (str): Path to the GWAS summary statistics.\n            output_path (str): Output path for the QC results.\n            pval_threshold (float): P-value threshold for the QC. Default is 1e-8.\n\n        \"\"\"\n        gwas = SummaryStatistics.from_parquet(session, path=gwas_path)\n\n        (\n            SummaryStatisticsQC.get_quality_control_metrics(\n                gwas=gwas, pval_threshold=pval_threshold\n            )\n            .write.mode(session.write_mode)\n            .parquet(output_path)\n        )\n</code></pre>"},{"location":"python_api/steps/summary_statistics_qc/#gentropy.sumstat_qc_step.SummaryStatisticsQCStep.__init__","title":"<code>__init__(session: Session, gwas_path: str, output_path: str, pval_threshold: float = 1e-08) -&gt; None</code>","text":"<p>Calculating quality control metrics on the provided GWAS study.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session</p> required <code>gwas_path</code> <code>str</code> <p>Path to the GWAS summary statistics.</p> required <code>output_path</code> <code>str</code> <p>Output path for the QC results.</p> required <code>pval_threshold</code> <code>float</code> <p>P-value threshold for the QC. Default is 1e-8.</p> <code>1e-08</code> Source code in <code>src/gentropy/sumstat_qc_step.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    gwas_path: str,\n    output_path: str,\n    pval_threshold: float = 1e-8,\n) -&gt; None:\n    \"\"\"Calculating quality control metrics on the provided GWAS study.\n\n    Args:\n        session (Session): Spark session\n        gwas_path (str): Path to the GWAS summary statistics.\n        output_path (str): Output path for the QC results.\n        pval_threshold (float): P-value threshold for the QC. Default is 1e-8.\n\n    \"\"\"\n    gwas = SummaryStatistics.from_parquet(session, path=gwas_path)\n\n    (\n        SummaryStatisticsQC.get_quality_control_metrics(\n            gwas=gwas, pval_threshold=pval_threshold\n        )\n        .write.mode(session.write_mode)\n        .parquet(output_path)\n    )\n</code></pre>"},{"location":"python_api/steps/ukb_ppp_eur_sumstat_preprocess/","title":"ukb_ppp_eur_sumstat_preprocess","text":""},{"location":"python_api/steps/ukb_ppp_eur_sumstat_preprocess/#gentropy.ukb_ppp_eur_sumstat_preprocess.UkbPppEurStep","title":"<code>gentropy.ukb_ppp_eur_sumstat_preprocess.UkbPppEurStep</code>","text":"<p>UKB PPP (EUR) data ingestion and harmonisation.</p> Source code in <code>src/gentropy/ukb_ppp_eur_sumstat_preprocess.py</code> <pre><code>class UkbPppEurStep:\n    \"\"\"UKB PPP (EUR) data ingestion and harmonisation.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        raw_study_index_path_from_tsv: str,\n        raw_summary_stats_path: str,\n        variant_annotation_path: str,\n        tmp_variant_annotation_path: str,\n        study_index_output_path: str,\n        summary_stats_output_path: str,\n    ) -&gt; None:\n        \"\"\"Run UKB PPP (EUR) data ingestion and harmonisation step.\n\n        Args:\n            session (Session): Session object.\n            raw_study_index_path_from_tsv (str): Input raw study index path.\n            raw_summary_stats_path (str): Input raw summary stats path.\n            variant_annotation_path (str): Input variant annotation dataset path.\n            tmp_variant_annotation_path (str): Temporary output path for variant annotation dataset.\n            study_index_output_path (str): Study index output path.\n            summary_stats_output_path (str): Summary stats output path.\n        \"\"\"\n        session.logger.info(\n            \"Pre-compute the direct and flipped variant annotation dataset.\"\n        )\n        prepare_va(session, variant_annotation_path, tmp_variant_annotation_path)\n\n        session.logger.info(\"Process study index.\")\n        (\n            UkbPppEurStudyIndex.from_source(\n                spark=session.spark,\n                raw_study_index_path_from_tsv=raw_study_index_path_from_tsv,\n                raw_summary_stats_path=raw_summary_stats_path,\n            )\n            .df.write.mode(\"overwrite\")\n            .parquet(study_index_output_path)\n        )\n\n        session.logger.info(\"Process and harmonise summary stats.\")\n        process_summary_stats_per_chromosome(\n            session,\n            UkbPppEurSummaryStats,\n            raw_summary_stats_path,\n            tmp_variant_annotation_path,\n            summary_stats_output_path,\n            study_index_output_path,\n        )\n</code></pre>"},{"location":"python_api/steps/ukb_ppp_eur_sumstat_preprocess/#gentropy.ukb_ppp_eur_sumstat_preprocess.UkbPppEurStep.__init__","title":"<code>__init__(session: Session, raw_study_index_path_from_tsv: str, raw_summary_stats_path: str, variant_annotation_path: str, tmp_variant_annotation_path: str, study_index_output_path: str, summary_stats_output_path: str) -&gt; None</code>","text":"<p>Run UKB PPP (EUR) data ingestion and harmonisation step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>raw_study_index_path_from_tsv</code> <code>str</code> <p>Input raw study index path.</p> required <code>raw_summary_stats_path</code> <code>str</code> <p>Input raw summary stats path.</p> required <code>variant_annotation_path</code> <code>str</code> <p>Input variant annotation dataset path.</p> required <code>tmp_variant_annotation_path</code> <code>str</code> <p>Temporary output path for variant annotation dataset.</p> required <code>study_index_output_path</code> <code>str</code> <p>Study index output path.</p> required <code>summary_stats_output_path</code> <code>str</code> <p>Summary stats output path.</p> required Source code in <code>src/gentropy/ukb_ppp_eur_sumstat_preprocess.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    raw_study_index_path_from_tsv: str,\n    raw_summary_stats_path: str,\n    variant_annotation_path: str,\n    tmp_variant_annotation_path: str,\n    study_index_output_path: str,\n    summary_stats_output_path: str,\n) -&gt; None:\n    \"\"\"Run UKB PPP (EUR) data ingestion and harmonisation step.\n\n    Args:\n        session (Session): Session object.\n        raw_study_index_path_from_tsv (str): Input raw study index path.\n        raw_summary_stats_path (str): Input raw summary stats path.\n        variant_annotation_path (str): Input variant annotation dataset path.\n        tmp_variant_annotation_path (str): Temporary output path for variant annotation dataset.\n        study_index_output_path (str): Study index output path.\n        summary_stats_output_path (str): Summary stats output path.\n    \"\"\"\n    session.logger.info(\n        \"Pre-compute the direct and flipped variant annotation dataset.\"\n    )\n    prepare_va(session, variant_annotation_path, tmp_variant_annotation_path)\n\n    session.logger.info(\"Process study index.\")\n    (\n        UkbPppEurStudyIndex.from_source(\n            spark=session.spark,\n            raw_study_index_path_from_tsv=raw_study_index_path_from_tsv,\n            raw_summary_stats_path=raw_summary_stats_path,\n        )\n        .df.write.mode(\"overwrite\")\n        .parquet(study_index_output_path)\n    )\n\n    session.logger.info(\"Process and harmonise summary stats.\")\n    process_summary_stats_per_chromosome(\n        session,\n        UkbPppEurSummaryStats,\n        raw_summary_stats_path,\n        tmp_variant_annotation_path,\n        summary_stats_output_path,\n        study_index_output_path,\n    )\n</code></pre>"},{"location":"python_api/steps/variant_annotation_step/","title":"GnomAD variant data ingestion","text":""},{"location":"python_api/steps/variant_annotation_step/#gentropy.gnomad_ingestion.GnomadVariantIndexStep","title":"<code>gentropy.gnomad_ingestion.GnomadVariantIndexStep</code>","text":"<p>A step to generate variant index dataset from gnomad data.</p> <p>Variant annotation step produces a dataset of the type <code>VariantIndex</code> derived from gnomADs <code>gnomad.genomes.vX.X.X.sites.ht</code> Hail's table. This dataset is used to validate variants and as a source of annotation.</p> Source code in <code>src/gentropy/gnomad_ingestion.py</code> <pre><code>class GnomadVariantIndexStep:\n    \"\"\"A step to generate variant index dataset from gnomad data.\n\n    Variant annotation step produces a dataset of the type `VariantIndex` derived from gnomADs `gnomad.genomes.vX.X.X.sites.ht` Hail's table.\n    This dataset is used to validate variants and as a source of annotation.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        variant_annotation_path: str = GnomadVariantConfig().variant_annotation_path,\n        gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path,\n        gnomad_variant_populations: list[\n            VariantPopulation | str\n        ] = GnomadVariantConfig().gnomad_variant_populations,\n    ) -&gt; None:\n        \"\"\"Run Variant Annotation step.\n\n        Args:\n            session (Session): Session object.\n            variant_annotation_path (str): Path to resulting dataset.\n            gnomad_genomes_path (str): Path to gnomAD genomes hail table, e.g. `gs://gcp-public-data--gnomad/release/4.0/ht/genomes/gnomad.genomes.v4.0.sites.ht/`.\n            gnomad_variant_populations (list[VariantPopulation | str]): List of populations to include.\n\n        All defaults are stored in the GnomadVariantConfig.\n        \"\"\"\n        # amend data source version to output path\n        session.logger.info(\"Gnomad variant annotation path:\")\n        session.logger.info(variant_annotation_path)\n        # Parse variant info from source.\n        (\n            GnomADVariants(\n                gnomad_genomes_path=gnomad_genomes_path,\n                gnomad_variant_populations=gnomad_variant_populations,\n            )\n            # Convert data to variant index:\n            .as_variant_index()\n            # Write file:\n            .df.repartitionByRange(\"chromosome\", \"position\")\n            .sortWithinPartitions(\"chromosome\", \"position\")\n            .write.mode(session.write_mode)\n            .parquet(variant_annotation_path)\n        )\n</code></pre>"},{"location":"python_api/steps/variant_annotation_step/#gentropy.gnomad_ingestion.GnomadVariantIndexStep.__init__","title":"<code>__init__(session: Session, variant_annotation_path: str = GnomadVariantConfig().variant_annotation_path, gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path, gnomad_variant_populations: list[VariantPopulation | str] = GnomadVariantConfig().gnomad_variant_populations) -&gt; None</code>","text":"<p>Run Variant Annotation step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>variant_annotation_path</code> <code>str</code> <p>Path to resulting dataset.</p> <code>variant_annotation_path</code> <code>gnomad_genomes_path</code> <code>str</code> <p>Path to gnomAD genomes hail table, e.g. <code>gs://gcp-public-data--gnomad/release/4.0/ht/genomes/gnomad.genomes.v4.0.sites.ht/</code>.</p> <code>gnomad_genomes_path</code> <code>gnomad_variant_populations</code> <code>list[VariantPopulation | str]</code> <p>List of populations to include.</p> <code>gnomad_variant_populations</code> <p>All defaults are stored in the GnomadVariantConfig.</p> Source code in <code>src/gentropy/gnomad_ingestion.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    variant_annotation_path: str = GnomadVariantConfig().variant_annotation_path,\n    gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path,\n    gnomad_variant_populations: list[\n        VariantPopulation | str\n    ] = GnomadVariantConfig().gnomad_variant_populations,\n) -&gt; None:\n    \"\"\"Run Variant Annotation step.\n\n    Args:\n        session (Session): Session object.\n        variant_annotation_path (str): Path to resulting dataset.\n        gnomad_genomes_path (str): Path to gnomAD genomes hail table, e.g. `gs://gcp-public-data--gnomad/release/4.0/ht/genomes/gnomad.genomes.v4.0.sites.ht/`.\n        gnomad_variant_populations (list[VariantPopulation | str]): List of populations to include.\n\n    All defaults are stored in the GnomadVariantConfig.\n    \"\"\"\n    # amend data source version to output path\n    session.logger.info(\"Gnomad variant annotation path:\")\n    session.logger.info(variant_annotation_path)\n    # Parse variant info from source.\n    (\n        GnomADVariants(\n            gnomad_genomes_path=gnomad_genomes_path,\n            gnomad_variant_populations=gnomad_variant_populations,\n        )\n        # Convert data to variant index:\n        .as_variant_index()\n        # Write file:\n        .df.repartitionByRange(\"chromosome\", \"position\")\n        .sortWithinPartitions(\"chromosome\", \"position\")\n        .write.mode(session.write_mode)\n        .parquet(variant_annotation_path)\n    )\n</code></pre>"},{"location":"python_api/steps/variant_index_step/","title":"variant_index","text":""},{"location":"python_api/steps/variant_index_step/#gentropy.variant_index.VariantIndexStep","title":"<code>gentropy.variant_index.VariantIndexStep</code>","text":"<p>Generate variant index based on a VEP output in json format.</p> <p>The variant index is a dataset that contains variant annotations extracted from VEP output. It is expected that all variants in the VEP output are present in the variant index. There's an option to provide extra variant annotations to be added to the variant index eg. allele frequencies from GnomAD.</p> Source code in <code>src/gentropy/variant_index.py</code> <pre><code>class VariantIndexStep:\n    \"\"\"Generate variant index based on a VEP output in json format.\n\n    The variant index is a dataset that contains variant annotations extracted from VEP output. It is expected that all variants in the VEP output are present in the variant index.\n    There's an option to provide extra variant annotations to be added to the variant index eg. allele frequencies from GnomAD.\n    \"\"\"\n\n    def __init__(\n        self: VariantIndexStep,\n        session: Session,\n        vep_output_json_path: str,\n        variant_index_path: str,\n        hash_threshold: int,\n        variant_annotations_path: list[str] | None = None,\n        amino_acid_change_annotations: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Run VariantIndex step.\n\n        Args:\n            session (Session): Session object.\n            vep_output_json_path (str): Variant effect predictor output path (in json format).\n            variant_index_path (str): Variant index dataset path to save resulting data.\n            hash_threshold (int): Hash threshold for variant identifier length.\n            variant_annotations_path (list[str] | None): List of paths to extra variant annotation datasets.\n            amino_acid_change_annotations (list[str] | None): list of paths to amino-acid based variant annotations.\n        \"\"\"\n        # Extract variant annotations from VEP output:\n        variant_index = VariantEffectPredictorParser.extract_variant_index_from_vep(\n            session.spark, vep_output_json_path, hash_threshold\n        )\n\n        # Process variant annotations if provided:\n        if variant_annotations_path:\n            for annotation_path in variant_annotations_path:\n                # Read variant annotations from parquet:\n                annotations = VariantIndex.from_parquet(\n                    session=session,\n                    path=annotation_path,\n                    recursiveFileLookup=True,\n                    id_threshold=hash_threshold,\n                )\n\n                # Update index with extra annotations:\n                variant_index = variant_index.add_annotation(annotations)\n\n        # If provided read amino-acid based annotation and enrich variant index:\n        if amino_acid_change_annotations:\n            for annotation_path in amino_acid_change_annotations:\n                annotation_data = AminoAcidVariants.from_parquet(\n                    session, annotation_path\n                )\n\n                # Update index with extra annotations:\n                variant_index = variant_index.annotate_with_amino_acid_consequences(\n                    annotation_data\n                )\n\n        (\n            variant_index.df.repartitionByRange(\n                session.output_partitions, \"chromosome\", \"position\"\n            )\n            .sortWithinPartitions(\"chromosome\", \"position\")\n            .write.mode(session.write_mode)\n            .parquet(variant_index_path)\n        )\n</code></pre>"},{"location":"python_api/steps/variant_index_step/#gentropy.variant_index.VariantIndexStep.__init__","title":"<code>__init__(session: Session, vep_output_json_path: str, variant_index_path: str, hash_threshold: int, variant_annotations_path: list[str] | None = None, amino_acid_change_annotations: list[str] | None = None) -&gt; None</code>","text":"<p>Run VariantIndex step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>vep_output_json_path</code> <code>str</code> <p>Variant effect predictor output path (in json format).</p> required <code>variant_index_path</code> <code>str</code> <p>Variant index dataset path to save resulting data.</p> required <code>hash_threshold</code> <code>int</code> <p>Hash threshold for variant identifier length.</p> required <code>variant_annotations_path</code> <code>list[str] | None</code> <p>List of paths to extra variant annotation datasets.</p> <code>None</code> <code>amino_acid_change_annotations</code> <code>list[str] | None</code> <p>list of paths to amino-acid based variant annotations.</p> <code>None</code> Source code in <code>src/gentropy/variant_index.py</code> <pre><code>def __init__(\n    self: VariantIndexStep,\n    session: Session,\n    vep_output_json_path: str,\n    variant_index_path: str,\n    hash_threshold: int,\n    variant_annotations_path: list[str] | None = None,\n    amino_acid_change_annotations: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Run VariantIndex step.\n\n    Args:\n        session (Session): Session object.\n        vep_output_json_path (str): Variant effect predictor output path (in json format).\n        variant_index_path (str): Variant index dataset path to save resulting data.\n        hash_threshold (int): Hash threshold for variant identifier length.\n        variant_annotations_path (list[str] | None): List of paths to extra variant annotation datasets.\n        amino_acid_change_annotations (list[str] | None): list of paths to amino-acid based variant annotations.\n    \"\"\"\n    # Extract variant annotations from VEP output:\n    variant_index = VariantEffectPredictorParser.extract_variant_index_from_vep(\n        session.spark, vep_output_json_path, hash_threshold\n    )\n\n    # Process variant annotations if provided:\n    if variant_annotations_path:\n        for annotation_path in variant_annotations_path:\n            # Read variant annotations from parquet:\n            annotations = VariantIndex.from_parquet(\n                session=session,\n                path=annotation_path,\n                recursiveFileLookup=True,\n                id_threshold=hash_threshold,\n            )\n\n            # Update index with extra annotations:\n            variant_index = variant_index.add_annotation(annotations)\n\n    # If provided read amino-acid based annotation and enrich variant index:\n    if amino_acid_change_annotations:\n        for annotation_path in amino_acid_change_annotations:\n            annotation_data = AminoAcidVariants.from_parquet(\n                session, annotation_path\n            )\n\n            # Update index with extra annotations:\n            variant_index = variant_index.annotate_with_amino_acid_consequences(\n                annotation_data\n            )\n\n    (\n        variant_index.df.repartitionByRange(\n            session.output_partitions, \"chromosome\", \"position\"\n        )\n        .sortWithinPartitions(\"chromosome\", \"position\")\n        .write.mode(session.write_mode)\n        .parquet(variant_index_path)\n    )\n</code></pre>"},{"location":"python_api/steps/window_based_clumping/","title":"window_based_clumping","text":""},{"location":"python_api/steps/window_based_clumping/#gentropy.window_based_clumping.WindowBasedClumpingStep","title":"<code>gentropy.window_based_clumping.WindowBasedClumpingStep</code>","text":"<p>Apply window based clumping on summary statistics datasets.</p> Source code in <code>src/gentropy/window_based_clumping.py</code> <pre><code>class WindowBasedClumpingStep:\n    \"\"\"Apply window based clumping on summary statistics datasets.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        summary_statistics_input_path: str,\n        study_locus_output_path: str,\n        distance: int = WindowBasedClumpingStepConfig().distance,\n        gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance,\n        collect_locus: bool = WindowBasedClumpingStepConfig().collect_locus,\n        collect_locus_distance: int = WindowBasedClumpingStepConfig().collect_locus_distance,\n        inclusion_list_path: str\n        | None = WindowBasedClumpingStepConfig().inclusion_list_path,\n    ) -&gt; None:\n        \"\"\"Run window-based clumping step.\n\n        Args:\n            session (Session): Session object.\n            summary_statistics_input_path (str): Path to the harmonized summary statistics dataset.\n            study_locus_output_path (str): Output path for the resulting study locus dataset.\n            distance (int): Distance, within which tagging variants are collected around the semi-index. Optional.\n            gwas_significance (float): GWAS significance threshold. Defaults to 5e-8.\n            collect_locus (bool): Whether to collect locus around semi-indices. Optional.\n            collect_locus_distance (int): Distance, within which tagging variants are collected around the semi-index. Optional.\n            inclusion_list_path (str | None): Path to the inclusion list (list of white-listed study identifier). Optional.\n\n        Check WindowBasedClumpingStepConfig object for default values.\n        \"\"\"\n        # If inclusion list path is provided, only these studies will be read:\n        if inclusion_list_path:\n            study_ids_to_ingest = [\n                f'{summary_statistics_input_path}/{row[\"studyId\"]}.parquet'\n                for row in session.spark.read.parquet(inclusion_list_path).collect()\n            ]\n        else:\n            # If no inclusion list is provided, read all summary stats in folder:\n            study_ids_to_ingest = [summary_statistics_input_path]\n\n        ss = SummaryStatistics.from_parquet(\n            session,\n            study_ids_to_ingest,\n            recursiveFileLookup=True,\n        )\n\n        # Clumping:\n        study_locus = ss.window_based_clumping(\n            distance=distance, gwas_significance=gwas_significance\n        )\n\n        # Optional locus collection:\n        if collect_locus:\n            # Collecting locus around semi-indices:\n            study_locus = study_locus.annotate_locus_statistics(\n                ss, collect_locus_distance=collect_locus_distance\n            )\n\n        study_locus.df.write.mode(session.write_mode).parquet(study_locus_output_path)\n</code></pre>"},{"location":"python_api/steps/window_based_clumping/#gentropy.window_based_clumping.WindowBasedClumpingStep.__init__","title":"<code>__init__(session: Session, summary_statistics_input_path: str, study_locus_output_path: str, distance: int = WindowBasedClumpingStepConfig().distance, gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance, collect_locus: bool = WindowBasedClumpingStepConfig().collect_locus, collect_locus_distance: int = WindowBasedClumpingStepConfig().collect_locus_distance, inclusion_list_path: str | None = WindowBasedClumpingStepConfig().inclusion_list_path) -&gt; None</code>","text":"<p>Run window-based clumping step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>summary_statistics_input_path</code> <code>str</code> <p>Path to the harmonized summary statistics dataset.</p> required <code>study_locus_output_path</code> <code>str</code> <p>Output path for the resulting study locus dataset.</p> required <code>distance</code> <code>int</code> <p>Distance, within which tagging variants are collected around the semi-index. Optional.</p> <code>distance</code> <code>gwas_significance</code> <code>float</code> <p>GWAS significance threshold. Defaults to 5e-8.</p> <code>gwas_significance</code> <code>collect_locus</code> <code>bool</code> <p>Whether to collect locus around semi-indices. Optional.</p> <code>collect_locus</code> <code>collect_locus_distance</code> <code>int</code> <p>Distance, within which tagging variants are collected around the semi-index. Optional.</p> <code>collect_locus_distance</code> <code>inclusion_list_path</code> <code>str | None</code> <p>Path to the inclusion list (list of white-listed study identifier). Optional.</p> <code>inclusion_list_path</code> <p>Check WindowBasedClumpingStepConfig object for default values.</p> Source code in <code>src/gentropy/window_based_clumping.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    summary_statistics_input_path: str,\n    study_locus_output_path: str,\n    distance: int = WindowBasedClumpingStepConfig().distance,\n    gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance,\n    collect_locus: bool = WindowBasedClumpingStepConfig().collect_locus,\n    collect_locus_distance: int = WindowBasedClumpingStepConfig().collect_locus_distance,\n    inclusion_list_path: str\n    | None = WindowBasedClumpingStepConfig().inclusion_list_path,\n) -&gt; None:\n    \"\"\"Run window-based clumping step.\n\n    Args:\n        session (Session): Session object.\n        summary_statistics_input_path (str): Path to the harmonized summary statistics dataset.\n        study_locus_output_path (str): Output path for the resulting study locus dataset.\n        distance (int): Distance, within which tagging variants are collected around the semi-index. Optional.\n        gwas_significance (float): GWAS significance threshold. Defaults to 5e-8.\n        collect_locus (bool): Whether to collect locus around semi-indices. Optional.\n        collect_locus_distance (int): Distance, within which tagging variants are collected around the semi-index. Optional.\n        inclusion_list_path (str | None): Path to the inclusion list (list of white-listed study identifier). Optional.\n\n    Check WindowBasedClumpingStepConfig object for default values.\n    \"\"\"\n    # If inclusion list path is provided, only these studies will be read:\n    if inclusion_list_path:\n        study_ids_to_ingest = [\n            f'{summary_statistics_input_path}/{row[\"studyId\"]}.parquet'\n            for row in session.spark.read.parquet(inclusion_list_path).collect()\n        ]\n    else:\n        # If no inclusion list is provided, read all summary stats in folder:\n        study_ids_to_ingest = [summary_statistics_input_path]\n\n    ss = SummaryStatistics.from_parquet(\n        session,\n        study_ids_to_ingest,\n        recursiveFileLookup=True,\n    )\n\n    # Clumping:\n    study_locus = ss.window_based_clumping(\n        distance=distance, gwas_significance=gwas_significance\n    )\n\n    # Optional locus collection:\n    if collect_locus:\n        # Collecting locus around semi-indices:\n        study_locus = study_locus.annotate_locus_statistics(\n            ss, collect_locus_distance=collect_locus_distance\n        )\n\n    study_locus.df.write.mode(session.write_mode).parquet(study_locus_output_path)\n</code></pre>"}]}