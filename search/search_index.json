{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Targets Gentropy","text":"<p>Open Targets Gentropy is a Python package to facilitate the interpretation and analysis of GWAS and functional genomic studies for target identification. This package contains a toolkit for the harmonisation, statistical analysis and prioritisation of genetic signals to assist drug discovery.</p>"},{"location":"#key-features","title":"Key Features:","text":"<ul> <li>Specialized Datatypes: Introduces essential genetics datatypes like StudyLocus, LocusToGene, and SummaryStatistics.</li> <li>Performance-Oriented: Optimized for large-scale genetic data analysis, including locus-to-gene scoring, fine mapping, and colocalization analysis.</li> <li>User-Friendly: The package is designed to be intuitive, allowing both beginners and experienced researchers to conduct complex genetic with ease.</li> </ul>"},{"location":"#about-open-targets","title":"About Open Targets","text":"<p>Open Targets is a pre-competitive, public-private partnership that uses human genetics and genomics data to systematically identify and prioritise drug targets. Through large-scale genomic experiments and the development of innovative computational techniques, the partnership aims to help researchers select the best targets for the development of new therapies. For more information, visit the Open Targets website.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<p>To install gentropy one needs to have pre installed:</p> <ul> <li>Python &gt;=3.11, &lt;3.14</li> <li>PySpark &gt;=3.5.0, &lt;3.6</li> <li>Java 11, 17 (for hail support Java 11 is recommended, see troubleshooting for more details)</li> </ul>"},{"location":"installation/#installation","title":"Installation","text":"<p>To install Gentropy we recommend using uv, which is a tool for managing Python environments and dependencies.</p> <pre><code>uv add gentropy\n</code></pre>"},{"location":"installation/#pypi","title":"Pypi","text":"<p>We recommend installing Open Targets Gentropy using Pypi:</p> <pre><code>pip install gentropy\n</code></pre>"},{"location":"installation/#source","title":"Source","text":"<p>Alternatively, you can install Open Targets Gentropy from source. Check the contributing section for more information.</p> <p>For any issues with the installation, check the troubleshooting section.</p>"},{"location":"installation/#xgboost","title":"xgboost","text":"<p>To use gentropy <code>LocusToGene</code> model the <code>xgboost</code> package is required. To reduce the size of the dependencies, gentropy uses the full <code>xgboost</code> package only when <code>xgboost-cpu</code> is not available:</p> <ul> <li><code>amd64</code> and <code>x86_64</code> will utilize <code>xgboost-cpu</code>.</li> <li><code>arm64</code> and <code>aarch64</code> will utilize <code>xgboost</code>.</li> </ul>"},{"location":"development/_development/","title":"Development","text":"<p>This section contains various technical information on how to develop and run the code.</p>"},{"location":"development/contributing/","title":"Contributing guidelines","text":""},{"location":"development/contributing/#one-time-configuration","title":"One-time configuration","text":"<p>The steps in this section only ever need to be done once on any particular system.</p> <p>For Google Cloud configuration:</p> <ol> <li> <p>Install Google Cloud SDK: https://cloud.google.com/sdk/docs/install.</p> </li> <li> <p>Log in to your work Google Account: run <code>gcloud auth login</code> and follow instructions.</p> </li> <li> <p>Obtain Google application credentials: run <code>gcloud auth application-default login</code> and follow instructions.</p> </li> </ol> <p>Check that you have the <code>make</code> utility installed, and if not (which is unlikely), install it using your system package manager.</p> <p>Java support</p> <p>Check that you have <code>java</code> installed. To be able to use all features including hail support use java 11 (for handling multiple java versions, consider using <code>sdkman</code>).</p>"},{"location":"development/contributing/#environment-configuration","title":"Environment configuration","text":"<p>Run <code>make setup-dev</code> to install/update the necessary packages (including required python version for development) and activate the development environment. You need to do it just once.</p> <p>It is recommended to use VS Code as an IDE for development.</p>"},{"location":"development/contributing/#how-to-create-gentropy-step","title":"How to create gentropy step","text":"<p>All gentropy steps can be invoked after successful environment configuration by running</p> <pre><code>uv run gentropy step=&lt;step_name&gt;\n</code></pre> <ol> <li> <p>Create a new step config in the <code>src/gentropy/config.py</code> that inherits from <code>StepConfig</code> class.</p> </li> <li> <p>Register new step configuration to <code>ConfigStore</code>.</p> </li> <li> <p>Create a step class that holds the business logic in new file in the <code>src/gentropy/{your_step_name}.py</code>.</p> </li> </ol>"},{"location":"development/contributing/#contributing-checklist","title":"Contributing checklist","text":"<p>When making changes, and especially when implementing a new module or feature, it's essential to ensure that all relevant sections of the code base are modified.</p> <ul> <li> Run <code>make check</code>. This will run the linter and formatter to ensure that the code is compliant with the project conventions.</li> <li> Develop unit tests for your code and run <code>make test</code>. This will run all unit tests in the repository, including the examples appended in the docstrings of some methods.</li> <li> Update the configuration if necessary.</li> <li> Update the documentation and check it with <code>make build-documentation</code>. This will start a local server to browse it (URL will be printed, usually <code>http://127.0.0.1:8000/</code>)</li> </ul> <p>For more details on each of these steps, see the sections below.</p>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<ul> <li>If during development you had a question which wasn't covered in the documentation, and someone explained it to you, add it to the documentation. The same applies if you encountered any instructions in the documentation which were obsolete or incorrect.</li> <li>Documentation autogeneration expressions start with <code>:::</code>. They will automatically generate sections of the documentation based on class and method docstrings. Be sure to update them for:</li> <li>Datasource main page, for example: <code>docs/python_api/datasources/finngen/_finngen.md</code></li> <li>Dataset definitions, for example: <code>docs/python_api/datasources/finngen/study_index.md</code></li> <li>Step definition, for example: <code>docs/python_api/steps/finngen_sumstat_preprocess.md</code></li> </ul>"},{"location":"development/contributing/#configuration","title":"Configuration","text":"<ul> <li>step default configuration in the <code>src/gentropy/config/</code> <code>StepConfig</code> derived classes.</li> </ul>"},{"location":"development/contributing/#classes","title":"Classes","text":"<ul> <li>Datasource init, for example: <code>src/gentropy/datasource/finngen/__init__.py</code></li> <li>Dataset classes, for example: <code>src/gentropy/datasource/finngen/study_index.py</code> \u2192 <code>FinnGenStudyIndex</code></li> <li>Step main running class, for example: <code>src/gentropy/finngen_sumstat_preprocess.py</code></li> </ul>"},{"location":"development/contributing/#tests","title":"Tests","text":"<ul> <li>Test study fixture in <code>tests/conftest.py</code>, for example: <code>mock_study_index_finngen</code> in that module</li> <li>Test sample data, for example: <code>tests/gentropy/data_samples/finngen_studies_sample.json</code></li> <li>Test definition, for example: <code>tests/dataset/test_study_index.py</code> \u2192 <code>test_study_index_finngen_creation</code>)</li> </ul>"},{"location":"development/contributing/#airflow-dags","title":"Airflow dags","text":"<ul> <li>Upstream of version 2.0.0 airflow orchestration layer was moved to the orchestration repository</li> </ul>"},{"location":"development/contributing/#support-for-python-versions","title":"Support for python versions","text":"<p>As of version 2.1.X gentropy supports multiple python versions. To ensure compatibility with all supported versions, unit tests are run for each of the minor python release from 3.10 to 3.12. Make sure your changes are compatible with all supported versions.</p>"},{"location":"development/contributing/#development-process","title":"Development process","text":"<p>The development follows simplified Git Flow process that includes usage of</p> <ul> <li><code>dev</code> (development branch)</li> <li><code>feature</code> branches</li> <li><code>main</code> (production branch)</li> </ul> <p>The development starts with creating new <code>feature</code> branch based on the <code>dev</code> branch. Once the feature is ready, the Pull Request for the <code>dev</code> branch is created and CI/CD Checks are performed to ensure that the code is compliant with the project conventions. Once the PR is approved, the feature branch is merged into the <code>dev</code> branch.</p>"},{"location":"development/contributing/#development-releases","title":"Development releases","text":"<p>One can create the dev release tagged by <code>vX.Y.Z-dev.V</code> tag. This release will not trigger the CI/CD pipeline to publish the package to the PyPi repository. The release is done by triggering the <code>Release</code> GitHub action.</p>"},{"location":"development/contributing/#production-releases","title":"Production releases","text":"<p>Once per week, the <code>Trigger PR for release</code> github action creates a Pull Request from <code>dev</code> to <code>main</code> branch, when the PR is approved, the <code>Release</code> GitHub action is triggered to create a production release tagged by <code>vX.Y.Z</code> tag. This release triggers the CI/CD pipeline to publish the package to the TestPyPi repository. If it is successful, then the actual deployment to the PyPI repository is done. The deployment to the PyPi repository must be verified by the gentropy maintainer.</p> <p>Below you can find a simplified diagram of the development process.</p>"},{"location":"development/troubleshooting/","title":"Troubleshooting","text":""},{"location":"development/troubleshooting/#blaslapack","title":"BLAS/LAPACK","text":"<p>If you see errors related to BLAS/LAPACK libraries, see this StackOverflow post for guidance.</p>"},{"location":"development/troubleshooting/#uv","title":"UV","text":"<p>The default python version and gentropy dependencies are managed by uv. To perform a fresh installation run <code>make setup-dev</code>.</p>"},{"location":"development/troubleshooting/#adding-new-dependencies-or-updating-existing-ones","title":"Adding new dependencies or updating existing ones","text":"<p>To add new dependencies or update existing ones, you need to update the <code>pyproject.toml</code> file. This can be done automatically with <code>uv add ${package}</code> command. Refer to the uv documentation for more information.</p>"},{"location":"development/troubleshooting/#java","title":"Java","text":"<p>Officially, PySpark requires Java version 8, or 11, 17. To support hail (gentropy dependency) it is recommended to use Java 11.</p>"},{"location":"development/troubleshooting/#setting-java-with-sdkman","title":"setting Java with sdkman","text":"<p>sdkman is a tool for managing parallel versions of multiple java SDK on most Unix based systems. It can be used to install and manage Java versions. See sdkman documentation for more information.</p>"},{"location":"development/troubleshooting/#pre-commit","title":"Pre-commit","text":"<p>If you see an error message thrown by pre-commit, which looks like this (<code>SyntaxError: Unexpected token '?'</code>), followed by a JavaScript traceback, the issue is likely with your system NodeJS version.</p> <p>One solution which can help in this case is to upgrade your system NodeJS version. However, this may not always be possible. For example, Ubuntu repository is several major versions behind the latest version as of July 2023.</p> <p>Another solution which helps is to remove Node, NodeJS, and npm from your system entirely. In this case, pre-commit will not try to rely on a system version of NodeJS and will install its own, suitable one.</p> <p>On Ubuntu, this can be done using <code>sudo apt remove node nodejs npm</code>, followed by <code>sudo apt autoremove</code>. But in some cases, depending on your existing installation, you may need to also manually remove some files. See this StackOverflow answer for guidance.</p>"},{"location":"development/troubleshooting/#macos","title":"MacOS","text":"<ul> <li> <p>To run L2G trainer on MacOS you need to install <code>libomp</code> using <code>brew install libomp</code>.</p> </li> <li> <p>Some functions on MacOS may throw a java error:</p> </li> </ul> <p><code>python3.10/site-packages/py4j/protocol.py:326: Py4JJavaError</code></p> <p>This can be resolved by adding the follow line to your <code>~/.zshrc</code>:</p> <p><code>export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES</code></p>"},{"location":"development/troubleshooting/#creating-development-dataproc-cluster-ot-users-only","title":"Creating development dataproc cluster (OT users only)","text":"<p>Requirements</p> <p>To create the cluster, you need to auth to the google cloud</p> <pre><code>gcloud auth login\n</code></pre> <p>To start dataproc cluster in the development mode run.</p> <pre><code>make create-dev-cluster REF=dev\n</code></pre> <p><code>REF</code> - remote branch available at the gentropy repository</p> <p>During cluster initialization actions the <code>utils/install_dependencies_on_cluster.sh</code> script is run, that installs <code>gentropy</code> package from the remote repository by using VCS support, hence it does not require the gentropy package whl artifact to be prepared in the Google Cloud Storage before the make command can be run.</p> <p>Check details how to make a package installable by VCS in pip documentation.</p> <p>How <code>create-dev-cluster</code> works</p> <p>This command will work, provided you have done one of:</p> <ul> <li>run <code>make create-dev-cluster REF=dev</code>, since the REF is requested, the cluster will attempt to install it from the remote repository.</li> <li>run <code>make create-dev-cluster</code> without specifying the REF or specifying REF that points to your local branch will request branch name you are checkout on your local repository, if any changes are pending locally, the cluster can not be created, it requires stashing or pushing the changes to the remote.</li> </ul> <p>The command will create a new dataproc cluster with the following configuration:</p> <ul> <li>package installed from the requested REF (for example <code>dev</code> or <code>feature/xxx</code>)</li> <li>uv installed in the cluster (to speed up the installation and dependency resolution process)</li> <li>cli script to run gentropy steps</li> </ul> <p>Dataproc cluster timeout</p> <p>By default the cluster will delete itself when running for 60 minutes after the last submitted job to the cluster was successfully completed (running jobs interactively via Jupyter or Jupyter lab is not treated as submitted job). To preserve the cluster for arbitrary period (for instance when the cluster is used only for interactive jobs) increase the cluster timeout:</p> <pre><code>make create-dev-cluster CLUSTER_TIMEOUT=1d REF=dev # 60m 1h 1d (by default 60m)\n</code></pre> <p>For the reference on timeout format check gcloud documentation</p>"},{"location":"howto/_howto/","title":"How-to","text":"<p>This page contains a collection of how-to guides for the project.</p> <ul> <li>Command line interface: Learn how to use the Gentropy CLI.</li> <li>tutorials: Learn how to use the Gentropy Python package.</li> </ul> <p>For additional information please visit https://community.opentargets.org/</p>"},{"location":"howto/command_line/_command_line/","title":"Command line","text":""},{"location":"howto/command_line/_command_line/#command-line-interface","title":"Command line interface","text":"<p>Gentropy steps can be run using the command line interface (CLI). This section contains a collection of how-to guides for the CLI.</p>"},{"location":"howto/command_line/run_step_in_cli/","title":"Run step in cli","text":""},{"location":"howto/command_line/run_step_in_cli/#run-step-in-cli","title":"Run step in CLI","text":"<p>To run a step in the command line interface (CLI), you need to know the step's name. To list what steps are avaiable in your current environment, simply run <code>gentropy</code> with no arguments. This will list all the steps:</p> <pre><code>You must specify 'step', e.g, step=&lt;OPTION&gt;\nAvailable options:\n        clump\n        colocalisation\n        eqtl_catalogue\n        finngen_studies\n        finngen_sumstat_preprocess\n        gwas_catalog_ingestion\n        gwas_catalog_sumstat_preprocess\n        ld_index\n        locus_to_gene\n        overlaps\n        pics\n        ukbiobank\n        variant_annotation\n        variant_index\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n</code></pre> <p>As indicated, you can run a step by specifying the step's name with the <code>step</code> argument. For example, to run the <code>gwas_catalog_sumstat_preprocess</code> step, you can run:</p> <pre><code>gentropy step=gwas_catalog_sumstat_preprocess\n</code></pre> <p>In most occassions, some mandatory values will be required to run the step. For example, the <code>gwas_catalog_sumstat_preprocess</code> step requires the <code>step.raw_sumstats_path</code> and <code>step.out_sumstats_path</code> argument to be specified. You can complete the necessary arguments by adding them to the command line:</p> <pre><code>gentropy step=gwas_catalog_sumstat_preprocess step.raw_sumstats_path=/path/to/raw_sumstats step.out_sumstats_path=/path/to/out_sumstats\n</code></pre> <p>You can find more about the available steps in the documentation.</p>"},{"location":"howto/command_line/run_step_using_config/","title":"Run step using config","text":""},{"location":"howto/command_line/run_step_using_config/#run-step-using-yaml-config","title":"Run step using YAML config","text":"<p>It's possible to parametrise the functionality of a step using a YAML configuration file. This is useful when you want to run a step multiple times with different parameters or simply to avoid having to specify the same parameters every time you run a step.</p> <p>Info</p> <p>The package uses Hydra to handle configuration files. For more information, please visit the Hydra documentation.</p> <p>To run a step using a configuration file, you need to create a configuration file in YAML format.</p> <pre><code>config/\n\u251c\u2500 step/\n\u2502  \u2514\u2500 my_gwas_catalog_sumstat_preprocess.md\n\u2514\u2500 my_config.yml\n</code></pre> <p>The configuration file should contain the parameters you want to use to run the step. For example, to run the <code>gwas_catalog_sumstat_preprocess</code> step, you need to specify the <code>step.raw_sumstats_path</code> and <code>step.out_sumstats_path</code> parameters. The configuration file should look like this:</p> my_config.yamlstep/my_gwas_catalog_sumstat_preprocess.md <pre><code>defaults:\n    - config\n    - _self_\n</code></pre> <p>This config file will specify that your configuration file will inherit the default configuration (<code>config</code>) and everything provided (<code>_self_</code>) will overwrite the default configuration.</p> <pre><code>defaults:\n    - gwas_catalog_sumstat_preprocess\n\nraw_sumstats_path: /path/to/raw_sumstats\nout_sumstats_path: /path/to/out_sumstats\n</code></pre> <p>This config file will inherit the default configuration for the <code>gwas_catalog_sumstat_preprocess</code> step and overwrite the <code>raw_sumstats_path</code> and <code>out_sumstats_path</code> parameters.</p> <p>Once you have created the configuration file, you can run your own new <code>my_gwas_catalog_sumstat_preprocess</code>:</p> <pre><code>gentropy step=my_gwas_catalog_sumstat_preprocess --config-dir=config --config-name=my_config\n</code></pre>"},{"location":"howto/tutorials/_tutorials/","title":"Python API","text":"<p>This section explains how to use gentropy in a Python environment providing a foundational understanding on how to perform genetics analyses using the package. This section can be useful for users wishing to use Gentropy in their own projects.</p>"},{"location":"howto/tutorials/a_creating_spark_session/","title":"Creating a Spark Session","text":"<p>In this section, we'll guide you through creating a Spark session using Gentropy's Session class. Gentropy uses Apache PySpark as the underlying framework for distributed computing. The Session class provides a convenient way to initialize a Spark session with pre-configured settings.</p>"},{"location":"howto/tutorials/a_creating_spark_session/#creating-a-default-session","title":"Creating a Default Session","text":"<p>To begin your journey with Gentropy, start by creating a default Spark session. This is the simplest way to initialize your environment.</p> <pre><code>from gentropy import Session\n\n# Create a default Spark Session\nsession = Session()\n</code></pre> <p>The above code snippet sets up a default Spark session with pre-configured settings. This is ideal for getting started quickly without needing to tweak any configurations.</p>"},{"location":"howto/tutorials/a_creating_spark_session/#customizing-your-spark-session","title":"Customizing Your Spark Session","text":"<p>Gentropy allows you to customize the Spark session to suit your specific needs. You can modify various parameters such as memory allocation, number of executors, and more. This flexibility is particularly useful for optimizing performance in steps that are more computationally intensive.</p>"},{"location":"howto/tutorials/a_creating_spark_session/#example-increasing-driver-memory","title":"Example: Increasing Driver Memory","text":"<p>If you require more memory for the Spark driver, you can easily adjust this setting:</p> <pre><code>from gentropy import Session\n\n# Create a Spark session with increased driver memory\nsession = Session(extended_spark_conf={\"spark.driver.memory\": \"4g\"})\n</code></pre> <p>This code snippet demonstrates how to increase the memory allocated to the Spark driver to 16 gigabytes. You can customize other Spark settings similarly, according to your project's requirements.</p>"},{"location":"howto/tutorials/a_creating_spark_session/#whats-next","title":"What's next?","text":"<p>Now that you've created a Spark session, you're ready to start using Gentropy. In the next section, we'll show you how to process a large dataset using Gentropy's powerful SummaryStatistics datatype.</p>"},{"location":"howto/tutorials/b_create_dataset/","title":"Create a dataset","text":""},{"location":"howto/tutorials/b_create_dataset/#datasets","title":"Datasets","text":"<p>Gentropy Datasets are the most basic concept that allows to represent various abstract data modalities: Variant, Gene, Locus, etc.</p> <p>The full list of <code>Dataset</code>s is available in the Python API documentation.</p> <p>Any instance of Dataset will have 2 common attributes</p> <ul> <li>df: the Spark DataFrame that contains the data</li> <li>schema: the definition of the data structure in Spark format</li> </ul>"},{"location":"howto/tutorials/b_create_dataset/#dataset-implementation-pyspark-dataframe","title":"Dataset implementation - pyspark DataFrame","text":"<p>Datasets are implemented as Classes that are composed of the PySpark DataFrames, this means that the <code>Dataset</code> has a <code>df</code> attribute that references a dataframe with the specific schema.</p>"},{"location":"howto/tutorials/b_create_dataset/#dataset-schemas-contract-with-the-user","title":"Dataset schemas - contract with the user","text":"<p>Each dataset specifies the contract that has to be met by the data provided by the package user in order to run methods implemented in Gentropy.</p> <p>The dataset contract is implemented as a pyspark DataFrame schema under <code>schema</code> attribute and includes the table field names, types and allows for specifying if a field is required or optional to construct the dataset. All dataset schemas can be found in the corresponding documentation pages under the Dataset API.</p>"},{"location":"howto/tutorials/b_create_dataset/#dataset-initialization","title":"Dataset initialization","text":"<p>In this section you'll learn the different ways of how to create a <code>Dataset</code> instances.</p>"},{"location":"howto/tutorials/b_create_dataset/#initializing-datasets-from-parquet-files","title":"Initializing datasets from parquet files","text":"<p>Each dataset has a method to read the <code>parquet</code> files into the <code>Dataset</code> instance with schema validation. This is implemented in the <code>Dataset.from_parquet</code> abstract method.</p> <pre><code># Create a SummaryStatistics object by loading data from the specified path\nfrom gentropy import SummaryStatistics\n\npath = \"path/to/summary/stats\"\nsummary_stats = SummaryStatistics.from_parquet(session, path)\n</code></pre> <p>Parquet files</p> <p>Parquet is a columnar storage format that is widely used in the Spark ecosystem. It is the recommended format for storing large datasets. For more information about parquet, please visit https://parquet.apache.org/.</p> <p>Reading multiple files</p> <p>If you have multiple parquet files, you can pass a</p> <ul> <li>directory path like <code>path/to/summary/stats</code> - reading all parquet files from <code>stats</code> directory.</li> <li>glob pattern like <code>path/to/summary/stats/*h.parquet</code> - reading all files that ends with <code>.parquet</code> from `stats directory.</li> </ul> <p>to the <code>from_parquet</code> method. The method will read all the parquet files in the directory and return a <code>Dataset</code> instance.</p>"},{"location":"howto/tutorials/b_create_dataset/#initializing-datasets-from-pyspark-dataframes","title":"Initializing datasets from pyspark DataFrames","text":"<p>Once one already has a pyspark DataFrame, it can be converted to a dataset using the default <code>Dataset</code> constructor. The constructor also validates the schema of the provided DataFrame against the dataset schema.</p>"},{"location":"howto/tutorials/b_create_dataset/#initializing-datasets-from-a-data-source","title":"Initializing datasets from a data source","text":"<p>Alternatively, <code>Dataset</code>s can be created using a data source harmonisation method. For example, to create a <code>SummaryStatistics</code> object from Finngen's raw summary statistics, you can use the <code>FinnGen</code> data source.</p> <pre><code># Create a SummaryStatistics object by loading raw data from Finngen\nfrom gentropy.datasource.finngen.summary_stats import FinnGenSummaryStats\n\npath = \"path/to/finngen/summary/stats\"\nsummary_stats = FinnGenSummaryStats.from_source(session.spark, path)\n</code></pre>"},{"location":"howto/tutorials/b_create_dataset/#initializing-datasets-from-a-pandas-dataframe","title":"Initializing datasets from a pandas DataFrame","text":"<p>If none of our data sources fit your needs, you can create a <code>Dataset</code> object from your own data. To do so, you need to transform your data to fit the <code>Dataset</code> schema.</p> <p>The schema of a Dataset is defined in Spark format</p> <p>The Dataset schemas can be found in the documentation of each Dataset. For example, the schema of the <code>SummaryStatistics</code> dataset can be found here.</p> <p>You can also create a <code>Dataset</code> from a pandas DataFrame. This is useful when you want to create a <code>Dataset</code> from a small dataset that fits in memory.</p> <pre><code>import pandas as pd\n\nfrom gentropy import SummaryStatistics\n\n\n# Load your transformed data into a pandas DataFrame\npath = \"path/to/your/data\"\ncustom_summary_stats_pandas_df = pd.read_csv(path)\n\n# Create a SummaryStatistics object specifying the data and schema\ncustom_summary_stats_df = session.spark.createDataFrame(\n    custom_summary_stats_pandas_df, schema=SummaryStatistics.get_schema()\n)\ncustom_summary_stats = SummaryStatistics(_df=custom_summary_stats_df)\n</code></pre>"},{"location":"howto/tutorials/b_create_dataset/#whats-next","title":"What's next?","text":"<p>In the next section, we will explore how to apply well-established algorithms that transform and analyse genetic data within the Gentropy framework.</p>"},{"location":"howto/tutorials/c_applying_methods/","title":"Applying methods","text":"<p>The available methods implement well established algorithms that transform and analyse data. Methods usually take as input predefined <code>Dataset</code>(s) and produce one or several <code>Dataset</code>(s) as output. This section explains how to apply methods to your data.</p> <p>The full list of available methods can be found in the Python API documentation.</p>"},{"location":"howto/tutorials/c_applying_methods/#apply-a-class-method","title":"Apply a class method","text":"<p>Some methods are implemented as class methods. For example, the <code>finemap</code> method is a class method of the <code>PICS</code> class. This method performs fine-mapping using the PICS algorithm. These methods usually take as input one or several <code>Dataset</code>(s) and produce one or several <code>Dataset</code>(s) as output.</p> <pre><code>from gentropy.method.pics import PICS\n\nfinemapped_study_locus = PICS.finemap(\n    study_locus_ld_annotated\n).annotate_credible_sets()\n</code></pre>"},{"location":"howto/tutorials/c_applying_methods/#apply-a-dataset-instance-method","title":"Apply a <code>Dataset</code> instance method","text":"<p>Some methods are implemented as instance methods of the <code>Dataset</code> class. For example, the <code>window_based_clumping</code> method is an instance method of the <code>SummaryStatistics</code> class. This method performs window-based clumping on summary statistics.</p> <pre><code># Perform window-based clumping on summary statistics\n# By default, the method uses a 1Mb window and a p-value threshold of 5e-8\nclumped_summary_statistics = summary_stats.window_based_clumping()\n</code></pre> <p>The <code>window_based_clumping</code> method is also available as a class method</p> <p>The <code>window_based_clumping</code> method is also available as a class method of the <code>WindowBasedClumping</code> class. This method performs window-based clumping on summary statistics.</p> <pre><code># Perform window-based clumping on summary statistics\nfrom gentropy.method.window_based_clumping import WindowBasedClumping\n\nclumped_summary_statistics = WindowBasedClumping.clump(\n    summary_stats, distance=250_000\n)\n</code></pre>"},{"location":"howto/tutorials/c_applying_methods/#whats-next","title":"What's next?","text":"<p>Up next, we'll show you how to inspect your data to ensure its integrity and the success of your transformations.</p>"},{"location":"howto/tutorials/d_inspect_dataset/","title":"Inspect a dataset","text":"<p>We have seen how to create and transform a <code>Dataset</code> instance. This section guides you through inspecting your data to ensure its integrity and the success of your transformations.</p>"},{"location":"howto/tutorials/d_inspect_dataset/#inspect-data-in-a-dataset","title":"Inspect data in a <code>Dataset</code>","text":"<p>The <code>df</code> attribute of a Dataset instance is key to interacting with and inspecting the stored data.</p> <p>By accessing the df attribute, you can apply any method that you would typically use on a PySpark DataFrame. See the PySpark documentation for more information.</p>"},{"location":"howto/tutorials/d_inspect_dataset/#view-data-samples","title":"View data samples","text":"<pre><code># Inspect the first 10 rows of the data\nsummary_stats.df.show(10)\n</code></pre> <p>This method displays the first 10 rows of your dataset, giving you a snapshot of your data's structure and content.</p>"},{"location":"howto/tutorials/d_inspect_dataset/#filter-data","title":"Filter data","text":"<pre><code>import pyspark.sql.functions as f\n\n# Filter summary statistics to only include associations in chromosome 22\nfiltered = summary_stats.filter(condition=f.col(\"chromosome\") == \"22\")\n</code></pre> <p>This method allows you to filter your data based on specific conditions, such as the value of a column. The application of any filter will create a new instance of the <code>Dataset</code> with the filtered data.</p>"},{"location":"howto/tutorials/d_inspect_dataset/#understand-the-schema","title":"Understand the schema","text":"<pre><code># Get the Spark schema of any `Dataset` as a `StructType` object\nschema = summary_stats.get_schema()\n\n# Inspect the first 10 rows of the data\nsummary_stats.df.show(10)\n</code></pre>"},{"location":"howto/tutorials/d_inspect_dataset/#write-a-dataset-to-disk","title":"Write a <code>Dataset</code> to disk","text":"<pre><code># Write the data to disk in parquet format\nsummary_stats.df.write.parquet(\"path/to/summary/stats\")\n\n# Write the data to disk in csv format\nsummary_stats.df.write.csv(\"path/to/summary/stats\")\n</code></pre> <p>Consider the format's compatibility with your tools, and the partitioning strategy for large datasets to optimize performance.</p>"},{"location":"python_api/_python_api/","title":"Python API","text":"<p>Open Targets Gentropy is a Python package to facilitate the interpretation and analysis of GWAS and functional genomic studies for target identification.</p> <p>The package contains a toolkit for the harmonisation, statistical analysis and prioritisation of genetic signals to assist drug discovery.</p>"},{"location":"python_api/_python_api/#data-model","title":"Data model","text":"<p>Gentropy data model is build upon following concepts:</p> <ul> <li>Data Sources: data sources harmonisation tools</li> <li>Datasets: data model</li> <li>Methods: statistical analysis tools</li> <li>Steps: pipeline steps</li> <li>Common: Common classes</li> <li>Assets: Assets used in gentropy package</li> </ul>"},{"location":"python_api/assets/_assets/","title":"Assets","text":"<p>Assets related with gentropy package.</p> <ul> <li>Variant Consequences: Variant Consequences used for variant annotation.</li> </ul>"},{"location":"python_api/assets/variant_consequences/","title":"Variant Consequences","text":""},{"location":"python_api/assets/variant_consequences/#gentropy.assets.variant_consequences","title":"<code>gentropy.assets.variant_consequences</code>","text":"<p>Common module representing Ensembl Variation Variant Consequences.</p> <p>This module contains the <code>Consequence</code> dataclass and the <code>VariantConsequence</code> enum. The <code>VariantConsequence</code> enum contains all <code>Consequence</code> instances defined by the Ensembl Variation API (used by Variant Effect Predictor - VEP). The full definition of the consequence was derived from the Ensembl Variation API.</p>"},{"location":"python_api/assets/variant_consequences/#gentropy.assets.variant_consequences.Consequence","title":"<code>Consequence</code>  <code>dataclass</code>","text":"<p>Base class for the variant consequence term.</p> Note <p>This class is used as a base class for the <code>VariantConsequence</code> enum, which contains all the valid consequence terms defined by the Ensembl Variation API.</p> Warning <p>Building new instances of this class is not recommended, as it may lead to inconsistencies in the way the <code>Consequence.score</code> is calculated. Rather then creating new instances of this class it is recommended to subclass it and override the <code>score</code> property with custom logic.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c = VariantConsequence.MISSENSE_VARIANT.value\n&gt;&gt;&gt; c.id\n'SO_0001583'\n&gt;&gt;&gt; c.label\n'missense_variant'\n&gt;&gt;&gt; c.impact\n'MODERATE'\n&gt;&gt;&gt; c.score\n0.68\n&gt;&gt;&gt; c.rank\n13\n&gt;&gt;&gt; str(c)\n'Consequence(id=SO_0001583, label=missense_variant, impact=MODERATE, rank=13)'\n</code></pre> Source code in <code>src/gentropy/assets/variant_consequences.py</code> <pre><code>@dataclass(frozen=True)\nclass Consequence:\n    \"\"\"Base class for the variant consequence term.\n\n    Note:\n        This class is used as a base class for the `VariantConsequence` enum, which\n        contains all the valid consequence terms defined by the Ensembl Variation API.\n\n    Warning:\n        **Building new instances of this class is not recommended**, as it may lead to inconsistencies\n        in the way the `Consequence.score` is calculated.\n        Rather then creating new instances of this class it is recommended to subclass it\n        and override the `score` property with custom logic.\n\n    Examples:\n        &gt;&gt;&gt; c = VariantConsequence.MISSENSE_VARIANT.value\n        &gt;&gt;&gt; c.id\n        'SO_0001583'\n        &gt;&gt;&gt; c.label\n        'missense_variant'\n        &gt;&gt;&gt; c.impact\n        'MODERATE'\n        &gt;&gt;&gt; c.score\n        0.68\n        &gt;&gt;&gt; c.rank\n        13\n        &gt;&gt;&gt; str(c)\n        'Consequence(id=SO_0001583, label=missense_variant, impact=MODERATE, rank=13)'\n    \"\"\"\n\n    id: str\n    label: str\n    impact: str\n    rank: int\n\n    @property\n    def score(self) -&gt; float:\n        r\"\"\"Scores the impact of a variant consequence.\n\n        Note:\n            The consequence scores are derived from the rank introduced by the `ensembl-variation` ranking of sequence ontology consequence terms.\n            The ranking is derived from [Constants.pm](https://github.com/Ensembl/ensembl-variation/blob/ac9116d964b33253cb34f727cad8c1c022d28672/modules/Bio/EnsEMBL/Variation/Utils/Constants.pm#L375).\n            The scoring that follows the inverse of the ranking is based on the formula.\n            ```math\n            score=1 - \\frac{rank}{max(rank)}\n            ```\n            where $max(rank)$ is the maximum rank of the consequences (41 in this case).\n            The score is then rounded to 2 decimal places.\n\n        * The score derived this way follows the VEP consequence ranking, which means that the consequence score is in sync to the actual mostSevereConsequence term\n        * The score is **different for each consequence term**\n        * The score follow the severity measure (0.98 - highest severity, 0 - lowest severity)\n        \"\"\"\n        return round(1 - (self.rank / len(VariantConsequence)), 2)\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of the consequence.\n\n        Returns:\n            str: String representation of the consequence.\n        \"\"\"\n        return f\"Consequence(id={self.id}, label={self.label}, impact={self.impact}, rank={self.rank})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Official string representation of the consequence.\n\n        Returns:\n            str: String representation of the consequence.\n        \"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"python_api/assets/variant_consequences/#gentropy.assets.variant_consequences.Consequence.score","title":"<code>score: float</code>  <code>property</code>","text":"<p>Scores the impact of a variant consequence.</p> Note <p>The consequence scores are derived from the rank introduced by the <code>ensembl-variation</code> ranking of sequence ontology consequence terms. The ranking is derived from Constants.pm. The scoring that follows the inverse of the ranking is based on the formula. <p>score=1\u2212rankmax(rank) score=1 - \\frac{rank}{max(rank)} score=1\u2212max(rank)rank\u200b</p> where $max(rank)$ is the maximum rank of the consequences (41 in this case). The score is then rounded to 2 decimal places.</p> <ul> <li>The score derived this way follows the VEP consequence ranking, which means that the consequence score is in sync to the actual mostSevereConsequence term</li> <li>The score is different for each consequence term</li> <li>The score follow the severity measure (0.98 - highest severity, 0 - lowest severity)</li> </ul>"},{"location":"python_api/assets/variant_consequences/#gentropy.assets.variant_consequences.VariantConsequence","title":"<code>VariantConsequence</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum representing Ensembl Variation Variant Consequences.</p> <p>This enum contains all <code>Consequence</code> instances defined by the Ensembl Variation API (used by Variant Effect Predictor - VEP).</p> <p>The full definition of the consequence was derived from the Ensembl Variation API. See issue for more details.</p> <p>Attributes:</p> Name Type Description <code>TRANSCRIPT_ABLATION</code> <code>Consequence</code> <p>A feature ablation whereby the deleted region includes a transcript feature</p> <code>SPLICE_ACCEPTOR_VARIANT</code> <code>Consequence</code> <p>A splice variant that changes the 2 base region at the 3' end of an intron</p> <code>SPLICE_DONOR_VARIANT</code> <code>Consequence</code> <p>A splice variant that changes the 2 base region at the 5' end of an intron</p> <code>STOP_GAINED</code> <code>Consequence</code> <p>A sequence variant whereby at least one base of a codon is changed, resulting in a premature stop codon, leading to a shortened transcript</p> <code>FRAMESHIFT_VARIANT</code> <code>Consequence</code> <p>A sequence variant which causes a disruption of the translational reading frame, because the number of nucleotides inserted or deleted is not a multiple of three</p> <code>STOP_LOST</code> <code>Consequence</code> <p>A sequence variant where at least one base of the terminator codon (stop) is changed, resulting in an elongated transcript</p> <code>START_LOST</code> <code>Consequence</code> <p>A codon variant that changes at least one base of the canonical start codon</p> <code>TRANSCRIPT_AMPLIFICATION</code> <code>Consequence</code> <p>A feature amplification of a region containing a transcript</p> <code>FEATURE_ELONGATION</code> <code>Consequence</code> <p>A sequence variant that causes the extension of a genomic feature, with regard to the reference sequence</p> <code>FEATURE_TRUNCATION</code> <code>Consequence</code> <p>A sequence variant that causes the reduction of a genomic feature, with regard to the reference sequence</p> <code>INFRAME_INSERTION</code> <code>Consequence</code> <p>An inframe non synonymous variant that inserts bases into in the coding sequence</p> <code>INFRAME_DELETION</code> <code>Consequence</code> <p>An inframe non synonymous variant that deletes bases from the coding sequence</p> <code>MISSENSE_VARIANT</code> <code>Consequence</code> <p>A sequence variant, that changes one or more bases, resulting in a different amino acid sequence but where the length is preserved</p> <code>PROTEIN_ALTERING_VARIANT</code> <code>Consequence</code> <p>A sequence_variant which is predicted to change the protein encoded in the coding sequence</p> <code>SPLICE_DONOR_5TH_BASE_VARIANT</code> <code>Consequence</code> <p>A sequence variant that causes a change at the 5th base pair after the start of the intron in the orientation of the transcript</p> <code>SPLICE_REGION_VARIANT</code> <code>Consequence</code> <p>A sequence variant in which a change has occurred within the region of the splice site, either within 1-3 bases of the exon or 3-8 bases of the intron</p> <code>SPLICE_DONOR_REGION_VARIANT</code> <code>Consequence</code> <p>A sequence variant that falls in the region between the 3rd and 6th base after splice junction (5' end of intron).</p> <code>SPLICE_POLYPYRIMIDINE_TRACT_VARIANT</code> <code>Consequence</code> <p>A sequence variant that falls in the polypyrimidine tract at 3' end of intron between 17 and 3 bases from the end (acceptor -3 to acceptor -17)</p> <code>INCOMPLETE_TERMINAL_CODON_VARIANT</code> <code>Consequence</code> <p>A sequence variant where at least one base of the final codon of an incompletely annotated transcript is changed</p> <code>START_RETAINED_VARIANT</code> <code>Consequence</code> <p>A sequence variant where at least one base in the start codon is changed, but the start remains</p> <code>STOP_RETAINED_VARIANT</code> <code>Consequence</code> <p>A sequence variant where at least one base in the terminator codon is changed, but the terminator remains</p> <code>SYNONYMOUS_VARIANT</code> <code>Consequence</code> <p>A sequence variant where there is no resulting change to the encoded amino acid</p> <code>CODING_SEQUENCE_VARIANT</code> <code>Consequence</code> <p>A sequence variant that changes the coding sequence</p> <code>MATURE_MIRNA_VARIANT</code> <code>Consequence</code> <p>A transcript variant located with the sequence of the mature miRNA</p> <code>5_PRIME_UTR_VARIANT</code> <code>Consequence</code> <p>A UTR variant of the 5' UTR</p> <code>3_PRIME_UTR_VARIANT</code> <code>Consequence</code> <p>A UTR variant of the 3' UTR</p> <code>NON_CODING_TRANSCRIPT_EXON_VARIANT</code> <code>Consequence</code> <p>A sequence variant that changes non-coding exon sequence in a non-coding transcript</p> <code>INTRON_VARIANT</code> <code>Consequence</code> <p>A transcript variant occurring within an intron</p> <code>NMD_TRANSCRIPT_VARIANT</code> <code>Consequence</code> <p>A variant in a transcript that is the target of NMD</p> <code>NON_CODING_TRANSCRIPT_VARIANT</code> <code>Consequence</code> <p>A transcript variant of a non coding RNA gene</p> <code>CODING_TRANSCRIPT_VARIANT</code> <code>Consequence</code> <p>A transcript variant of a protein coding gene</p> <code>UPSTREAM_GENE_VARIANT</code> <code>Consequence</code> <p>A sequence variant located 5' of a gene</p> <code>DOWNSTREAM_GENE_VARIANT</code> <code>Consequence</code> <p>A sequence variant located 3' of a gene</p> <code>TFBS_ABLATION</code> <code>Consequence</code> <p>A feature ablation whereby the deleted region includes a transcription factor binding site</p> <code>TFBS_AMPLIFICATION</code> <code>Consequence</code> <p>A feature amplification of a region containing a transcription factor binding site</p> <code>TF_BINDING_SITE_VARIANT</code> <code>Consequence</code> <p>A sequence variant located within a transcription factor binding site</p> <code>REGULATORY_REGION_ABLATION</code> <code>ConsequeSO_0001893nce</code> <p>A feature ablation whereby the deleted region includes a regulatory region</p> <code>REGULATORY_REGION_AMPLIFICATION</code> <code>Consequence</code> <p>A feature amplification of a region containing a regulatory region</p> <code>REGULATORY_REGION_VARIANT</code> <code>Consequence</code> <p>A sequence variant located within a regulatory region</p> <code>INTERGENIC_VARIANT</code> <code>Consequence</code> <p>A sequence variant located in the intergenic region, between genes</p> <code>SEQUENCE_VARIANT</code> <code>Consequence</code> <p>A sequence_variant is a non exact copy of a sequence_feature or genome exhibiting one or more sequence_alterations</p> Source code in <code>src/gentropy/assets/variant_consequences.py</code> <pre><code>class VariantConsequence(Enum):\n    \"\"\"Enum representing Ensembl Variation Variant Consequences.\n\n    This enum contains all `Consequence` instances defined by the Ensembl Variation API (used by Variant Effect Predictor - VEP).\n\n    The full definition of the consequence was derived from the Ensembl Variation API.\n    See [issue](https://github.com/opentargets/issues/issues/3952) for more details.\n\n    Attributes:\n        TRANSCRIPT_ABLATION (Consequence): A feature ablation whereby the deleted region includes a transcript feature\n        SPLICE_ACCEPTOR_VARIANT (Consequence): A splice variant that changes the 2 base region at the 3' end of an intron\n        SPLICE_DONOR_VARIANT (Consequence): A splice variant that changes the 2 base region at the 5' end of an intron\n        STOP_GAINED (Consequence): A sequence variant whereby at least one base of a codon is changed, resulting in a premature stop codon, leading to a shortened transcript\n        FRAMESHIFT_VARIANT (Consequence): A sequence variant which causes a disruption of the translational reading frame, because the number of nucleotides inserted or deleted is not a multiple of three\n        STOP_LOST (Consequence): A sequence variant where at least one base of the terminator codon (stop) is changed, resulting in an elongated transcript\n        START_LOST (Consequence): A codon variant that changes at least one base of the canonical start codon\n        TRANSCRIPT_AMPLIFICATION (Consequence): A feature amplification of a region containing a transcript\n        FEATURE_ELONGATION (Consequence): A sequence variant that causes the extension of a genomic feature, with regard to the reference sequence\n        FEATURE_TRUNCATION (Consequence): A sequence variant that causes the reduction of a genomic feature, with regard to the reference sequence\n        INFRAME_INSERTION (Consequence): An inframe non synonymous variant that inserts bases into in the coding sequence\n        INFRAME_DELETION (Consequence): An inframe non synonymous variant that deletes bases from the coding sequence\n        MISSENSE_VARIANT (Consequence): A sequence variant, that changes one or more bases, resulting in a different amino acid sequence but where the length is preserved\n        PROTEIN_ALTERING_VARIANT (Consequence): A sequence_variant which is predicted to change the protein encoded in the coding sequence\n        SPLICE_DONOR_5TH_BASE_VARIANT (Consequence): A sequence variant that causes a change at the 5th base pair after the start of the intron in the orientation of the transcript\n        SPLICE_REGION_VARIANT (Consequence): A sequence variant in which a change has occurred within the region of the splice site, either within 1-3 bases of the exon or 3-8 bases of the intron\n        SPLICE_DONOR_REGION_VARIANT (Consequence): A sequence variant that falls in the region between the 3rd and 6th base after splice junction (5' end of intron).\n        SPLICE_POLYPYRIMIDINE_TRACT_VARIANT (Consequence): A sequence variant that falls in the polypyrimidine tract at 3' end of intron between 17 and 3 bases from the end (acceptor -3 to acceptor -17)\n        INCOMPLETE_TERMINAL_CODON_VARIANT (Consequence): A sequence variant where at least one base of the final codon of an incompletely annotated transcript is changed\n        START_RETAINED_VARIANT (Consequence): A sequence variant where at least one base in the start codon is changed, but the start remains\n        STOP_RETAINED_VARIANT (Consequence): A sequence variant where at least one base in the terminator codon is changed, but the terminator remains\n        SYNONYMOUS_VARIANT (Consequence): A sequence variant where there is no resulting change to the encoded amino acid\n        CODING_SEQUENCE_VARIANT (Consequence): A sequence variant that changes the coding sequence\n        MATURE_MIRNA_VARIANT (Consequence): A transcript variant located with the sequence of the mature miRNA\n        5_PRIME_UTR_VARIANT (Consequence): A UTR variant of the 5' UTR\n        3_PRIME_UTR_VARIANT (Consequence): A UTR variant of the 3' UTR\n        NON_CODING_TRANSCRIPT_EXON_VARIANT (Consequence): A sequence variant that changes non-coding exon sequence in a non-coding transcript\n        INTRON_VARIANT (Consequence): A transcript variant occurring within an intron\n        NMD_TRANSCRIPT_VARIANT (Consequence): A variant in a transcript that is the target of NMD\n        NON_CODING_TRANSCRIPT_VARIANT (Consequence): A transcript variant of a non coding RNA gene\n        CODING_TRANSCRIPT_VARIANT (Consequence): A transcript variant of a protein coding gene\n        UPSTREAM_GENE_VARIANT (Consequence): A sequence variant located 5' of a gene\n        DOWNSTREAM_GENE_VARIANT (Consequence): A sequence variant located 3' of a gene\n        TFBS_ABLATION (Consequence): A feature ablation whereby the deleted region includes a transcription factor binding site\n        TFBS_AMPLIFICATION (Consequence): A feature amplification of a region containing a transcription factor binding site\n        TF_BINDING_SITE_VARIANT (Consequence): A sequence variant located within a transcription factor binding site\n        REGULATORY_REGION_ABLATION (ConsequeSO_0001893nce): A feature ablation whereby the deleted region includes a regulatory region\n        REGULATORY_REGION_AMPLIFICATION (Consequence): A feature amplification of a region containing a regulatory region\n        REGULATORY_REGION_VARIANT (Consequence): A sequence variant located within a regulatory region\n        INTERGENIC_VARIANT (Consequence): A sequence variant located in the intergenic region, between genes\n        SEQUENCE_VARIANT (Consequence): A sequence_variant is a non exact copy of a sequence_feature or genome exhibiting one or more sequence_alterations\n    \"\"\"\n\n    TRANSCRIPT_ABLATION = Consequence(\n        id=\"SO_0001893\", label=\"transcript_ablation\", impact=\"HIGH\", rank=1\n    )\n    SPLICE_ACCEPTOR_VARIANT = Consequence(\n        id=\"SO_0001574\", label=\"splice_acceptor_variant\", impact=\"HIGH\", rank=2\n    )\n    SPLICE_DONOR_VARIANT = Consequence(\n        id=\"SO_0001575\", label=\"splice_donor_variant\", impact=\"HIGH\", rank=3\n    )\n    STOP_GAINED = Consequence(\n        id=\"SO_0001587\", label=\"stop_gained\", impact=\"HIGH\", rank=4\n    )\n    FRAMESHIFT_VARIANT = Consequence(\n        id=\"SO_0001589\", label=\"frameshift_variant\", impact=\"HIGH\", rank=5\n    )\n    STOP_LOST = Consequence(id=\"SO_0001578\", label=\"stop_lost\", impact=\"HIGH\", rank=6)\n    START_LOST = Consequence(id=\"SO_0002012\", label=\"start_lost\", impact=\"HIGH\", rank=7)\n    TRANSCRIPT_AMPLIFICATION = Consequence(\n        id=\"SO_0001889\", label=\"transcript_amplification\", impact=\"HIGH\", rank=8\n    )\n    FEATURE_ELONGATION = Consequence(\n        id=\"SO_0001907\", label=\"feature_elongation\", impact=\"HIGH\", rank=9\n    )\n    FEATURE_TRUNCATION = Consequence(\n        id=\"SO_0001906\", label=\"feature_truncation\", impact=\"HIGH\", rank=10\n    )\n    INFRAME_INSERTION = Consequence(\n        id=\"SO_0001821\", label=\"inframe_insertion\", impact=\"MODERATE\", rank=11\n    )\n    INFRAME_DELETION = Consequence(\n        id=\"SO_0001822\", label=\"inframe_deletion\", impact=\"MODERATE\", rank=12\n    )\n    MISSENSE_VARIANT = Consequence(\n        id=\"SO_0001583\", label=\"missense_variant\", impact=\"MODERATE\", rank=13\n    )\n    PROTEIN_ALTERING_VARIANT = Consequence(\n        id=\"SO_0001818\", label=\"protein_altering_variant\", impact=\"MODERATE\", rank=14\n    )\n    SPLICE_DONOR_5TH_BASE_VARIANT = Consequence(\n        id=\"SO_0001787\", label=\"splice_donor_5th_base_variant\", impact=\"LOW\", rank=15\n    )\n    SPLICE_REGION_VARIANT = Consequence(\n        id=\"SO_0001630\", label=\"splice_region_variant\", impact=\"LOW\", rank=16\n    )\n    SPLICE_DONOR_REGION_VARIANT = Consequence(\n        id=\"SO_0002170\", label=\"splice_donor_region_variant\", impact=\"LOW\", rank=17\n    )\n    SPLICE_POLYPYRIMIDINE_TRACT_VARIANT = Consequence(\n        id=\"SO_0002169\",\n        label=\"splice_polypyrimidine_tract_variant\",\n        impact=\"LOW\",\n        rank=18,\n    )\n    INCOMPLETE_labelINAL_CODON_VARIANT = Consequence(\n        id=\"SO_0001626\",\n        label=\"incomplete_labelinal_codon_variant\",\n        impact=\"LOW\",\n        rank=19,\n    )\n    START_RETAINED_VARIANT = Consequence(\n        id=\"SO_0002019\", label=\"start_retained_variant\", impact=\"LOW\", rank=20\n    )\n    STOP_RETAINED_VARIANT = Consequence(\n        id=\"SO_0001567\", label=\"stop_retained_variant\", impact=\"LOW\", rank=21\n    )\n    SYNONYMOUS_VARIANT = Consequence(\n        id=\"SO_0001819\", label=\"synonymous_variant\", impact=\"LOW\", rank=22\n    )\n    CODING_SEQUENCE_VARIANT = Consequence(\n        id=\"SO_0001580\", label=\"coding_sequence_variant\", impact=\"MODIFIER\", rank=23\n    )\n    MATURE_MIRNA_VARIANT = Consequence(\n        id=\"SO_0001620\", label=\"mature_miRNA_variant\", impact=\"MODIFIER\", rank=24\n    )\n    FIVE_PRIME_UTR_VARIANT = Consequence(\n        id=\"SO_0001623\", label=\"5_prime_UTR_variant\", impact=\"MODIFIER\", rank=25\n    )\n    THREE_PRIME_UTR_VARIANT = Consequence(\n        id=\"SO_0001624\", label=\"3_prime_UTR_variant\", impact=\"MODIFIER\", rank=26\n    )\n    NON_CODING_TRANSCRIPT_EXON_VARIANT = Consequence(\n        id=\"SO_0001792\",\n        label=\"non_coding_transcript_exon_variant\",\n        impact=\"MODIFIER\",\n        rank=27,\n    )\n    INTRON_VARIANT = Consequence(\n        id=\"SO_0001627\", label=\"intron_variant\", impact=\"MODIFIER\", rank=28\n    )\n    NMD_TRANSCRIPT_VARIANT = Consequence(\n        id=\"SO_0001621\", label=\"NMD_transcript_variant\", impact=\"MODIFIER\", rank=29\n    )\n    NON_CODING_TRANSCRIPT_VARIANT = Consequence(\n        id=\"SO_0001619\",\n        label=\"non_coding_transcript_variant\",\n        impact=\"MODIFIER\",\n        rank=30,\n    )\n    CODING_TRANSCRIPT_VARIANT = Consequence(\n        id=\"SO_0001968\", label=\"coding_transcript_variant\", impact=\"MODIFIER\", rank=31\n    )\n    UPSTREAM_GENE_VARIANT = Consequence(\n        id=\"SO_0001631\", label=\"upstream_gene_variant\", impact=\"MODIFIER\", rank=32\n    )\n    DOWNSTREAM_GENE_VARIANT = Consequence(\n        id=\"SO_0001632\", label=\"downstream_gene_variant\", impact=\"MODIFIER\", rank=33\n    )\n    TFBS_ABLATION = Consequence(\n        id=\"SO_0001895\", label=\"TFBS_ablation\", impact=\"MODERATE\", rank=34\n    )\n    TFBS_AMPLIFICATION = Consequence(\n        id=\"SO_0001892\", label=\"TFBS_amplification\", impact=\"MODIFIER\", rank=35\n    )\n    TF_BINDING_SITE_VARIANT = Consequence(\n        id=\"SO_0001782\", label=\"TF_binding_site_variant\", impact=\"MODIFIER\", rank=36\n    )\n    REGULATORY_REGION_ABLATION = Consequence(\n        id=\"SO_0001894\", label=\"regulatory_region_ablation\", impact=\"MODIFIER\", rank=37\n    )\n    REGULATORY_REGION_AMPLIFICATION = Consequence(\n        id=\"SO_0001891\",\n        label=\"regulatory_region_amplification\",\n        impact=\"MODIFIER\",\n        rank=38,\n    )\n    REGULATORY_REGION_VARIANT = Consequence(\n        id=\"SO_0001566\", label=\"regulatory_region_variant\", impact=\"MODIFIER\", rank=39\n    )\n    INTERGENIC_VARIANT = Consequence(\n        id=\"SO_0001628\", label=\"intergenic_variant\", impact=\"MODIFIER\", rank=40\n    )\n    SEQUENCE_VARIANT = Consequence(\n        id=\"SO_0001060\", label=\"sequence_variant\", impact=\"MODIFIER\", rank=41\n    )\n\n    @classmethod\n    def map_sequence_ontology(cls) -&gt; dict[str, str]:\n        \"\"\"Return the mapping of the `Consequence.label` (key) and `Consequence.id` (value) representing Sequence Ontology term.\n\n        Returns:\n            dict[str, str]: Mapping of consequence label to ID.\n\n        Examples:\n            &gt;&gt;&gt; m = VariantConsequence.map_sequence_ontology()\n            &gt;&gt;&gt; m[\"missense_variant\"]\n            'SO_0001583'\n            &gt;&gt;&gt; len(m)\n            41\n        \"\"\"\n        return {\n            consequence.value.label: consequence.value.id\n            for consequence in cls.__members__.values()\n        }\n\n    @classmethod\n    def map_score(cls) -&gt; dict[str, float]:\n        \"\"\"Return the mapping of the `Consequence.label` (key) and `Consequence.score` (value).\n\n        Returns:\n            dict[str, float]: Mapping of consequence label to score.\n\n        Examples:\n            &gt;&gt;&gt; s = VariantConsequence.map_score()\n            &gt;&gt;&gt; s[\"missense_variant\"]\n            0.68\n            &gt;&gt;&gt; len(s)\n            41\n        \"\"\"\n        return {\n            consequence.value.label: consequence.value.score\n            for consequence in cls.__members__.values()\n        }\n</code></pre>"},{"location":"python_api/assets/variant_consequences/#gentropy.assets.variant_consequences.VariantConsequence.map_score","title":"<code>map_score() -&gt; dict[str, float]</code>  <code>classmethod</code>","text":"<p>Return the mapping of the <code>Consequence.label</code> (key) and <code>Consequence.score</code> (value).</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Mapping of consequence label to score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s = VariantConsequence.map_score()\n&gt;&gt;&gt; s[\"missense_variant\"]\n0.68\n&gt;&gt;&gt; len(s)\n41\n</code></pre> Source code in <code>src/gentropy/assets/variant_consequences.py</code> <pre><code>@classmethod\ndef map_score(cls) -&gt; dict[str, float]:\n    \"\"\"Return the mapping of the `Consequence.label` (key) and `Consequence.score` (value).\n\n    Returns:\n        dict[str, float]: Mapping of consequence label to score.\n\n    Examples:\n        &gt;&gt;&gt; s = VariantConsequence.map_score()\n        &gt;&gt;&gt; s[\"missense_variant\"]\n        0.68\n        &gt;&gt;&gt; len(s)\n        41\n    \"\"\"\n    return {\n        consequence.value.label: consequence.value.score\n        for consequence in cls.__members__.values()\n    }\n</code></pre>"},{"location":"python_api/assets/variant_consequences/#gentropy.assets.variant_consequences.VariantConsequence.map_sequence_ontology","title":"<code>map_sequence_ontology() -&gt; dict[str, str]</code>  <code>classmethod</code>","text":"<p>Return the mapping of the <code>Consequence.label</code> (key) and <code>Consequence.id</code> (value) representing Sequence Ontology term.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: Mapping of consequence label to ID.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = VariantConsequence.map_sequence_ontology()\n&gt;&gt;&gt; m[\"missense_variant\"]\n'SO_0001583'\n&gt;&gt;&gt; len(m)\n41\n</code></pre> Source code in <code>src/gentropy/assets/variant_consequences.py</code> <pre><code>@classmethod\ndef map_sequence_ontology(cls) -&gt; dict[str, str]:\n    \"\"\"Return the mapping of the `Consequence.label` (key) and `Consequence.id` (value) representing Sequence Ontology term.\n\n    Returns:\n        dict[str, str]: Mapping of consequence label to ID.\n\n    Examples:\n        &gt;&gt;&gt; m = VariantConsequence.map_sequence_ontology()\n        &gt;&gt;&gt; m[\"missense_variant\"]\n        'SO_0001583'\n        &gt;&gt;&gt; len(m)\n        41\n    \"\"\"\n    return {\n        consequence.value.label: consequence.value.id\n        for consequence in cls.__members__.values()\n    }\n</code></pre>"},{"location":"python_api/common/_common/","title":"Common","text":"<p>Common utilities used in gentropy package.</p> <ul> <li>Session: Spark Session wrapper.</li> <li>Genomic Region</li> <li>Types: Literal types.</li> <li>User Defined PySpark functions User Defined Pandas functions (Pandas UDFS).</li> <li>Stats: Statistical functions.</li> <li>Processing: Common functions for gentropy dataset processing (data business logic).</li> <li>Spark Helpers: Common functions for Spark DataFrame processing (Pyspark extensions).</li> </ul>"},{"location":"python_api/common/genomic_region/","title":"genomic_region","text":""},{"location":"python_api/common/genomic_region/#genomic-region-processing","title":"Genomic Region processing","text":""},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.KnownGenomicRegions","title":"<code>gentropy.common.genomic_region.KnownGenomicRegions</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Known genomic regions in the human genome in string format.</p> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>class KnownGenomicRegions(Enum):\n    \"\"\"Known genomic regions in the human genome in string format.\"\"\"\n\n    MHC = \"chr6:25726063-33400556\"\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.GenomicRegion","title":"<code>gentropy.common.genomic_region.GenomicRegion</code>","text":"<p>Genomic regions of interest.</p> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>class GenomicRegion:\n    \"\"\"Genomic regions of interest.\"\"\"\n\n    def __init__(self, chromosome: str, start: int, end: int) -&gt; None:\n        \"\"\"Class constructor.\n\n        Args:\n            chromosome (str): Chromosome.\n            start (int): Start position.\n            end (int): End position.\n        \"\"\"\n        self.chromosome = chromosome\n        self.start = start\n        self.end = end\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of the genomic region.\n\n        Returns:\n            str: Genomic region in chr:start-end format.\n        \"\"\"\n        return f\"{self.chromosome}:{self.start}-{self.end}\"\n\n    @classmethod\n    def from_string(cls: type[GenomicRegion], region: str) -&gt; GenomicRegion:\n        \"\"\"Parse region string to chr:start-end.\n\n        Args:\n            region (str): Genomic region expected to follow chr##:#,###-#,### format or ##:####-#####.\n\n        Returns:\n            GenomicRegion: Genomic region object.\n\n        Raises:\n            ValueError: If the end and start positions cannot be casted to integer or not all three values value error is raised.\n\n        Examples:\n            &gt;&gt;&gt; print(GenomicRegion.from_string('chr6:28,510,120-33,480,577'))\n            6:28510120-33480577\n            &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-33480577'))\n            6:28510120-33480577\n            &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120'))\n            Traceback (most recent call last):\n                ...\n            ValueError: Genomic region should follow a ##:####-#### format.\n            &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-foo'))\n            Traceback (most recent call last):\n                ...\n            ValueError: Start and the end position of the region has to be integer.\n        \"\"\"\n        region = region.replace(\":\", \"-\").replace(\",\", \"\")\n        try:\n            chromosome, start_position, end_position = region.split(\"-\")\n        except ValueError as err:\n            raise ValueError(\n                \"Genomic region should follow a ##:####-#### format.\"\n            ) from err\n\n        try:\n            return cls(\n                chromosome=chromosome.replace(\"chr\", \"\"),\n                start=int(start_position),\n                end=int(end_position),\n            )\n        except ValueError as err:\n            raise ValueError(\n                \"Start and the end position of the region has to be integer.\"\n            ) from err\n\n    @classmethod\n    def from_known_genomic_region(\n        cls: type[GenomicRegion], region: KnownGenomicRegions\n    ) -&gt; GenomicRegion:\n        \"\"\"Get known genomic region.\n\n        Args:\n            region (KnownGenomicRegions): Known genomic region.\n\n        Returns:\n            GenomicRegion: Genomic region object.\n\n        Examples:\n            &gt;&gt;&gt; print(GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC))\n            6:25726063-33400556\n        \"\"\"\n        return GenomicRegion.from_string(region.value)\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.GenomicRegion.__init__","title":"<code>__init__(chromosome: str, start: int, end: int) -&gt; None</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>chromosome</code> <code>str</code> <p>Chromosome.</p> required <code>start</code> <code>int</code> <p>Start position.</p> required <code>end</code> <code>int</code> <p>End position.</p> required Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>def __init__(self, chromosome: str, start: int, end: int) -&gt; None:\n    \"\"\"Class constructor.\n\n    Args:\n        chromosome (str): Chromosome.\n        start (int): Start position.\n        end (int): End position.\n    \"\"\"\n    self.chromosome = chromosome\n    self.start = start\n    self.end = end\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.GenomicRegion.from_known_genomic_region","title":"<code>from_known_genomic_region(region: KnownGenomicRegions) -&gt; GenomicRegion</code>  <code>classmethod</code>","text":"<p>Get known genomic region.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>KnownGenomicRegions</code> <p>Known genomic region.</p> required <p>Returns:</p> Name Type Description <code>GenomicRegion</code> <code>GenomicRegion</code> <p>Genomic region object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC))\n6:25726063-33400556\n</code></pre> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>@classmethod\ndef from_known_genomic_region(\n    cls: type[GenomicRegion], region: KnownGenomicRegions\n) -&gt; GenomicRegion:\n    \"\"\"Get known genomic region.\n\n    Args:\n        region (KnownGenomicRegions): Known genomic region.\n\n    Returns:\n        GenomicRegion: Genomic region object.\n\n    Examples:\n        &gt;&gt;&gt; print(GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC))\n        6:25726063-33400556\n    \"\"\"\n    return GenomicRegion.from_string(region.value)\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.GenomicRegion.from_string","title":"<code>from_string(region: str) -&gt; GenomicRegion</code>  <code>classmethod</code>","text":"<p>Parse region string to chr:start-end.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>str</code> <p>Genomic region expected to follow chr##:#,###-#,### format or ##:####-#####.</p> required <p>Returns:</p> Name Type Description <code>GenomicRegion</code> <code>GenomicRegion</code> <p>Genomic region object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the end and start positions cannot be casted to integer or not all three values value error is raised.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(GenomicRegion.from_string('chr6:28,510,120-33,480,577'))\n6:28510120-33480577\n&gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-33480577'))\n6:28510120-33480577\n&gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120'))\nTraceback (most recent call last):\n    ...\nValueError: Genomic region should follow a ##:####-#### format.\n&gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-foo'))\nTraceback (most recent call last):\n    ...\nValueError: Start and the end position of the region has to be integer.\n</code></pre> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>@classmethod\ndef from_string(cls: type[GenomicRegion], region: str) -&gt; GenomicRegion:\n    \"\"\"Parse region string to chr:start-end.\n\n    Args:\n        region (str): Genomic region expected to follow chr##:#,###-#,### format or ##:####-#####.\n\n    Returns:\n        GenomicRegion: Genomic region object.\n\n    Raises:\n        ValueError: If the end and start positions cannot be casted to integer or not all three values value error is raised.\n\n    Examples:\n        &gt;&gt;&gt; print(GenomicRegion.from_string('chr6:28,510,120-33,480,577'))\n        6:28510120-33480577\n        &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-33480577'))\n        6:28510120-33480577\n        &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120'))\n        Traceback (most recent call last):\n            ...\n        ValueError: Genomic region should follow a ##:####-#### format.\n        &gt;&gt;&gt; print(GenomicRegion.from_string('6:28510120-foo'))\n        Traceback (most recent call last):\n            ...\n        ValueError: Start and the end position of the region has to be integer.\n    \"\"\"\n    region = region.replace(\":\", \"-\").replace(\",\", \"\")\n    try:\n        chromosome, start_position, end_position = region.split(\"-\")\n    except ValueError as err:\n        raise ValueError(\n            \"Genomic region should follow a ##:####-#### format.\"\n        ) from err\n\n    try:\n        return cls(\n            chromosome=chromosome.replace(\"chr\", \"\"),\n            start=int(start_position),\n            end=int(end_position),\n        )\n    except ValueError as err:\n        raise ValueError(\n            \"Start and the end position of the region has to be integer.\"\n        ) from err\n</code></pre>"},{"location":"python_api/common/genomic_region/#liftover","title":"LiftOver","text":""},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.LiftOverSpark","title":"<code>gentropy.common.genomic_region.LiftOverSpark</code>","text":"<p>LiftOver class for mapping genomic coordinates to an other genome build.</p> <p>The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided.</p> <p>Logic:</p> <ul> <li>The mapping is dropped if the mapped chromosome is not on the same as the source.</li> <li>The mapping is dropped if the mapping is ambiguous (more than one mapping is available).</li> <li>If regions are provided, the mapping is dropped if the new region is reversed (mapped_start &gt; mapped_end).</li> <li>If regions are provided, the mapping is dropped if the difference of the length of the mapped region and original is larger than a threshold.</li> <li>When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe.</li> </ul> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>class LiftOverSpark:\n    \"\"\"LiftOver class for mapping genomic coordinates to an other genome build.\n\n    The input is a Spark DataFrame with a chromosome and position column. This classs can\n    also map regions, if a start and end positions are provided.\n\n    **Logic**:\n\n    - The mapping is dropped if the mapped chromosome is not on the same as the source.\n    - The mapping is dropped if the mapping is ambiguous (more than one mapping is available).\n    - If regions are provided, the mapping is dropped if the new region is reversed (mapped_start &gt; mapped_end).\n    - If regions are provided, the mapping is dropped if the difference of the length of the mapped region and original is larger than a threshold.\n    - When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe.\n    \"\"\"\n\n    def __init__(\n        self: LiftOverSpark, chain_file: str, max_difference: int = 100\n    ) -&gt; None:\n        \"\"\"Initialize the LiftOver object.\n\n        Args:\n            chain_file (str): Path to the chain file\n            max_difference (int, optional): The maximum tolerated difference in the resulting length. Defaults to 100.\n        \"\"\"\n        self.chain_file = chain_file\n        self.max_difference = max_difference\n\n        # Initializing liftover object by opening the chain file - LiftOver only supports local files:\n        if chain_file.startswith(\"gs://\"):\n            bucket_name = chain_file.split(\"/\")[2]\n            blob_name = \"/\".join(chain_file.split(\"/\")[3:])\n            with tempfile.NamedTemporaryFile(delete=True) as temp_file:\n                storage.Client().bucket(bucket_name).blob(\n                    blob_name\n                ).download_to_filename(temp_file.name)\n                self.lo = LiftOver(temp_file.name)\n        else:\n            self.lo = LiftOver(chain_file)\n\n        # UDF to do map genomic coordinates to liftover coordinates:\n        self.liftover_udf = f.udf(\n            lambda chrom, pos: self.lo.convert_coordinate(chrom, pos),\n            t.ArrayType(t.ArrayType(t.StringType())),\n        )\n\n    def convert_intervals(\n        self: LiftOverSpark,\n        df: DataFrame,\n        chrom_col: str,\n        start_col: str,\n        end_col: str,\n        filter: bool = True,\n    ) -&gt; DataFrame:\n        \"\"\"Convert genomic intervals to liftover coordinates.\n\n        Args:\n            df (DataFrame): spark Dataframe with chromosome, start and end columns.\n            chrom_col (str): Name of the chromosome column.\n            start_col (str): Name of the start column.\n            end_col (str): Name of the end column.\n            filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True.\n\n        Returns:\n            DataFrame: Liftovered intervals\n        \"\"\"\n        # Lift over start coordinates, changing to 1-based coordinates:\n        start_df = (\n            df.withColumn(start_col, f.col(start_col) + 1)\n            .select(chrom_col, start_col)\n            .distinct()\n        )\n        start_df = self.convert_coordinates(\n            start_df, chrom_col, start_col\n        ).withColumnRenamed(\"mapped_pos\", f\"mapped_{start_col}\")\n\n        # Lift over end coordinates:\n        end_df = df.select(chrom_col, end_col).distinct()\n        end_df = self.convert_coordinates(end_df, chrom_col, end_col).withColumnRenamed(\n            \"mapped_pos\", f\"mapped_{end_col}\"\n        )\n\n        # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates):\n        mapped_df = df.join(\n            start_df.withColumn(start_col, f.col(start_col) - 1),\n            on=[chrom_col, start_col],\n            how=\"left\",\n        ).join(end_df, on=[chrom_col, end_col], how=\"left\")\n\n        # The filter option allows to get all the data and filter it afterwards.\n        if filter:\n            return (\n                mapped_df\n                # Select only rows where the start is smaller than the end:\n                .filter(\n                    # Drop rows with no mappings:\n                    f.col(f\"mapped_{start_col}\").isNotNull()\n                    &amp; f.col(f\"mapped_{end_col}\").isNotNull()\n                    # Drop rows where the start is larger than the end:\n                    &amp; (f.col(f\"mapped_{end_col}\") &gt;= f.col(f\"mapped_{start_col}\"))\n                    # Drop rows where the difference of the length of the regions are larger than the threshold:\n                    &amp; (\n                        f.abs(\n                            (f.col(end_col) - f.col(start_col))\n                            - (\n                                f.col(f\"mapped_{end_col}\")\n                                - f.col(f\"mapped_{start_col}\")\n                            )\n                        )\n                        &lt;= self.max_difference\n                    )\n                ).persist()\n            )\n        else:\n            return mapped_df.persist()\n\n    def convert_coordinates(\n        self: LiftOverSpark, df: DataFrame, chrom_name: str, pos_name: str\n    ) -&gt; DataFrame:\n        \"\"\"Converts genomic coordinates to coordinates on an other build.\n\n        Args:\n            df (DataFrame): Spark Dataframe with chromosome and position columns.\n            chrom_name (str): Name of the chromosome column.\n            pos_name (str): Name of the position column.\n\n        Returns:\n            DataFrame: Spark Dataframe with the mapped position column.\n        \"\"\"\n        mapped = (\n            df.withColumn(\n                \"mapped\", self.liftover_udf(f.col(chrom_name), f.col(pos_name))\n            )\n            .filter((f.col(\"mapped\").isNotNull()) &amp; (f.size(f.col(\"mapped\")) == 1))\n            # Extracting mapped corrdinates:\n            .withColumn(\"mapped_\" + chrom_name, f.col(\"mapped\")[0][0])\n            .withColumn(\"mapped_\" + pos_name, f.col(\"mapped\")[0][1])\n            # Drop rows that mapped to the other chromosomes:\n            .filter(\n                f.col(\"mapped_\" + chrom_name)\n                == f.concat(f.lit(\"chr\"), f.col(chrom_name))\n            )\n            # Dropping unused columns:\n            .drop(\"mapped\", \"mapped_\" + chrom_name)\n            .persist()\n        )\n\n        return mapped\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.LiftOverSpark.__init__","title":"<code>__init__(chain_file: str, max_difference: int = 100) -&gt; None</code>","text":"<p>Initialize the LiftOver object.</p> <p>Parameters:</p> Name Type Description Default <code>chain_file</code> <code>str</code> <p>Path to the chain file</p> required <code>max_difference</code> <code>int</code> <p>The maximum tolerated difference in the resulting length. Defaults to 100.</p> <code>100</code> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>def __init__(\n    self: LiftOverSpark, chain_file: str, max_difference: int = 100\n) -&gt; None:\n    \"\"\"Initialize the LiftOver object.\n\n    Args:\n        chain_file (str): Path to the chain file\n        max_difference (int, optional): The maximum tolerated difference in the resulting length. Defaults to 100.\n    \"\"\"\n    self.chain_file = chain_file\n    self.max_difference = max_difference\n\n    # Initializing liftover object by opening the chain file - LiftOver only supports local files:\n    if chain_file.startswith(\"gs://\"):\n        bucket_name = chain_file.split(\"/\")[2]\n        blob_name = \"/\".join(chain_file.split(\"/\")[3:])\n        with tempfile.NamedTemporaryFile(delete=True) as temp_file:\n            storage.Client().bucket(bucket_name).blob(\n                blob_name\n            ).download_to_filename(temp_file.name)\n            self.lo = LiftOver(temp_file.name)\n    else:\n        self.lo = LiftOver(chain_file)\n\n    # UDF to do map genomic coordinates to liftover coordinates:\n    self.liftover_udf = f.udf(\n        lambda chrom, pos: self.lo.convert_coordinate(chrom, pos),\n        t.ArrayType(t.ArrayType(t.StringType())),\n    )\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.LiftOverSpark.convert_coordinates","title":"<code>convert_coordinates(df: DataFrame, chrom_name: str, pos_name: str) -&gt; DataFrame</code>","text":"<p>Converts genomic coordinates to coordinates on an other build.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark Dataframe with chromosome and position columns.</p> required <code>chrom_name</code> <code>str</code> <p>Name of the chromosome column.</p> required <code>pos_name</code> <code>str</code> <p>Name of the position column.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Spark Dataframe with the mapped position column.</p> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>def convert_coordinates(\n    self: LiftOverSpark, df: DataFrame, chrom_name: str, pos_name: str\n) -&gt; DataFrame:\n    \"\"\"Converts genomic coordinates to coordinates on an other build.\n\n    Args:\n        df (DataFrame): Spark Dataframe with chromosome and position columns.\n        chrom_name (str): Name of the chromosome column.\n        pos_name (str): Name of the position column.\n\n    Returns:\n        DataFrame: Spark Dataframe with the mapped position column.\n    \"\"\"\n    mapped = (\n        df.withColumn(\n            \"mapped\", self.liftover_udf(f.col(chrom_name), f.col(pos_name))\n        )\n        .filter((f.col(\"mapped\").isNotNull()) &amp; (f.size(f.col(\"mapped\")) == 1))\n        # Extracting mapped corrdinates:\n        .withColumn(\"mapped_\" + chrom_name, f.col(\"mapped\")[0][0])\n        .withColumn(\"mapped_\" + pos_name, f.col(\"mapped\")[0][1])\n        # Drop rows that mapped to the other chromosomes:\n        .filter(\n            f.col(\"mapped_\" + chrom_name)\n            == f.concat(f.lit(\"chr\"), f.col(chrom_name))\n        )\n        # Dropping unused columns:\n        .drop(\"mapped\", \"mapped_\" + chrom_name)\n        .persist()\n    )\n\n    return mapped\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.LiftOverSpark.convert_intervals","title":"<code>convert_intervals(df: DataFrame, chrom_col: str, start_col: str, end_col: str, filter: bool = True) -&gt; DataFrame</code>","text":"<p>Convert genomic intervals to liftover coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>spark Dataframe with chromosome, start and end columns.</p> required <code>chrom_col</code> <code>str</code> <p>Name of the chromosome column.</p> required <code>start_col</code> <code>str</code> <p>Name of the start column.</p> required <code>end_col</code> <code>str</code> <p>Name of the end column.</p> required <code>filter</code> <code>bool</code> <p>If True, filter is applied on the mapped data, otherwise return everything. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Liftovered intervals</p> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>def convert_intervals(\n    self: LiftOverSpark,\n    df: DataFrame,\n    chrom_col: str,\n    start_col: str,\n    end_col: str,\n    filter: bool = True,\n) -&gt; DataFrame:\n    \"\"\"Convert genomic intervals to liftover coordinates.\n\n    Args:\n        df (DataFrame): spark Dataframe with chromosome, start and end columns.\n        chrom_col (str): Name of the chromosome column.\n        start_col (str): Name of the start column.\n        end_col (str): Name of the end column.\n        filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True.\n\n    Returns:\n        DataFrame: Liftovered intervals\n    \"\"\"\n    # Lift over start coordinates, changing to 1-based coordinates:\n    start_df = (\n        df.withColumn(start_col, f.col(start_col) + 1)\n        .select(chrom_col, start_col)\n        .distinct()\n    )\n    start_df = self.convert_coordinates(\n        start_df, chrom_col, start_col\n    ).withColumnRenamed(\"mapped_pos\", f\"mapped_{start_col}\")\n\n    # Lift over end coordinates:\n    end_df = df.select(chrom_col, end_col).distinct()\n    end_df = self.convert_coordinates(end_df, chrom_col, end_col).withColumnRenamed(\n        \"mapped_pos\", f\"mapped_{end_col}\"\n    )\n\n    # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates):\n    mapped_df = df.join(\n        start_df.withColumn(start_col, f.col(start_col) - 1),\n        on=[chrom_col, start_col],\n        how=\"left\",\n    ).join(end_df, on=[chrom_col, end_col], how=\"left\")\n\n    # The filter option allows to get all the data and filter it afterwards.\n    if filter:\n        return (\n            mapped_df\n            # Select only rows where the start is smaller than the end:\n            .filter(\n                # Drop rows with no mappings:\n                f.col(f\"mapped_{start_col}\").isNotNull()\n                &amp; f.col(f\"mapped_{end_col}\").isNotNull()\n                # Drop rows where the start is larger than the end:\n                &amp; (f.col(f\"mapped_{end_col}\") &gt;= f.col(f\"mapped_{start_col}\"))\n                # Drop rows where the difference of the length of the regions are larger than the threshold:\n                &amp; (\n                    f.abs(\n                        (f.col(end_col) - f.col(start_col))\n                        - (\n                            f.col(f\"mapped_{end_col}\")\n                            - f.col(f\"mapped_{start_col}\")\n                        )\n                    )\n                    &lt;= self.max_difference\n                )\n            ).persist()\n        )\n    else:\n        return mapped_df.persist()\n</code></pre>"},{"location":"python_api/common/genomic_region/#gentropy.common.genomic_region.liftover_loci","title":"<code>gentropy.common.genomic_region.liftover_loci(variant_index: Table, chain_path: str, dest_reference_genome: str) -&gt; Table</code>","text":"<p>Liftover a Hail table containing variant information from GRCh37 to GRCh38 or vice versa.</p> <p>Parameters:</p> Name Type Description Default <code>variant_index</code> <code>Table</code> <p>Variants to be lifted over</p> required <code>chain_path</code> <code>str</code> <p>Path to chain file for liftover</p> required <code>dest_reference_genome</code> <code>str</code> <p>Destination reference genome. It can be either GRCh37 or GRCh38.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>LD variant index with coordinates in the new reference genome</p> Warning <p>This function assumes hail is initialized in Session.</p> Source code in <code>src/gentropy/common/genomic_region.py</code> <pre><code>def liftover_loci(\n    variant_index: Table, chain_path: str, dest_reference_genome: str\n) -&gt; Table:\n    \"\"\"Liftover a Hail table containing variant information from GRCh37 to GRCh38 or vice versa.\n\n    Args:\n        variant_index (Table): Variants to be lifted over\n        chain_path (str): Path to chain file for liftover\n        dest_reference_genome (str): Destination reference genome. It can be either GRCh37 or GRCh38.\n\n    Returns:\n        Table: LD variant index with coordinates in the new reference genome\n\n    Warning:\n        This function assumes hail is initialized in Session.\n    \"\"\"\n    import hail as hl\n    if not hl.get_reference(\"GRCh37\").has_liftover(\n        \"GRCh38\"\n    ):  # True when a chain file has already been registered\n        rg37 = hl.get_reference(\"GRCh37\")\n        rg38 = hl.get_reference(\"GRCh38\")\n        if dest_reference_genome == \"GRCh38\":\n            rg37.add_liftover(chain_path, rg38)\n        elif dest_reference_genome == \"GRCh37\":\n            rg38.add_liftover(chain_path, rg37)\n    # Dynamically create the new field with transmute\n    new_locus = f\"locus_{dest_reference_genome}\"\n    return variant_index.transmute(\n        **{new_locus: hl.liftover(variant_index.locus, dest_reference_genome)}\n    )\n</code></pre>"},{"location":"python_api/common/processing/","title":"processing","text":""},{"location":"python_api/common/processing/#common-functions-for-gentropy-dataset-processing-data-business-logic","title":"Common functions for gentropy dataset processing (data business logic)","text":""},{"location":"python_api/common/processing/#gentropy.common.processing.parse_efos","title":"<code>gentropy.common.processing.parse_efos(efo_uris: Column) -&gt; Column</code>","text":"<p>Extracting EFO identifiers.</p> <p>This function parses EFO identifiers from a comma-separated list of EFO URIs.</p> <p>Parameters:</p> Name Type Description Default <code>efo_uris</code> <code>Column</code> <p>column with a list of EFO URIs</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>column with a sorted and unique list of parsed EFO IDs</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = [(\"http://www.ebi.ac.uk/efo/EFO_0000001,http://purl.obolibrary.org/obo/OBA_VT0001253,http://www.orpha.net/ORDO/Orphanet_101953,http://www.ebi.ac.uk/efo/EFO_0000001\",)]\n&gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"efos\")\n&gt;&gt;&gt; df.select(parse_efos(f.col(\"efos\")).alias('col')).show(truncate=False)\n+---------------------------------------------+\n|col                                          |\n+---------------------------------------------+\n|[EFO_0000001, OBA_VT0001253, Orphanet_101953]|\n+---------------------------------------------+\n</code></pre> Source code in <code>src/gentropy/common/processing.py</code> <pre><code>def parse_efos(efo_uris: Column) -&gt; Column:\n    \"\"\"Extracting EFO identifiers.\n\n    This function parses EFO identifiers from a comma-separated list of EFO URIs.\n\n    Args:\n        efo_uris (Column): column with a list of EFO URIs\n\n    Returns:\n        Column: column with a sorted and unique list of parsed EFO IDs\n\n    Examples:\n        &gt;&gt;&gt; d = [(\"http://www.ebi.ac.uk/efo/EFO_0000001,http://purl.obolibrary.org/obo/OBA_VT0001253,http://www.orpha.net/ORDO/Orphanet_101953,http://www.ebi.ac.uk/efo/EFO_0000001\",)]\n        &gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"efos\")\n        &gt;&gt;&gt; df.select(parse_efos(f.col(\"efos\")).alias('col')).show(truncate=False)\n        +---------------------------------------------+\n        |col                                          |\n        +---------------------------------------------+\n        |[EFO_0000001, OBA_VT0001253, Orphanet_101953]|\n        +---------------------------------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return f.array_distinct(\n        f.transform(\n            # Splitting colun values to individual URIs:\n            f.split(efo_uris, \",\"),\n            # Each URI is further split, and the last component is returned:\n            lambda uri: f.element_at(f.split(uri, \"/\"), -1),\n        )\n    )\n</code></pre>"},{"location":"python_api/common/processing/#gentropy.common.processing.extract_chromosome","title":"<code>gentropy.common.processing.extract_chromosome(variant_id: Column) -&gt; Column</code>","text":"<p>Extract chromosome from variant ID.</p> <p>This function extracts the chromosome from a variant ID. The variantId is expected to be in the format <code>chromosome_position_ref_alt</code>. The function does not convert the GENCODE to Ensembl chromosome notation. See https://genome.ucsc.edu/FAQ/FAQgenes.html#:~:text=maps%20only%20once.-,The%20differences,-Some%20of%20our</p> <p>Parameters:</p> Name Type Description Default <code>variant_id</code> <code>Column</code> <p>Variant ID</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Chromosome</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = [(\"chr1_12345_A_T\",),(\"15_KI270850v1_alt_48777_C_T\",),]\n&gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"variantId\")\n&gt;&gt;&gt; df.withColumn(\"chromosome\", extract_chromosome(f.col(\"variantId\"))).show(truncate=False)\n+---------------------------+-----------------+\n|variantId                  |chromosome       |\n+---------------------------+-----------------+\n|chr1_12345_A_T             |chr1             |\n|15_KI270850v1_alt_48777_C_T|15_KI270850v1_alt|\n+---------------------------+-----------------+\n</code></pre> Source code in <code>src/gentropy/common/processing.py</code> <pre><code>def extract_chromosome(variant_id: Column) -&gt; Column:\n    \"\"\"Extract chromosome from variant ID.\n\n    This function extracts the chromosome from a variant ID. The variantId is expected to be in the format `chromosome_position_ref_alt`.\n    The function does not convert the GENCODE to Ensembl chromosome notation.\n    See https://genome.ucsc.edu/FAQ/FAQgenes.html#:~:text=maps%20only%20once.-,The%20differences,-Some%20of%20our\n\n    Args:\n        variant_id (Column): Variant ID\n\n    Returns:\n        Column: Chromosome\n\n    Examples:\n        &gt;&gt;&gt; d = [(\"chr1_12345_A_T\",),(\"15_KI270850v1_alt_48777_C_T\",),]\n        &gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"variantId\")\n        &gt;&gt;&gt; df.withColumn(\"chromosome\", extract_chromosome(f.col(\"variantId\"))).show(truncate=False)\n        +---------------------------+-----------------+\n        |variantId                  |chromosome       |\n        +---------------------------+-----------------+\n        |chr1_12345_A_T             |chr1             |\n        |15_KI270850v1_alt_48777_C_T|15_KI270850v1_alt|\n        +---------------------------+-----------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return f.regexp_extract(variant_id, r\"^(.*)_\\d+_.*$\", 1)\n</code></pre>"},{"location":"python_api/common/processing/#gentropy.common.processing.extract_position","title":"<code>gentropy.common.processing.extract_position(variant_id: Column) -&gt; Column</code>","text":"<p>Extract position from variant ID.</p> <p>This function extracts the position from a variant ID. The variantId is expected to be in the format <code>chromosome_position_ref_alt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>variant_id</code> <code>Column</code> <p>Variant ID</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Position</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = [(\"chr1_12345_A_T\",),(\"15_KI270850v1_alt_48777_C_T\",),]\n&gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"variantId\")\n&gt;&gt;&gt; df.withColumn(\"position\", extract_position(f.col(\"variantId\"))).show(truncate=False)\n+---------------------------+--------+\n|variantId                  |position|\n+---------------------------+--------+\n|chr1_12345_A_T             |12345   |\n|15_KI270850v1_alt_48777_C_T|48777   |\n+---------------------------+--------+\n</code></pre> Source code in <code>src/gentropy/common/processing.py</code> <pre><code>def extract_position(variant_id: Column) -&gt; Column:\n    \"\"\"Extract position from variant ID.\n\n    This function extracts the position from a variant ID. The variantId is expected to be in the format `chromosome_position_ref_alt`.\n\n    Args:\n        variant_id (Column): Variant ID\n\n    Returns:\n        Column: Position\n\n    Examples:\n        &gt;&gt;&gt; d = [(\"chr1_12345_A_T\",),(\"15_KI270850v1_alt_48777_C_T\",),]\n        &gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"variantId\")\n        &gt;&gt;&gt; df.withColumn(\"position\", extract_position(f.col(\"variantId\"))).show(truncate=False)\n        +---------------------------+--------+\n        |variantId                  |position|\n        +---------------------------+--------+\n        |chr1_12345_A_T             |12345   |\n        |15_KI270850v1_alt_48777_C_T|48777   |\n        +---------------------------+--------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return f.regexp_extract(variant_id, r\"^.*_(\\d+)_.*$\", 1)\n</code></pre>"},{"location":"python_api/common/processing/#gentropy.common.processing.normalize_chromosome","title":"<code>gentropy.common.processing.normalize_chromosome(chromosome: Column) -&gt; Column</code>","text":"<p>Normalize chromosome notation.</p> <p>This function normalizes chromosome notation by     1. Removing the \"chr\" prefix if present.     2. Converting \"M\" to \"MT\".     3. Converting \"23\" to \"X\".     4. Converting \"24\" to \"Y\".</p> <p>Parameters:</p> Name Type Description Default <code>chromosome</code> <code>Column</code> <p>Chromosome column</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Normalized chromosome column</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = [(\"chr1\",),(\"2\",),(\"chrX\",),(\"Y\",),(\"chrM\",),(\"23\",),(\"24\",)]\n&gt;&gt;&gt; df = spark.createDataFrame(data=d, schema=\"chromosome STRING\")\n&gt;&gt;&gt; df.withColumn(\"normalized_chromosome\", normalize_chromosome(f.col(\"chromosome\"))).show(truncate=False)\n+----------+---------------------+\n|chromosome|normalized_chromosome|\n+----------+---------------------+\n|chr1      |1                    |\n|2         |2                    |\n|chrX      |X                    |\n|Y         |Y                    |\n|chrM      |MT                   |\n|23        |X                    |\n|24        |Y                    |\n+----------+---------------------+\n</code></pre> Source code in <code>src/gentropy/common/processing.py</code> <pre><code>def normalize_chromosome(chromosome: Column) -&gt; Column:\n    \"\"\"Normalize chromosome notation.\n\n    This function normalizes chromosome notation by\n        1. Removing the \"chr\" prefix if present.\n        2. Converting \"M\" to \"MT\".\n        3. Converting \"23\" to \"X\".\n        4. Converting \"24\" to \"Y\".\n\n    Args:\n        chromosome (Column): Chromosome column\n\n    Returns:\n        Column: Normalized chromosome column\n\n    Examples:\n        &gt;&gt;&gt; d = [(\"chr1\",),(\"2\",),(\"chrX\",),(\"Y\",),(\"chrM\",),(\"23\",),(\"24\",)]\n        &gt;&gt;&gt; df = spark.createDataFrame(data=d, schema=\"chromosome STRING\")\n        &gt;&gt;&gt; df.withColumn(\"normalized_chromosome\", normalize_chromosome(f.col(\"chromosome\"))).show(truncate=False)\n        +----------+---------------------+\n        |chromosome|normalized_chromosome|\n        +----------+---------------------+\n        |chr1      |1                    |\n        |2         |2                    |\n        |chrX      |X                    |\n        |Y         |Y                    |\n        |chrM      |MT                   |\n        |23        |X                    |\n        |24        |Y                    |\n        +----------+---------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    chr_str = chromosome.cast(t.StringType())\n    ensembl_chr = f.regexp_replace(chr_str, r\"^chr\", \"\")\n    return (\n        f.when(ensembl_chr == \"M\", \"MT\")\n        .when(ensembl_chr == \"23\", \"X\")\n        .when(ensembl_chr == \"24\", \"Y\")\n        .otherwise(ensembl_chr)\n    ).alias(\"chromosome\")\n</code></pre>"},{"location":"python_api/common/processing/#gentropy.common.processing.maf","title":"<code>gentropy.common.processing.maf(af: Column, scale: int = 10) -&gt; Column</code>","text":"<p>Calculate minor allele frequency from allele frequency.</p> <p>Parameters:</p> Name Type Description Default <code>af</code> <code>Column</code> <p>Allele frequency column.</p> required <code>scale</code> <code>int</code> <p>Scale for DecimalType.</p> <code>10</code> Note <p>the DecimalType is represented by two parameters: precision and scale. The precision is the total number of digits that can be stored and the scale is the number of digits to the right of the decimal point.</p> <p>For AF the value can be only between 0 and 1.0, so we limit the scale to the number of digits after the decimal point. The precision is set to scale + 1 to ensure we can represent values like 1.0 with the given scale.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Minor allele frequency column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"v1\", 0.1), (\"v2\", 0.9), (\"v3\", None),]\n&gt;&gt;&gt; schema = \"variantId STRING, af DOUBLE\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+---------+----+\n|variantId|af  |\n+---------+----+\n|v1       |0.1 |\n|v2       |0.9 |\n|v3       |NULL|\n+---------+----+\n\n&gt;&gt;&gt; df.withColumn(\"minorAlleleFrequency\", maf(f.col(\"af\"))).show(truncate=False)\n+---------+----+--------------------+\n|variantId|af  |minorAlleleFrequency|\n+---------+----+--------------------+\n|v1       |0.1 |0.1000000000        |\n|v2       |0.9 |0.1000000000        |\n|v3       |NULL|NULL                |\n+---------+----+--------------------+\n</code></pre> Source code in <code>src/gentropy/common/processing.py</code> <pre><code>def maf(af: Column, scale: int = 10) -&gt; Column:\n    \"\"\"Calculate minor allele frequency from allele frequency.\n\n    Args:\n        af (Column): Allele frequency column.\n        scale (int): Scale for DecimalType.\n\n    Note:\n        the DecimalType is represented by two parameters: precision and scale.\n        The precision is the total number of digits that can be stored and the scale\n        is the number of digits to the right of the decimal point.\n\n        For AF the value can be only between 0 and 1.0, so we limit the scale to the\n        number of digits after the decimal point. The precision is set to scale + 1\n        to ensure we can represent values like 1.0 with the given scale.\n\n    Returns:\n        Column: Minor allele frequency column.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"v1\", 0.1), (\"v2\", 0.9), (\"v3\", None),]\n        &gt;&gt;&gt; schema = \"variantId STRING, af DOUBLE\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------+----+\n        |variantId|af  |\n        +---------+----+\n        |v1       |0.1 |\n        |v2       |0.9 |\n        |v3       |NULL|\n        +---------+----+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.withColumn(\"minorAlleleFrequency\", maf(f.col(\"af\"))).show(truncate=False)\n        +---------+----+--------------------+\n        |variantId|af  |minorAlleleFrequency|\n        +---------+----+--------------------+\n        |v1       |0.1 |0.1000000000        |\n        |v2       |0.9 |0.1000000000        |\n        |v3       |NULL|NULL                |\n        +---------+----+--------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    precision = scale + 1  # to ensure we can represent values like 1.0000\n    scaled_af = af.cast(t.DecimalType(precision, scale))\n    max_af = f.lit(1.0).cast(t.DecimalType(precision, scale))\n    return (\n        f.when(af.isNotNull() &amp; (af &lt;= 0.5), scaled_af)\n        .when(af.isNotNull(), max_af - scaled_af)\n        .otherwise(f.lit(None))\n        .alias(\"minorAlleleFrequency\")\n    )\n</code></pre>"},{"location":"python_api/common/processing/#gentropy.common.processing.mac","title":"<code>gentropy.common.processing.mac(maf: Column, n: Column) -&gt; Column</code>","text":"<p>Calculate minor allele count from minor allele frequency and sample size.</p> <p>Parameters:</p> Name Type Description Default <code>maf</code> <code>Column</code> <p>Minor allele frequency column.</p> required <code>n</code> <code>Column</code> <p>Sample size column.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Minor allele count column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"v1\", 0.1, 100), (\"v2\", 0.2, 200), (\"v3\", None, 150), (\"v4\", 0.3, None),]\n&gt;&gt;&gt; schema = \"variantId STRING, maf DOUBLE, n INT\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+---------+----+----+\n|variantId|maf |n   |\n+---------+----+----+\n|v1       |0.1 |100 |\n|v2       |0.2 |200 |\n|v3       |NULL|150 |\n|v4       |0.3 |NULL|\n+---------+----+----+\n</code></pre> Source code in <code>src/gentropy/common/processing.py</code> <pre><code>def mac(maf: Column, n: Column) -&gt; Column:\n    \"\"\"Calculate minor allele count from minor allele frequency and sample size.\n\n    Args:\n        maf (Column): Minor allele frequency column.\n        n (Column): Sample size column.\n\n    Returns:\n        Column: Minor allele count column.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"v1\", 0.1, 100), (\"v2\", 0.2, 200), (\"v3\", None, 150), (\"v4\", 0.3, None),]\n        &gt;&gt;&gt; schema = \"variantId STRING, maf DOUBLE, n INT\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------+----+----+\n        |variantId|maf |n   |\n        +---------+----+----+\n        |v1       |0.1 |100 |\n        |v2       |0.2 |200 |\n        |v3       |NULL|150 |\n        |v4       |0.3 |NULL|\n        +---------+----+----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return (\n        f.when(maf.isNotNull() &amp; n.isNotNull(), (maf * n * 2).cast(t.IntegerType()))\n        .otherwise(f.lit(None))\n        .alias(\"minorAlleleCount\")\n    )\n</code></pre>"},{"location":"python_api/common/processing/#harmonisation","title":"Harmonisation","text":""},{"location":"python_api/common/processing/#gentropy.common.processing.harmonise_summary_stats","title":"<code>gentropy.common.processing.harmonise_summary_stats(spark: SparkSession, raw_summary_stats_path: str, tmp_variant_annotation_path: str, chromosome: str, colname_position: str, colname_allele0: str, colname_allele1: str, colname_a1freq: str | None, colname_info: str | None, colname_beta: str, colname_se: str, colname_mlog10p: str, colname_n: str | None) -&gt; DataFrame</code>","text":"<p>Ingest and harmonise the summary stats.</p> <ol> <li>Rename chromosome 23 to X.</li> <li>Filter out low INFO rows.</li> <li>Filter out low frequency rows.</li> <li>Assign variant types.</li> <li>Create variant ID for joining the variant annotation dataset.</li> <li>Join with the Variant Annotation dataset.</li> <li>Drop bad quality variants.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>raw_summary_stats_path</code> <code>str</code> <p>Input raw summary stats path.</p> required <code>tmp_variant_annotation_path</code> <code>str</code> <p>Path to the Variant Annotation dataset which has been further prepared and processed by the per_chromosome module (previous PR in the chain) to speed up the joins in the harmonisation phase. It includes all variants in both the direct (A0/A1) and reverse (A1/A0) orientations, so that the direction of the variant can be easily determined on joining.</p> required <code>chromosome</code> <code>str</code> <p>Which chromosome to process.</p> required <code>colname_position</code> <code>str</code> <p>Column name for position.</p> required <code>colname_allele0</code> <code>str</code> <p>Column name for allele0.</p> required <code>colname_allele1</code> <code>str</code> <p>Column name for allele1.</p> required <code>colname_a1freq</code> <code>str | None</code> <p>Column name for allele1 frequency (optional).</p> required <code>colname_info</code> <code>str | None</code> <p>Column name for INFO, reflecting variant quality (optional).</p> required <code>colname_beta</code> <code>str</code> <p>Column name for beta.</p> required <code>colname_se</code> <code>str</code> <p>Column name for beta standard error.</p> required <code>colname_mlog10p</code> <code>str</code> <p>Column name for -log10(p).</p> required <code>colname_n</code> <code>str | None</code> <p>Column name for the number of samples (optional).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A harmonised summary stats dataframe.</p> Source code in <code>src/gentropy/common/processing.py</code> <pre><code>def harmonise_summary_stats(\n    spark: SparkSession,\n    raw_summary_stats_path: str,\n    tmp_variant_annotation_path: str,\n    chromosome: str,\n    colname_position: str,\n    colname_allele0: str,\n    colname_allele1: str,\n    colname_a1freq: str | None,\n    colname_info: str | None,\n    colname_beta: str,\n    colname_se: str,\n    colname_mlog10p: str,\n    colname_n: str | None,\n) -&gt; DataFrame:\n    \"\"\"Ingest and harmonise the summary stats.\n\n    1. Rename chromosome 23 to X.\n    2. Filter out low INFO rows.\n    3. Filter out low frequency rows.\n    4. Assign variant types.\n    5. Create variant ID for joining the variant annotation dataset.\n    6. Join with the Variant Annotation dataset.\n    7. Drop bad quality variants.\n\n    Args:\n        spark (SparkSession): Spark session object.\n        raw_summary_stats_path (str): Input raw summary stats path.\n        tmp_variant_annotation_path (str): Path to the Variant Annotation dataset which has been further prepared and processed by the per_chromosome module (previous PR in the chain) to speed up the joins in the harmonisation phase. It includes all variants in both the direct (A0/A1) and reverse (A1/A0) orientations, so that the direction of the variant can be easily determined on joining.\n        chromosome (str): Which chromosome to process.\n        colname_position (str): Column name for position.\n        colname_allele0 (str): Column name for allele0.\n        colname_allele1 (str): Column name for allele1.\n        colname_a1freq (str | None): Column name for allele1 frequency (optional).\n        colname_info (str | None): Column name for INFO, reflecting variant quality (optional).\n        colname_beta (str): Column name for beta.\n        colname_se (str): Column name for beta standard error.\n        colname_mlog10p (str): Column name for -log10(p).\n        colname_n (str | None): Column name for the number of samples (optional).\n\n    Returns:\n        DataFrame: A harmonised summary stats dataframe.\n    \"\"\"\n    # Read the precomputed variant annotation dataset.\n    va_df = (\n        spark.read.parquet(tmp_variant_annotation_path)\n        .filter(f.col(\"vaChromosome\") == (\"X\" if chromosome == \"23\" else chromosome))\n        .persist()\n    )\n\n    # Read and process the summary stats dataset.\n    df = (\n        spark.read.parquet(raw_summary_stats_path)\n        .filter(f.col(\"chromosome\") == chromosome)\n        # Harmonise, 1: Rename chromosome 23 to X.\n        .withColumn(\n            \"chromosome\",\n            f.when(f.col(\"chromosome\") == \"23\", \"X\").otherwise(f.col(\"chromosome\")),\n        )\n    )\n    if colname_info:\n        # Harmonise, 2: Filter out low INFO rows.\n        df = df.filter(f.col(colname_info) &gt;= 0.8)\n    if colname_a1freq:\n        # Harmonise, 3: Filter out low frequency rows.\n        df = (\n            df.withColumn(\n                \"MAF\",\n                f.when(f.col(colname_a1freq) &lt; 0.5, f.col(colname_a1freq)).otherwise(\n                    1 - f.col(colname_a1freq)\n                ),\n            )\n            .filter(f.col(\"MAF\") &gt;= 0.0001)\n            .drop(\"MAF\")\n        )\n    df = (\n        df\n        # Harmonise, 4: Assign variant types.\n        # There are three possible variant types:\n        # 1. `snp_c` means an SNP converting a base into its complementary base: A&lt;&gt;T or G&gt;&lt;C.\n        # 2. `snp_n` means any other SNP where the length of each allele is still exactly 1.\n        # 3. `indel` means any other variant where the length of at least one allele is greater than 1.\n        .withColumn(\n            \"variant_type\",\n            f.when(\n                (f.length(colname_allele0) == 1) &amp; (f.length(colname_allele1) == 1),\n                f.when(\n                    ((f.col(colname_allele0) == \"A\") &amp; (f.col(colname_allele1) == \"T\"))\n                    | (\n                        (f.col(colname_allele0) == \"T\")\n                        &amp; (f.col(colname_allele1) == \"A\")\n                    )\n                    | (\n                        (f.col(colname_allele0) == \"G\")\n                        &amp; (f.col(colname_allele1) == \"C\")\n                    )\n                    | (\n                        (f.col(colname_allele0) == \"C\")\n                        &amp; (f.col(colname_allele1) == \"G\")\n                    ),\n                    \"snp_c\",\n                ).otherwise(\"snp_n\"),\n            ).otherwise(\"indel\"),\n        )\n        # Harmonise, 5: Create variant ID for joining the variant annotation dataset.\n        .withColumn(colname_position, f.col(colname_position).cast(\"integer\"))\n        .withColumn(\n            \"summary_stats_id\",\n            f.concat_ws(\n                \"_\",\n                f.col(\"chromosome\"),\n                f.col(colname_position),\n                f.col(colname_allele0),\n                f.col(colname_allele1),\n            ),\n        )\n    )\n    # Harmonise, 6: Join with the Variant Annotation dataset.\n    df = (\n        df.join(\n            va_df,\n            (df[\"chromosome\"] == va_df[\"vaChromosome\"])\n            &amp; (df[\"summary_stats_id\"] == va_df[\"summary_stats_id\"]),\n            \"inner\",\n        )\n        .drop(\"vaChromosome\", \"summary_stats_id\")\n        .withColumn(\n            \"beta\",\n            f.when(\n                f.col(\"direction\") == \"direct\", f.col(colname_beta).cast(\"double\")\n            ).otherwise(-f.col(colname_beta).cast(\"double\")),\n        )\n    )\n    if colname_a1freq:\n        df = df.withColumn(\n            \"effectAlleleFrequencyFromSource\",\n            f.when(\n                f.col(\"direction\") == \"direct\", f.col(colname_a1freq).cast(\"float\")\n            ).otherwise(1 - f.col(colname_a1freq).cast(\"float\")),\n        )\n    df = (\n        # Harmonise, 7: Drop bad quality variants.\n        df.filter(\n            ~((f.col(\"variant_type\") == \"snp_c\") &amp; (f.col(\"direction\") == \"flip\"))\n        )\n    )\n\n    # Prepare the fields according to schema.\n    select_expr = [\n        f.col(\"studyId\"),\n        f.col(\"chromosome\"),\n        f.col(\"variantId\"),\n        f.col(\"beta\"),\n        f.col(colname_position).cast(t.IntegerType()).alias(\"position\"),\n        # Parse p-value into mantissa and exponent.\n        *pvalue_from_neglogpval(f.col(colname_mlog10p).cast(t.DoubleType())),\n        # Add standard error and sample size information.\n        f.col(colname_se).cast(\"double\").alias(\"standardError\"),\n    ]\n    if colname_n:\n        select_expr.append(f.col(colname_n).cast(\"integer\").alias(\"sampleSize\"))\n\n    df = (\n        df.select(*select_expr)\n        # Dropping associations where no harmonized position is available:\n        .filter(f.col(\"position\").isNotNull())\n        # We are not interested in associations empty beta values:\n        .filter(f.col(\"beta\").isNotNull())\n        # We are not interested in associations with zero effect:\n        .filter(f.col(\"beta\") != 0)\n        # Drop rows which don't have proper position or beta value.\n    )\n    # NOTE! In case the standard error is empty, recompute it from p-value and beta.\n    # If we leave the standard error empty for all fields, we will cause the sanity filter\n    # to skip all rows.\n    # Make sure the beta is non empty before computation.\n    computed_chi2 = chi2_from_pvalue(f.col(\"pValueMantissa\"), f.col(\"pValueExponent\"))\n    computed_stderr = stderr_from_chi2_and_effect_size(computed_chi2, f.col(\"beta\"))\n    df = df.withColumn(\n        \"standardError\", f.coalesce(f.col(\"standardError\"), computed_stderr)\n    ).orderBy(f.col(\"chromosome\"), f.col(\"position\"))\n\n    # Return the dataframe.\n    return df\n</code></pre>"},{"location":"python_api/common/processing/#gentropy.common.processing.prepare_va","title":"<code>gentropy.common.processing.prepare_va(session: Session, variant_annotation_path: str, tmp_variant_annotation_path: str) -&gt; None</code>","text":"<p>Prepare the Variant Annotation dataset for efficient per-chromosome joins.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>The gentropy session wrapper to be used for reading and writing data.</p> required <code>variant_annotation_path</code> <code>str</code> <p>The path to the input variant annotation dataset.</p> required <code>tmp_variant_annotation_path</code> <code>str</code> <p>The path to store the temporary output for the repartitioned annotation dataset.</p> required Source code in <code>src/gentropy/common/processing.py</code> <pre><code>def prepare_va(\n    session: Session, variant_annotation_path: str, tmp_variant_annotation_path: str\n) -&gt; None:\n    \"\"\"Prepare the Variant Annotation dataset for efficient per-chromosome joins.\n\n    Args:\n        session (Session): The gentropy session wrapper to be used for reading and writing data.\n        variant_annotation_path (str): The path to the input variant annotation dataset.\n        tmp_variant_annotation_path (str): The path to store the temporary output for the repartitioned annotation dataset.\n    \"\"\"\n    va_df = session.spark.read.parquet(variant_annotation_path)\n    va_df_direct = va_df.select(\n        f.col(\"chromosome\").alias(\"vaChromosome\"),\n        f.col(\"variantId\"),\n        f.concat_ws(\n            \"_\",\n            f.col(\"chromosome\"),\n            f.col(\"position\"),\n            f.col(\"referenceAllele\"),\n            f.col(\"alternateAllele\"),\n        ).alias(\"summary_stats_id\"),\n        f.lit(\"direct\").alias(\"direction\"),\n    )\n    va_df_flip = va_df.select(\n        f.col(\"chromosome\").alias(\"vaChromosome\"),\n        f.col(\"variantId\"),\n        f.concat_ws(\n            \"_\",\n            f.col(\"chromosome\"),\n            f.col(\"position\"),\n            f.col(\"alternateAllele\"),\n            f.col(\"referenceAllele\"),\n        ).alias(\"summary_stats_id\"),\n        f.lit(\"flip\").alias(\"direction\"),\n    )\n    (\n        va_df_direct.union(va_df_flip)\n        .coalesce(1)\n        .repartition(\"vaChromosome\")\n        .write.partitionBy(\"vaChromosome\")\n        .mode(\"overwrite\")\n        .parquet(tmp_variant_annotation_path)\n    )\n</code></pre>"},{"location":"python_api/common/session/","title":"session","text":""},{"location":"python_api/common/session/#spark-session-wrapper-for-gentropy","title":"Spark Session wrapper for gentropy","text":""},{"location":"python_api/common/session/#gentropy.common.session.Session","title":"<code>gentropy.common.session.Session</code>","text":"<p>This class provides a Spark session and logger.</p> Source code in <code>src/gentropy/common/session.py</code> <pre><code>class Session:\n    \"\"\"This class provides a Spark session and logger.\"\"\"\n\n    def __init__(\n        self: Session,\n        spark_uri: str = \"local[*]\",\n        write_mode: str = \"errorifexists\",\n        app_name: str = \"gentropy\",\n        hail_home: str | None = None,\n        start_hail: bool = False,\n        extended_spark_conf: dict[str, str] | None = None,\n        output_partitions: int = 200,\n        use_enhanced_bgzip_codec: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialises spark session and logger.\n\n        Args:\n            spark_uri (str): Spark URI. Defaults to \"local[*]\".\n            write_mode (str): Spark write mode. Defaults to \"errorifexists\".\n            app_name (str): Spark application name. Defaults to \"gentropy\".\n            hail_home (str | None): Path to Hail installation. Defaults to None.\n            start_hail (bool): Whether to start Hail. Defaults to False.\n            extended_spark_conf (dict[str, str] | None): Extended Spark configuration. Defaults to None.\n            output_partitions (int): Number of partitions for output datasets. Defaults to 200.\n            use_enhanced_bgzip_codec (bool): Whether to use the BGZFEnhancedGzipCodec for reading block gzipped files. Defaults to False.\n        \"\"\"\n        merged_conf = self._create_merged_config(\n            start_hail,\n            use_enhanced_bgzip_codec,\n            extended_spark_conf,\n            hail_home,\n        )\n\n        self.spark = (\n            SparkSession.Builder()\n            .config(conf=merged_conf)\n            .master(spark_uri)\n            .appName(app_name)\n            .getOrCreate()\n        )\n        self.logger = Log4j(self.spark)\n\n        self.write_mode = write_mode\n\n        self.hail_home = hail_home\n        self.start_hail = start_hail\n        self.use_enhanced_bgzip_codec = use_enhanced_bgzip_codec\n        if start_hail:\n            hl.init(sc=self.spark.sparkContext, log=\"/dev/null\")\n        self.output_partitions = output_partitions\n\n    def _default_config(self: Session) -&gt; SparkConf:\n        \"\"\"Default spark configuration.\n\n        Returns:\n            SparkConf: Default spark configuration.\n        \"\"\"\n        return (\n            SparkConf()\n            # Dynamic allocation\n            .set(\"spark.dynamicAllocation.enabled\", \"true\")\n            .set(\"spark.dynamicAllocation.minExecutors\", \"2\")\n            .set(\"spark.dynamicAllocation.initialExecutors\", \"2\")\n            .set(\"spark.shuffle.service.enabled\", \"true\")\n        )\n\n    def _bgzip_config(self: Session) -&gt; SparkConf:\n        \"\"\"Spark configuration for reading block gzipped files.\n\n        Configuration that adds the hadoop-bam package and sets the BGZFEnhancedGzipCodec.\n        Based on hadoop-bam jar artifact from [maven](https://mvnrepository.com/artifact/org.seqdoop/hadoop-bam/7.10.0).\n\n        Note:\n            Full details of the codec can be found in [hadoop-bam](https://github.com/HadoopGenomics/Hadoop-BAM/blob/7.10.0/src/main/java/org/seqdoop/hadoop_bam/util/BGZFEnhancedGzipCodec.java)\n\n        This codec implements:\n        (1) SplittableCompressionCodec allowing parallel reading of bgzip files.\n        (2) GzipCodec allowing reading of standard gzip files.\n\n        Returns:\n            SparkConf: Spark configuration for reading block gzipped files.\n        \"\"\"\n        return (\n            SparkConf()\n            .set(\"spark.jars.packages\", \"org.seqdoop:hadoop-bam:7.10.0\")\n            .set(\n                \"spark.hadoop.io.compression.codecs\",\n                \"org.seqdoop.hadoop_bam.util.BGZFEnhancedGzipCodec\",\n            )\n        )\n\n    def _hail_config(self: Session, hail_home: str) -&gt; SparkConf:\n        \"\"\"Returns the Hail specific Spark configuration.\n\n        Args:\n            hail_home (str): Path to Hail installation.\n\n        Returns:\n            SparkConf: Hail specific Spark configuration.\n        \"\"\"\n        return (\n            SparkConf()\n            .set(\"spark.jars\", f\"{hail_home}/backend/hail-all-spark.jar\")\n            .set(\n                \"spark.driver.extraClassPath\", f\"{hail_home}/backend/hail-all-spark.jar\"\n            )\n            .set(\"spark.executor.extraClassPath\", \"./hail-all-spark.jar\")\n            .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n            .set(\"spark.kryo.registrator\", \"is.hail.kryo.HailKryoRegistrator\")\n        )\n\n    def _create_merged_config(\n        self: Session,\n        start_hail: bool,\n        use_enhanced_bgzip_codec: bool,\n        extended_spark_conf: dict[str, str] | None,\n        hail_home: str | None = None,\n    ) -&gt; SparkConf:\n        \"\"\"Merges the default, and optionally the Hail and extended configurations if provided.\n\n        Args:\n            start_hail (bool): Whether to start Hail.\n            use_enhanced_bgzip_codec (bool): Whether to use the BGZFEnhancedGzipCodec for reading block gzipped files.\n            extended_spark_conf (dict[str, str] | None): Extended Spark configuration.\n            hail_home (str | None): Path to Hail installation.\n\n        Raises:\n            ValueError: If Hail home is not specified but Hail is requested.\n\n        Returns:\n            SparkConf: Merged Spark configuration.\n        \"\"\"\n        all_settings = self._default_config().getAll()\n        if start_hail:\n            if not hail_home:\n                raise ValueError(\"Hail home must be specified to start Hail.\")\n            all_settings += self._hail_config(hail_home).getAll()\n        if use_enhanced_bgzip_codec:\n            all_settings += self._bgzip_config().getAll()\n        if extended_spark_conf is not None:\n            all_settings += list(extended_spark_conf.items())\n        return SparkConf().setAll(all_settings)\n\n    def load_data(\n        self: Session,\n        path: str | list[str],\n        format: str = \"parquet\",\n        schema: StructType | str | None = None,\n        **kwargs: bool | float | int | str | None,\n    ) -&gt; DataFrame:\n        \"\"\"Generic function to read a file or folder into a Spark dataframe.\n\n        The `recursiveFileLookup` flag when set to True will skip all partition columns, but read files from all subdirectories.\n\n        Args:\n            path (str | list[str]): path to the dataset\n            format (str): file format. Defaults to parquet.\n            schema (StructType | str | None): Schema to use when reading the data.\n            **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.load. `mergeSchema` is set to True, `recursiveFileLookup` is set to False by default.\n\n        Returns:\n            DataFrame: Dataframe\n        \"\"\"\n        # Set default kwargs\n        if schema is None:\n            kwargs[\"inferSchema\"] = kwargs.get(\"inferSchema\", True)\n        kwargs[\"mergeSchema\"] = kwargs.get(\"mergeSchema\", True)\n        kwargs[\"recursiveFileLookup\"] = kwargs.get(\"recursiveFileLookup\", False)\n        return self.spark.read.load(path, format=format, schema=schema, **kwargs)\n</code></pre>"},{"location":"python_api/common/session/#gentropy.common.session.Session.__init__","title":"<code>__init__(spark_uri: str = 'local[*]', write_mode: str = 'errorifexists', app_name: str = 'gentropy', hail_home: str | None = None, start_hail: bool = False, extended_spark_conf: dict[str, str] | None = None, output_partitions: int = 200, use_enhanced_bgzip_codec: bool = False) -&gt; None</code>","text":"<p>Initialises spark session and logger.</p> <p>Parameters:</p> Name Type Description Default <code>spark_uri</code> <code>str</code> <p>Spark URI. Defaults to \"local[*]\".</p> <code>'local[*]'</code> <code>write_mode</code> <code>str</code> <p>Spark write mode. Defaults to \"errorifexists\".</p> <code>'errorifexists'</code> <code>app_name</code> <code>str</code> <p>Spark application name. Defaults to \"gentropy\".</p> <code>'gentropy'</code> <code>hail_home</code> <code>str | None</code> <p>Path to Hail installation. Defaults to None.</p> <code>None</code> <code>start_hail</code> <code>bool</code> <p>Whether to start Hail. Defaults to False.</p> <code>False</code> <code>extended_spark_conf</code> <code>dict[str, str] | None</code> <p>Extended Spark configuration. Defaults to None.</p> <code>None</code> <code>output_partitions</code> <code>int</code> <p>Number of partitions for output datasets. Defaults to 200.</p> <code>200</code> <code>use_enhanced_bgzip_codec</code> <code>bool</code> <p>Whether to use the BGZFEnhancedGzipCodec for reading block gzipped files. Defaults to False.</p> <code>False</code> Source code in <code>src/gentropy/common/session.py</code> <pre><code>def __init__(\n    self: Session,\n    spark_uri: str = \"local[*]\",\n    write_mode: str = \"errorifexists\",\n    app_name: str = \"gentropy\",\n    hail_home: str | None = None,\n    start_hail: bool = False,\n    extended_spark_conf: dict[str, str] | None = None,\n    output_partitions: int = 200,\n    use_enhanced_bgzip_codec: bool = False,\n) -&gt; None:\n    \"\"\"Initialises spark session and logger.\n\n    Args:\n        spark_uri (str): Spark URI. Defaults to \"local[*]\".\n        write_mode (str): Spark write mode. Defaults to \"errorifexists\".\n        app_name (str): Spark application name. Defaults to \"gentropy\".\n        hail_home (str | None): Path to Hail installation. Defaults to None.\n        start_hail (bool): Whether to start Hail. Defaults to False.\n        extended_spark_conf (dict[str, str] | None): Extended Spark configuration. Defaults to None.\n        output_partitions (int): Number of partitions for output datasets. Defaults to 200.\n        use_enhanced_bgzip_codec (bool): Whether to use the BGZFEnhancedGzipCodec for reading block gzipped files. Defaults to False.\n    \"\"\"\n    merged_conf = self._create_merged_config(\n        start_hail,\n        use_enhanced_bgzip_codec,\n        extended_spark_conf,\n        hail_home,\n    )\n\n    self.spark = (\n        SparkSession.Builder()\n        .config(conf=merged_conf)\n        .master(spark_uri)\n        .appName(app_name)\n        .getOrCreate()\n    )\n    self.logger = Log4j(self.spark)\n\n    self.write_mode = write_mode\n\n    self.hail_home = hail_home\n    self.start_hail = start_hail\n    self.use_enhanced_bgzip_codec = use_enhanced_bgzip_codec\n    if start_hail:\n        hl.init(sc=self.spark.sparkContext, log=\"/dev/null\")\n    self.output_partitions = output_partitions\n</code></pre>"},{"location":"python_api/common/session/#gentropy.common.session.Session.load_data","title":"<code>load_data(path: str | list[str], format: str = 'parquet', schema: StructType | str | None = None, **kwargs: bool | float | int | str | None) -&gt; DataFrame</code>","text":"<p>Generic function to read a file or folder into a Spark dataframe.</p> <p>The <code>recursiveFileLookup</code> flag when set to True will skip all partition columns, but read files from all subdirectories.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | list[str]</code> <p>path to the dataset</p> required <code>format</code> <code>str</code> <p>file format. Defaults to parquet.</p> <code>'parquet'</code> <code>schema</code> <code>StructType | str | None</code> <p>Schema to use when reading the data.</p> <code>None</code> <code>**kwargs</code> <code>bool | float | int | str | None</code> <p>Additional arguments to pass to spark.read.load. <code>mergeSchema</code> is set to True, <code>recursiveFileLookup</code> is set to False by default.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Dataframe</p> Source code in <code>src/gentropy/common/session.py</code> <pre><code>def load_data(\n    self: Session,\n    path: str | list[str],\n    format: str = \"parquet\",\n    schema: StructType | str | None = None,\n    **kwargs: bool | float | int | str | None,\n) -&gt; DataFrame:\n    \"\"\"Generic function to read a file or folder into a Spark dataframe.\n\n    The `recursiveFileLookup` flag when set to True will skip all partition columns, but read files from all subdirectories.\n\n    Args:\n        path (str | list[str]): path to the dataset\n        format (str): file format. Defaults to parquet.\n        schema (StructType | str | None): Schema to use when reading the data.\n        **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.load. `mergeSchema` is set to True, `recursiveFileLookup` is set to False by default.\n\n    Returns:\n        DataFrame: Dataframe\n    \"\"\"\n    # Set default kwargs\n    if schema is None:\n        kwargs[\"inferSchema\"] = kwargs.get(\"inferSchema\", True)\n    kwargs[\"mergeSchema\"] = kwargs.get(\"mergeSchema\", True)\n    kwargs[\"recursiveFileLookup\"] = kwargs.get(\"recursiveFileLookup\", False)\n    return self.spark.read.load(path, format=format, schema=schema, **kwargs)\n</code></pre>"},{"location":"python_api/common/session/#gentropy.common.session.Log4j","title":"<code>gentropy.common.session.Log4j</code>","text":"<p>Log4j logger class.</p> Source code in <code>src/gentropy/common/session.py</code> <pre><code>class Log4j:\n    \"\"\"Log4j logger class.\"\"\"\n\n    def __init__(self, spark: SparkSession) -&gt; None:\n        \"\"\"Log4j logger class. This class provides a wrapper around the Log4j logging system.\n\n        Args:\n            spark (SparkSession): The Spark session used to access Spark context and Log4j logging.\n        \"\"\"\n        log4j: Any = spark.sparkContext._jvm.org.apache.log4j  # pyright: ignore[reportAttributeAccessIssue, reportOptionalMemberAccess]\n        # Cast to our protocol type for type safety\n        self.logger: JavaLogger = log4j.LogManager.getLogger(__name__)\n\n    def error(self, message: str) -&gt; None:\n        \"\"\"Log an error.\n\n        Args:\n            message (str): Error message to write to log\n        \"\"\"\n        self.logger.error(message)\n\n    def warning(self, message: str) -&gt; None:\n        \"\"\"Log a warning.\n\n        Args:\n            message (str): Warning message to write to log\n        \"\"\"\n        self.logger.warn(message)  # noqa: G010\n\n    def info(self, message: str) -&gt; None:\n        \"\"\"Log information.\n\n        Args:\n            message (str): Information message to write to log\n        \"\"\"\n        self.logger.info(message)\n</code></pre>"},{"location":"python_api/common/session/#gentropy.common.session.Log4j.__init__","title":"<code>__init__(spark: SparkSession) -&gt; None</code>","text":"<p>Log4j logger class. This class provides a wrapper around the Log4j logging system.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark session used to access Spark context and Log4j logging.</p> required Source code in <code>src/gentropy/common/session.py</code> <pre><code>def __init__(self, spark: SparkSession) -&gt; None:\n    \"\"\"Log4j logger class. This class provides a wrapper around the Log4j logging system.\n\n    Args:\n        spark (SparkSession): The Spark session used to access Spark context and Log4j logging.\n    \"\"\"\n    log4j: Any = spark.sparkContext._jvm.org.apache.log4j  # pyright: ignore[reportAttributeAccessIssue, reportOptionalMemberAccess]\n    # Cast to our protocol type for type safety\n    self.logger: JavaLogger = log4j.LogManager.getLogger(__name__)\n</code></pre>"},{"location":"python_api/common/session/#gentropy.common.session.Log4j.error","title":"<code>error(message: str) -&gt; None</code>","text":"<p>Log an error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message to write to log</p> required Source code in <code>src/gentropy/common/session.py</code> <pre><code>def error(self, message: str) -&gt; None:\n    \"\"\"Log an error.\n\n    Args:\n        message (str): Error message to write to log\n    \"\"\"\n    self.logger.error(message)\n</code></pre>"},{"location":"python_api/common/session/#gentropy.common.session.Log4j.info","title":"<code>info(message: str) -&gt; None</code>","text":"<p>Log information.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Information message to write to log</p> required Source code in <code>src/gentropy/common/session.py</code> <pre><code>def info(self, message: str) -&gt; None:\n    \"\"\"Log information.\n\n    Args:\n        message (str): Information message to write to log\n    \"\"\"\n    self.logger.info(message)\n</code></pre>"},{"location":"python_api/common/session/#gentropy.common.session.Log4j.warning","title":"<code>warning(message: str) -&gt; None</code>","text":"<p>Log a warning.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message to write to log</p> required Source code in <code>src/gentropy/common/session.py</code> <pre><code>def warning(self, message: str) -&gt; None:\n    \"\"\"Log a warning.\n\n    Args:\n        message (str): Warning message to write to log\n    \"\"\"\n    self.logger.warn(message)  # noqa: G010\n</code></pre>"},{"location":"python_api/common/spark/","title":"spark","text":""},{"location":"python_api/common/spark/#operations-on-pyspark-columns","title":"Operations on PySpark columns","text":""},{"location":"python_api/common/spark/#gentropy.common.spark.nullify_empty_array","title":"<code>gentropy.common.spark.nullify_empty_array(column: Column) -&gt; Column</code>","text":"<p>Returns null when a Spark Column has an array of size 0, otherwise return the array.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>The Spark Column to be processed.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Nullified column when the array is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([[], [1, 2, 3]], \"array&lt;int&gt;\")\n&gt;&gt;&gt; df.withColumn(\"new\", nullify_empty_array(df.value)).show()\n+---------+---------+\n|    value|      new|\n+---------+---------+\n|       []|     NULL|\n|[1, 2, 3]|[1, 2, 3]|\n+---------+---------+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def nullify_empty_array(column: Column) -&gt; Column:\n    \"\"\"Returns null when a Spark Column has an array of size 0, otherwise return the array.\n\n    Args:\n        column (Column): The Spark Column to be processed.\n\n    Returns:\n        Column: Nullified column when the array is empty.\n\n    Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([[], [1, 2, 3]], \"array&lt;int&gt;\")\n        &gt;&gt;&gt; df.withColumn(\"new\", nullify_empty_array(df.value)).show()\n        +---------+---------+\n        |    value|      new|\n        +---------+---------+\n        |       []|     NULL|\n        |[1, 2, 3]|[1, 2, 3]|\n        +---------+---------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.when(f.size(column) != 0, column)\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.get_top_ranked_in_window","title":"<code>gentropy.common.spark.get_top_ranked_in_window(df: DataFrame, w: WindowSpec) -&gt; DataFrame</code>","text":"<p>Returns the record with the top rank within each group of the window.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be processed.</p> required <code>w</code> <code>WindowSpec</code> <p>The window to be used for ranking.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame with the record with the top rank within each group of the window.</p> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def get_top_ranked_in_window(df: DataFrame, w: WindowSpec) -&gt; DataFrame:\n    \"\"\"Returns the record with the top rank within each group of the window.\n\n    Args:\n        df (DataFrame): The DataFrame to be processed.\n        w (WindowSpec): The window to be used for ranking.\n\n    Returns:\n        DataFrame: The DataFrame with the record with the top rank within each group of the window.\n    \"\"\"\n    return (\n        df.withColumn(\"row_number\", f.row_number().over(w))\n        .filter(f.col(\"row_number\") == 1)\n        .drop(\"row_number\")\n    )\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.get_record_with_minimum_value","title":"<code>gentropy.common.spark.get_record_with_minimum_value(df: DataFrame, grouping_col: Column | str | list[Column | str], sorting_col: str) -&gt; DataFrame</code>","text":"<p>Returns the record with the minimum value of the sorting column within each group of the grouping column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be processed.</p> required <code>grouping_col</code> <code>Column | str | list[Column | str]</code> <p>The column(s) to group the DataFrame by.</p> required <code>sorting_col</code> <code>str</code> <p>The column name to sort the DataFrame by.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame with the record with the minimum value of the sorting column within each group of the grouping column.</p> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def get_record_with_minimum_value(\n    df: DataFrame,\n    grouping_col: Column | str | list[Column | str],\n    sorting_col: str,\n) -&gt; DataFrame:\n    \"\"\"Returns the record with the minimum value of the sorting column within each group of the grouping column.\n\n    Args:\n        df (DataFrame): The DataFrame to be processed.\n        grouping_col (Column | str | list[Column | str]): The column(s) to group the DataFrame by.\n        sorting_col (str): The column name to sort the DataFrame by.\n\n    Returns:\n        DataFrame: The DataFrame with the record with the minimum value of the sorting column within each group of the grouping column.\n    \"\"\"\n    w = Window.partitionBy(grouping_col).orderBy(sorting_col)\n    return get_top_ranked_in_window(df, w)\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.get_record_with_maximum_value","title":"<code>gentropy.common.spark.get_record_with_maximum_value(df: DataFrame, grouping_col: str | list[str], sorting_col: str) -&gt; DataFrame</code>","text":"<p>Returns the record with the maximum value of the sorting column within each group of the grouping column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be processed.</p> required <code>grouping_col</code> <code>str | list[str]</code> <p>The column(s) to group the DataFrame by.</p> required <code>sorting_col</code> <code>str</code> <p>The column name to sort the DataFrame by.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame with the record with the maximum value of the sorting column within each group of the grouping column.</p> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def get_record_with_maximum_value(\n    df: DataFrame,\n    grouping_col: str | list[str],\n    sorting_col: str,\n) -&gt; DataFrame:\n    \"\"\"Returns the record with the maximum value of the sorting column within each group of the grouping column.\n\n    Args:\n        df (DataFrame): The DataFrame to be processed.\n        grouping_col (str | list[str]): The column(s) to group the DataFrame by.\n        sorting_col (str): The column name to sort the DataFrame by.\n\n    Returns:\n        DataFrame: The DataFrame with the record with the maximum value of the sorting column within each group of the grouping column.\n    \"\"\"\n    w = Window.partitionBy(grouping_col).orderBy(f.col(sorting_col).desc())\n    return get_top_ranked_in_window(df, w)\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.normalise_column","title":"<code>gentropy.common.spark.normalise_column(df: DataFrame, input_col_name: str, output_col_name: str) -&gt; DataFrame</code>","text":"<p>Normalises a numerical column to a value between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be processed.</p> required <code>input_col_name</code> <code>str</code> <p>The name of the column to be normalised.</p> required <code>output_col_name</code> <code>str</code> <p>The name of the column to store the normalised values.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame with the normalised column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([5, 50, 1000], \"int\")\n&gt;&gt;&gt; df.transform(lambda df: normalise_column(df, \"value\", \"norm_value\")).show()\n+-----+----------+\n|value|norm_value|\n+-----+----------+\n|    5|       0.0|\n|   50|      0.05|\n| 1000|       1.0|\n+-----+----------+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def normalise_column(\n    df: DataFrame, input_col_name: str, output_col_name: str\n) -&gt; DataFrame:\n    \"\"\"Normalises a numerical column to a value between 0 and 1.\n\n    Args:\n        df (DataFrame): The DataFrame to be processed.\n        input_col_name (str): The name of the column to be normalised.\n        output_col_name (str): The name of the column to store the normalised values.\n\n    Returns:\n        DataFrame: The DataFrame with the normalised column.\n\n    Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([5, 50, 1000], \"int\")\n        &gt;&gt;&gt; df.transform(lambda df: normalise_column(df, \"value\", \"norm_value\")).show()\n        +-----+----------+\n        |value|norm_value|\n        +-----+----------+\n        |    5|       0.0|\n        |   50|      0.05|\n        | 1000|       1.0|\n        +-----+----------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    vec_assembler = VectorAssembler(\n        inputCols=[input_col_name], outputCol=\"feature_vector\"\n    )\n    scaler = MinMaxScaler(inputCol=\"feature_vector\", outputCol=\"norm_vector\")\n    unvector_score = f.round(vector_to_array(f.col(\"norm_vector\"))[0], 2).alias(\n        output_col_name\n    )\n    pipeline = Pipeline(stages=[vec_assembler, scaler])\n    return (\n        pipeline.fit(df)\n        .transform(df)\n        .select(\"*\", unvector_score)\n        .drop(\"feature_vector\", \"norm_vector\")\n    )\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.string2camelcase","title":"<code>gentropy.common.spark.string2camelcase(col_name: str) -&gt; str</code>","text":"<p>Converting a string to camelcase.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>a random string</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Camel cased string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; string2camelcase(\"hello_world\")\n'helloWorld'\n&gt;&gt;&gt; string2camelcase(\"hello world\")\n'helloWorld'\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def string2camelcase(col_name: str) -&gt; str:\n    \"\"\"Converting a string to camelcase.\n\n    Args:\n        col_name (str): a random string\n\n    Returns:\n        str: Camel cased string\n\n    Examples:\n        &gt;&gt;&gt; string2camelcase(\"hello_world\")\n        'helloWorld'\n        &gt;&gt;&gt; string2camelcase(\"hello world\")\n        'helloWorld'\n    \"\"\"\n    # Removing a bunch of unwanted characters from the column names:\n    col_name_normalised = re.sub(r\"[\\/\\(\\)\\-]+\", \" \", col_name)\n\n    first, *rest = re.split(\"[ _-]\", col_name_normalised)\n    return \"\".join([first.lower(), *map(str.capitalize, rest)])\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.column2camel_case","title":"<code>gentropy.common.spark.column2camel_case(col_name: str) -&gt; str</code>","text":"<p>A helper function to convert column names to camel cases.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>a single column name</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>spark expression to select and rename the column</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; column2camel_case(\"hello_world\")\n'`hello_world` as helloWorld'\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def column2camel_case(col_name: str) -&gt; str:\n    \"\"\"A helper function to convert column names to camel cases.\n\n    Args:\n        col_name (str): a single column name\n\n    Returns:\n        str: spark expression to select and rename the column\n\n    Examples:\n        &gt;&gt;&gt; column2camel_case(\"hello_world\")\n        '`hello_world` as helloWorld'\n    \"\"\"\n    return f\"`{col_name}` as {string2camelcase(col_name)}\"\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.get_value_from_row","title":"<code>gentropy.common.spark.get_value_from_row(row: Row, column: str) -&gt; Any</code>","text":"<p>Extract index value from a row if exists.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>One row from a dataframe</p> required <code>column</code> <code>str</code> <p>column label we want to extract.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>value of the column in the row</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the column is not in the row</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_value_from_row(Row(geneName=\"AR\", chromosome=\"X\"), \"chromosome\")\n'X'\n&gt;&gt;&gt; get_value_from_row(Row(geneName=\"AR\", chromosome=\"X\"), \"disease\")\nTraceback (most recent call last):\n...\nValueError: Column disease not found in row Row(geneName='AR', chromosome='X')\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def get_value_from_row(row: Row, column: str) -&gt; Any:\n    \"\"\"Extract index value from a row if exists.\n\n    Args:\n        row (Row): One row from a dataframe\n        column (str): column label we want to extract.\n\n    Returns:\n        Any: value of the column in the row\n\n    Raises:\n        ValueError: if the column is not in the row\n\n    Examples:\n        &gt;&gt;&gt; get_value_from_row(Row(geneName=\"AR\", chromosome=\"X\"), \"chromosome\")\n        'X'\n        &gt;&gt;&gt; get_value_from_row(Row(geneName=\"AR\", chromosome=\"X\"), \"disease\")\n        Traceback (most recent call last):\n        ...\n        ValueError: Column disease not found in row Row(geneName='AR', chromosome='X')\n    \"\"\"\n    if column not in row:\n        raise ValueError(f\"Column {column} not found in row {row}\")\n    return row[column]\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.safe_array_union","title":"<code>gentropy.common.spark.safe_array_union(a: Column, b: Column, fields_order: list[str] | None = None) -&gt; Column</code>","text":"<p>Merge the content of two optional columns.</p> <p>The function assumes the array columns have the same schema. If the <code>fields_order</code> is passed, the function assumes that it deals with array of structs and sorts the nested struct fields by the provided <code>fields_order</code> before conducting array_merge. If the <code>fields_order</code> is not passed and both columns are &lt;array&lt;struct&lt;...&gt;&gt; type then function assumes struct fields have the same order, otherwise the function will raise an AnalysisException.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Column</code> <p>One optional array column.</p> required <code>b</code> <code>Column</code> <p>The other optional array column.</p> required <code>fields_order</code> <code>list[str] | None</code> <p>The order of the fields in the struct. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>array column with merged content.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(['a'], ['b']), (['c'], None), (None, ['d']), (None, None)]\n&gt;&gt;&gt; (\n...    spark.createDataFrame(data, ['col1', 'col2'])\n...    .select(\n...        safe_array_union(f.col('col1'), f.col('col2')).alias('merged')\n...    )\n...    .show()\n... )\n+------+\n|merged|\n+------+\n|[a, b]|\n|   [c]|\n|   [d]|\n|  NULL|\n+------+\n\n&gt;&gt;&gt; schema=\"arr2: array&lt;struct&lt;b:int,a:string&gt;&gt;, arr: array&lt;struct&lt;a:string,b:int&gt;&gt;\"\n&gt;&gt;&gt; data = [([(1,\"a\",), (2, \"c\")],[(\"a\", 1,)]),]\n&gt;&gt;&gt; df = spark.createDataFrame(data=data, schema=schema)\n&gt;&gt;&gt; df.select(safe_array_union(f.col(\"arr\"), f.col(\"arr2\"), fields_order=[\"a\", \"b\"]).alias(\"merged\")).show()\n+----------------+\n|          merged|\n+----------------+\n|[{a, 1}, {c, 2}]|\n+----------------+\n\n&gt;&gt;&gt; schema=\"arr2: array&lt;struct&lt;b:int,a:string&gt;&gt;, arr: array&lt;struct&lt;a:string,b:int&gt;&gt;\"\n&gt;&gt;&gt; data = [([(1,\"a\",), (2, \"c\")],[(\"a\", 1,)]),]\n&gt;&gt;&gt; df = spark.createDataFrame(data=data, schema=schema)\n&gt;&gt;&gt; df.select(safe_array_union(f.col(\"arr\"), f.col(\"arr2\")).alias(\"merged\")).show()\nTraceback (most recent call last):\npyspark.sql.utils.AnalysisException: ...\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def safe_array_union(\n    a: Column, b: Column, fields_order: list[str] | None = None\n) -&gt; Column:\n    \"\"\"Merge the content of two optional columns.\n\n    The function assumes the array columns have the same schema.\n    If the `fields_order` is passed, the function assumes that it deals with array of structs and sorts the nested\n    struct fields by the provided `fields_order` before conducting array_merge.\n    If the `fields_order` is not passed and both columns are &lt;array&lt;struct&lt;...&gt;&gt; type then function assumes struct fields have the same order,\n    otherwise the function will raise an AnalysisException.\n\n    Args:\n        a (Column): One optional array column.\n        b (Column): The other optional array column.\n        fields_order (list[str] | None): The order of the fields in the struct. Defaults to None.\n\n    Returns:\n        Column: array column with merged content.\n\n    Examples:\n        &gt;&gt;&gt; data = [(['a'], ['b']), (['c'], None), (None, ['d']), (None, None)]\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame(data, ['col1', 'col2'])\n        ...    .select(\n        ...        safe_array_union(f.col('col1'), f.col('col2')).alias('merged')\n        ...    )\n        ...    .show()\n        ... )\n        +------+\n        |merged|\n        +------+\n        |[a, b]|\n        |   [c]|\n        |   [d]|\n        |  NULL|\n        +------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; schema=\"arr2: array&lt;struct&lt;b:int,a:string&gt;&gt;, arr: array&lt;struct&lt;a:string,b:int&gt;&gt;\"\n        &gt;&gt;&gt; data = [([(1,\"a\",), (2, \"c\")],[(\"a\", 1,)]),]\n        &gt;&gt;&gt; df = spark.createDataFrame(data=data, schema=schema)\n        &gt;&gt;&gt; df.select(safe_array_union(f.col(\"arr\"), f.col(\"arr2\"), fields_order=[\"a\", \"b\"]).alias(\"merged\")).show()\n        +----------------+\n        |          merged|\n        +----------------+\n        |[{a, 1}, {c, 2}]|\n        +----------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; schema=\"arr2: array&lt;struct&lt;b:int,a:string&gt;&gt;, arr: array&lt;struct&lt;a:string,b:int&gt;&gt;\"\n        &gt;&gt;&gt; data = [([(1,\"a\",), (2, \"c\")],[(\"a\", 1,)]),]\n        &gt;&gt;&gt; df = spark.createDataFrame(data=data, schema=schema)\n        &gt;&gt;&gt; df.select(safe_array_union(f.col(\"arr\"), f.col(\"arr2\")).alias(\"merged\")).show() # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n        pyspark.sql.utils.AnalysisException: ...\n    \"\"\"\n    if fields_order:\n        # sort the nested struct fields by the provided order\n        a = sort_array_struct_by_columns(a, fields_order)\n        b = sort_array_struct_by_columns(b, fields_order)\n    return f.when(a.isNotNull() &amp; b.isNotNull(), f.array_union(a, b)).otherwise(\n        f.coalesce(a, b)\n    )\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.sort_array_struct_by_columns","title":"<code>gentropy.common.spark.sort_array_struct_by_columns(column: Column, fields_order: list[str]) -&gt; Column</code>","text":"<p>Sort nested struct fields by provided fields order.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>Column with array of structs.</p> required <code>fields_order</code> <code>list[str]</code> <p>List of field names to sort by.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Sorted column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; schema=\"arr: array&lt;struct&lt;b:int,a:string&gt;&gt;\"\n&gt;&gt;&gt; data = [([(1,\"a\",), (2, \"c\")],)]\n&gt;&gt;&gt; fields_order = [\"a\", \"b\"]\n&gt;&gt;&gt; df = spark.createDataFrame(data=data, schema=schema)\n&gt;&gt;&gt; df.select(sort_array_struct_by_columns(f.col(\"arr\"), fields_order).alias(\"sorted\")).show()\n+----------------+\n|          sorted|\n+----------------+\n|[{c, 2}, {a, 1}]|\n+----------------+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def sort_array_struct_by_columns(column: Column, fields_order: list[str]) -&gt; Column:\n    \"\"\"Sort nested struct fields by provided fields order.\n\n    Args:\n        column (Column): Column with array of structs.\n        fields_order (list[str]): List of field names to sort by.\n\n    Returns:\n        Column: Sorted column.\n\n    Examples:\n        &gt;&gt;&gt; schema=\"arr: array&lt;struct&lt;b:int,a:string&gt;&gt;\"\n        &gt;&gt;&gt; data = [([(1,\"a\",), (2, \"c\")],)]\n        &gt;&gt;&gt; fields_order = [\"a\", \"b\"]\n        &gt;&gt;&gt; df = spark.createDataFrame(data=data, schema=schema)\n        &gt;&gt;&gt; df.select(sort_array_struct_by_columns(f.col(\"arr\"), fields_order).alias(\"sorted\")).show()\n        +----------------+\n        |          sorted|\n        +----------------+\n        |[{c, 2}, {a, 1}]|\n        +----------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    column_name = extract_column_name(column)\n    fields_order_expr = \", \".join([f\"x.{field}\" for field in fields_order])\n    return f.expr(\n        f\"sort_array(transform({column_name}, x -&gt; struct({fields_order_expr})), False)\"\n    ).alias(column_name)\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.extract_column_name","title":"<code>gentropy.common.spark.extract_column_name(column: Column) -&gt; str</code>","text":"<p>Extract column name from a column expression.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>Column expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Column name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the column name cannot be extracted.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; extract_column_name(f.col('col1'))\n'col1'\n&gt;&gt;&gt; extract_column_name(f.sort_array(f.col('col1')))\n'sort_array(col1, true)'\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def extract_column_name(column: Column) -&gt; str:\n    \"\"\"Extract column name from a column expression.\n\n    Args:\n        column (Column): Column expression.\n\n    Returns:\n        str: Column name.\n\n    Raises:\n        ValueError: If the column name cannot be extracted.\n\n    Examples:\n        &gt;&gt;&gt; extract_column_name(f.col('col1'))\n        'col1'\n        &gt;&gt;&gt; extract_column_name(f.sort_array(f.col('col1')))\n        'sort_array(col1, true)'\n    \"\"\"\n    pattern = re.compile(\"^Column&lt;'(?P&lt;name&gt;.*)'&gt;?\")\n\n    _match = pattern.search(str(column))\n    if not _match:\n        raise ValueError(f\"Cannot extract column name from {column}\")\n    return _match.group(\"name\")\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.order_array_of_structs_by_field","title":"<code>gentropy.common.spark.order_array_of_structs_by_field(column_name: str, field_name: str) -&gt; Column</code>","text":"<p>Sort a column of array of structs by a field in descending order, nulls last.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>Column name</p> required <code>field_name</code> <code>str</code> <p>Field name</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Sorted column</p> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def order_array_of_structs_by_field(column_name: str, field_name: str) -&gt; Column:\n    \"\"\"Sort a column of array of structs by a field in descending order, nulls last.\n\n    Args:\n        column_name (str): Column name\n        field_name (str): Field name\n\n    Returns:\n        Column: Sorted column\n    \"\"\"\n    return f.expr(\n        f\"\"\"\n        array_sort(\n        {column_name},\n        (left, right) -&gt; case\n                        when left.{field_name} is null and right.{field_name} is null then 0\n                        when left.{field_name} is null then 1\n                        when right.{field_name} is null then -1\n                        when left.{field_name} &lt; right.{field_name} then 1\n                        when left.{field_name} &gt; right.{field_name} then -1\n                        else 0\n                end)\n        \"\"\"\n    )\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.order_array_of_structs_by_two_fields","title":"<code>gentropy.common.spark.order_array_of_structs_by_two_fields(array_name: str, descending_column: str, ascending_column: str) -&gt; Column</code>","text":"<p>Sort array of structs by a field in descending order and by an other field in an ascending order.</p> <p>This function doesn't deal with null values, assumes the sort columns are not nullable. The sorting function compares the descending_column first, in case when two values from descending_column are equal it compares the ascending_column. When values in both columns are equal, the rows order is preserved.</p> <p>Parameters:</p> Name Type Description Default <code>array_name</code> <code>str</code> <p>Column name with array of structs</p> required <code>descending_column</code> <code>str</code> <p>Name of the first keys sorted in descending order</p> required <code>ascending_column</code> <code>str</code> <p>Name of the second keys sorted in ascending order</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Sorted column</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(1.0, 45, 'First'), (0.5, 232, 'Third'), (0.5, 233, 'Fourth'), (1.0, 125, 'Second'),]\n&gt;&gt;&gt; (\n...    spark.createDataFrame(data, ['col1', 'col2', 'ranking'])\n...    .groupBy(f.lit('c'))\n...    .agg(f.collect_list(f.struct('col1','col2', 'ranking')).alias('list'))\n...    .select(order_array_of_structs_by_two_fields('list', 'col1', 'col2').alias('sorted_list'))\n...    .show(truncate=False)\n... )\n+-----------------------------------------------------------------------------+\n|sorted_list                                                                  |\n+-----------------------------------------------------------------------------+\n|[{1.0, 45, First}, {1.0, 125, Second}, {0.5, 232, Third}, {0.5, 233, Fourth}]|\n+-----------------------------------------------------------------------------+\n\n&gt;&gt;&gt; data = [(1.0, 45, 'First'), (1.0, 45, 'Second'), (0.5, 233, 'Fourth'), (1.0, 125, 'Third'),]\n&gt;&gt;&gt; (\n...    spark.createDataFrame(data, ['col1', 'col2', 'ranking'])\n...    .groupBy(f.lit('c'))\n...    .agg(f.collect_list(f.struct('col1','col2', 'ranking')).alias('list'))\n...    .select(order_array_of_structs_by_two_fields('list', 'col1', 'col2').alias('sorted_list'))\n...    .show(truncate=False)\n... )\n+----------------------------------------------------------------------------+\n|sorted_list                                                                 |\n+----------------------------------------------------------------------------+\n|[{1.0, 45, First}, {1.0, 45, Second}, {1.0, 125, Third}, {0.5, 233, Fourth}]|\n+----------------------------------------------------------------------------+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def order_array_of_structs_by_two_fields(\n    array_name: str, descending_column: str, ascending_column: str\n) -&gt; Column:\n    \"\"\"Sort array of structs by a field in descending order and by an other field in an ascending order.\n\n    This function doesn't deal with null values, assumes the sort columns are not nullable.\n    The sorting function compares the descending_column first, in case when two values from descending_column are equal\n    it compares the ascending_column. When values in both columns are equal, the rows order is preserved.\n\n    Args:\n        array_name (str): Column name with array of structs\n        descending_column (str): Name of the first keys sorted in descending order\n        ascending_column (str): Name of the second keys sorted in ascending order\n\n    Returns:\n        Column: Sorted column\n\n    Examples:\n        &gt;&gt;&gt; data = [(1.0, 45, 'First'), (0.5, 232, 'Third'), (0.5, 233, 'Fourth'), (1.0, 125, 'Second'),]\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame(data, ['col1', 'col2', 'ranking'])\n        ...    .groupBy(f.lit('c'))\n        ...    .agg(f.collect_list(f.struct('col1','col2', 'ranking')).alias('list'))\n        ...    .select(order_array_of_structs_by_two_fields('list', 'col1', 'col2').alias('sorted_list'))\n        ...    .show(truncate=False)\n        ... )\n        +-----------------------------------------------------------------------------+\n        |sorted_list                                                                  |\n        +-----------------------------------------------------------------------------+\n        |[{1.0, 45, First}, {1.0, 125, Second}, {0.5, 232, Third}, {0.5, 233, Fourth}]|\n        +-----------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; data = [(1.0, 45, 'First'), (1.0, 45, 'Second'), (0.5, 233, 'Fourth'), (1.0, 125, 'Third'),]\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame(data, ['col1', 'col2', 'ranking'])\n        ...    .groupBy(f.lit('c'))\n        ...    .agg(f.collect_list(f.struct('col1','col2', 'ranking')).alias('list'))\n        ...    .select(order_array_of_structs_by_two_fields('list', 'col1', 'col2').alias('sorted_list'))\n        ...    .show(truncate=False)\n        ... )\n        +----------------------------------------------------------------------------+\n        |sorted_list                                                                 |\n        +----------------------------------------------------------------------------+\n        |[{1.0, 45, First}, {1.0, 45, Second}, {1.0, 125, Third}, {0.5, 233, Fourth}]|\n        +----------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.expr(\n        f\"\"\"\n        array_sort(\n        {array_name},\n        (left, right) -&gt; case\n                when left.{descending_column} is null and right.{descending_column} is null then 0\n                when left.{ascending_column} is null and right.{ascending_column} is null then 0\n\n                when left.{descending_column} is null then 1\n                when right.{descending_column} is null then -1\n\n                when left.{ascending_column} is null then 1\n                when right.{ascending_column} is null then -1\n\n                when left.{descending_column} &lt; right.{descending_column} then 1\n                when left.{descending_column} &gt; right.{descending_column} then -1\n                when left.{descending_column} == right.{descending_column} and left.{ascending_column} &gt; right.{ascending_column} then 1\n                when left.{descending_column} == right.{descending_column} and left.{ascending_column} &lt; right.{ascending_column} then -1\n                when left.{ascending_column} == right.{ascending_column} and left.{descending_column} == right.{descending_column} then 0\n        end)\n        \"\"\"\n    )\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.map_column_by_dictionary","title":"<code>gentropy.common.spark.map_column_by_dictionary(col: Column, mapping_dict: dict[str, Any]) -&gt; Column</code>","text":"<p>Map column values to dictionary values by key.</p> <p>Missing consequence label will be converted to None, unmapped consequences will be mapped as None.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>Column containing labels to map.</p> required <code>mapping_dict</code> <code>dict[str, Any]</code> <p>Dictionary with mapping key/value pairs.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Column with mapped values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [('consequence_1',),('unmapped_consequence',),(None,)]\n&gt;&gt;&gt; m = {'consequence_1': 'SO:000000'}\n&gt;&gt;&gt; (\n...    spark.createDataFrame(data, ['label'])\n...    .select('label',map_column_by_dictionary(f.col('label'),m).alias('id'))\n...    .show()\n... )\n+--------------------+---------+\n|               label|       id|\n+--------------------+---------+\n|       consequence_1|SO:000000|\n|unmapped_consequence|     NULL|\n|                NULL|     NULL|\n+--------------------+---------+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def map_column_by_dictionary(col: Column, mapping_dict: dict[str, Any]) -&gt; Column:\n    \"\"\"Map column values to dictionary values by key.\n\n    Missing consequence label will be converted to None, unmapped consequences will be mapped as None.\n\n    Args:\n        col (Column): Column containing labels to map.\n        mapping_dict (dict[str, Any]): Dictionary with mapping key/value pairs.\n\n    Returns:\n        Column: Column with mapped values.\n\n    Examples:\n        &gt;&gt;&gt; data = [('consequence_1',),('unmapped_consequence',),(None,)]\n        &gt;&gt;&gt; m = {'consequence_1': 'SO:000000'}\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame(data, ['label'])\n        ...    .select('label',map_column_by_dictionary(f.col('label'),m).alias('id'))\n        ...    .show()\n        ... )\n        +--------------------+---------+\n        |               label|       id|\n        +--------------------+---------+\n        |       consequence_1|SO:000000|\n        |unmapped_consequence|     NULL|\n        |                NULL|     NULL|\n        +--------------------+---------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    map_expr = f.create_map(*[f.lit(x) for x in chain(*mapping_dict.items())])\n\n    return map_expr[col]\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.create_empty_column_if_not_exists","title":"<code>gentropy.common.spark.create_empty_column_if_not_exists(col_name: str, col_schema: t.DataType = t.NullType()) -&gt; Column</code>","text":"<p>Create a column if it does not exist in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to be created.</p> required <code>col_schema</code> <code>DataType</code> <p>The schema of the column to be created. Defaults to NullType.</p> <code>NullType()</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>The expression to create the column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(1, 2),], ['col1', 'col2'])\n&gt;&gt;&gt; df.select(\"*\", create_empty_column_if_not_exists('col3', t.IntegerType())).show()\n+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|   1|   2|NULL|\n+----+----+----+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def create_empty_column_if_not_exists(\n    col_name: str, col_schema: t.DataType = t.NullType()\n) -&gt; Column:\n    \"\"\"Create a column if it does not exist in the DataFrame.\n\n    Args:\n        col_name (str): The name of the column to be created.\n        col_schema (t.DataType): The schema of the column to be created. Defaults to NullType.\n\n    Returns:\n        Column: The expression to create the column.\n\n    Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, 2),], ['col1', 'col2'])\n        &gt;&gt;&gt; df.select(\"*\", create_empty_column_if_not_exists('col3', t.IntegerType())).show()\n        +----+----+----+\n        |col1|col2|col3|\n        +----+----+----+\n        |   1|   2|NULL|\n        +----+----+----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.lit(None).cast(col_schema).alias(col_name)\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.calculate_harmonic_sum","title":"<code>gentropy.common.spark.calculate_harmonic_sum(input_array: Column) -&gt; Column</code>","text":"<p>Calculate the harmonic sum of an array.</p> <p>Parameters:</p> Name Type Description Default <code>input_array</code> <code>Column</code> <p>input array of doubles</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>column of harmonic sums</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import Row\n&gt;&gt;&gt; df = spark.createDataFrame([\n...     Row([0.3, 0.8, 1.0]),\n...     Row([0.7, 0.2, 0.9]),\n...     ], [\"input_array\"]\n... )\n&gt;&gt;&gt; df.select(\"*\", f.round(calculate_harmonic_sum(f.col(\"input_array\")), 2).alias(\"harmonic_sum\")).show()\n+---------------+------------+\n|    input_array|harmonic_sum|\n+---------------+------------+\n|[0.3, 0.8, 1.0]|        0.75|\n|[0.7, 0.2, 0.9]|        0.67|\n+---------------+------------+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def calculate_harmonic_sum(input_array: Column) -&gt; Column:\n    \"\"\"Calculate the harmonic sum of an array.\n\n    Args:\n        input_array (Column): input array of doubles\n\n    Returns:\n        Column: column of harmonic sums\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; df = spark.createDataFrame([\n        ...     Row([0.3, 0.8, 1.0]),\n        ...     Row([0.7, 0.2, 0.9]),\n        ...     ], [\"input_array\"]\n        ... )\n        &gt;&gt;&gt; df.select(\"*\", f.round(calculate_harmonic_sum(f.col(\"input_array\")), 2).alias(\"harmonic_sum\")).show()\n        +---------------+------------+\n        |    input_array|harmonic_sum|\n        +---------------+------------+\n        |[0.3, 0.8, 1.0]|        0.75|\n        |[0.7, 0.2, 0.9]|        0.67|\n        +---------------+------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.aggregate(\n        f.arrays_zip(\n            f.sort_array(input_array, False).alias(\"score\"),\n            f.sequence(f.lit(1), f.size(input_array)).alias(\"pos\"),\n        ),\n        f.lit(0.0),\n        lambda acc, x: acc\n        + x[\"score\"]\n        / f.pow(x[\"pos\"], 2)\n        / f.lit(sum(1 / ((i + 1) ** 2) for i in range(1000))),\n    )\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.clean_strings_from_symbols","title":"<code>gentropy.common.spark.clean_strings_from_symbols(source: Column) -&gt; Column</code>","text":"<p>To make strings URL-safe and consistent by lower-casing and replace special characters with underscores.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Column</code> <p>Source string</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Cleaned string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = [(\"AbCd-12.2\",),(\"AaBb..123?\",),(\"cDd!@#$%^&amp;*()\",),]\n&gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"source\")\n&gt;&gt;&gt; df.withColumn(\"cleaned\", clean_strings_from_symbols(f.col(\"source\"))).show(truncate=False)\n+-------------+---------+\n|source       |cleaned  |\n+-------------+---------+\n|AbCd-12.2    |abcd-12_2|\n|AaBb..123?   |aabb_123_|\n|cDd!@#$%^&amp;*()|cdd_     |\n+-------------+---------+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def clean_strings_from_symbols(source: Column) -&gt; Column:\n    \"\"\"To make strings URL-safe and consistent by lower-casing and replace special characters with underscores.\n\n    Args:\n        source (Column): Source string\n\n    Returns:\n        Column: Cleaned string\n\n    Examples:\n        &gt;&gt;&gt; d = [(\"AbCd-12.2\",),(\"AaBb..123?\",),(\"cDd!@#$%^&amp;*()\",),]\n        &gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"source\")\n        &gt;&gt;&gt; df.withColumn(\"cleaned\", clean_strings_from_symbols(f.col(\"source\"))).show(truncate=False)\n        +-------------+---------+\n        |source       |cleaned  |\n        +-------------+---------+\n        |AbCd-12.2    |abcd-12_2|\n        |AaBb..123?   |aabb_123_|\n        |cDd!@#$%^&amp;*()|cdd_     |\n        +-------------+---------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    characters_to_replace = r\"[^a-z0-9-_]+\"\n    return f.regexp_replace(f.lower(source), characters_to_replace, \"_\")\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.filter_array_struct","title":"<code>gentropy.common.spark.filter_array_struct(array_struct: Column | str, key_column: Column | str, key: Column | str | int | bool | float, value_column: Column | str) -&gt; Column</code>","text":"<p>Extract a value from an array of structs based on a key.</p> <p>This function searches for the predicate <code>key_column</code> that matches the <code>key</code> within the <code>array_struct</code> and returns the corresponding <code>value_column</code> from the struct with the same index as predicate.</p> Warning <p>Only the first match will be returned. If there are multiple matches, one need to sort the array first.</p> Warning <p>The function will not work if the <code>key_column</code> or <code>value_column</code> are not present in the <code>array_struct</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>array_struct</code> <code>Column | str</code> <p>The array of structs to be searched.</p> required <code>key_column</code> <code>Column | str</code> <p>The column name to be used as a key.</p> required <code>key</code> <code>Column | str | int | bool | float</code> <p>The key to be searched for.</p> required <code>value_column</code> <code>Column | str</code> <p>The column name to be returned.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>The value_column from the struct from the same array element as the matched key_column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [([{\"a\": 1, \"b\": 2.0, \"c\": \"c\", \"d\": True}, {\"a\": 3, \"b\": 4.0, \"c\": \"c\", \"d\": False}], \"c\")]\n&gt;&gt;&gt; schema = 'col array&lt;struct&lt;a:int,b:float,c:string, d:boolean&gt;&gt;, col2 string'\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+---------------------------------------+----+\n|col                                    |col2|\n+---------------------------------------+----+\n|[{1, 2.0, c, true}, {3, 4.0, c, false}]|c   |\n+---------------------------------------+----+\n</code></pre> <pre><code>&gt;&gt;&gt; df.printSchema()\nroot\n |-- col: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- a: integer (nullable = true)\n |    |    |-- b: float (nullable = true)\n |    |    |-- c: string (nullable = true)\n |    |    |-- d: boolean (nullable = true)\n |-- col2: string (nullable = true)\n</code></pre> <p>** Key can be an int **</p> <pre><code>&gt;&gt;&gt; array_struct = \"col\"\n&gt;&gt;&gt; key_column = \"a\"\n&gt;&gt;&gt; key = 1\n&gt;&gt;&gt; value_column = \"b\"\n&gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n&gt;&gt;&gt; result.show()\n+---+\n|  b|\n+---+\n|2.0|\n+---+\n</code></pre> <p>** Key can be a float **</p> <pre><code>&gt;&gt;&gt; key_column = \"b\"\n&gt;&gt;&gt; key = 2.0\n&gt;&gt;&gt; value_column = \"a\"\n&gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n&gt;&gt;&gt; result.show()\n+---+\n|  a|\n+---+\n|  1|\n+---+\n</code></pre> <p>** Key can be a string **</p> <pre><code>&gt;&gt;&gt; key_column = \"c\"\n&gt;&gt;&gt; key = \"c\"\n&gt;&gt;&gt; value_column = \"a\"\n&gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n</code></pre> <p>The first match will be returned, even if array have multiple matches to the key.</p> <pre><code>&gt;&gt;&gt; result.show()\n+---+\n|  a|\n+---+\n|  1|\n+---+\n</code></pre> <p>** Key can be a boolean **</p> <pre><code>&gt;&gt;&gt; key_column = \"d\"\n&gt;&gt;&gt; key = True\n&gt;&gt;&gt; value_column = \"a\"\n&gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n&gt;&gt;&gt; result.show()\n+---+\n|  a|\n+---+\n|  1|\n+---+\n</code></pre> <p>** Key can be a column**</p> <pre><code>&gt;&gt;&gt; array_struct = f.col(\"col\")\n&gt;&gt;&gt; key_column = \"c\"\n&gt;&gt;&gt; key = f.col(\"col2\")\n&gt;&gt;&gt; value_column = \"b\"\n&gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n&gt;&gt;&gt; result.show()\n+---+\n|  b|\n+---+\n|2.0|\n+---+\n</code></pre> <p>** All paramters are columns **</p> <pre><code>&gt;&gt;&gt; array_struct = f.col(\"col\")\n&gt;&gt;&gt; key_column = f.col(\"c\")\n&gt;&gt;&gt; key = f.col(\"col2\")\n&gt;&gt;&gt; value_column = f.col(\"a\")\n&gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n&gt;&gt;&gt; result.show()\n+---+\n|  a|\n+---+\n|  1|\n+---+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def filter_array_struct(\n    array_struct: Column | str,\n    key_column: Column | str,\n    key: Column | str | int | bool | float,\n    value_column: Column | str,\n) -&gt; Column:\n    \"\"\"Extract a value from an array of structs based on a key.\n\n    This function searches for the predicate `key_column` that matches the `key` within the\n    `array_struct` and returns the corresponding `value_column` from the struct with\n    the same index as predicate.\n\n    Warning:\n        Only the first match will be returned. If there are multiple matches, one need to\n        sort the array first.\n\n    Warning:\n        The function will not work if the `key_column` or `value_column` are not present in the\n        `array_struct` schema.\n\n    Args:\n        array_struct (Column | str): The array of structs to be searched.\n        key_column (Column | str): The column name to be used as a key.\n        key (Column | str | int | bool | float): The key to be searched for.\n        value_column (Column | str): The column name to be returned.\n\n    Returns:\n        Column: The value_column from the struct from the same array element as the matched key_column.\n\n    Examples:\n        &gt;&gt;&gt; data = [([{\"a\": 1, \"b\": 2.0, \"c\": \"c\", \"d\": True}, {\"a\": 3, \"b\": 4.0, \"c\": \"c\", \"d\": False}], \"c\")]\n        &gt;&gt;&gt; schema = 'col array&lt;struct&lt;a:int,b:float,c:string, d:boolean&gt;&gt;, col2 string'\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------------------------------------+----+\n        |col                                    |col2|\n        +---------------------------------------+----+\n        |[{1, 2.0, c, true}, {3, 4.0, c, false}]|c   |\n        +---------------------------------------+----+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- col: array (nullable = true)\n         |    |-- element: struct (containsNull = true)\n         |    |    |-- a: integer (nullable = true)\n         |    |    |-- b: float (nullable = true)\n         |    |    |-- c: string (nullable = true)\n         |    |    |-- d: boolean (nullable = true)\n         |-- col2: string (nullable = true)\n        &lt;BLANKLINE&gt;\n\n        ** Key can be an int **\n\n        &gt;&gt;&gt; array_struct = \"col\"\n        &gt;&gt;&gt; key_column = \"a\"\n        &gt;&gt;&gt; key = 1\n        &gt;&gt;&gt; value_column = \"b\"\n        &gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n        &gt;&gt;&gt; result.show()\n        +---+\n        |  b|\n        +---+\n        |2.0|\n        +---+\n        &lt;BLANKLINE&gt;\n\n        ** Key can be a float **\n\n        &gt;&gt;&gt; key_column = \"b\"\n        &gt;&gt;&gt; key = 2.0\n        &gt;&gt;&gt; value_column = \"a\"\n        &gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n        &gt;&gt;&gt; result.show()\n        +---+\n        |  a|\n        +---+\n        |  1|\n        +---+\n        &lt;BLANKLINE&gt;\n\n        ** Key can be a string **\n\n        &gt;&gt;&gt; key_column = \"c\"\n        &gt;&gt;&gt; key = \"c\"\n        &gt;&gt;&gt; value_column = \"a\"\n        &gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n\n        The first match will be returned, even if array have multiple matches to the key.\n\n        &gt;&gt;&gt; result.show()\n        +---+\n        |  a|\n        +---+\n        |  1|\n        +---+\n        &lt;BLANKLINE&gt;\n\n        ** Key can be a boolean **\n\n        &gt;&gt;&gt; key_column = \"d\"\n        &gt;&gt;&gt; key = True\n        &gt;&gt;&gt; value_column = \"a\"\n        &gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n        &gt;&gt;&gt; result.show()\n        +---+\n        |  a|\n        +---+\n        |  1|\n        +---+\n        &lt;BLANKLINE&gt;\n\n        ** Key can be a column**\n\n        &gt;&gt;&gt; array_struct = f.col(\"col\")\n        &gt;&gt;&gt; key_column = \"c\"\n        &gt;&gt;&gt; key = f.col(\"col2\")\n        &gt;&gt;&gt; value_column = \"b\"\n        &gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n        &gt;&gt;&gt; result.show()\n        +---+\n        |  b|\n        +---+\n        |2.0|\n        +---+\n        &lt;BLANKLINE&gt;\n\n        ** All paramters are columns **\n        &gt;&gt;&gt; array_struct = f.col(\"col\")\n        &gt;&gt;&gt; key_column = f.col(\"c\")\n        &gt;&gt;&gt; key = f.col(\"col2\")\n        &gt;&gt;&gt; value_column = f.col(\"a\")\n        &gt;&gt;&gt; result = df.select(filter_array_struct(array_struct, key_column, key, value_column))\n        &gt;&gt;&gt; result.show()\n        +---+\n        |  a|\n        +---+\n        |  1|\n        +---+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    if not isinstance(key, Column):\n        key = f.lit(key)\n\n    if not isinstance(array_struct, Column):\n        array_struct = f.col(array_struct)\n\n    if isinstance(key_column, Column):\n        key_column = extract_column_name(key_column)\n    if isinstance(value_column, Column):\n        value_column = extract_column_name(value_column)\n\n    return (\n        f.filter(\n            array_struct,\n            lambda x: x.getField(key_column) == key,\n        )\n        .getItem(0)\n        .getField(value_column)\n        .alias(value_column)\n    )\n</code></pre>"},{"location":"python_api/common/spark/#operations-on-pyspark-schemas-meta-transformations","title":"Operations on PySpark schemas (meta transformations)","text":""},{"location":"python_api/common/spark/#gentropy.common.spark.enforce_schema","title":"<code>gentropy.common.spark.enforce_schema(expected_schema: t.ArrayType | t.StructType | Column | str) -&gt; Callable[..., Any]</code>","text":"<p>A function to enforce the schema of a function output follows expectation.</p> Behavior <ul> <li>Fields that are not present in the expected schema will be dropped.</li> <li>Expected but missing fields will be added with Null values.</li> <li>Fields with incorrect data types will be casted to the expected data type.</li> </ul> <p>This is a decorator function and expected to be used like this:</p> <p>@enforce_schema(spark_schema) def my_function() -&gt; t.StructType:     return ...</p> <p>Parameters:</p> Name Type Description Default <code>expected_schema</code> <code>ArrayType | StructType | Column | str</code> <p>The expected schema of the output.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>Callable[..., Any]: A decorator function.</p> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def enforce_schema(\n    expected_schema: t.ArrayType | t.StructType | Column | str,\n) -&gt; Callable[..., Any]:\n    \"\"\"A function to enforce the schema of a function output follows expectation.\n\n    Behavior:\n        - Fields that are not present in the expected schema will be dropped.\n        - Expected but missing fields will be added with Null values.\n        - Fields with incorrect data types will be casted to the expected data type.\n\n    This is a decorator function and expected to be used like this:\n\n    @enforce_schema(spark_schema)\n    def my_function() -&gt; t.StructType:\n        return ...\n\n    Args:\n        expected_schema (t.ArrayType | t.StructType | Column | str): The expected schema of the output.\n\n    Returns:\n        Callable[..., Any]: A decorator function.\n    \"\"\"\n    T = TypeVar(\"T\", str, Column)\n\n    def decorator(function: Callable[..., T]) -&gt; Callable[..., T]:\n        \"\"\"A decorator function to enforce the schema of a function output follows expectation.\n\n        Args:\n            function (Callable[..., T]): The function to be decorated.\n\n        Returns:\n            Callable[..., T]: The decorated function.\n        \"\"\"\n\n        @wraps(function)\n        def wrapper(*args: str, **kwargs: str) -&gt; Any:\n            return f.from_json(f.to_json(function(*args, **kwargs)), expected_schema)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.get_nested_struct_schema","title":"<code>gentropy.common.spark.get_nested_struct_schema(dtype: t.DataType) -&gt; t.StructType</code>","text":"<p>Get the bottom StructType from a nested ArrayType type.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>DataType</code> <p>The nested data structure.</p> required <p>Returns:</p> Type Description <code>StructType</code> <p>t.StructType: The nested struct schema.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input data type is not a nested struct.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_nested_struct_schema(t.ArrayType(t.StructType([t.StructField('a', t.StringType())])))\nStructType([StructField('a', StringType(), True)])\n</code></pre> <pre><code>&gt;&gt;&gt; get_nested_struct_schema(t.ArrayType(t.ArrayType(t.StructType([t.StructField(\"a\", t.StringType())]))))\nStructType([StructField('a', StringType(), True)])\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def get_nested_struct_schema(dtype: t.DataType) -&gt; t.StructType:\n    \"\"\"Get the bottom StructType from a nested ArrayType type.\n\n    Args:\n        dtype (t.DataType): The nested data structure.\n\n    Returns:\n        t.StructType: The nested struct schema.\n\n    Raises:\n        TypeError: If the input data type is not a nested struct.\n\n    Examples:\n        &gt;&gt;&gt; get_nested_struct_schema(t.ArrayType(t.StructType([t.StructField('a', t.StringType())])))\n        StructType([StructField('a', StringType(), True)])\n\n        &gt;&gt;&gt; get_nested_struct_schema(t.ArrayType(t.ArrayType(t.StructType([t.StructField(\"a\", t.StringType())]))))\n        StructType([StructField('a', StringType(), True)])\n    \"\"\"\n    if isinstance(dtype, t.StructField):\n        dtype = dtype.dataType\n\n    match dtype:\n        case t.StructType(fields=_):\n            return dtype\n        case t.ArrayType(elementType=dtype):\n            return get_nested_struct_schema(dtype)\n        case _:\n            raise TypeError(\"The input data type must be a nested struct.\")\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.get_struct_field_schema","title":"<code>gentropy.common.spark.get_struct_field_schema(schema: t.StructType, name: str) -&gt; t.DataType</code>","text":"<p>Get schema for underlying struct field.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>StructType</code> <p>Provided schema where the name should be looked in.</p> required <code>name</code> <code>str</code> <p>Name of the field to look in the schema</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>t.DataType: Data type of the StructField with provided name</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provided name is not present in the input schema</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_struct_field_schema(t.StructType([t.StructField(\"a\", t.StringType())]), \"a\")\nStringType()\n</code></pre> <pre><code>&gt;&gt;&gt; get_struct_field_schema(t.StructType([t.StructField(\"a\", t.StringType())]), \"b\")\nTraceback (most recent call last):\n...\nValueError: Provided name b is not present in the schema\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def get_struct_field_schema(schema: t.StructType, name: str) -&gt; t.DataType:\n    \"\"\"Get schema for underlying struct field.\n\n    Args:\n        schema (t.StructType): Provided schema where the name should be looked in.\n        name (str): Name of the field to look in the schema\n\n    Returns:\n        t.DataType: Data type of the StructField with provided name\n\n    Raises:\n        ValueError: If provided name is not present in the input schema\n\n    Examples:\n        &gt;&gt;&gt; get_struct_field_schema(t.StructType([t.StructField(\"a\", t.StringType())]), \"a\")\n        StringType()\n\n        &gt;&gt;&gt; get_struct_field_schema(t.StructType([t.StructField(\"a\", t.StringType())]), \"b\") # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n        ...\n        ValueError: Provided name b is not present in the schema\n\n    \"\"\"\n    matching_fields = [f for f in schema.fields if f.name == name]\n    if not matching_fields:\n        raise ValueError(\"Provided name %s is not present in the schema.\", name)\n    return matching_fields[0].dataType\n</code></pre>"},{"location":"python_api/common/spark/#dataframe-transformations","title":"DataFrame transformations","text":""},{"location":"python_api/common/spark/#gentropy.common.spark.convert_from_wide_to_long","title":"<code>gentropy.common.spark.convert_from_wide_to_long(df: DataFrame, id_vars: Iterable[str], var_name: str, value_name: str, value_vars: Iterable[str] | None = None) -&gt; DataFrame</code>","text":"<p>Converts a dataframe from wide to long format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to melt</p> required <code>id_vars</code> <code>Iterable[str]</code> <p>List of fixed columns to keep</p> required <code>var_name</code> <code>str</code> <p>Name of the column containing the variable names</p> required <code>value_name</code> <code>str</code> <p>Name of the column containing the values</p> required <code>value_vars</code> <code>Iterable[str] | None</code> <p>List of columns to melt. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Melted dataframe</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"a\", 1, 2)], [\"id\", \"feature_1\", \"feature_2\"])\n&gt;&gt;&gt; convert_from_wide_to_long(df, [\"id\"], \"feature\", \"value\").show()\n+---+---------+-----+\n| id|  feature|value|\n+---+---------+-----+\n|  a|feature_1|  1.0|\n|  a|feature_2|  2.0|\n+---+---------+-----+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def convert_from_wide_to_long(\n    df: DataFrame,\n    id_vars: Iterable[str],\n    var_name: str,\n    value_name: str,\n    value_vars: Iterable[str] | None = None,\n) -&gt; DataFrame:\n    \"\"\"Converts a dataframe from wide to long format.\n\n    Args:\n        df (DataFrame): Dataframe to melt\n        id_vars (Iterable[str]): List of fixed columns to keep\n        var_name (str): Name of the column containing the variable names\n        value_name (str): Name of the column containing the values\n        value_vars (Iterable[str] | None): List of columns to melt. Defaults to None.\n\n    Returns:\n        DataFrame: Melted dataframe\n\n    Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"a\", 1, 2)], [\"id\", \"feature_1\", \"feature_2\"])\n        &gt;&gt;&gt; convert_from_wide_to_long(df, [\"id\"], \"feature\", \"value\").show()\n        +---+---------+-----+\n        | id|  feature|value|\n        +---+---------+-----+\n        |  a|feature_1|  1.0|\n        |  a|feature_2|  2.0|\n        +---+---------+-----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    if not value_vars:\n        value_vars = [c for c in df.columns if c not in id_vars]\n    _vars_and_vals = f.array(\n        *(\n            f.struct(\n                f.lit(c).alias(var_name), f.col(c).cast(t.FloatType()).alias(value_name)\n            )\n            for c in value_vars\n        )\n    )\n\n    # Add to the DataFrame and explode to convert into rows\n    _tmp = df.withColumn(\"_vars_and_vals\", f.explode(_vars_and_vals))\n\n    cols = list(id_vars) + [\n        f.col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]\n    ]\n    return _tmp.select(*cols)\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.convert_from_long_to_wide","title":"<code>gentropy.common.spark.convert_from_long_to_wide(df: DataFrame, id_vars: list[str], var_name: str, value_name: str) -&gt; DataFrame</code>","text":"<p>Converts a dataframe from long to wide format using Spark pivot built-in function.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to pivot</p> required <code>id_vars</code> <code>list[str]</code> <p>List of fixed columns to keep</p> required <code>var_name</code> <code>str</code> <p>Name of the column to pivot on</p> required <code>value_name</code> <code>str</code> <p>Name of the column containing the values</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Pivoted dataframe</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"a\", \"feature_1\", 1), (\"a\", \"feature_2\", 2)], [\"id\", \"featureName\", \"featureValue\"])\n&gt;&gt;&gt; convert_from_long_to_wide(df, [\"id\"], \"featureName\", \"featureValue\").show()\n+---+---------+---------+\n| id|feature_1|feature_2|\n+---+---------+---------+\n|  a|        1|        2|\n+---+---------+---------+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def convert_from_long_to_wide(\n    df: DataFrame, id_vars: list[str], var_name: str, value_name: str\n) -&gt; DataFrame:\n    \"\"\"Converts a dataframe from long to wide format using Spark pivot built-in function.\n\n    Args:\n        df (DataFrame): Dataframe to pivot\n        id_vars (list[str]): List of fixed columns to keep\n        var_name (str): Name of the column to pivot on\n        value_name (str): Name of the column containing the values\n\n    Returns:\n        DataFrame: Pivoted dataframe\n\n    Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"a\", \"feature_1\", 1), (\"a\", \"feature_2\", 2)], [\"id\", \"featureName\", \"featureValue\"])\n        &gt;&gt;&gt; convert_from_long_to_wide(df, [\"id\"], \"featureName\", \"featureValue\").show()\n        +---+---------+---------+\n        | id|feature_1|feature_2|\n        +---+---------+---------+\n        |  a|        1|        2|\n        +---+---------+---------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return df.groupBy(id_vars).pivot(var_name).agg(f.first(value_name))\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.pivot_df","title":"<code>gentropy.common.spark.pivot_df(df: DataFrame, pivot_col: str, value_col: str, grouping_cols: list[Column]) -&gt; DataFrame</code>","text":"<p>Pivot a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to pivot</p> required <code>pivot_col</code> <code>str</code> <p>Column to pivot on</p> required <code>value_col</code> <code>str</code> <p>Column to pivot</p> required <code>grouping_cols</code> <code>list[Column]</code> <p>Columns to group by</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Pivoted dataframe</p> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def pivot_df(\n    df: DataFrame,\n    pivot_col: str,\n    value_col: str,\n    grouping_cols: list[Column],\n) -&gt; DataFrame:\n    \"\"\"Pivot a dataframe.\n\n    Args:\n        df (DataFrame): Dataframe to pivot\n        pivot_col (str): Column to pivot on\n        value_col (str): Column to pivot\n        grouping_cols (list[Column]): Columns to group by\n\n    Returns:\n        DataFrame: Pivoted dataframe\n    \"\"\"\n    pivot_values = df.select(pivot_col).distinct().rdd.flatMap(lambda x: x).collect()\n    return (\n        df.groupBy(grouping_cols)\n        .pivot(pivot_col)\n        .agg({value_col: \"first\"})\n        .select(\n            grouping_cols\n            + [\n                f.when(f.col(x).isNull(), None)\n                .otherwise(f.col(x))\n                .alias(f\"{x}_{value_col}\")\n                for x in pivot_values\n            ],\n        )\n    )\n</code></pre>"},{"location":"python_api/common/spark/#gentropy.common.spark.rename_all_columns","title":"<code>gentropy.common.spark.rename_all_columns(df: DataFrame, prefix: str) -&gt; DataFrame</code>","text":"<p>Given a prefix, rename all columns of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be processed.</p> required <code>prefix</code> <code>str</code> <p>The prefix to be added to the column names.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame with all columns renamed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [('a', 1.2, True),('b', 0.0, False),('c', None, None),]\n&gt;&gt;&gt; prefix = 'prefix_'\n&gt;&gt;&gt; rename_all_columns(spark.createDataFrame(data, ['col1', 'col2', 'col3']), prefix).show()\n+-----------+-----------+-----------+\n|prefix_col1|prefix_col2|prefix_col3|\n+-----------+-----------+-----------+\n|          a|        1.2|       true|\n|          b|        0.0|      false|\n|          c|       NULL|       NULL|\n+-----------+-----------+-----------+\n</code></pre> Source code in <code>src/gentropy/common/spark.py</code> <pre><code>def rename_all_columns(df: DataFrame, prefix: str) -&gt; DataFrame:\n    \"\"\"Given a prefix, rename all columns of a DataFrame.\n\n    Args:\n        df (DataFrame): The DataFrame to be processed.\n        prefix (str): The prefix to be added to the column names.\n\n    Returns:\n        DataFrame: The DataFrame with all columns renamed.\n\n    Examples:\n        &gt;&gt;&gt; data = [('a', 1.2, True),('b', 0.0, False),('c', None, None),]\n        &gt;&gt;&gt; prefix = 'prefix_'\n        &gt;&gt;&gt; rename_all_columns(spark.createDataFrame(data, ['col1', 'col2', 'col3']), prefix).show()\n        +-----------+-----------+-----------+\n        |prefix_col1|prefix_col2|prefix_col3|\n        +-----------+-----------+-----------+\n        |          a|        1.2|       true|\n        |          b|        0.0|      false|\n        |          c|       NULL|       NULL|\n        +-----------+-----------+-----------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return reduce(\n        lambda df, col: df.withColumnRenamed(col, f\"{prefix}{col}\"),\n        df.columns,\n        df,\n    )\n</code></pre>"},{"location":"python_api/common/stats/","title":"stats","text":""},{"location":"python_api/common/stats/#statistical-methods-used-for-gwas-processing","title":"Statistical methods used for GWAS processing","text":"<p>The functions below are used during the harmonisation of summary statistics effect size, p-value and confidence intervals, standard error calculations.</p> <p>NOTE: Due to low p-value values, the functions work with pvalue in one of two formats:</p> <ul> <li>as negative log10 p-value (neglogoval)</li> <li>as mantissa and exponent (2 separate columns)</li> </ul>"},{"location":"python_api/common/stats/#gentropy.common.stats","title":"<code>gentropy.common.stats</code>","text":"<p>Statistic calculations.</p>"},{"location":"python_api/common/stats/#gentropy.common.stats.chi2_from_pvalue","title":"<code>chi2_from_pvalue(p_value_mantissa: Column, p_value_exponent: Column) -&gt; Column</code>","text":"<p>Calculate chi2 from p-value.</p> <p>This function calculates the chi2 value from the p-value mantissa and exponent. In case the p-value is very small (exponent &lt; -300), it uses an approximation based on a linear regression model. The approximation is based on the formula: -5.367 * neglog_pval + 4.596, where neglog_pval is the negative log10 of the p-value mantissa.</p> <p>Parameters:</p> Name Type Description Default <code>p_value_mantissa</code> <code>Column</code> <p>Mantissa of the p-value (float)</p> required <code>p_value_exponent</code> <code>Column</code> <p>Exponent of the p-value (integer)</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Chi2 value (float)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(5.0, -8), (9.0, -300), (9.0, -301)]\n&gt;&gt;&gt; schema = \"pValueMantissa FLOAT, pValueExponent INT\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show()\n+--------------+--------------+\n|pValueMantissa|pValueExponent|\n+--------------+--------------+\n|           5.0|            -8|\n|           9.0|          -300|\n|           9.0|          -301|\n+--------------+--------------+\n</code></pre> <pre><code>&gt;&gt;&gt; mantissa = f.col(\"pValueMantissa\")\n&gt;&gt;&gt; exponent = f.col(\"pValueExponent\")\n&gt;&gt;&gt; chi2 = f.round(chi2_from_pvalue(mantissa, exponent), 2).alias(\"chi2\")\n&gt;&gt;&gt; df2 = df.select(mantissa, exponent, chi2)\n&gt;&gt;&gt; df2.show()\n+--------------+--------------+-------+\n|pValueMantissa|pValueExponent|   chi2|\n+--------------+--------------+-------+\n|           5.0|            -8|  29.72|\n|           9.0|          -300|1369.48|\n|           9.0|          -301|1373.64|\n+--------------+--------------+-------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def chi2_from_pvalue(p_value_mantissa: Column, p_value_exponent: Column) -&gt; Column:\n    \"\"\"Calculate chi2 from p-value.\n\n    This function calculates the chi2 value from the p-value mantissa and exponent.\n    In case the p-value is very small (exponent &lt; -300), it uses an approximation based on a linear regression model.\n    The approximation is based on the formula: -5.367 * neglog_pval + 4.596, where neglog_pval is the negative log10 of the p-value mantissa.\n\n\n    Args:\n        p_value_mantissa (Column): Mantissa of the p-value (float)\n        p_value_exponent (Column): Exponent of the p-value (integer)\n\n    Returns:\n        Column: Chi2 value (float)\n\n    Examples:\n        &gt;&gt;&gt; data = [(5.0, -8), (9.0, -300), (9.0, -301)]\n        &gt;&gt;&gt; schema = \"pValueMantissa FLOAT, pValueExponent INT\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show()\n        +--------------+--------------+\n        |pValueMantissa|pValueExponent|\n        +--------------+--------------+\n        |           5.0|            -8|\n        |           9.0|          -300|\n        |           9.0|          -301|\n        +--------------+--------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; mantissa = f.col(\"pValueMantissa\")\n        &gt;&gt;&gt; exponent = f.col(\"pValueExponent\")\n        &gt;&gt;&gt; chi2 = f.round(chi2_from_pvalue(mantissa, exponent), 2).alias(\"chi2\")\n        &gt;&gt;&gt; df2 = df.select(mantissa, exponent, chi2)\n        &gt;&gt;&gt; df2.show()\n        +--------------+--------------+-------+\n        |pValueMantissa|pValueExponent|   chi2|\n        +--------------+--------------+-------+\n        |           5.0|            -8|  29.72|\n        |           9.0|          -300|1369.48|\n        |           9.0|          -301|1373.64|\n        +--------------+--------------+-------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    PVAL_EXP_THRESHOLD = f.lit(-300)\n    APPROX_INTERCEPT = f.lit(-5.367)\n    APPROX_COEF = f.lit(4.596)\n    neglog_pval = neglogpval_from_pvalue(p_value_mantissa, p_value_exponent)\n    p_value = p_value_mantissa * f.pow(10, p_value_exponent)\n    neglog_approx = (neglog_pval * APPROX_COEF + APPROX_INTERCEPT).cast(t.DoubleType())\n\n    return (\n        f.when(p_value_exponent &lt; PVAL_EXP_THRESHOLD, neglog_approx)\n        .otherwise(chi2_inverse_survival_function(p_value))\n        .alias(\"chi2\")\n    )\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.ci","title":"<code>ci(pvalue_mantissa: Column, pvalue_exponent: Column, beta: Column, standard_error: Column) -&gt; tuple[Column, Column]</code>","text":"<p>Calculate the confidence interval for the effect based on the p-value and the effect size.</p> <p>If the standard error already available, don't re-calculate from p-value.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue_mantissa</code> <code>Column</code> <p>p-value mantissa (float)</p> required <code>pvalue_exponent</code> <code>Column</code> <p>p-value exponent (integer)</p> required <code>beta</code> <code>Column</code> <p>effect size in beta (float)</p> required <code>standard_error</code> <code>Column</code> <p>standard error.</p> required <p>Returns:</p> Type Description <code>tuple[Column, Column]</code> <p>tuple[Column, Column]: betaConfidenceIntervalLower (float), betaConfidenceIntervalUpper (float)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([\n...     (2.5, -10, 0.5, 0.2),\n...     (3.0, -5, 1.0, None),\n...     (1.5, -8, -0.2, 0.1)\n...     ], [\"pvalue_mantissa\", \"pvalue_exponent\", \"beta\", \"standard_error\"]\n... )\n&gt;&gt;&gt; df.select(\"*\", *ci(f.col(\"pvalue_mantissa\"), f.col(\"pvalue_exponent\"), f.col(\"beta\"), f.col(\"standard_error\"))).show()\n+---------------+---------------+----+--------------+---------------------------+---------------------------+\n|pvalue_mantissa|pvalue_exponent|beta|standard_error|betaConfidenceIntervalLower|betaConfidenceIntervalUpper|\n+---------------+---------------+----+--------------+---------------------------+---------------------------+\n|            2.5|            -10| 0.5|           0.2|        0.10799999999999998|                      0.892|\n|            3.0|             -5| 1.0|          NULL|         0.5303664052547075|         1.4696335947452925|\n|            1.5|             -8|-0.2|           0.1|                     -0.396|       -0.00400000000000...|\n+---------------+---------------+----+--------------+---------------------------+---------------------------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def ci(\n    pvalue_mantissa: Column,\n    pvalue_exponent: Column,\n    beta: Column,\n    standard_error: Column,\n) -&gt; tuple[Column, Column]:\n    \"\"\"Calculate the confidence interval for the effect based on the p-value and the effect size.\n\n    If the standard error already available, don't re-calculate from p-value.\n\n    Args:\n        pvalue_mantissa (Column): p-value mantissa (float)\n        pvalue_exponent (Column): p-value exponent (integer)\n        beta (Column): effect size in beta (float)\n        standard_error (Column): standard error.\n\n    Returns:\n        tuple[Column, Column]: betaConfidenceIntervalLower (float), betaConfidenceIntervalUpper (float)\n\n    Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([\n        ...     (2.5, -10, 0.5, 0.2),\n        ...     (3.0, -5, 1.0, None),\n        ...     (1.5, -8, -0.2, 0.1)\n        ...     ], [\"pvalue_mantissa\", \"pvalue_exponent\", \"beta\", \"standard_error\"]\n        ... )\n        &gt;&gt;&gt; df.select(\"*\", *ci(f.col(\"pvalue_mantissa\"), f.col(\"pvalue_exponent\"), f.col(\"beta\"), f.col(\"standard_error\"))).show()\n        +---------------+---------------+----+--------------+---------------------------+---------------------------+\n        |pvalue_mantissa|pvalue_exponent|beta|standard_error|betaConfidenceIntervalLower|betaConfidenceIntervalUpper|\n        +---------------+---------------+----+--------------+---------------------------+---------------------------+\n        |            2.5|            -10| 0.5|           0.2|        0.10799999999999998|                      0.892|\n        |            3.0|             -5| 1.0|          NULL|         0.5303664052547075|         1.4696335947452925|\n        |            1.5|             -8|-0.2|           0.1|                     -0.396|       -0.00400000000000...|\n        +---------------+---------------+----+--------------+---------------------------+---------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    # Calculate p-value from mantissa and exponent:\n    pvalue = pvalue_mantissa * f.pow(10, pvalue_exponent)\n\n    # Fix p-value underflow:\n    pvalue = f.when(pvalue == 0, sys.float_info.min).otherwise(pvalue)\n\n    # Compute missing standard error:\n    standard_error = f.when(\n        standard_error.isNull(), f.abs(beta) / f.abs(zscore_from_pvalue(pvalue, beta))\n    ).otherwise(standard_error)\n\n    # Calculate upper and lower confidence interval:\n    z_score_095 = 1.96\n    ci_lower = (beta - z_score_095 * standard_error).alias(\n        \"betaConfidenceIntervalLower\"\n    )\n    ci_upper = (beta + z_score_095 * standard_error).alias(\n        \"betaConfidenceIntervalUpper\"\n    )\n\n    return (ci_lower, ci_upper)\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.get_logsum","title":"<code>get_logsum(arr: NDArray[np.float64]) -&gt; float</code>","text":"<p>Calculates logarithm of the sum of exponents of a vector. The max is extracted to ensure that the sum is not Inf.</p> <p>This function emulates scipy's logsumexp expression.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>NDArray[float64]</code> <p>input array</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>logsumexp of the input array</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; l = [0.2, 0.1, 0.05, 0]\n&gt;&gt;&gt; round(get_logsum(l), 6)\n1.476557\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def get_logsum(arr: NDArray[np.float64]) -&gt; float:\n    \"\"\"Calculates logarithm of the sum of exponents of a vector. The max is extracted to ensure that the sum is not Inf.\n\n    This function emulates scipy's logsumexp expression.\n\n    Args:\n        arr (NDArray[np.float64]): input array\n\n    Returns:\n        float: logsumexp of the input array\n\n    Examples:\n        &gt;&gt;&gt; l = [0.2, 0.1, 0.05, 0]\n        &gt;&gt;&gt; round(get_logsum(l), 6)\n        1.476557\n    \"\"\"\n    MAX = np.max(arr)\n    result = MAX + np.log(np.sum(np.exp(arr - MAX)))\n    return float(result)\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.neglogpval_from_pvalue","title":"<code>neglogpval_from_pvalue(p_value_mantissa: Column, p_value_exponent: Column) -&gt; Column</code>","text":"<p>Compute the negative log p-value.</p> <p>Parameters:</p> Name Type Description Default <code>p_value_mantissa</code> <code>Column</code> <p>P-value mantissa</p> required <code>p_value_exponent</code> <code>Column</code> <p>P-value exponent</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Negative log p-value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = [(1, 1), (5, -2), (1, -1000)]\n&gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"p_value_mantissa\", \"p_value_exponent\")\n&gt;&gt;&gt; df.withColumn(\"neg_log_p\", neglogpval_from_pvalue(f.col(\"p_value_mantissa\"), f.col(\"p_value_exponent\"))).show()\n+----------------+----------------+------------------+\n|p_value_mantissa|p_value_exponent|         neg_log_p|\n+----------------+----------------+------------------+\n|               1|               1|              -1.0|\n|               5|              -2|1.3010299956639813|\n|               1|           -1000|            1000.0|\n+----------------+----------------+------------------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def neglogpval_from_pvalue(\n    p_value_mantissa: Column, p_value_exponent: Column\n) -&gt; Column:\n    \"\"\"Compute the negative log p-value.\n\n    Args:\n        p_value_mantissa (Column): P-value mantissa\n        p_value_exponent (Column): P-value exponent\n\n    Returns:\n        Column: Negative log p-value\n\n    Examples:\n        &gt;&gt;&gt; d = [(1, 1), (5, -2), (1, -1000)]\n        &gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"p_value_mantissa\", \"p_value_exponent\")\n        &gt;&gt;&gt; df.withColumn(\"neg_log_p\", neglogpval_from_pvalue(f.col(\"p_value_mantissa\"), f.col(\"p_value_exponent\"))).show()\n        +----------------+----------------+------------------+\n        |p_value_mantissa|p_value_exponent|         neg_log_p|\n        +----------------+----------------+------------------+\n        |               1|               1|              -1.0|\n        |               5|              -2|1.3010299956639813|\n        |               1|           -1000|            1000.0|\n        +----------------+----------------+------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return -1 * (f.log10(p_value_mantissa) + p_value_exponent)\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.neglogpval_from_z2","title":"<code>neglogpval_from_z2(z2: Column) -&gt; Column</code>","text":"<p>Calculate negative log10 of p-value from squared Z-score following chi2 distribution.</p> <p>The Z-score^2 is equal to the chi2 with 1 degree of freedom.</p> <p>In case of very large Z-score (very small corresponding p-value), the function uses a linear approximation.</p> <p>Parameters:</p> Name Type Description Default <code>z2</code> <code>Column</code> <p>Z-score squared.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>negative log of p-value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(1.0,), (2000.0,)]\n&gt;&gt;&gt; schema = \"z2 FLOAT\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show()\n+------+\n|    z2|\n+------+\n|   1.0|\n|2000.0|\n+------+\n</code></pre> <pre><code>&gt;&gt;&gt; neglogpval = f.round(neglogpval_from_z2(f.col(\"z2\")), 2).alias(\"neglogpval\")\n&gt;&gt;&gt; df2 = df.select(f.col(\"z2\"), neglogpval)\n&gt;&gt;&gt; df2.show()\n+------+----------+\n|    z2|neglogpval|\n+------+----------+\n|   1.0|       0.5|\n|2000.0|    436.02|\n+------+----------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def neglogpval_from_z2(z2: Column) -&gt; Column:\n    \"\"\"Calculate negative log10 of p-value from squared Z-score following chi2 distribution.\n\n    **The Z-score^2 is equal to the chi2 with 1 degree of freedom.**\n\n    In case of very large Z-score (very small corresponding p-value), the function uses a linear approximation.\n\n    Args:\n        z2 (Column): Z-score squared.\n\n    Returns:\n        Column:  negative log of p-value.\n\n    Examples:\n        &gt;&gt;&gt; data = [(1.0,), (2000.0,)]\n        &gt;&gt;&gt; schema = \"z2 FLOAT\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show()\n        +------+\n        |    z2|\n        +------+\n        |   1.0|\n        |2000.0|\n        +------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; neglogpval = f.round(neglogpval_from_z2(f.col(\"z2\")), 2).alias(\"neglogpval\")\n        &gt;&gt;&gt; df2 = df.select(f.col(\"z2\"), neglogpval)\n        &gt;&gt;&gt; df2.show()\n        +------+----------+\n        |    z2|neglogpval|\n        +------+----------+\n        |   1.0|       0.5|\n        |2000.0|    436.02|\n        +------+----------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    MAX_EXACT_Z2 = f.lit(1400)\n    APPROX_INTERCEPT = f.lit(1.4190)\n    APPROX_COEFF = f.lit(0.2173)\n    approximate_neglogpval_from_z2 = APPROX_INTERCEPT + APPROX_COEFF * z2\n    computed_neglogpval_from_z2 = -1 * f.log10(chi2_survival_function(z2))\n    return f.when(z2 &lt;= MAX_EXACT_Z2, computed_neglogpval_from_z2).otherwise(\n        approximate_neglogpval_from_z2\n    )\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.normalise_gwas_statistics","title":"<code>normalise_gwas_statistics(beta: Column, odds_ratio: Column, standard_error: Column, ci_upper: Column, ci_lower: Column, mantissa: Column, exponent: Column) -&gt; GWASEffect</code>","text":"<p>Normalise beta and standard error from given values.</p> <p>This function attempts to harmonise Effect and Standard Error given various inputs.</p> Note <p>Effect (Beta) harmonisation:</p> <ul> <li>If beta is not null, it is kept as is.</li> <li>If beta is null, but odds ratio is not null, odds ratio is converted to beta</li> </ul> Note <p>Effect Standard Error (std(beta)) harmonisation</p> <p>Prefer calculation from p-value and beta, if available, as the confidence interval is usually rounded and may lead to loss of precision:</p> <ul> <li>If standard error is not null, it is kept as is.</li> <li>If standard error is null, but beta, pval-mantissa, pval-exponent are not null, convert pval components and beta to standard error</li> <li>If standard error is null, but ci-upper and ci-lower are not null and they come from odds ratio, convert them to standard error.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>Column</code> <p>Effect in beta.</p> required <code>odds_ratio</code> <code>Column</code> <p>Effect in odds ratio.</p> required <code>standard_error</code> <code>Column</code> <p>Standard error of the effect.</p> required <code>ci_upper</code> <code>Column</code> <p>Upper bound of the confidence interval.</p> required <code>ci_lower</code> <code>Column</code> <p>Lower bound of the confidence interval.</p> required <code>mantissa</code> <code>Column</code> <p>Mantissa of the p-value.</p> required <code>exponent</code> <code>Column</code> <p>Exponent of the p-value.</p> required <p>Returns:</p> Name Type Description <code>GWASEffect</code> <code>GWASEffect</code> <p>named tuple with standardError and beta columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x1 = (0.1, 1.1, 0.1, None, None, 9.0, -100) # keep beta, keep std error\n&gt;&gt;&gt; x2 = (None, 1.1, 0.1, None, None, 9.0, -100) # convert odds ratio to beta, keep std error\n&gt;&gt;&gt; x3 = (None, 1.1, None, 1.30, 0.90, None, None) # convert odds ratio to beta, convert ci to standard error\n&gt;&gt;&gt; x4 = (0.1, 1.1, None, 1.30, 0.90, None, None) # keep beta, convert ci to standard error\n&gt;&gt;&gt; x5 = (None, 1.1, None, 1.30, 0.90, 9.0, -100) # convert beta to odds ratio, convert p-value and beta to standard error\n&gt;&gt;&gt; x6 = (0.1, None, None, None, None, 9.0, -100) # keep beta, convert p-value and beta to standard error\n&gt;&gt;&gt; x7 = (None, None, None, 1.3, 0.9, 9.0, -100) # keep beta NULL, without beta we do not want to compute the standard error\n&gt;&gt;&gt; data = [x1, x2, x3, x4, x5, x6, x7]\n</code></pre> <pre><code>&gt;&gt;&gt; schema = \"beta FLOAT, oddsRatio FLOAT, standardError FLOAT, ci_upper FLOAT, ci_lower FLOAT, mantissa FLOAT, exp INT\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show()\n+----+---------+-------------+--------+--------+--------+----+\n|beta|oddsRatio|standardError|ci_upper|ci_lower|mantissa| exp|\n+----+---------+-------------+--------+--------+--------+----+\n| 0.1|      1.1|          0.1|    NULL|    NULL|     9.0|-100|\n|NULL|      1.1|          0.1|    NULL|    NULL|     9.0|-100|\n|NULL|      1.1|         NULL|     1.3|     0.9|    NULL|NULL|\n| 0.1|      1.1|         NULL|     1.3|     0.9|    NULL|NULL|\n|NULL|      1.1|         NULL|     1.3|     0.9|     9.0|-100|\n| 0.1|     NULL|         NULL|    NULL|    NULL|     9.0|-100|\n|NULL|     NULL|         NULL|     1.3|     0.9|     9.0|-100|\n+----+---------+-------------+--------+--------+--------+----+\n</code></pre> <pre><code>&gt;&gt;&gt; beta = f.col(\"beta\")\n&gt;&gt;&gt; odds_ratio = f.col(\"oddsRatio\")\n&gt;&gt;&gt; se = f.col(\"standardError\")\n&gt;&gt;&gt; ci_upper = f.col(\"ci_upper\")\n&gt;&gt;&gt; ci_lower = f.col(\"ci_lower\")\n&gt;&gt;&gt; mantissa = f.col(\"mantissa\")\n&gt;&gt;&gt; exponent = f.col(\"exp\")\n&gt;&gt;&gt; cols = normalise_gwas_statistics(\n...     beta, odds_ratio, se, ci_upper, ci_lower, mantissa, exponent\n... )\n&gt;&gt;&gt; beta_computed = f.round(cols.beta, 2).alias(\"beta\")\n&gt;&gt;&gt; standard_error_computed = f.round(cols.standard_error, 2).alias(\"standardError\")\n&gt;&gt;&gt; df.select(beta_computed, standard_error_computed).show()\n+----+-------------+\n|beta|standardError|\n+----+-------------+\n| 0.1|          0.1|\n| 0.1|          0.1|\n| 0.1|         0.09|\n| 0.1|         0.09|\n| 0.1|          0.0|\n| 0.1|          0.0|\n|NULL|         NULL|\n+----+-------------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def normalise_gwas_statistics(\n    beta: Column,\n    odds_ratio: Column,\n    standard_error: Column,\n    ci_upper: Column,\n    ci_lower: Column,\n    mantissa: Column,\n    exponent: Column,\n) -&gt; GWASEffect:\n    \"\"\"Normalise beta and standard error from given values.\n\n    This function attempts to harmonise Effect and Standard Error given various inputs.\n\n    Note:\n        Effect (Beta) harmonisation:\n\n        - If beta is not null, it is kept as is.\n        - If beta is null, but odds ratio is not null, odds ratio is converted to beta\n\n    Note:\n        Effect Standard Error (std(beta)) harmonisation\n\n        **Prefer calculation from p-value and beta, if available, as the confidence interval is usually rounded and may lead to loss of precision**:\n\n        - If standard error is not null, it is kept as is.\n        - If standard error is null, but beta, pval-mantissa, pval-exponent are not null, convert pval components and beta to standard error\n        - If standard error is null, but ci-upper and ci-lower are not null and they come from odds ratio, convert them to standard error.\n\n\n    Args:\n        beta (Column): Effect in beta.\n        odds_ratio (Column): Effect in odds ratio.\n        standard_error (Column): Standard error of the effect.\n        ci_upper (Column): Upper bound of the confidence interval.\n        ci_lower (Column): Lower bound of the confidence interval.\n        mantissa (Column): Mantissa of the p-value.\n        exponent (Column): Exponent of the p-value.\n\n    Returns:\n        GWASEffect: named tuple with standardError and beta columns.\n\n    Examples:\n        &gt;&gt;&gt; x1 = (0.1, 1.1, 0.1, None, None, 9.0, -100) # keep beta, keep std error\n        &gt;&gt;&gt; x2 = (None, 1.1, 0.1, None, None, 9.0, -100) # convert odds ratio to beta, keep std error\n        &gt;&gt;&gt; x3 = (None, 1.1, None, 1.30, 0.90, None, None) # convert odds ratio to beta, convert ci to standard error\n        &gt;&gt;&gt; x4 = (0.1, 1.1, None, 1.30, 0.90, None, None) # keep beta, convert ci to standard error\n        &gt;&gt;&gt; x5 = (None, 1.1, None, 1.30, 0.90, 9.0, -100) # convert beta to odds ratio, convert p-value and beta to standard error\n        &gt;&gt;&gt; x6 = (0.1, None, None, None, None, 9.0, -100) # keep beta, convert p-value and beta to standard error\n        &gt;&gt;&gt; x7 = (None, None, None, 1.3, 0.9, 9.0, -100) # keep beta NULL, without beta we do not want to compute the standard error\n        &gt;&gt;&gt; data = [x1, x2, x3, x4, x5, x6, x7]\n\n        &gt;&gt;&gt; schema = \"beta FLOAT, oddsRatio FLOAT, standardError FLOAT, ci_upper FLOAT, ci_lower FLOAT, mantissa FLOAT, exp INT\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show()\n        +----+---------+-------------+--------+--------+--------+----+\n        |beta|oddsRatio|standardError|ci_upper|ci_lower|mantissa| exp|\n        +----+---------+-------------+--------+--------+--------+----+\n        | 0.1|      1.1|          0.1|    NULL|    NULL|     9.0|-100|\n        |NULL|      1.1|          0.1|    NULL|    NULL|     9.0|-100|\n        |NULL|      1.1|         NULL|     1.3|     0.9|    NULL|NULL|\n        | 0.1|      1.1|         NULL|     1.3|     0.9|    NULL|NULL|\n        |NULL|      1.1|         NULL|     1.3|     0.9|     9.0|-100|\n        | 0.1|     NULL|         NULL|    NULL|    NULL|     9.0|-100|\n        |NULL|     NULL|         NULL|     1.3|     0.9|     9.0|-100|\n        +----+---------+-------------+--------+--------+--------+----+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; beta = f.col(\"beta\")\n        &gt;&gt;&gt; odds_ratio = f.col(\"oddsRatio\")\n        &gt;&gt;&gt; se = f.col(\"standardError\")\n        &gt;&gt;&gt; ci_upper = f.col(\"ci_upper\")\n        &gt;&gt;&gt; ci_lower = f.col(\"ci_lower\")\n        &gt;&gt;&gt; mantissa = f.col(\"mantissa\")\n        &gt;&gt;&gt; exponent = f.col(\"exp\")\n        &gt;&gt;&gt; cols = normalise_gwas_statistics(\n        ...     beta, odds_ratio, se, ci_upper, ci_lower, mantissa, exponent\n        ... )\n        &gt;&gt;&gt; beta_computed = f.round(cols.beta, 2).alias(\"beta\")\n        &gt;&gt;&gt; standard_error_computed = f.round(cols.standard_error, 2).alias(\"standardError\")\n        &gt;&gt;&gt; df.select(beta_computed, standard_error_computed).show()\n        +----+-------------+\n        |beta|standardError|\n        +----+-------------+\n        | 0.1|          0.1|\n        | 0.1|          0.1|\n        | 0.1|         0.09|\n        | 0.1|         0.09|\n        | 0.1|          0.0|\n        | 0.1|          0.0|\n        |NULL|         NULL|\n        +----+-------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    beta = (\n        f.when(beta.isNotNull(), beta)\n        .when(odds_ratio.isNotNull(), f.log(odds_ratio))\n        .otherwise(f.lit(None))\n        .alias(\"beta\")\n    )\n    chi2 = chi2_from_pvalue(mantissa, exponent)\n\n    standard_error = (\n        f.when(standard_error.isNotNull(), standard_error)\n        .when(\n            standard_error.isNull()\n            &amp; mantissa.isNotNull()\n            &amp; exponent.isNotNull()\n            &amp; beta.isNotNull(),\n            stderr_from_chi2_and_effect_size(chi2, beta),\n        )\n        .when(\n            standard_error.isNull()\n            &amp; ci_lower.isNotNull()\n            &amp; ci_upper.isNotNull()\n            &amp; odds_ratio.isNotNull(),\n            stderr_from_ci(ci_upper, ci_lower),\n        )\n        .otherwise(f.lit(None))\n        .alias(\"standardError\")\n    )\n\n    return GWASEffect(standard_error=standard_error, beta=beta)\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.pvalue_from_neglogpval","title":"<code>pvalue_from_neglogpval(p_value: Column) -&gt; PValComponents</code>","text":"<p>Computing p-value mantissa and exponent based on the negative 10 based logarithm of the p-value.</p> <p>Parameters:</p> Name Type Description Default <code>p_value</code> <code>Column</code> <p>Neg-log p-value (string)</p> required <p>Returns:</p> Name Type Description <code>PValComponents</code> <code>PValComponents</code> <p>mantissa and exponent of the p-value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; (\n... spark.createDataFrame([(4.56, 'a'),(2109.23, 'b'),(None,'c')], ['negLogPv', 'label'])\n... .select('negLogPv',*pvalue_from_neglogpval(f.col('negLogPv')))\n... .show()\n... )\n+--------+--------------+--------------+\n|negLogPv|pValueMantissa|pValueExponent|\n+--------+--------------+--------------+\n|    4.56|     2.7542286|            -5|\n| 2109.23|     5.8884363|         -2110|\n|    NULL|          NULL|          NULL|\n+--------+--------------+--------------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def pvalue_from_neglogpval(p_value: Column) -&gt; PValComponents:\n    \"\"\"Computing p-value mantissa and exponent based on the negative 10 based logarithm of the p-value.\n\n    Args:\n        p_value (Column): Neg-log p-value (string)\n\n    Returns:\n        PValComponents: mantissa and exponent of the p-value\n\n    Examples:\n        &gt;&gt;&gt; (\n        ... spark.createDataFrame([(4.56, 'a'),(2109.23, 'b'),(None,'c')], ['negLogPv', 'label'])\n        ... .select('negLogPv',*pvalue_from_neglogpval(f.col('negLogPv')))\n        ... .show()\n        ... )\n        +--------+--------------+--------------+\n        |negLogPv|pValueMantissa|pValueExponent|\n        +--------+--------------+--------------+\n        |    4.56|     2.7542286|            -5|\n        | 2109.23|     5.8884363|         -2110|\n        |    NULL|          NULL|          NULL|\n        +--------+--------------+--------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    exponent: Column = f.ceil(p_value)\n    mantissa: Column = f.pow(f.lit(10), (exponent - p_value))\n\n    return PValComponents(\n        mantissa=mantissa.cast(t.FloatType()).alias(\"pValueMantissa\"),\n        exponent=(-1 * exponent).cast(t.IntegerType()).alias(\"pValueExponent\"),\n    )\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.split_pvalue","title":"<code>split_pvalue(pvalue: float) -&gt; tuple[float, int]</code>","text":"<p>Convert a float to 10 based exponent and mantissa.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue</code> <code>float</code> <p>p-value</p> required <p>Returns:</p> Type Description <code>tuple[float, int]</code> <p>tuple[float, int]: Tuple with mantissa and exponent</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If p-value is not between 0 and 1</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; split_pvalue(0.00001234)\n(1.234, -5)\n</code></pre> <pre><code>&gt;&gt;&gt; split_pvalue(1)\n(1.0, 0)\n</code></pre> <pre><code>&gt;&gt;&gt; split_pvalue(0.123)\n(1.23, -1)\n</code></pre> <pre><code>&gt;&gt;&gt; split_pvalue(0.99)\n(9.9, -1)\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def split_pvalue(pvalue: float) -&gt; tuple[float, int]:\n    \"\"\"Convert a float to 10 based exponent and mantissa.\n\n    Args:\n        pvalue (float): p-value\n\n    Returns:\n        tuple[float, int]: Tuple with mantissa and exponent\n\n    Raises:\n        ValueError: If p-value is not between 0 and 1\n\n    Examples:\n        &gt;&gt;&gt; split_pvalue(0.00001234)\n        (1.234, -5)\n\n        &gt;&gt;&gt; split_pvalue(1)\n        (1.0, 0)\n\n        &gt;&gt;&gt; split_pvalue(0.123)\n        (1.23, -1)\n\n        &gt;&gt;&gt; split_pvalue(0.99)\n        (9.9, -1)\n    \"\"\"\n    if pvalue &lt; 0.0 or pvalue &gt; 1.0:\n        raise ValueError(\"P-value must be between 0 and 1\")\n\n    exponent = floor(log10(pvalue)) if pvalue != 0 else 0\n    mantissa = round(pvalue / 10**exponent, 3)\n    return (mantissa, exponent)\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.split_pvalue_column","title":"<code>split_pvalue_column(pv: Column) -&gt; PValComponents</code>","text":"<p>This function takes a p-value string and returns two columns mantissa (float), exponent (integer).</p> <p>Parameters:</p> Name Type Description Default <code>pv</code> <code>Column</code> <p>P-value as string</p> required <p>Returns:</p> Name Type Description <code>PValComponents</code> <code>PValComponents</code> <p>pValueMantissa (float), pValueExponent (integer)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = [(\"0.01\",),(\"4.2E-45\",),(\"43.2E5\",),(\"0\",),(\"1\",)]\n&gt;&gt;&gt; spark.createDataFrame(d, ['pval']).select('pval',*split_pvalue_column(f.col('pval'))).show()\n+-------+--------------+--------------+\n|   pval|pValueMantissa|pValueExponent|\n+-------+--------------+--------------+\n|   0.01|           1.0|            -2|\n|4.2E-45|           4.2|           -45|\n| 43.2E5|          43.2|             5|\n|      0|         2.225|          -308|\n|      1|           1.0|             0|\n+-------+--------------+--------------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def split_pvalue_column(pv: Column) -&gt; PValComponents:\n    \"\"\"This function takes a p-value string and returns two columns mantissa (float), exponent (integer).\n\n    Args:\n        pv (Column): P-value as string\n\n    Returns:\n        PValComponents: pValueMantissa (float), pValueExponent (integer)\n\n    Examples:\n        &gt;&gt;&gt; d = [(\"0.01\",),(\"4.2E-45\",),(\"43.2E5\",),(\"0\",),(\"1\",)]\n        &gt;&gt;&gt; spark.createDataFrame(d, ['pval']).select('pval',*split_pvalue_column(f.col('pval'))).show()\n        +-------+--------------+--------------+\n        |   pval|pValueMantissa|pValueExponent|\n        +-------+--------------+--------------+\n        |   0.01|           1.0|            -2|\n        |4.2E-45|           4.2|           -45|\n        | 43.2E5|          43.2|             5|\n        |      0|         2.225|          -308|\n        |      1|           1.0|             0|\n        +-------+--------------+--------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    # Making sure there's a number in the string:\n    pv = f.when(\n        pv == f.lit(\"0\"), f.lit(sys.float_info.min).cast(t.StringType())\n    ).otherwise(pv)\n\n    # Get exponent:\n    exponent = f.when(\n        f.upper(pv).contains(\"E\"),\n        f.split(f.upper(pv), \"E\").getItem(1),\n    ).otherwise(f.floor(f.log10(pv)))\n\n    # Get mantissa:\n    mantissa = f.when(\n        f.upper(pv).contains(\"E\"),\n        f.split(f.upper(pv), \"E\").getItem(0),\n    ).otherwise(pv / (10**exponent))\n\n    # Round value:\n    mantissa = f.round(mantissa, 3)\n\n    return PValComponents(\n        mantissa=mantissa.cast(t.FloatType()).alias(\"pValueMantissa\"),\n        exponent=exponent.cast(t.IntegerType()).alias(\"pValueExponent\"),\n    )\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.stderr_from_chi2_and_effect_size","title":"<code>stderr_from_chi2_and_effect_size(chi2_col: Column, beta: Column) -&gt; Column</code>","text":"<p>Calculate standard error from chi2 and beta.</p> <p>This function calculates the standard error from the chi2 value and beta.</p> <p>Parameters:</p> Name Type Description Default <code>chi2_col</code> <code>Column</code> <p>Chi2 value (float)</p> required <code>beta</code> <code>Column</code> <p>Beta value (float)</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Standard error (float)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(29.72, 3.0), (3.84, 1.0)]\n&gt;&gt;&gt; schema = \"chi2 FLOAT, beta FLOAT\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show()\n+-----+----+\n| chi2|beta|\n+-----+----+\n|29.72| 3.0|\n| 3.84| 1.0|\n+-----+----+\n</code></pre> <pre><code>&gt;&gt;&gt; chi2_col = f.col(\"chi2\")\n&gt;&gt;&gt; beta = f.col(\"beta\")\n&gt;&gt;&gt; standard_error = f.round(stderr_from_chi2_and_effect_size(chi2_col, beta), 2).alias(\"standardError\")\n&gt;&gt;&gt; df2 = df.select(chi2_col, beta, standard_error)\n&gt;&gt;&gt; df2.show()\n+-----+----+-------------+\n| chi2|beta|standardError|\n+-----+----+-------------+\n|29.72| 3.0|         0.55|\n| 3.84| 1.0|         0.51|\n+-----+----+-------------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def stderr_from_chi2_and_effect_size(chi2_col: Column, beta: Column) -&gt; Column:\n    \"\"\"Calculate standard error from chi2 and beta.\n\n    This function calculates the standard error from the chi2 value and beta.\n\n    Args:\n        chi2_col (Column): Chi2 value (float)\n        beta (Column): Beta value (float)\n\n    Returns:\n        Column: Standard error (float)\n\n    Examples:\n        &gt;&gt;&gt; data = [(29.72, 3.0), (3.84, 1.0)]\n        &gt;&gt;&gt; schema = \"chi2 FLOAT, beta FLOAT\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show()\n        +-----+----+\n        | chi2|beta|\n        +-----+----+\n        |29.72| 3.0|\n        | 3.84| 1.0|\n        +-----+----+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; chi2_col = f.col(\"chi2\")\n        &gt;&gt;&gt; beta = f.col(\"beta\")\n        &gt;&gt;&gt; standard_error = f.round(stderr_from_chi2_and_effect_size(chi2_col, beta), 2).alias(\"standardError\")\n        &gt;&gt;&gt; df2 = df.select(chi2_col, beta, standard_error)\n        &gt;&gt;&gt; df2.show()\n        +-----+----+-------------+\n        | chi2|beta|standardError|\n        +-----+----+-------------+\n        |29.72| 3.0|         0.55|\n        | 3.84| 1.0|         0.51|\n        +-----+----+-------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return (f.abs(beta) / f.sqrt(chi2_col)).alias(\"standardError\")\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.stderr_from_ci","title":"<code>stderr_from_ci(ci_upper: Column, ci_lower: Column, odds_ratio_based: bool = True) -&gt; Column</code>","text":"<p>Calculate standard error from confidence interval.</p> <p>This function calculates the standard error from the confidence interval upper and lower bounds.</p> <p>Parameters:</p> Name Type Description Default <code>ci_upper</code> <code>Column</code> <p>Upper bound of the confidence interval (float)</p> required <code>ci_lower</code> <code>Column</code> <p>Lower bound of the confidence interval (float)</p> required <code>odds_ratio_based</code> <code>bool</code> <p>If True, the function assumes that the confidence interval is based on odds ratio. use log difference (default), else it assumes that the confidence interval is based on beta.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Standard error (float)</p> Note <p>Absolute value of the log difference is used to ensure that the standard error is always positive, even if the ci bounds are inverted.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(0.5, 0.1), (1.0, 0.5)]\n&gt;&gt;&gt; schema = \"ci_upper FLOAT, ci_lower FLOAT\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show()\n+--------+--------+\n|ci_upper|ci_lower|\n+--------+--------+\n|     0.5|     0.1|\n|     1.0|     0.5|\n+--------+--------+\n</code></pre> <pre><code>&gt;&gt;&gt; ci_upper = f.col(\"ci_upper\")\n&gt;&gt;&gt; ci_lower = f.col(\"ci_lower\")\n&gt;&gt;&gt; standard_error = f.round(stderr_from_ci(ci_upper, ci_lower), 2).alias(\"standardError\")\n&gt;&gt;&gt; df2 = df.select(ci_upper, ci_lower, standard_error)\n&gt;&gt;&gt; df2.show()\n+--------+--------+-------------+\n|ci_upper|ci_lower|standardError|\n+--------+--------+-------------+\n|     0.5|     0.1|         0.41|\n|     1.0|     0.5|         0.18|\n+--------+--------+-------------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def stderr_from_ci(\n    ci_upper: Column, ci_lower: Column, odds_ratio_based: bool = True\n) -&gt; Column:\n    \"\"\"Calculate standard error from confidence interval.\n\n    This function calculates the standard error from the confidence interval upper and lower bounds.\n\n    Args:\n        ci_upper (Column): Upper bound of the confidence interval (float)\n        ci_lower (Column): Lower bound of the confidence interval (float)\n        odds_ratio_based (bool): If True, the function assumes that the confidence interval is based on odds ratio.\n            use log difference (default), else it assumes that the confidence interval is based on beta.\n\n    Returns:\n        Column: Standard error (float)\n\n    Note:\n        Absolute value of the log difference is used to ensure that the standard error is always positive, even if the ci bounds are inverted.\n\n\n    Examples:\n        &gt;&gt;&gt; data = [(0.5, 0.1), (1.0, 0.5)]\n        &gt;&gt;&gt; schema = \"ci_upper FLOAT, ci_lower FLOAT\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show()\n        +--------+--------+\n        |ci_upper|ci_lower|\n        +--------+--------+\n        |     0.5|     0.1|\n        |     1.0|     0.5|\n        +--------+--------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; ci_upper = f.col(\"ci_upper\")\n        &gt;&gt;&gt; ci_lower = f.col(\"ci_lower\")\n        &gt;&gt;&gt; standard_error = f.round(stderr_from_ci(ci_upper, ci_lower), 2).alias(\"standardError\")\n        &gt;&gt;&gt; df2 = df.select(ci_upper, ci_lower, standard_error)\n        &gt;&gt;&gt; df2.show()\n        +--------+--------+-------------+\n        |ci_upper|ci_lower|standardError|\n        +--------+--------+-------------+\n        |     0.5|     0.1|         0.41|\n        |     1.0|     0.5|         0.18|\n        +--------+--------+-------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    if odds_ratio_based:\n        return (f.abs(f.log(ci_upper) - f.log(ci_lower)) / (2 * 1.96)).alias(\n            \"standardError\"\n        )\n    return (f.abs(ci_upper - ci_lower) / (2 * 1.96)).alias(\"standardError\")\n</code></pre>"},{"location":"python_api/common/stats/#gentropy.common.stats.zscore_from_pvalue","title":"<code>zscore_from_pvalue(pval_col: Column, beta: Column) -&gt; Column</code>","text":"<p>Convert p-value column to z-score column.</p> <p>Parameters:</p> Name Type Description Default <code>pval_col</code> <code>Column</code> <p>p-value</p> required <code>beta</code> <code>Column</code> <p>Effect size in beta - used to derive the sign of the z-score.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>p-values transformed to z-scores</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"1.0\", -1.0), (\"0.9\", -1.0), (\"0.05\", 1.0), (\"1e-300\", 1.0), (\"1e-1000\", None), (None, 1.0)]\n&gt;&gt;&gt; schema = \"pval STRING, beta FLOAT\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show()\n+-------+----+\n|   pval|beta|\n+-------+----+\n|    1.0|-1.0|\n|    0.9|-1.0|\n|   0.05| 1.0|\n| 1e-300| 1.0|\n|1e-1000|NULL|\n|   NULL| 1.0|\n+-------+----+\n</code></pre> <pre><code>&gt;&gt;&gt; df.withColumn(\"zscore\", f.round(zscore_from_pvalue(f.col(\"pval\"), f.col(\"beta\")), 2)).show()\n+-------+----+------+\n|   pval|beta|zscore|\n+-------+----+------+\n|    1.0|-1.0|   0.0|\n|    0.9|-1.0| -0.13|\n|   0.05| 1.0|  1.96|\n| 1e-300| 1.0| 37.07|\n|1e-1000|NULL| 67.75|\n|   NULL| 1.0|  NULL|\n+-------+----+------+\n</code></pre> Source code in <code>src/gentropy/common/stats.py</code> <pre><code>def zscore_from_pvalue(pval_col: Column, beta: Column) -&gt; Column:\n    \"\"\"Convert p-value column to z-score column.\n\n    Args:\n        pval_col (Column): p-value\n        beta (Column): Effect size in beta - used to derive the sign of the z-score.\n\n    Returns:\n        Column: p-values transformed to z-scores\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"1.0\", -1.0), (\"0.9\", -1.0), (\"0.05\", 1.0), (\"1e-300\", 1.0), (\"1e-1000\", None), (None, 1.0)]\n        &gt;&gt;&gt; schema = \"pval STRING, beta FLOAT\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show()\n        +-------+----+\n        |   pval|beta|\n        +-------+----+\n        |    1.0|-1.0|\n        |    0.9|-1.0|\n        |   0.05| 1.0|\n        | 1e-300| 1.0|\n        |1e-1000|NULL|\n        |   NULL| 1.0|\n        +-------+----+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df.withColumn(\"zscore\", f.round(zscore_from_pvalue(f.col(\"pval\"), f.col(\"beta\")), 2)).show()\n        +-------+----+------+\n        |   pval|beta|zscore|\n        +-------+----+------+\n        |    1.0|-1.0|   0.0|\n        |    0.9|-1.0| -0.13|\n        |   0.05| 1.0|  1.96|\n        | 1e-300| 1.0| 37.07|\n        |1e-1000|NULL| 67.75|\n        |   NULL| 1.0|  NULL|\n        +-------+----+------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    mantissa, exponent = split_pvalue_column(pval_col)\n    sign = (\n        f.when(beta &gt; 0, f.lit(1))\n        .when(beta &lt; 0, f.lit(-1))\n        .when(beta.isNull(), f.lit(1))\n    )\n    return (sign * f.sqrt(chi2_from_pvalue(mantissa, exponent))).alias(\"zscore\")\n</code></pre>"},{"location":"python_api/common/types/","title":"types","text":""},{"location":"python_api/common/types/#gentropy.common.types","title":"<code>gentropy.common.types</code>","text":"<p>Types and type aliases used in the package.</p>"},{"location":"python_api/common/types/#gentropy.common.types.GWASEffect","title":"<code>GWASEffect</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Components of GWAS effect.</p> <p>Attributes:</p> Name Type Description <code>beta</code> <code>Column</code> <p>Effect.</p> <code>standard_error</code> <code>Column</code> <p>Effect standard error.</p> Source code in <code>src/gentropy/common/types.py</code> <pre><code>class GWASEffect(NamedTuple):\n    \"\"\"Components of GWAS effect.\n\n    Attributes:\n        beta (Column): Effect.\n        standard_error (Column): Effect standard error.\n    \"\"\"\n\n    beta: Column\n    standard_error: Column\n</code></pre>"},{"location":"python_api/common/types/#gentropy.common.types.PValComponents","title":"<code>PValComponents</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Components of p-value.</p> <p>Attributes:</p> Name Type Description <code>mantissa</code> <code>Column</code> <p>Mantissa of the p-value.</p> <code>exponent</code> <code>Column</code> <p>Exponent of the p-value.</p> Source code in <code>src/gentropy/common/types.py</code> <pre><code>class PValComponents(NamedTuple):\n    \"\"\"Components of p-value.\n\n    Attributes:\n        mantissa (Column): Mantissa of the p-value.\n        exponent (Column): Exponent of the p-value.\n    \"\"\"\n\n    mantissa: Column\n    exponent: Column\n</code></pre>"},{"location":"python_api/common/udf/","title":"udf","text":""},{"location":"python_api/common/udf/#gentropy.common.udf","title":"<code>gentropy.common.udf</code>","text":"<p>Common UDF functions.</p> <p>The functions in this module are designed to be run as PySpark UDFs with parallel execution provided by pandas and numpy.</p> Note <p>The decorated function(s) in this module with signature f(pd.Series) -&gt; pd.Series has to be annotated with the functionType parameter, as it is currently only way to distinguish between many -&gt; many and one -&gt; one UDFs. The API is not consistent between different pyspark versions, next versions may deprecate the functionType parameter in advance of python type hints. See - https://issues.apache.org/jira/browse/SPARK-28264 and the draft for new API - https://docs.google.com/document/d/1-kV0FS_LF2zvaRh_GhkV32Uqksm_Sq8SvnBBmRyxm30/edit?tab=t.0.</p>"},{"location":"python_api/common/udf/#gentropy.common.udf.chi2_inverse_survival_function","title":"<code>chi2_inverse_survival_function(x: pd.Series) -&gt; pd.Series</code>","text":"<p>Calculate the inverse survival function of the chi2 distribution with 1 degree of freedom.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>A pandas Series containing p-values (between 0 and 1).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A pandas Series containing the chi2 statistic corresponding to the input p-values.</p> <p>Get the chi2 statistic for a given p-value (x).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(0.1,), (0.05,), (0.001,)]\n&gt;&gt;&gt; schema = \"pValue Float\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema=schema)\n&gt;&gt;&gt; df.show()\n+------+\n|pValue|\n+------+\n|   0.1|\n|  0.05|\n| 0.001|\n+------+\n</code></pre> <pre><code>&gt;&gt;&gt; import pyspark.sql.functions as f\n&gt;&gt;&gt; chi2 = f.round(chi2_inverse_survival_function(\"pValue\"), 2).alias(\"chi2_stat\")\n&gt;&gt;&gt; df.select(\"pValue\", chi2).show()\n+------+---------+\n|pValue|chi2_stat|\n+------+---------+\n|   0.1|     2.71|\n|  0.05|     3.84|\n| 0.001|    10.83|\n+------+---------+\n</code></pre> Source code in <code>src/gentropy/common/udf.py</code> <pre><code>@pandas_udf(returnType=t.DoubleType(), functionType=PandasUDFType.SCALAR)\ndef chi2_inverse_survival_function(x: pd.Series) -&gt; pd.Series:\n    \"\"\"Calculate the inverse survival function of the chi2 distribution with 1 degree of freedom.\n\n    Args:\n        x (pd.Series): A pandas Series containing p-values (between 0 and 1).\n\n    Returns:\n        pd.Series: A pandas Series containing the chi2 statistic corresponding to the input p-values.\n\n    Get the chi2 statistic for a given p-value (x).\n\n    Examples:\n        &gt;&gt;&gt; data = [(0.1,), (0.05,), (0.001,)]\n        &gt;&gt;&gt; schema = \"pValue Float\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema=schema)\n        &gt;&gt;&gt; df.show()\n        +------+\n        |pValue|\n        +------+\n        |   0.1|\n        |  0.05|\n        | 0.001|\n        +------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; import pyspark.sql.functions as f\n        &gt;&gt;&gt; chi2 = f.round(chi2_inverse_survival_function(\"pValue\"), 2).alias(\"chi2_stat\")\n        &gt;&gt;&gt; df.select(\"pValue\", chi2).show()\n        +------+---------+\n        |pValue|chi2_stat|\n        +------+---------+\n        |   0.1|     2.71|\n        |  0.05|     3.84|\n        | 0.001|    10.83|\n        +------+---------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return pd.Series(chi2.isf(x, df=1).astype(np.float64))\n</code></pre>"},{"location":"python_api/common/udf/#gentropy.common.udf.chi2_survival_function","title":"<code>chi2_survival_function(x: pd.Series) -&gt; pd.Series</code>","text":"<p>Calculate the survival function of the chi2 distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>A pandas Series containing chi2 statistics or z-scores squared.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A pandas Series containing the p-values corresponding to the input chi2 statistics or z-scores squared.</p> <p>Useful to convert the z-score^2 or chi2 statistic to a p-value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(1.0, 1.0), (-1.0, 1.0), (10.0, 100.0)]\n&gt;&gt;&gt; schema = \"zScore Float, chi2 Float\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema=schema)\n&gt;&gt;&gt; df.show()\n+------+-----+\n|zScore| chi2|\n+------+-----+\n|   1.0|  1.0|\n|  -1.0|  1.0|\n|  10.0|100.0|\n+------+-----+\n</code></pre> <pre><code>&gt;&gt;&gt; import pyspark.sql.functions as f\n&gt;&gt;&gt; pval_from_z2 = f.round(chi2_survival_function(f.col(\"zScore\")**2), 2).alias(\"pValueZscore^2\")\n&gt;&gt;&gt; pval_from_chi2 = f.round(chi2_survival_function(\"chi2\"), 2).alias(\"pValueChi2\")\n&gt;&gt;&gt; df.select(\"zScore\", pval_from_z2, \"chi2\", pval_from_chi2).show()\n+------+--------------+-----+----------+\n|zScore|pValueZscore^2| chi2|pValueChi2|\n+------+--------------+-----+----------+\n|   1.0|          0.32|  1.0|      0.32|\n|  -1.0|          0.32|  1.0|      0.32|\n|  10.0|           0.0|100.0|       0.0|\n+------+--------------+-----+----------+\n</code></pre> Source code in <code>src/gentropy/common/udf.py</code> <pre><code>@pandas_udf(returnType=t.DoubleType(), functionType=PandasUDFType.SCALAR)\ndef chi2_survival_function(x: pd.Series) -&gt; pd.Series:\n    \"\"\"Calculate the survival function of the chi2 distribution.\n\n    Args:\n        x (pd.Series): A pandas Series containing chi2 statistics or z-scores squared.\n\n    Returns:\n        pd.Series: A pandas Series containing the p-values corresponding to the input chi2 statistics or z-scores squared.\n\n    Useful to convert the z-score^2 or chi2 statistic to a p-value.\n\n    Examples:\n        &gt;&gt;&gt; data = [(1.0, 1.0), (-1.0, 1.0), (10.0, 100.0)]\n        &gt;&gt;&gt; schema = \"zScore Float, chi2 Float\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema=schema)\n        &gt;&gt;&gt; df.show()\n        +------+-----+\n        |zScore| chi2|\n        +------+-----+\n        |   1.0|  1.0|\n        |  -1.0|  1.0|\n        |  10.0|100.0|\n        +------+-----+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; import pyspark.sql.functions as f\n        &gt;&gt;&gt; pval_from_z2 = f.round(chi2_survival_function(f.col(\"zScore\")**2), 2).alias(\"pValueZscore^2\")\n        &gt;&gt;&gt; pval_from_chi2 = f.round(chi2_survival_function(\"chi2\"), 2).alias(\"pValueChi2\")\n        &gt;&gt;&gt; df.select(\"zScore\", pval_from_z2, \"chi2\", pval_from_chi2).show()\n        +------+--------------+-----+----------+\n        |zScore|pValueZscore^2| chi2|pValueChi2|\n        +------+--------------+-----+----------+\n        |   1.0|          0.32|  1.0|      0.32|\n        |  -1.0|          0.32|  1.0|      0.32|\n        |  10.0|           0.0|100.0|       0.0|\n        +------+--------------+-----+----------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return pd.Series(chi2.sf(x, df=1).astype(np.float64))\n</code></pre>"},{"location":"python_api/datasets/_datasets/","title":"Datasets","text":"<p>The Dataset classes define the data model behind Open Targets Gentropy. Every class inherits from the <code>Dataset</code> class and contains a dataframe with a predefined schema that can be found in the respective classes.</p>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset","title":"<code>gentropy.dataset.dataset.Dataset</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Open Targets Gentropy Dataset Interface.</p> <p>The <code>Dataset</code> interface is a wrapper around a Spark DataFrame with a predefined schema. Class allows for overwriting the schema with <code>_schema</code> parameter. If the <code>_schema</code> is not provided, the schema is inferred from the Dataset.get_schema specific method which must be implemented by the child classes.</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@dataclass\nclass Dataset(ABC):\n    \"\"\"Open Targets Gentropy Dataset Interface.\n\n    The `Dataset` interface is a wrapper around a Spark DataFrame with a predefined schema.\n    Class allows for overwriting the schema with `_schema` parameter.\n    If the `_schema` is not provided, the schema is inferred from the Dataset.get_schema specific\n    method which must be implemented by the child classes.\n    \"\"\"\n\n    _df: DataFrame\n    _schema: StructType | None = None\n\n    def __post_init__(self: Dataset) -&gt; None:\n        \"\"\"Post init.\n\n        Raises:\n            TypeError: If the type of the _df or _schema is not valid\n        \"\"\"\n        match self._df:\n            case DataFrame():\n                pass\n            case _:\n                raise TypeError(f\"Invalid type for _df: {type(self._df)}\")\n\n        match self._schema:\n            case None | t.StructType():\n                self.validate_schema()\n            case _:\n                raise TypeError(f\"Invalid type for _schema: {type(self._schema)}\")\n\n    @property\n    def df(self: Dataset) -&gt; DataFrame:\n        \"\"\"Dataframe included in the Dataset.\n\n        Returns:\n            DataFrame: Dataframe included in the Dataset\n        \"\"\"\n        return self._df\n\n    @df.setter\n    def df(self: Dataset, new_df: DataFrame) -&gt; None:  # noqa: CCE001\n        \"\"\"Dataframe setter.\n\n        Args:\n            new_df (DataFrame): New dataframe to be included in the Dataset\n        \"\"\"\n        self._df: DataFrame = new_df\n        self.validate_schema()\n\n    @property\n    def schema(self: Dataset) -&gt; StructType:\n        \"\"\"Dataframe expected schema.\n\n        Returns:\n            StructType: Dataframe expected schema\n        \"\"\"\n        return self._schema or self.get_schema()\n\n    @classmethod\n    def _process_class_params(\n        cls, params: dict[str, Any]\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"Separate class initialization parameters from spark session parameters.\n\n        Args:\n            params (dict[str, Any]): Combined parameters dictionary\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: (class_params, spark_params)\n        \"\"\"\n        # Get all field names from the class (including parent classes)\n        class_field_names = {\n            field.name\n            for cls_ in cls.__mro__\n            if hasattr(cls_, \"__dataclass_fields__\")\n            for field in cls_.__dataclass_fields__.values()\n        }\n        # Separate parameters\n        class_params = {k: v for k, v in params.items() if k in class_field_names}\n        spark_params = {k: v for k, v in params.items() if k not in class_field_names}\n        return class_params, spark_params\n\n    @classmethod\n    @abstractmethod\n    def get_schema(cls: type[Self]) -&gt; StructType:\n        \"\"\"Abstract method to get the schema. Must be implemented by child classes.\n\n        Returns:\n            StructType: Schema for the Dataset\n\n        Raises:\n                NotImplementedError: Must be implemented in the child classes\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in the child classes\")\n\n    @classmethod\n    def get_QC_column_name(cls: type[Self]) -&gt; str | None:\n        \"\"\"Abstract method to get the QC column name. Assumes None unless overriden by child classes.\n\n        Returns:\n            str | None: Column name\n        \"\"\"\n        return None\n\n    @classmethod\n    def get_QC_mappings(cls: type[Self]) -&gt; dict[str, str]:\n        \"\"\"Method to get the mapping between QC flag and corresponding QC category value.\n\n        Returns empty dict unless overriden by child classes.\n\n        Returns:\n            dict[str, str]: Mapping between flag name and QC column category value.\n        \"\"\"\n        return {}\n\n    @classmethod\n    def from_parquet(\n        cls: type[Self],\n        session: Session,\n        path: str | list[str],\n        **kwargs: bool | float | int | str | None,\n    ) -&gt; Self:\n        \"\"\"Reads parquet into a Dataset with a given schema.\n\n        Args:\n            session (Session): Spark session\n            path (str | list[str]): Path to the parquet dataset\n            **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.parquet\n\n        Returns:\n            Self: Dataset with the parquet file contents\n\n        Raises:\n            ValueError: Parquet file is empty\n        \"\"\"\n        schema = cls.get_schema()\n\n        # Separate class params from spark params\n        class_params, spark_params = cls._process_class_params(kwargs)\n\n        df = session.load_data(path, format=\"parquet\", schema=schema, **spark_params)\n        if df.isEmpty():\n            raise ValueError(f\"Parquet file is empty: {path}\")\n        return cls(_df=df, _schema=schema, **class_params)\n\n    def filter(self: Self, condition: Column) -&gt; Self:\n        \"\"\"Creates a new instance of a Dataset with the DataFrame filtered by the condition.\n\n        Preserves all attributes from the original instance.\n\n        Args:\n            condition (Column): Condition to filter the DataFrame\n\n        Returns:\n            Self: Filtered Dataset with preserved attributes\n        \"\"\"\n        filtered_df = self._df.filter(condition)\n        attrs = {k: v for k, v in self.__dict__.items() if k != \"_df\"}\n        return self.__class__(_df=filtered_df, **attrs)\n\n    def validate_schema(self: Dataset) -&gt; None:\n        \"\"\"Validate DataFrame schema against expected class schema.\n\n        Raises:\n            SchemaValidationError: If the DataFrame schema does not match the expected schema\n        \"\"\"\n        expected_schema = self.schema\n        observed_schema = self._df.schema\n\n        # Unexpected fields in dataset\n        if discrepancies := compare_struct_schemas(observed_schema, expected_schema):\n            raise SchemaValidationError(\n                f\"Schema validation failed for {type(self).__name__}\", discrepancies\n            )\n\n    def valid_rows(self: Self, invalid_flags: list[str], invalid: bool = False) -&gt; Self:\n        \"\"\"Filters `Dataset` according to a list of quality control flags. Only `Dataset` classes with a QC column can be validated.\n\n        This method checks do following steps:\n        - Check if the Dataset contains a QC column.\n        - Check if the invalid_flags exist in the QC mappings flags.\n        - Filter the Dataset according to the invalid_flags and invalid parameters.\n\n        Args:\n            invalid_flags (list[str]): List of quality control flags to be excluded.\n            invalid (bool): If True returns the invalid rows, instead of the valid. Defaults to False.\n\n        Returns:\n            Self: filtered dataset.\n\n        Raises:\n            ValueError: If the Dataset does not contain a QC column or if the invalid_flags elements do not exist in QC mappings flags.\n        \"\"\"\n        # If the invalid flags are not valid quality checks (enum) for this Dataset we raise an error:\n        invalid_reasons = []\n        for flag in invalid_flags:\n            if flag not in self.get_QC_mappings():\n                raise ValueError(\n                    f\"{flag} is not a valid QC flag for {type(self).__name__} ({self.get_QC_mappings()}).\"\n                )\n            reason = self.get_QC_mappings()[flag]\n            invalid_reasons.append(reason)\n\n        qc_column_name = self.get_QC_column_name()\n        # If Dataset (class) does not contain QC column we raise an error:\n        if not qc_column_name:\n            raise ValueError(\n                f\"{type(self).__name__} objects do not contain a QC column to filter by.\"\n            )\n        else:\n            column: str = qc_column_name\n            # If QC column (nullable) is not available in the dataframe we create an empty array:\n            qc = f.when(f.col(column).isNull(), f.array()).otherwise(f.col(column))\n\n        filterCondition = ~f.arrays_overlap(\n            f.array([f.lit(i) for i in invalid_reasons]), qc\n        )\n        # Returning the filtered dataset:\n        if invalid:\n            return self.filter(~filterCondition)\n        else:\n            return self.filter(filterCondition)\n\n    def drop_infinity_values(self: Self, *cols: str) -&gt; Self:\n        \"\"\"Drop infinity values from Double typed column.\n\n        Infinity type reference - https://spark.apache.org/docs/latest/sql-ref-datatypes.html#floating-point-special-values\n        The implementation comes from https://stackoverflow.com/questions/34432998/how-to-replace-infinity-in-pyspark-dataframe\n\n        Args:\n            *cols (str): names of the columns to check for infinite values, these should be of DoubleType only!\n\n        Returns:\n            Self: Dataset after removing infinite values\n        \"\"\"\n        if len(cols) == 0:\n            return self\n        inf_strings = (\"Inf\", \"+Inf\", \"-Inf\", \"Infinity\", \"+Infinity\", \"-Infinity\")\n        inf_values = [f.lit(v).cast(t.DoubleType()) for v in inf_strings]\n        conditions = [f.col(c).isin(inf_values) for c in cols]\n        # reduce individual filter expressions with or statement\n        # to col(\"beta\").isin([lit(Inf)]) | col(\"beta\").isin([lit(Inf)])...\n        condition = reduce(lambda a, b: a | b, conditions)\n        self.df = self._df.filter(~condition)\n        return self\n\n    def persist(self: Self) -&gt; Self:\n        \"\"\"Persist in memory the DataFrame included in the Dataset.\n\n        Returns:\n            Self: Persisted Dataset\n        \"\"\"\n        self.df = self._df.persist()\n        return self\n\n    def unpersist(self: Self) -&gt; Self:\n        \"\"\"Remove the persisted DataFrame from memory.\n\n        Returns:\n            Self: Unpersisted Dataset\n        \"\"\"\n        self.df = self._df.unpersist()\n        return self\n\n    def coalesce(self: Self, num_partitions: int, **kwargs: Any) -&gt; Self:\n        \"\"\"Coalesce the DataFrame included in the Dataset.\n\n        Coalescing is efficient for decreasing the number of partitions because it avoids a full shuffle of the data.\n\n        Args:\n            num_partitions (int): Number of partitions to coalesce to\n            **kwargs (Any): Arguments to pass to the coalesce method\n\n        Returns:\n            Self: Coalesced Dataset\n        \"\"\"\n        self.df = self._df.coalesce(num_partitions, **kwargs)\n        return self\n\n    def repartition(self: Self, num_partitions: int, **kwargs: Any) -&gt; Self:\n        \"\"\"Repartition the DataFrame included in the Dataset.\n\n        Repartitioning creates new partitions with data that is distributed evenly.\n\n        Args:\n            num_partitions (int): Number of partitions to repartition to\n            **kwargs (Any): Arguments to pass to the repartition method\n\n        Returns:\n            Self: Repartitioned Dataset\n        \"\"\"\n        self.df = self._df.repartition(num_partitions, **kwargs)\n        return self\n\n    @staticmethod\n    def update_quality_flag(\n        qc: Column, flag_condition: Column, flag_text: Enum\n    ) -&gt; Column:\n        \"\"\"Update the provided quality control list with a new flag if condition is met.\n\n        Args:\n            qc (Column): Array column with the current list of qc flags.\n            flag_condition (Column): This is a column of booleans, signing which row should be flagged\n            flag_text (Enum): Text for the new quality control flag\n\n        Returns:\n            Column: Array column with the updated list of qc flags.\n\n\n        Examples:\n            &gt;&gt;&gt; s = \"study STRING, qualityControls ARRAY&lt;STRING&gt;, condition BOOLEAN\"\n            &gt;&gt;&gt; d =  [(\"S1\", [\"qc1\"], True), (\"S2\", [\"qc3\"], False)]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, s)\n            &gt;&gt;&gt; df.show()\n            +-----+---------------+---------+\n            |study|qualityControls|condition|\n            +-----+---------------+---------+\n            |   S1|          [qc1]|     true|\n            |   S2|          [qc3]|    false|\n            +-----+---------------+---------+\n            &lt;BLANKLINE&gt;\n\n            &gt;&gt;&gt; class QC(Enum):\n            ...     QC1 = \"qc1\"\n            ...     QC2 = \"qc2\"\n            ...     QC3 = \"qc3\"\n\n            &gt;&gt;&gt; condition = f.col(\"condition\")\n            &gt;&gt;&gt; new_qc = Dataset.update_quality_flag(f.col(\"qualityControls\"), condition, QC.QC2)\n            &gt;&gt;&gt; df.withColumn(\"qualityControls\", new_qc).show()\n            +-----+---------------+---------+\n            |study|qualityControls|condition|\n            +-----+---------------+---------+\n            |   S1|     [qc1, qc2]|     true|\n            |   S2|          [qc3]|    false|\n            +-----+---------------+---------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        qc = f.when(qc.isNull(), f.array()).otherwise(qc)\n        return f.when(\n            flag_condition,\n            f.array_sort(\n                f.array_distinct(f.array_union(qc, f.array(f.lit(flag_text.value))))\n            ),\n        ).otherwise(qc)\n\n    @staticmethod\n    def flag_duplicates(test_column: Column) -&gt; Column:\n        \"\"\"Return True for rows, where the value was already seen in column.\n\n        This implementation allows keeping the first occurrence of the value.\n\n        Args:\n            test_column (Column): Column to check for duplicates\n\n        Returns:\n            Column: Column with a boolean flag for duplicates\n        \"\"\"\n        return (\n            f.row_number().over(Window.partitionBy(test_column).orderBy(f.rand())) &gt; 1\n        )\n\n    @staticmethod\n    def generate_identifier(uniqueness_defining_columns: list[str]) -&gt; Column:\n        \"\"\"Hashes the provided columns to generate a unique identifier.\n\n        Args:\n            uniqueness_defining_columns (list[str]): list of columns defining uniqueness\n\n        Returns:\n            Column: column with a unique identifier\n        \"\"\"\n        hashable_columns = [\n            f.when(f.col(column).cast(\"string\").isNull(), f.lit(\"None\")).otherwise(\n                f.col(column).cast(\"string\")\n            )\n            for column in uniqueness_defining_columns\n        ]\n        return f.md5(f.concat(*hashable_columns))\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.df","title":"<code>df: DataFrame</code>  <code>property</code> <code>writable</code>","text":"<p>Dataframe included in the Dataset.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Dataframe included in the Dataset</p>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.schema","title":"<code>schema: StructType</code>  <code>property</code>","text":"<p>Dataframe expected schema.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Dataframe expected schema</p>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.coalesce","title":"<code>coalesce(num_partitions: int, **kwargs: Any) -&gt; Self</code>","text":"<p>Coalesce the DataFrame included in the Dataset.</p> <p>Coalescing is efficient for decreasing the number of partitions because it avoids a full shuffle of the data.</p> <p>Parameters:</p> Name Type Description Default <code>num_partitions</code> <code>int</code> <p>Number of partitions to coalesce to</p> required <code>**kwargs</code> <code>Any</code> <p>Arguments to pass to the coalesce method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Coalesced Dataset</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def coalesce(self: Self, num_partitions: int, **kwargs: Any) -&gt; Self:\n    \"\"\"Coalesce the DataFrame included in the Dataset.\n\n    Coalescing is efficient for decreasing the number of partitions because it avoids a full shuffle of the data.\n\n    Args:\n        num_partitions (int): Number of partitions to coalesce to\n        **kwargs (Any): Arguments to pass to the coalesce method\n\n    Returns:\n        Self: Coalesced Dataset\n    \"\"\"\n    self.df = self._df.coalesce(num_partitions, **kwargs)\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.drop_infinity_values","title":"<code>drop_infinity_values(*cols: str) -&gt; Self</code>","text":"<p>Drop infinity values from Double typed column.</p> <p>Infinity type reference - https://spark.apache.org/docs/latest/sql-ref-datatypes.html#floating-point-special-values The implementation comes from https://stackoverflow.com/questions/34432998/how-to-replace-infinity-in-pyspark-dataframe</p> <p>Parameters:</p> Name Type Description Default <code>*cols</code> <code>str</code> <p>names of the columns to check for infinite values, these should be of DoubleType only!</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Dataset after removing infinite values</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def drop_infinity_values(self: Self, *cols: str) -&gt; Self:\n    \"\"\"Drop infinity values from Double typed column.\n\n    Infinity type reference - https://spark.apache.org/docs/latest/sql-ref-datatypes.html#floating-point-special-values\n    The implementation comes from https://stackoverflow.com/questions/34432998/how-to-replace-infinity-in-pyspark-dataframe\n\n    Args:\n        *cols (str): names of the columns to check for infinite values, these should be of DoubleType only!\n\n    Returns:\n        Self: Dataset after removing infinite values\n    \"\"\"\n    if len(cols) == 0:\n        return self\n    inf_strings = (\"Inf\", \"+Inf\", \"-Inf\", \"Infinity\", \"+Infinity\", \"-Infinity\")\n    inf_values = [f.lit(v).cast(t.DoubleType()) for v in inf_strings]\n    conditions = [f.col(c).isin(inf_values) for c in cols]\n    # reduce individual filter expressions with or statement\n    # to col(\"beta\").isin([lit(Inf)]) | col(\"beta\").isin([lit(Inf)])...\n    condition = reduce(lambda a, b: a | b, conditions)\n    self.df = self._df.filter(~condition)\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.filter","title":"<code>filter(condition: Column) -&gt; Self</code>","text":"<p>Creates a new instance of a Dataset with the DataFrame filtered by the condition.</p> <p>Preserves all attributes from the original instance.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Column</code> <p>Condition to filter the DataFrame</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Filtered Dataset with preserved attributes</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def filter(self: Self, condition: Column) -&gt; Self:\n    \"\"\"Creates a new instance of a Dataset with the DataFrame filtered by the condition.\n\n    Preserves all attributes from the original instance.\n\n    Args:\n        condition (Column): Condition to filter the DataFrame\n\n    Returns:\n        Self: Filtered Dataset with preserved attributes\n    \"\"\"\n    filtered_df = self._df.filter(condition)\n    attrs = {k: v for k, v in self.__dict__.items() if k != \"_df\"}\n    return self.__class__(_df=filtered_df, **attrs)\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.flag_duplicates","title":"<code>flag_duplicates(test_column: Column) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Return True for rows, where the value was already seen in column.</p> <p>This implementation allows keeping the first occurrence of the value.</p> <p>Parameters:</p> Name Type Description Default <code>test_column</code> <code>Column</code> <p>Column to check for duplicates</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Column with a boolean flag for duplicates</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@staticmethod\ndef flag_duplicates(test_column: Column) -&gt; Column:\n    \"\"\"Return True for rows, where the value was already seen in column.\n\n    This implementation allows keeping the first occurrence of the value.\n\n    Args:\n        test_column (Column): Column to check for duplicates\n\n    Returns:\n        Column: Column with a boolean flag for duplicates\n    \"\"\"\n    return (\n        f.row_number().over(Window.partitionBy(test_column).orderBy(f.rand())) &gt; 1\n    )\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.from_parquet","title":"<code>from_parquet(session: Session, path: str | list[str], **kwargs: bool | float | int | str | None) -&gt; Self</code>  <code>classmethod</code>","text":"<p>Reads parquet into a Dataset with a given schema.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session</p> required <code>path</code> <code>str | list[str]</code> <p>Path to the parquet dataset</p> required <code>**kwargs</code> <code>bool | float | int | str | None</code> <p>Additional arguments to pass to spark.read.parquet</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Dataset with the parquet file contents</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Parquet file is empty</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@classmethod\ndef from_parquet(\n    cls: type[Self],\n    session: Session,\n    path: str | list[str],\n    **kwargs: bool | float | int | str | None,\n) -&gt; Self:\n    \"\"\"Reads parquet into a Dataset with a given schema.\n\n    Args:\n        session (Session): Spark session\n        path (str | list[str]): Path to the parquet dataset\n        **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.parquet\n\n    Returns:\n        Self: Dataset with the parquet file contents\n\n    Raises:\n        ValueError: Parquet file is empty\n    \"\"\"\n    schema = cls.get_schema()\n\n    # Separate class params from spark params\n    class_params, spark_params = cls._process_class_params(kwargs)\n\n    df = session.load_data(path, format=\"parquet\", schema=schema, **spark_params)\n    if df.isEmpty():\n        raise ValueError(f\"Parquet file is empty: {path}\")\n    return cls(_df=df, _schema=schema, **class_params)\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.generate_identifier","title":"<code>generate_identifier(uniqueness_defining_columns: list[str]) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Hashes the provided columns to generate a unique identifier.</p> <p>Parameters:</p> Name Type Description Default <code>uniqueness_defining_columns</code> <code>list[str]</code> <p>list of columns defining uniqueness</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>column with a unique identifier</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@staticmethod\ndef generate_identifier(uniqueness_defining_columns: list[str]) -&gt; Column:\n    \"\"\"Hashes the provided columns to generate a unique identifier.\n\n    Args:\n        uniqueness_defining_columns (list[str]): list of columns defining uniqueness\n\n    Returns:\n        Column: column with a unique identifier\n    \"\"\"\n    hashable_columns = [\n        f.when(f.col(column).cast(\"string\").isNull(), f.lit(\"None\")).otherwise(\n            f.col(column).cast(\"string\")\n        )\n        for column in uniqueness_defining_columns\n    ]\n    return f.md5(f.concat(*hashable_columns))\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.get_QC_column_name","title":"<code>get_QC_column_name() -&gt; str | None</code>  <code>classmethod</code>","text":"<p>Abstract method to get the QC column name. Assumes None unless overriden by child classes.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Column name</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@classmethod\ndef get_QC_column_name(cls: type[Self]) -&gt; str | None:\n    \"\"\"Abstract method to get the QC column name. Assumes None unless overriden by child classes.\n\n    Returns:\n        str | None: Column name\n    \"\"\"\n    return None\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.get_QC_mappings","title":"<code>get_QC_mappings() -&gt; dict[str, str]</code>  <code>classmethod</code>","text":"<p>Method to get the mapping between QC flag and corresponding QC category value.</p> <p>Returns empty dict unless overriden by child classes.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: Mapping between flag name and QC column category value.</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@classmethod\ndef get_QC_mappings(cls: type[Self]) -&gt; dict[str, str]:\n    \"\"\"Method to get the mapping between QC flag and corresponding QC category value.\n\n    Returns empty dict unless overriden by child classes.\n\n    Returns:\n        dict[str, str]: Mapping between flag name and QC column category value.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Abstract method to get the schema. Must be implemented by child classes.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the Dataset</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented in the child classes</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@classmethod\n@abstractmethod\ndef get_schema(cls: type[Self]) -&gt; StructType:\n    \"\"\"Abstract method to get the schema. Must be implemented by child classes.\n\n    Returns:\n        StructType: Schema for the Dataset\n\n    Raises:\n            NotImplementedError: Must be implemented in the child classes\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in the child classes\")\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.persist","title":"<code>persist() -&gt; Self</code>","text":"<p>Persist in memory the DataFrame included in the Dataset.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Persisted Dataset</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def persist(self: Self) -&gt; Self:\n    \"\"\"Persist in memory the DataFrame included in the Dataset.\n\n    Returns:\n        Self: Persisted Dataset\n    \"\"\"\n    self.df = self._df.persist()\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.repartition","title":"<code>repartition(num_partitions: int, **kwargs: Any) -&gt; Self</code>","text":"<p>Repartition the DataFrame included in the Dataset.</p> <p>Repartitioning creates new partitions with data that is distributed evenly.</p> <p>Parameters:</p> Name Type Description Default <code>num_partitions</code> <code>int</code> <p>Number of partitions to repartition to</p> required <code>**kwargs</code> <code>Any</code> <p>Arguments to pass to the repartition method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Repartitioned Dataset</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def repartition(self: Self, num_partitions: int, **kwargs: Any) -&gt; Self:\n    \"\"\"Repartition the DataFrame included in the Dataset.\n\n    Repartitioning creates new partitions with data that is distributed evenly.\n\n    Args:\n        num_partitions (int): Number of partitions to repartition to\n        **kwargs (Any): Arguments to pass to the repartition method\n\n    Returns:\n        Self: Repartitioned Dataset\n    \"\"\"\n    self.df = self._df.repartition(num_partitions, **kwargs)\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.unpersist","title":"<code>unpersist() -&gt; Self</code>","text":"<p>Remove the persisted DataFrame from memory.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Unpersisted Dataset</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def unpersist(self: Self) -&gt; Self:\n    \"\"\"Remove the persisted DataFrame from memory.\n\n    Returns:\n        Self: Unpersisted Dataset\n    \"\"\"\n    self.df = self._df.unpersist()\n    return self\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.update_quality_flag","title":"<code>update_quality_flag(qc: Column, flag_condition: Column, flag_text: Enum) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Update the provided quality control list with a new flag if condition is met.</p> <p>Parameters:</p> Name Type Description Default <code>qc</code> <code>Column</code> <p>Array column with the current list of qc flags.</p> required <code>flag_condition</code> <code>Column</code> <p>This is a column of booleans, signing which row should be flagged</p> required <code>flag_text</code> <code>Enum</code> <p>Text for the new quality control flag</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Array column with the updated list of qc flags.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s = \"study STRING, qualityControls ARRAY&lt;STRING&gt;, condition BOOLEAN\"\n&gt;&gt;&gt; d =  [(\"S1\", [\"qc1\"], True), (\"S2\", [\"qc3\"], False)]\n&gt;&gt;&gt; df = spark.createDataFrame(d, s)\n&gt;&gt;&gt; df.show()\n+-----+---------------+---------+\n|study|qualityControls|condition|\n+-----+---------------+---------+\n|   S1|          [qc1]|     true|\n|   S2|          [qc3]|    false|\n+-----+---------------+---------+\n</code></pre> <pre><code>&gt;&gt;&gt; class QC(Enum):\n...     QC1 = \"qc1\"\n...     QC2 = \"qc2\"\n...     QC3 = \"qc3\"\n</code></pre> <pre><code>&gt;&gt;&gt; condition = f.col(\"condition\")\n&gt;&gt;&gt; new_qc = Dataset.update_quality_flag(f.col(\"qualityControls\"), condition, QC.QC2)\n&gt;&gt;&gt; df.withColumn(\"qualityControls\", new_qc).show()\n+-----+---------------+---------+\n|study|qualityControls|condition|\n+-----+---------------+---------+\n|   S1|     [qc1, qc2]|     true|\n|   S2|          [qc3]|    false|\n+-----+---------------+---------+\n</code></pre> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>@staticmethod\ndef update_quality_flag(\n    qc: Column, flag_condition: Column, flag_text: Enum\n) -&gt; Column:\n    \"\"\"Update the provided quality control list with a new flag if condition is met.\n\n    Args:\n        qc (Column): Array column with the current list of qc flags.\n        flag_condition (Column): This is a column of booleans, signing which row should be flagged\n        flag_text (Enum): Text for the new quality control flag\n\n    Returns:\n        Column: Array column with the updated list of qc flags.\n\n\n    Examples:\n        &gt;&gt;&gt; s = \"study STRING, qualityControls ARRAY&lt;STRING&gt;, condition BOOLEAN\"\n        &gt;&gt;&gt; d =  [(\"S1\", [\"qc1\"], True), (\"S2\", [\"qc3\"], False)]\n        &gt;&gt;&gt; df = spark.createDataFrame(d, s)\n        &gt;&gt;&gt; df.show()\n        +-----+---------------+---------+\n        |study|qualityControls|condition|\n        +-----+---------------+---------+\n        |   S1|          [qc1]|     true|\n        |   S2|          [qc3]|    false|\n        +-----+---------------+---------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; class QC(Enum):\n        ...     QC1 = \"qc1\"\n        ...     QC2 = \"qc2\"\n        ...     QC3 = \"qc3\"\n\n        &gt;&gt;&gt; condition = f.col(\"condition\")\n        &gt;&gt;&gt; new_qc = Dataset.update_quality_flag(f.col(\"qualityControls\"), condition, QC.QC2)\n        &gt;&gt;&gt; df.withColumn(\"qualityControls\", new_qc).show()\n        +-----+---------------+---------+\n        |study|qualityControls|condition|\n        +-----+---------------+---------+\n        |   S1|     [qc1, qc2]|     true|\n        |   S2|          [qc3]|    false|\n        +-----+---------------+---------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    qc = f.when(qc.isNull(), f.array()).otherwise(qc)\n    return f.when(\n        flag_condition,\n        f.array_sort(\n            f.array_distinct(f.array_union(qc, f.array(f.lit(flag_text.value))))\n        ),\n    ).otherwise(qc)\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.valid_rows","title":"<code>valid_rows(invalid_flags: list[str], invalid: bool = False) -&gt; Self</code>","text":"<p>Filters <code>Dataset</code> according to a list of quality control flags. Only <code>Dataset</code> classes with a QC column can be validated.</p> <p>This method checks do following steps: - Check if the Dataset contains a QC column. - Check if the invalid_flags exist in the QC mappings flags. - Filter the Dataset according to the invalid_flags and invalid parameters.</p> <p>Parameters:</p> Name Type Description Default <code>invalid_flags</code> <code>list[str]</code> <p>List of quality control flags to be excluded.</p> required <code>invalid</code> <code>bool</code> <p>If True returns the invalid rows, instead of the valid. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>filtered dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Dataset does not contain a QC column or if the invalid_flags elements do not exist in QC mappings flags.</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def valid_rows(self: Self, invalid_flags: list[str], invalid: bool = False) -&gt; Self:\n    \"\"\"Filters `Dataset` according to a list of quality control flags. Only `Dataset` classes with a QC column can be validated.\n\n    This method checks do following steps:\n    - Check if the Dataset contains a QC column.\n    - Check if the invalid_flags exist in the QC mappings flags.\n    - Filter the Dataset according to the invalid_flags and invalid parameters.\n\n    Args:\n        invalid_flags (list[str]): List of quality control flags to be excluded.\n        invalid (bool): If True returns the invalid rows, instead of the valid. Defaults to False.\n\n    Returns:\n        Self: filtered dataset.\n\n    Raises:\n        ValueError: If the Dataset does not contain a QC column or if the invalid_flags elements do not exist in QC mappings flags.\n    \"\"\"\n    # If the invalid flags are not valid quality checks (enum) for this Dataset we raise an error:\n    invalid_reasons = []\n    for flag in invalid_flags:\n        if flag not in self.get_QC_mappings():\n            raise ValueError(\n                f\"{flag} is not a valid QC flag for {type(self).__name__} ({self.get_QC_mappings()}).\"\n            )\n        reason = self.get_QC_mappings()[flag]\n        invalid_reasons.append(reason)\n\n    qc_column_name = self.get_QC_column_name()\n    # If Dataset (class) does not contain QC column we raise an error:\n    if not qc_column_name:\n        raise ValueError(\n            f\"{type(self).__name__} objects do not contain a QC column to filter by.\"\n        )\n    else:\n        column: str = qc_column_name\n        # If QC column (nullable) is not available in the dataframe we create an empty array:\n        qc = f.when(f.col(column).isNull(), f.array()).otherwise(f.col(column))\n\n    filterCondition = ~f.arrays_overlap(\n        f.array([f.lit(i) for i in invalid_reasons]), qc\n    )\n    # Returning the filtered dataset:\n    if invalid:\n        return self.filter(~filterCondition)\n    else:\n        return self.filter(filterCondition)\n</code></pre>"},{"location":"python_api/datasets/_datasets/#gentropy.dataset.dataset.Dataset.validate_schema","title":"<code>validate_schema() -&gt; None</code>","text":"<p>Validate DataFrame schema against expected class schema.</p> <p>Raises:</p> Type Description <code>SchemaValidationError</code> <p>If the DataFrame schema does not match the expected schema</p> Source code in <code>src/gentropy/dataset/dataset.py</code> <pre><code>def validate_schema(self: Dataset) -&gt; None:\n    \"\"\"Validate DataFrame schema against expected class schema.\n\n    Raises:\n        SchemaValidationError: If the DataFrame schema does not match the expected schema\n    \"\"\"\n    expected_schema = self.schema\n    observed_schema = self._df.schema\n\n    # Unexpected fields in dataset\n    if discrepancies := compare_struct_schemas(observed_schema, expected_schema):\n        raise SchemaValidationError(\n            f\"Schema validation failed for {type(self).__name__}\", discrepancies\n        )\n</code></pre>"},{"location":"python_api/datasets/biosample_index/","title":"Biosample index","text":""},{"location":"python_api/datasets/biosample_index/#gentropy.dataset.biosample_index.BiosampleIndex","title":"<code>gentropy.dataset.biosample_index.BiosampleIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Biosample index dataset.</p> <p>A Biosample index dataset captures the metadata of the biosamples (e.g. tissues, cell types, cell lines, etc) such as alternate names and relationships with other biosamples.</p> Source code in <code>src/gentropy/dataset/biosample_index.py</code> <pre><code>@dataclass\nclass BiosampleIndex(Dataset):\n    \"\"\"Biosample index dataset.\n\n    A Biosample index dataset captures the metadata of the biosamples (e.g. tissues, cell types, cell lines, etc) such as alternate names and relationships with other biosamples.\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[BiosampleIndex]) -&gt; StructType:\n        \"\"\"Provide the schema for the BiosampleIndex dataset.\n\n        Returns:\n            StructType: The schema of the BiosampleIndex dataset.\n        \"\"\"\n        return parse_spark_schema(\"biosample_index.json\")\n\n    def merge_indices(\n        self: BiosampleIndex,\n        biosample_indices : list[BiosampleIndex]\n        ) -&gt; BiosampleIndex:\n        \"\"\"Merge a list of biosample indices into a single biosample index.\n\n        Where there are conflicts, in single values - the first value is taken. In list values, the union of all values is taken.\n\n        Args:\n            biosample_indices (list[BiosampleIndex]): Biosample indices to merge.\n\n        Returns:\n            BiosampleIndex: Merged biosample index.\n        \"\"\"\n        # Extract the DataFrames from the BiosampleIndex objects\n        biosample_dfs = [biosample_index.df for biosample_index in biosample_indices] + [self.df]\n\n        # Merge the DataFrames\n        merged_df = reduce(DataFrame.unionAll, biosample_dfs)\n\n        # Determine aggregation functions for each column\n        # Currently this will take the first value for single values and merge lists for list values\n        agg_funcs = []\n        for field in merged_df.schema.fields:\n            if field.name != \"biosampleId\":  # Skip the grouping column\n                if field.dataType == ArrayType(StringType()):\n                    agg_funcs.append(f.array_distinct(f.flatten(f.collect_list(field.name))).alias(field.name))\n                else:\n                    agg_funcs.append(f.first(f.col(field.name), ignorenulls=True).alias(field.name))\n\n        # Perform aggregation\n        aggregated_df = merged_df.groupBy(\"biosampleId\").agg(*agg_funcs)\n\n        return BiosampleIndex(\n            _df=aggregated_df,\n            _schema=BiosampleIndex.get_schema()\n            )\n\n    def retain_rows_with_ancestor_id(\n        self: BiosampleIndex,\n        ancestor_ids : list[str]\n        ) -&gt; BiosampleIndex:\n        \"\"\"Filter the biosample index to retain only rows with the given ancestor IDs.\n\n        Args:\n            ancestor_ids (list[str]): Ancestor IDs to filter on.\n\n        Returns:\n            BiosampleIndex: Filtered biosample index.\n        \"\"\"\n        # Create a Spark array of ancestor IDs prior to filtering\n        ancestor_ids_array = f.array(*[f.lit(id) for id in ancestor_ids])\n\n        return BiosampleIndex(\n            _df=self.df.filter(\n                f.size(f.array_intersect(f.col(\"ancestors\"), ancestor_ids_array)) &gt; 0\n            ),\n            _schema=BiosampleIndex.get_schema()\n            )\n</code></pre>"},{"location":"python_api/datasets/biosample_index/#gentropy.dataset.biosample_index.BiosampleIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provide the schema for the BiosampleIndex dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>The schema of the BiosampleIndex dataset.</p> Source code in <code>src/gentropy/dataset/biosample_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[BiosampleIndex]) -&gt; StructType:\n    \"\"\"Provide the schema for the BiosampleIndex dataset.\n\n    Returns:\n        StructType: The schema of the BiosampleIndex dataset.\n    \"\"\"\n    return parse_spark_schema(\"biosample_index.json\")\n</code></pre>"},{"location":"python_api/datasets/biosample_index/#gentropy.dataset.biosample_index.BiosampleIndex.merge_indices","title":"<code>merge_indices(biosample_indices: list[BiosampleIndex]) -&gt; BiosampleIndex</code>","text":"<p>Merge a list of biosample indices into a single biosample index.</p> <p>Where there are conflicts, in single values - the first value is taken. In list values, the union of all values is taken.</p> <p>Parameters:</p> Name Type Description Default <code>biosample_indices</code> <code>list[BiosampleIndex]</code> <p>Biosample indices to merge.</p> required <p>Returns:</p> Name Type Description <code>BiosampleIndex</code> <code>BiosampleIndex</code> <p>Merged biosample index.</p> Source code in <code>src/gentropy/dataset/biosample_index.py</code> <pre><code>def merge_indices(\n    self: BiosampleIndex,\n    biosample_indices : list[BiosampleIndex]\n    ) -&gt; BiosampleIndex:\n    \"\"\"Merge a list of biosample indices into a single biosample index.\n\n    Where there are conflicts, in single values - the first value is taken. In list values, the union of all values is taken.\n\n    Args:\n        biosample_indices (list[BiosampleIndex]): Biosample indices to merge.\n\n    Returns:\n        BiosampleIndex: Merged biosample index.\n    \"\"\"\n    # Extract the DataFrames from the BiosampleIndex objects\n    biosample_dfs = [biosample_index.df for biosample_index in biosample_indices] + [self.df]\n\n    # Merge the DataFrames\n    merged_df = reduce(DataFrame.unionAll, biosample_dfs)\n\n    # Determine aggregation functions for each column\n    # Currently this will take the first value for single values and merge lists for list values\n    agg_funcs = []\n    for field in merged_df.schema.fields:\n        if field.name != \"biosampleId\":  # Skip the grouping column\n            if field.dataType == ArrayType(StringType()):\n                agg_funcs.append(f.array_distinct(f.flatten(f.collect_list(field.name))).alias(field.name))\n            else:\n                agg_funcs.append(f.first(f.col(field.name), ignorenulls=True).alias(field.name))\n\n    # Perform aggregation\n    aggregated_df = merged_df.groupBy(\"biosampleId\").agg(*agg_funcs)\n\n    return BiosampleIndex(\n        _df=aggregated_df,\n        _schema=BiosampleIndex.get_schema()\n        )\n</code></pre>"},{"location":"python_api/datasets/biosample_index/#gentropy.dataset.biosample_index.BiosampleIndex.retain_rows_with_ancestor_id","title":"<code>retain_rows_with_ancestor_id(ancestor_ids: list[str]) -&gt; BiosampleIndex</code>","text":"<p>Filter the biosample index to retain only rows with the given ancestor IDs.</p> <p>Parameters:</p> Name Type Description Default <code>ancestor_ids</code> <code>list[str]</code> <p>Ancestor IDs to filter on.</p> required <p>Returns:</p> Name Type Description <code>BiosampleIndex</code> <code>BiosampleIndex</code> <p>Filtered biosample index.</p> Source code in <code>src/gentropy/dataset/biosample_index.py</code> <pre><code>def retain_rows_with_ancestor_id(\n    self: BiosampleIndex,\n    ancestor_ids : list[str]\n    ) -&gt; BiosampleIndex:\n    \"\"\"Filter the biosample index to retain only rows with the given ancestor IDs.\n\n    Args:\n        ancestor_ids (list[str]): Ancestor IDs to filter on.\n\n    Returns:\n        BiosampleIndex: Filtered biosample index.\n    \"\"\"\n    # Create a Spark array of ancestor IDs prior to filtering\n    ancestor_ids_array = f.array(*[f.lit(id) for id in ancestor_ids])\n\n    return BiosampleIndex(\n        _df=self.df.filter(\n            f.size(f.array_intersect(f.col(\"ancestors\"), ancestor_ids_array)) &gt; 0\n        ),\n        _schema=BiosampleIndex.get_schema()\n        )\n</code></pre>"},{"location":"python_api/datasets/biosample_index/#schema","title":"Schema","text":"<pre><code>root\n |-- biosampleId: string (nullable = false)\n |-- biosampleName: string (nullable = false)\n |-- description: string (nullable = true)\n |-- xrefs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- synonyms: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- parents: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- ancestors: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- descendants: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- children: array (nullable = true)\n |    |-- element: string (containsNull = true)\n</code></pre>"},{"location":"python_api/datasets/colocalisation/","title":"Colocalisation","text":""},{"location":"python_api/datasets/colocalisation/#gentropy.dataset.colocalisation.Colocalisation","title":"<code>gentropy.dataset.colocalisation.Colocalisation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Colocalisation results for pairs of overlapping study-locus.</p> Source code in <code>src/gentropy/dataset/colocalisation.py</code> <pre><code>@dataclass\nclass Colocalisation(Dataset):\n    \"\"\"Colocalisation results for pairs of overlapping study-locus.\"\"\"\n\n    @classmethod\n    def get_schema(cls: type[Colocalisation]) -&gt; StructType:\n        \"\"\"Provides the schema for the Colocalisation dataset.\n\n        Returns:\n            StructType: Schema for the Colocalisation dataset\n        \"\"\"\n        return parse_spark_schema(\"colocalisation.json\")\n\n    def extract_maximum_coloc_probability_per_region_and_gene(\n        self: Colocalisation,\n        study_locus: StudyLocus,\n        study_index: StudyIndex,\n        *,\n        filter_by_colocalisation_method: str,\n        filter_by_qtls: str | list[str] | None = None,\n    ) -&gt; DataFrame:\n        \"\"\"Get maximum colocalisation probability for a (studyLocus, gene) window.\n\n        Args:\n            study_locus (StudyLocus): Dataset containing study loci to filter the colocalisation dataset on and the geneId linked to the region\n            study_index (StudyIndex): Study index to use to get study metadata\n            filter_by_colocalisation_method (str): optional filter to apply on the colocalisation dataset\n            filter_by_qtls (str | list[str] | None): optional filter to apply on the colocalisation dataset\n\n        Returns:\n            DataFrame: table with the maximum colocalisation scores for the provided study loci\n\n        Raises:\n            ValueError: if filter_by_qtl is not in the list of valid QTL types or is not in the list of valid colocalisation methods\n        \"\"\"\n        from gentropy.colocalisation import ColocalisationStep\n\n        valid_qtls = list(\n            set(EqtlCatalogueStudyIndex.method_to_qtl_type_mapping.values())\n        ) + [\n            f\"sc{qtl}\"\n            for qtl in set(EqtlCatalogueStudyIndex.method_to_qtl_type_mapping.values())\n        ]\n\n        if filter_by_qtls:\n            filter_by_qtls = (\n                list(map(str.lower, [filter_by_qtls]))\n                if isinstance(filter_by_qtls, str)\n                else list(map(str.lower, filter_by_qtls))\n            )\n            if any(qtl not in valid_qtls for qtl in filter_by_qtls):\n                raise ValueError(f\"There are no studies with QTL type {filter_by_qtls}\")\n\n        if filter_by_colocalisation_method not in [\n            \"ECaviar\",\n            \"Coloc\",\n        ]:  # TODO: Write helper class to retrieve coloc method names\n            raise ValueError(\n                f\"Colocalisation method {filter_by_colocalisation_method} is not supported.\"\n            )\n        # Prepare the list of colocalisation methods that contain expected metrics\n        colocalisation_methods = [\n            filter_by_colocalisation_method.lower(),  # original method name Coloc or ECaviar to ensure backward compatibility\n            \"coloc_pip_ecaviar\",  # combined method name, coloc_pip_ecaviar contains both CLPP and H4\n        ]\n        method_colocalisation_metric = ColocalisationStep._get_colocalisation_class(\n            filter_by_colocalisation_method\n        ).METHOD_METRIC\n\n        coloc_filtering_expr = [\n            f.col(\"rightGeneId\").isNotNull(),\n            (f.lower(\"colocalisationMethod\").isin(colocalisation_methods)),\n        ]\n        if filter_by_qtls:\n            coloc_filtering_expr.append(f.lower(\"rightStudyType\").isin(filter_by_qtls))\n\n        filtered_colocalisation = (\n            # Bring rightStudyType and rightGeneId and filter by rows where the gene is null,\n            # which is equivalent to filtering studyloci from gwas on the right side\n            self.append_study_metadata(\n                study_locus,\n                study_index,\n                metadata_cols=[\"geneId\", \"studyType\"],\n                colocalisation_side=\"right\",\n            )\n            # it also filters based on method and qtl type\n            .filter(reduce(lambda a, b: a &amp; b, coloc_filtering_expr))\n            # and filters colocalisation results to only include the subset of studylocus that contains gwas studylocusid\n            .join(\n                study_locus.df.selectExpr(\"studyLocusId as leftStudyLocusId\"),\n                \"leftStudyLocusId\",\n            )\n        )\n\n        return get_record_with_maximum_value(\n            filtered_colocalisation.withColumnRenamed(\n                \"leftStudyLocusId\", \"studyLocusId\"\n            ).withColumnRenamed(\"rightGeneId\", \"geneId\"),\n            [\"studyLocusId\", \"geneId\"],\n            method_colocalisation_metric,\n        )\n\n    def append_study_metadata(\n        self: Colocalisation,\n        study_locus: StudyLocus,\n        study_index: StudyIndex,\n        *,\n        metadata_cols: list[str],\n        colocalisation_side: str = \"right\",\n    ) -&gt; DataFrame:\n        \"\"\"Appends metadata from the study to the requested side of the colocalisation dataset.\n\n        Args:\n            study_locus (StudyLocus): Dataset containing study loci that links the colocalisation dataset and the study index via the studyId\n            study_index (StudyIndex): Dataset containing study index that contains the metadata\n            metadata_cols (list[str]): List of study columns to append\n            colocalisation_side (str): Which side of the colocalisation dataset to append metadata to. Must be either 'right' or 'left'\n\n        Returns:\n            DataFrame: Colocalisation dataset with appended metadata of the study from the requested side\n\n        Raises:\n            ValueError: if colocalisation_side is not 'right' or 'left'\n        \"\"\"\n        metadata_cols = [\"studyId\", *metadata_cols]\n        if colocalisation_side not in [\"right\", \"left\"]:\n            raise ValueError(\n                f\"colocalisation_side must be either 'right' or 'left', got {colocalisation_side}\"\n            )\n\n        study_loci_w_metadata = (\n            study_locus.df.select(\"studyLocusId\", \"studyId\")\n            .join(\n                f.broadcast(study_index.df.select(\"studyId\", *metadata_cols)),\n                \"studyId\",\n            )\n            .distinct()\n        )\n        coloc_df = (\n            # drop `rightStudyType` in case it is requested\n            self.df.drop(\"rightStudyType\")\n            if \"studyType\" in metadata_cols and colocalisation_side == \"right\"\n            else self.df\n        )\n        return (\n            # Append that to the respective side of the colocalisation dataset\n            study_loci_w_metadata.selectExpr(\n                f\"studyLocusId as {colocalisation_side}StudyLocusId\",\n                *[\n                    f\"{col} as {colocalisation_side}{col[0].upper() + col[1:]}\"\n                    for col in metadata_cols\n                ],\n            ).join(coloc_df, f\"{colocalisation_side}StudyLocusId\", \"right\")\n        )\n\n    def drop_trans_effects(\n        self: Colocalisation, study_locus: StudyLocus\n    ) -&gt; Colocalisation:\n        \"\"\"Filters the colocalisation dataset to only include cis effects from QTLs (right study locus).\n\n        Args:\n            study_locus (StudyLocus): Dataset containing study loci that has metadata about the type of credible set\n\n        Returns:\n            Colocalisation: Colocalisation dataset filtered to only include cis effects from QTLs (right study locus)\n        \"\"\"\n        cis_study_loci = study_locus.filter(\n            (~f.col(\"isTransQtl\")) | (f.col(\"isTransQtl\").isNull())\n        ).df.select(\"studyLocusId\")\n        filtered_coloc = self.df.join(\n            cis_study_loci,\n            self.df.rightStudyLocusId == cis_study_loci.studyLocusId,\n            \"inner\",\n        ).drop(\"studyLocusId\")\n        return Colocalisation(\n            _df=filtered_coloc,\n            _schema=self.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/colocalisation/#gentropy.dataset.colocalisation.Colocalisation.append_study_metadata","title":"<code>append_study_metadata(study_locus: StudyLocus, study_index: StudyIndex, *, metadata_cols: list[str], colocalisation_side: str = 'right') -&gt; DataFrame</code>","text":"<p>Appends metadata from the study to the requested side of the colocalisation dataset.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus</code> <code>StudyLocus</code> <p>Dataset containing study loci that links the colocalisation dataset and the study index via the studyId</p> required <code>study_index</code> <code>StudyIndex</code> <p>Dataset containing study index that contains the metadata</p> required <code>metadata_cols</code> <code>list[str]</code> <p>List of study columns to append</p> required <code>colocalisation_side</code> <code>str</code> <p>Which side of the colocalisation dataset to append metadata to. Must be either 'right' or 'left'</p> <code>'right'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Colocalisation dataset with appended metadata of the study from the requested side</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if colocalisation_side is not 'right' or 'left'</p> Source code in <code>src/gentropy/dataset/colocalisation.py</code> <pre><code>def append_study_metadata(\n    self: Colocalisation,\n    study_locus: StudyLocus,\n    study_index: StudyIndex,\n    *,\n    metadata_cols: list[str],\n    colocalisation_side: str = \"right\",\n) -&gt; DataFrame:\n    \"\"\"Appends metadata from the study to the requested side of the colocalisation dataset.\n\n    Args:\n        study_locus (StudyLocus): Dataset containing study loci that links the colocalisation dataset and the study index via the studyId\n        study_index (StudyIndex): Dataset containing study index that contains the metadata\n        metadata_cols (list[str]): List of study columns to append\n        colocalisation_side (str): Which side of the colocalisation dataset to append metadata to. Must be either 'right' or 'left'\n\n    Returns:\n        DataFrame: Colocalisation dataset with appended metadata of the study from the requested side\n\n    Raises:\n        ValueError: if colocalisation_side is not 'right' or 'left'\n    \"\"\"\n    metadata_cols = [\"studyId\", *metadata_cols]\n    if colocalisation_side not in [\"right\", \"left\"]:\n        raise ValueError(\n            f\"colocalisation_side must be either 'right' or 'left', got {colocalisation_side}\"\n        )\n\n    study_loci_w_metadata = (\n        study_locus.df.select(\"studyLocusId\", \"studyId\")\n        .join(\n            f.broadcast(study_index.df.select(\"studyId\", *metadata_cols)),\n            \"studyId\",\n        )\n        .distinct()\n    )\n    coloc_df = (\n        # drop `rightStudyType` in case it is requested\n        self.df.drop(\"rightStudyType\")\n        if \"studyType\" in metadata_cols and colocalisation_side == \"right\"\n        else self.df\n    )\n    return (\n        # Append that to the respective side of the colocalisation dataset\n        study_loci_w_metadata.selectExpr(\n            f\"studyLocusId as {colocalisation_side}StudyLocusId\",\n            *[\n                f\"{col} as {colocalisation_side}{col[0].upper() + col[1:]}\"\n                for col in metadata_cols\n            ],\n        ).join(coloc_df, f\"{colocalisation_side}StudyLocusId\", \"right\")\n    )\n</code></pre>"},{"location":"python_api/datasets/colocalisation/#gentropy.dataset.colocalisation.Colocalisation.drop_trans_effects","title":"<code>drop_trans_effects(study_locus: StudyLocus) -&gt; Colocalisation</code>","text":"<p>Filters the colocalisation dataset to only include cis effects from QTLs (right study locus).</p> <p>Parameters:</p> Name Type Description Default <code>study_locus</code> <code>StudyLocus</code> <p>Dataset containing study loci that has metadata about the type of credible set</p> required <p>Returns:</p> Name Type Description <code>Colocalisation</code> <code>Colocalisation</code> <p>Colocalisation dataset filtered to only include cis effects from QTLs (right study locus)</p> Source code in <code>src/gentropy/dataset/colocalisation.py</code> <pre><code>def drop_trans_effects(\n    self: Colocalisation, study_locus: StudyLocus\n) -&gt; Colocalisation:\n    \"\"\"Filters the colocalisation dataset to only include cis effects from QTLs (right study locus).\n\n    Args:\n        study_locus (StudyLocus): Dataset containing study loci that has metadata about the type of credible set\n\n    Returns:\n        Colocalisation: Colocalisation dataset filtered to only include cis effects from QTLs (right study locus)\n    \"\"\"\n    cis_study_loci = study_locus.filter(\n        (~f.col(\"isTransQtl\")) | (f.col(\"isTransQtl\").isNull())\n    ).df.select(\"studyLocusId\")\n    filtered_coloc = self.df.join(\n        cis_study_loci,\n        self.df.rightStudyLocusId == cis_study_loci.studyLocusId,\n        \"inner\",\n    ).drop(\"studyLocusId\")\n    return Colocalisation(\n        _df=filtered_coloc,\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/colocalisation/#gentropy.dataset.colocalisation.Colocalisation.extract_maximum_coloc_probability_per_region_and_gene","title":"<code>extract_maximum_coloc_probability_per_region_and_gene(study_locus: StudyLocus, study_index: StudyIndex, *, filter_by_colocalisation_method: str, filter_by_qtls: str | list[str] | None = None) -&gt; DataFrame</code>","text":"<p>Get maximum colocalisation probability for a (studyLocus, gene) window.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus</code> <code>StudyLocus</code> <p>Dataset containing study loci to filter the colocalisation dataset on and the geneId linked to the region</p> required <code>study_index</code> <code>StudyIndex</code> <p>Study index to use to get study metadata</p> required <code>filter_by_colocalisation_method</code> <code>str</code> <p>optional filter to apply on the colocalisation dataset</p> required <code>filter_by_qtls</code> <code>str | list[str] | None</code> <p>optional filter to apply on the colocalisation dataset</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>table with the maximum colocalisation scores for the provided study loci</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if filter_by_qtl is not in the list of valid QTL types or is not in the list of valid colocalisation methods</p> Source code in <code>src/gentropy/dataset/colocalisation.py</code> <pre><code>def extract_maximum_coloc_probability_per_region_and_gene(\n    self: Colocalisation,\n    study_locus: StudyLocus,\n    study_index: StudyIndex,\n    *,\n    filter_by_colocalisation_method: str,\n    filter_by_qtls: str | list[str] | None = None,\n) -&gt; DataFrame:\n    \"\"\"Get maximum colocalisation probability for a (studyLocus, gene) window.\n\n    Args:\n        study_locus (StudyLocus): Dataset containing study loci to filter the colocalisation dataset on and the geneId linked to the region\n        study_index (StudyIndex): Study index to use to get study metadata\n        filter_by_colocalisation_method (str): optional filter to apply on the colocalisation dataset\n        filter_by_qtls (str | list[str] | None): optional filter to apply on the colocalisation dataset\n\n    Returns:\n        DataFrame: table with the maximum colocalisation scores for the provided study loci\n\n    Raises:\n        ValueError: if filter_by_qtl is not in the list of valid QTL types or is not in the list of valid colocalisation methods\n    \"\"\"\n    from gentropy.colocalisation import ColocalisationStep\n\n    valid_qtls = list(\n        set(EqtlCatalogueStudyIndex.method_to_qtl_type_mapping.values())\n    ) + [\n        f\"sc{qtl}\"\n        for qtl in set(EqtlCatalogueStudyIndex.method_to_qtl_type_mapping.values())\n    ]\n\n    if filter_by_qtls:\n        filter_by_qtls = (\n            list(map(str.lower, [filter_by_qtls]))\n            if isinstance(filter_by_qtls, str)\n            else list(map(str.lower, filter_by_qtls))\n        )\n        if any(qtl not in valid_qtls for qtl in filter_by_qtls):\n            raise ValueError(f\"There are no studies with QTL type {filter_by_qtls}\")\n\n    if filter_by_colocalisation_method not in [\n        \"ECaviar\",\n        \"Coloc\",\n    ]:  # TODO: Write helper class to retrieve coloc method names\n        raise ValueError(\n            f\"Colocalisation method {filter_by_colocalisation_method} is not supported.\"\n        )\n    # Prepare the list of colocalisation methods that contain expected metrics\n    colocalisation_methods = [\n        filter_by_colocalisation_method.lower(),  # original method name Coloc or ECaviar to ensure backward compatibility\n        \"coloc_pip_ecaviar\",  # combined method name, coloc_pip_ecaviar contains both CLPP and H4\n    ]\n    method_colocalisation_metric = ColocalisationStep._get_colocalisation_class(\n        filter_by_colocalisation_method\n    ).METHOD_METRIC\n\n    coloc_filtering_expr = [\n        f.col(\"rightGeneId\").isNotNull(),\n        (f.lower(\"colocalisationMethod\").isin(colocalisation_methods)),\n    ]\n    if filter_by_qtls:\n        coloc_filtering_expr.append(f.lower(\"rightStudyType\").isin(filter_by_qtls))\n\n    filtered_colocalisation = (\n        # Bring rightStudyType and rightGeneId and filter by rows where the gene is null,\n        # which is equivalent to filtering studyloci from gwas on the right side\n        self.append_study_metadata(\n            study_locus,\n            study_index,\n            metadata_cols=[\"geneId\", \"studyType\"],\n            colocalisation_side=\"right\",\n        )\n        # it also filters based on method and qtl type\n        .filter(reduce(lambda a, b: a &amp; b, coloc_filtering_expr))\n        # and filters colocalisation results to only include the subset of studylocus that contains gwas studylocusid\n        .join(\n            study_locus.df.selectExpr(\"studyLocusId as leftStudyLocusId\"),\n            \"leftStudyLocusId\",\n        )\n    )\n\n    return get_record_with_maximum_value(\n        filtered_colocalisation.withColumnRenamed(\n            \"leftStudyLocusId\", \"studyLocusId\"\n        ).withColumnRenamed(\"rightGeneId\", \"geneId\"),\n        [\"studyLocusId\", \"geneId\"],\n        method_colocalisation_metric,\n    )\n</code></pre>"},{"location":"python_api/datasets/colocalisation/#gentropy.dataset.colocalisation.Colocalisation.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the Colocalisation dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the Colocalisation dataset</p> Source code in <code>src/gentropy/dataset/colocalisation.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[Colocalisation]) -&gt; StructType:\n    \"\"\"Provides the schema for the Colocalisation dataset.\n\n    Returns:\n        StructType: Schema for the Colocalisation dataset\n    \"\"\"\n    return parse_spark_schema(\"colocalisation.json\")\n</code></pre>"},{"location":"python_api/datasets/colocalisation/#schema","title":"Schema","text":"<pre><code>root\n |-- leftStudyLocusId: string (nullable = false)\n |-- rightStudyLocusId: string (nullable = false)\n |-- rightStudyType: string (nullable = false)\n |-- chromosome: string (nullable = false)\n |-- colocalisationMethod: string (nullable = false)\n |-- numberColocalisingVariants: long (nullable = false)\n |-- h0: double (nullable = true)\n |-- h1: double (nullable = true)\n |-- h2: double (nullable = true)\n |-- h3: double (nullable = true)\n |-- h4: double (nullable = true)\n |-- clpp: double (nullable = true)\n |-- betaRatioSignAverage: double (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/intervals/","title":"Intervals","text":""},{"location":"python_api/datasets/intervals/#gentropy.dataset.intervals.Intervals","title":"<code>gentropy.dataset.intervals.Intervals</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Intervals dataset links genes to genomic regions based on genome interaction studies.</p> Source code in <code>src/gentropy/dataset/intervals.py</code> <pre><code>@dataclass\nclass Intervals(Dataset):\n    \"\"\"Intervals dataset links genes to genomic regions based on genome interaction studies.\"\"\"\n\n    @classmethod\n    def get_schema(cls: type[Intervals]) -&gt; StructType:\n        \"\"\"Provides the schema for the Intervals dataset.\n\n        Returns:\n            StructType: Schema for the Intervals dataset\n        \"\"\"\n        return parse_spark_schema(\"intervals.json\")\n\n    @classmethod\n    def from_source(\n        cls: type[Intervals],\n        spark: SparkSession,\n        source_name: str,\n        source_path: str,\n        target_index: TargetIndex,\n        biosample_index: BiosampleIndex,\n        biosample_mapping: DataFrame,\n    ) -&gt; Intervals:\n        \"\"\"Collect interval data for a particular source.\n\n        Args:\n            spark (SparkSession): Spark session\n            source_name (str): Name of the interval source\n            source_path (str): Path to the interval source file\n            target_index (TargetIndex): Target index\n            biosample_index (BiosampleIndex): Biosample index\n            biosample_mapping (DataFrame): Biosample mapping DataFrame\n\n        Returns:\n            Intervals: Intervals dataset\n\n        Raises:\n            ValueError: If the source name is not recognised\n        \"\"\"\n        from gentropy.datasource.intervals.e2g import IntervalsE2G\n        from gentropy.datasource.intervals.epiraction import IntervalsEpiraction\n\n        if source_name == \"e2g\":\n            raw = IntervalsE2G.read(spark, source_path)\n            return IntervalsE2G.parse(\n                raw_e2g_df=raw,\n                biosample_mapping=biosample_mapping,\n                target_index=target_index,\n                biosample_index=biosample_index,\n            )\n\n        if source_name == \"epiraction\":\n            raw = IntervalsEpiraction.read(spark, source_path)\n            return IntervalsEpiraction.parse(\n                raw_epiraction_df=raw,\n                target_index=target_index,\n            )\n\n        raise ValueError(f\"Unknown interval source: {source_name!r}\")\n</code></pre>"},{"location":"python_api/datasets/intervals/#gentropy.dataset.intervals.Intervals.from_source","title":"<code>from_source(spark: SparkSession, source_name: str, source_path: str, target_index: TargetIndex, biosample_index: BiosampleIndex, biosample_mapping: DataFrame) -&gt; Intervals</code>  <code>classmethod</code>","text":"<p>Collect interval data for a particular source.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>source_name</code> <code>str</code> <p>Name of the interval source</p> required <code>source_path</code> <code>str</code> <p>Path to the interval source file</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index</p> required <code>biosample_index</code> <code>BiosampleIndex</code> <p>Biosample index</p> required <code>biosample_mapping</code> <code>DataFrame</code> <p>Biosample mapping DataFrame</p> required <p>Returns:</p> Name Type Description <code>Intervals</code> <code>Intervals</code> <p>Intervals dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source name is not recognised</p> Source code in <code>src/gentropy/dataset/intervals.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[Intervals],\n    spark: SparkSession,\n    source_name: str,\n    source_path: str,\n    target_index: TargetIndex,\n    biosample_index: BiosampleIndex,\n    biosample_mapping: DataFrame,\n) -&gt; Intervals:\n    \"\"\"Collect interval data for a particular source.\n\n    Args:\n        spark (SparkSession): Spark session\n        source_name (str): Name of the interval source\n        source_path (str): Path to the interval source file\n        target_index (TargetIndex): Target index\n        biosample_index (BiosampleIndex): Biosample index\n        biosample_mapping (DataFrame): Biosample mapping DataFrame\n\n    Returns:\n        Intervals: Intervals dataset\n\n    Raises:\n        ValueError: If the source name is not recognised\n    \"\"\"\n    from gentropy.datasource.intervals.e2g import IntervalsE2G\n    from gentropy.datasource.intervals.epiraction import IntervalsEpiraction\n\n    if source_name == \"e2g\":\n        raw = IntervalsE2G.read(spark, source_path)\n        return IntervalsE2G.parse(\n            raw_e2g_df=raw,\n            biosample_mapping=biosample_mapping,\n            target_index=target_index,\n            biosample_index=biosample_index,\n        )\n\n    if source_name == \"epiraction\":\n        raw = IntervalsEpiraction.read(spark, source_path)\n        return IntervalsEpiraction.parse(\n            raw_epiraction_df=raw,\n            target_index=target_index,\n        )\n\n    raise ValueError(f\"Unknown interval source: {source_name!r}\")\n</code></pre>"},{"location":"python_api/datasets/intervals/#gentropy.dataset.intervals.Intervals.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the Intervals dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the Intervals dataset</p> Source code in <code>src/gentropy/dataset/intervals.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[Intervals]) -&gt; StructType:\n    \"\"\"Provides the schema for the Intervals dataset.\n\n    Returns:\n        StructType: Schema for the Intervals dataset\n    \"\"\"\n    return parse_spark_schema(\"intervals.json\")\n</code></pre>"},{"location":"python_api/datasets/intervals/#schema","title":"Schema","text":"<pre><code>root\n |-- chromosome: string (nullable = false)\n |-- start: string (nullable = false)\n |-- end: string (nullable = false)\n |-- geneId: string (nullable = true)\n |-- score: double (nullable = true)\n |-- distanceToTss: integer (nullable = true)\n |-- resourceScore: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- name: string (nullable = false)\n |    |    |-- value: float (nullable = false)\n |-- datasourceId: string (nullable = false)\n |-- intervalType: string (nullable = false)\n |-- pmid: string (nullable = true)\n |-- biofeature: string (nullable = true)\n |-- biosampleName: string (nullable = true)\n |-- biosampleId: string (nullable = true)\n |-- studyId: string (nullable = true)\n |-- intervalId: string (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/","title":"L2G Feature Matrix","text":""},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix","title":"<code>gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix</code>","text":"<p>Dataset with features for Locus to Gene prediction.</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>class L2GFeatureMatrix:\n    \"\"\"Dataset with features for Locus to Gene prediction.\"\"\"\n\n    def __init__(\n        self,\n        _df: DataFrame,\n        features_list: list[str] | None = None,\n        with_gold_standard: bool = False,\n        label_col: str = \"goldStandardSet\",\n    ) -&gt; None:\n        \"\"\"Post-initialisation to set the features list. If not provided, all columns except the fixed ones are used.\n\n        Args:\n            _df (DataFrame): Feature matrix dataset\n            features_list (list[str] | None): List of features to use. If None, all possible features are used.\n            with_gold_standard (bool): Whether to include the gold standard set in the feature matrix.\n            label_col (str): The target column when the feature matrix represents the gold standard\n\n        \"\"\"\n        self.with_gold_standard = with_gold_standard\n        self.fixed_cols = [\"studyLocusId\", \"geneId\"]\n        if self.with_gold_standard:\n            self.label_col = label_col\n            self.fixed_cols.append(label_col)\n        if \"traitFromSourceMappedId\" in _df.columns:\n            self.fixed_cols.append(\"traitFromSourceMappedId\")\n\n        self.features_list = features_list or [\n            col for col in _df.columns if col not in self.fixed_cols\n        ]\n        self._df = _df.selectExpr(\n            self.fixed_cols\n            + [\n                f\"CAST({feature} AS FLOAT) AS {feature}\"\n                for feature in self.features_list\n            ]\n        )\n\n    @classmethod\n    def from_features_list(\n        cls: type[L2GFeatureMatrix],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        features_list: list[str],\n        features_input_loader: L2GFeatureInputLoader,\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Generate features from the gentropy datasets by calling the feature factory that will instantiate the corresponding features.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): Study locus pairs to annotate\n            features_list (list[str]): List of feature names to be computed.\n            features_input_loader (L2GFeatureInputLoader): Object that contais features input.\n\n        Returns:\n            L2GFeatureMatrix: L2G feature matrix dataset\n        \"\"\"\n        features_long_df = reduce(\n            lambda x, y: x.unionByName(y, allowMissingColumns=True),\n            [\n                # Compute all features and merge them into a single dataframe\n                feature.df\n                for feature in FeatureFactory(\n                    study_loci_to_annotate, features_list\n                ).generate_features(features_input_loader)\n            ],\n        )\n        if isinstance(study_loci_to_annotate, L2GGoldStandard):\n            return cls(\n                _df=convert_from_long_to_wide(\n                    # Add gold standard set to the feature matrix\n                    features_long_df.join(\n                        study_loci_to_annotate.df.select(\n                            \"studyLocusId\", \"geneId\", \"goldStandardSet\"\n                        ),\n                        [\"studyLocusId\", \"geneId\"],\n                    ),\n                    [\"studyLocusId\", \"geneId\", \"goldStandardSet\"],\n                    \"featureName\",\n                    \"featureValue\",\n                ),\n                with_gold_standard=True,\n            )\n        return cls(\n            _df=convert_from_long_to_wide(\n                features_long_df,\n                [\"studyLocusId\", \"geneId\"],\n                \"featureName\",\n                \"featureValue\",\n            ),\n            with_gold_standard=False,\n        )\n\n    def calculate_feature_missingness_rate(\n        self: L2GFeatureMatrix,\n    ) -&gt; dict[str, float]:\n        \"\"\"Calculate the proportion of missing values in each feature.\n\n        Returns:\n            dict[str, float]: Dictionary of feature names and their missingness rate.\n\n        Raises:\n            ValueError: If no features are found.\n        \"\"\"\n        total_count = self._df.count()\n        if not self.features_list:\n            raise ValueError(\"No features found\")\n\n        return {\n            feature: (\n                self._df.filter(\n                    (self._df[feature].isNull()) | (self._df[feature] == 0)\n                ).count()\n                / total_count\n            )\n            for feature in self.features_list\n        }\n\n    def fill_na(\n        self: L2GFeatureMatrix, na_value: float = 0.0, subset: list[str] | None = None\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Fill missing values in a column with a given value.\n\n        For features that correspond to gene attributes, missing values are imputed using the mean of the column.\n\n        Args:\n            na_value (float): Value to replace missing values with. Defaults to 0.0.\n            subset (list[str] | None): Subset of columns to consider. Defaults to None.\n\n        Returns:\n            L2GFeatureMatrix: L2G feature matrix dataset\n        \"\"\"\n        cols_to_impute = [\n            \"proteinGeneCount500kb\",\n            \"geneCount500kb\",\n        ]\n        for col in cols_to_impute:\n            if col not in self._df.columns:\n                continue\n            else:\n                self._df = self._df.withColumn(\n                    col,\n                    f.when(\n                        f.col(col).isNull(),\n                        f.mean(f.col(col)).over(Window.partitionBy(\"studyLocusId\")),\n                    ).otherwise(f.col(col)),\n                )\n        self._df = self._df.fillna(na_value, subset=subset)\n        return self\n\n    def select_features(\n        self: L2GFeatureMatrix,\n        features_list: list[str] | None,\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Returns a new object with a subset of features from the original feature matrix.\n\n        Args:\n            features_list (list[str] | None): List of features to select\n\n        Returns:\n            L2GFeatureMatrix: L2G feature matrix dataset\n\n        Raises:\n            ValueError: If no features have been selected.\n        \"\"\"\n        if features_list := features_list or self.features_list:\n            # cast to float every feature in the features_list\n            return L2GFeatureMatrix(\n                _df=self._df.selectExpr(\n                    self.fixed_cols\n                    + [\n                        f\"CAST({feature} AS FLOAT) AS {feature}\"\n                        for feature in features_list\n                    ]\n                ),\n                features_list=features_list,\n                with_gold_standard=self.with_gold_standard,\n            )\n        raise ValueError(\"features_list cannot be None\")\n\n    def persist(self: Self) -&gt; Self:\n        \"\"\"Persist the feature matrix in memory.\n\n        Returns:\n            Self: Persisted Dataset\n        \"\"\"\n        self._df = self._df.persist()\n        return self\n\n    def append_null_features(self, features_list: list[str]) -&gt; L2GFeatureMatrix:\n        \"\"\"Add features from the list that are not already in the dataframe as null columns filled with 0.0.\n\n        Args:\n            features_list (list[str]): List of features to check and add if missing\n\n        Returns:\n            L2GFeatureMatrix: Updated feature matrix with additional features\n        \"\"\"\n        null_features = [\n            feature for feature in features_list if feature not in self._df.columns\n        ]\n        if null_features:\n            for feature in null_features:\n                self._df = self._df.withColumn(feature, f.lit(0.0))\n            self.features_list.extend(null_features)\n\n        return self\n\n    def generate_train_test_split(\n        self,\n        test_size: float,\n        verbose: bool,\n        label_encoder: dict[str, int],\n        label_col: str,\n    ) -&gt; tuple[pd_dataframe, pd_dataframe]:\n        \"\"\"Generate train and test splits for the feature matrix.\n\n        Args:\n            test_size (float): Proportion of the test set\n            verbose (bool): Whether to print verbose output\n            label_encoder (dict[str, int]): Label encoder for the gold standard set\n            label_col (str): Column name for the gold standard set\n\n        Returns:\n            tuple[pd_dataframe, pd_dataframe]: Train and test splits\n        \"\"\"\n        from gentropy.method.l2g.trainer import LocusToGeneTrainer\n\n        data_df = self._df.toPandas()\n\n        # Encode labels in `goldStandardSet` to a numeric value\n        data_df[label_col] = data_df[label_col].map(label_encoder)\n\n        # Generate train, held out sets\n        return LocusToGeneTrainer.hierarchical_split(\n            data_df, test_size=test_size, verbose=verbose\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.__init__","title":"<code>__init__(_df: DataFrame, features_list: list[str] | None = None, with_gold_standard: bool = False, label_col: str = 'goldStandardSet') -&gt; None</code>","text":"<p>Post-initialisation to set the features list. If not provided, all columns except the fixed ones are used.</p> <p>Parameters:</p> Name Type Description Default <code>_df</code> <code>DataFrame</code> <p>Feature matrix dataset</p> required <code>features_list</code> <code>list[str] | None</code> <p>List of features to use. If None, all possible features are used.</p> <code>None</code> <code>with_gold_standard</code> <code>bool</code> <p>Whether to include the gold standard set in the feature matrix.</p> <code>False</code> <code>label_col</code> <code>str</code> <p>The target column when the feature matrix represents the gold standard</p> <code>'goldStandardSet'</code> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def __init__(\n    self,\n    _df: DataFrame,\n    features_list: list[str] | None = None,\n    with_gold_standard: bool = False,\n    label_col: str = \"goldStandardSet\",\n) -&gt; None:\n    \"\"\"Post-initialisation to set the features list. If not provided, all columns except the fixed ones are used.\n\n    Args:\n        _df (DataFrame): Feature matrix dataset\n        features_list (list[str] | None): List of features to use. If None, all possible features are used.\n        with_gold_standard (bool): Whether to include the gold standard set in the feature matrix.\n        label_col (str): The target column when the feature matrix represents the gold standard\n\n    \"\"\"\n    self.with_gold_standard = with_gold_standard\n    self.fixed_cols = [\"studyLocusId\", \"geneId\"]\n    if self.with_gold_standard:\n        self.label_col = label_col\n        self.fixed_cols.append(label_col)\n    if \"traitFromSourceMappedId\" in _df.columns:\n        self.fixed_cols.append(\"traitFromSourceMappedId\")\n\n    self.features_list = features_list or [\n        col for col in _df.columns if col not in self.fixed_cols\n    ]\n    self._df = _df.selectExpr(\n        self.fixed_cols\n        + [\n            f\"CAST({feature} AS FLOAT) AS {feature}\"\n            for feature in self.features_list\n        ]\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.append_null_features","title":"<code>append_null_features(features_list: list[str]) -&gt; L2GFeatureMatrix</code>","text":"<p>Add features from the list that are not already in the dataframe as null columns filled with 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>features_list</code> <code>list[str]</code> <p>List of features to check and add if missing</p> required <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>Updated feature matrix with additional features</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def append_null_features(self, features_list: list[str]) -&gt; L2GFeatureMatrix:\n    \"\"\"Add features from the list that are not already in the dataframe as null columns filled with 0.0.\n\n    Args:\n        features_list (list[str]): List of features to check and add if missing\n\n    Returns:\n        L2GFeatureMatrix: Updated feature matrix with additional features\n    \"\"\"\n    null_features = [\n        feature for feature in features_list if feature not in self._df.columns\n    ]\n    if null_features:\n        for feature in null_features:\n            self._df = self._df.withColumn(feature, f.lit(0.0))\n        self.features_list.extend(null_features)\n\n    return self\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.calculate_feature_missingness_rate","title":"<code>calculate_feature_missingness_rate() -&gt; dict[str, float]</code>","text":"<p>Calculate the proportion of missing values in each feature.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Dictionary of feature names and their missingness rate.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no features are found.</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def calculate_feature_missingness_rate(\n    self: L2GFeatureMatrix,\n) -&gt; dict[str, float]:\n    \"\"\"Calculate the proportion of missing values in each feature.\n\n    Returns:\n        dict[str, float]: Dictionary of feature names and their missingness rate.\n\n    Raises:\n        ValueError: If no features are found.\n    \"\"\"\n    total_count = self._df.count()\n    if not self.features_list:\n        raise ValueError(\"No features found\")\n\n    return {\n        feature: (\n            self._df.filter(\n                (self._df[feature].isNull()) | (self._df[feature] == 0)\n            ).count()\n            / total_count\n        )\n        for feature in self.features_list\n    }\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.fill_na","title":"<code>fill_na(na_value: float = 0.0, subset: list[str] | None = None) -&gt; L2GFeatureMatrix</code>","text":"<p>Fill missing values in a column with a given value.</p> <p>For features that correspond to gene attributes, missing values are imputed using the mean of the column.</p> <p>Parameters:</p> Name Type Description Default <code>na_value</code> <code>float</code> <p>Value to replace missing values with. Defaults to 0.0.</p> <code>0.0</code> <code>subset</code> <code>list[str] | None</code> <p>Subset of columns to consider. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>L2G feature matrix dataset</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def fill_na(\n    self: L2GFeatureMatrix, na_value: float = 0.0, subset: list[str] | None = None\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Fill missing values in a column with a given value.\n\n    For features that correspond to gene attributes, missing values are imputed using the mean of the column.\n\n    Args:\n        na_value (float): Value to replace missing values with. Defaults to 0.0.\n        subset (list[str] | None): Subset of columns to consider. Defaults to None.\n\n    Returns:\n        L2GFeatureMatrix: L2G feature matrix dataset\n    \"\"\"\n    cols_to_impute = [\n        \"proteinGeneCount500kb\",\n        \"geneCount500kb\",\n    ]\n    for col in cols_to_impute:\n        if col not in self._df.columns:\n            continue\n        else:\n            self._df = self._df.withColumn(\n                col,\n                f.when(\n                    f.col(col).isNull(),\n                    f.mean(f.col(col)).over(Window.partitionBy(\"studyLocusId\")),\n                ).otherwise(f.col(col)),\n            )\n    self._df = self._df.fillna(na_value, subset=subset)\n    return self\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.from_features_list","title":"<code>from_features_list(study_loci_to_annotate: StudyLocus | L2GGoldStandard, features_list: list[str], features_input_loader: L2GFeatureInputLoader) -&gt; L2GFeatureMatrix</code>  <code>classmethod</code>","text":"<p>Generate features from the gentropy datasets by calling the feature factory that will instantiate the corresponding features.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>Study locus pairs to annotate</p> required <code>features_list</code> <code>list[str]</code> <p>List of feature names to be computed.</p> required <code>features_input_loader</code> <code>L2GFeatureInputLoader</code> <p>Object that contais features input.</p> required <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>L2G feature matrix dataset</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>@classmethod\ndef from_features_list(\n    cls: type[L2GFeatureMatrix],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    features_list: list[str],\n    features_input_loader: L2GFeatureInputLoader,\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Generate features from the gentropy datasets by calling the feature factory that will instantiate the corresponding features.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): Study locus pairs to annotate\n        features_list (list[str]): List of feature names to be computed.\n        features_input_loader (L2GFeatureInputLoader): Object that contais features input.\n\n    Returns:\n        L2GFeatureMatrix: L2G feature matrix dataset\n    \"\"\"\n    features_long_df = reduce(\n        lambda x, y: x.unionByName(y, allowMissingColumns=True),\n        [\n            # Compute all features and merge them into a single dataframe\n            feature.df\n            for feature in FeatureFactory(\n                study_loci_to_annotate, features_list\n            ).generate_features(features_input_loader)\n        ],\n    )\n    if isinstance(study_loci_to_annotate, L2GGoldStandard):\n        return cls(\n            _df=convert_from_long_to_wide(\n                # Add gold standard set to the feature matrix\n                features_long_df.join(\n                    study_loci_to_annotate.df.select(\n                        \"studyLocusId\", \"geneId\", \"goldStandardSet\"\n                    ),\n                    [\"studyLocusId\", \"geneId\"],\n                ),\n                [\"studyLocusId\", \"geneId\", \"goldStandardSet\"],\n                \"featureName\",\n                \"featureValue\",\n            ),\n            with_gold_standard=True,\n        )\n    return cls(\n        _df=convert_from_long_to_wide(\n            features_long_df,\n            [\"studyLocusId\", \"geneId\"],\n            \"featureName\",\n            \"featureValue\",\n        ),\n        with_gold_standard=False,\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.generate_train_test_split","title":"<code>generate_train_test_split(test_size: float, verbose: bool, label_encoder: dict[str, int], label_col: str) -&gt; tuple[pd_dataframe, pd_dataframe]</code>","text":"<p>Generate train and test splits for the feature matrix.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float</code> <p>Proportion of the test set</p> required <code>verbose</code> <code>bool</code> <p>Whether to print verbose output</p> required <code>label_encoder</code> <code>dict[str, int]</code> <p>Label encoder for the gold standard set</p> required <code>label_col</code> <code>str</code> <p>Column name for the gold standard set</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd_dataframe, pd_dataframe]: Train and test splits</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def generate_train_test_split(\n    self,\n    test_size: float,\n    verbose: bool,\n    label_encoder: dict[str, int],\n    label_col: str,\n) -&gt; tuple[pd_dataframe, pd_dataframe]:\n    \"\"\"Generate train and test splits for the feature matrix.\n\n    Args:\n        test_size (float): Proportion of the test set\n        verbose (bool): Whether to print verbose output\n        label_encoder (dict[str, int]): Label encoder for the gold standard set\n        label_col (str): Column name for the gold standard set\n\n    Returns:\n        tuple[pd_dataframe, pd_dataframe]: Train and test splits\n    \"\"\"\n    from gentropy.method.l2g.trainer import LocusToGeneTrainer\n\n    data_df = self._df.toPandas()\n\n    # Encode labels in `goldStandardSet` to a numeric value\n    data_df[label_col] = data_df[label_col].map(label_encoder)\n\n    # Generate train, held out sets\n    return LocusToGeneTrainer.hierarchical_split(\n        data_df, test_size=test_size, verbose=verbose\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.persist","title":"<code>persist() -&gt; Self</code>","text":"<p>Persist the feature matrix in memory.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>Persisted Dataset</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def persist(self: Self) -&gt; Self:\n    \"\"\"Persist the feature matrix in memory.\n\n    Returns:\n        Self: Persisted Dataset\n    \"\"\"\n    self._df = self._df.persist()\n    return self\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#gentropy.dataset.l2g_feature_matrix.L2GFeatureMatrix.select_features","title":"<code>select_features(features_list: list[str] | None) -&gt; L2GFeatureMatrix</code>","text":"<p>Returns a new object with a subset of features from the original feature matrix.</p> <p>Parameters:</p> Name Type Description Default <code>features_list</code> <code>list[str] | None</code> <p>List of features to select</p> required <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>L2G feature matrix dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no features have been selected.</p> Source code in <code>src/gentropy/dataset/l2g_feature_matrix.py</code> <pre><code>def select_features(\n    self: L2GFeatureMatrix,\n    features_list: list[str] | None,\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Returns a new object with a subset of features from the original feature matrix.\n\n    Args:\n        features_list (list[str] | None): List of features to select\n\n    Returns:\n        L2GFeatureMatrix: L2G feature matrix dataset\n\n    Raises:\n        ValueError: If no features have been selected.\n    \"\"\"\n    if features_list := features_list or self.features_list:\n        # cast to float every feature in the features_list\n        return L2GFeatureMatrix(\n            _df=self._df.selectExpr(\n                self.fixed_cols\n                + [\n                    f\"CAST({feature} AS FLOAT) AS {feature}\"\n                    for feature in features_list\n                ]\n            ),\n            features_list=features_list,\n            with_gold_standard=self.with_gold_standard,\n        )\n    raise ValueError(\"features_list cannot be None\")\n</code></pre>"},{"location":"python_api/datasets/l2g_feature_matrix/#schema","title":"Schema","text":""},{"location":"python_api/datasets/l2g_gold_standard/","title":"L2G Gold Standard","text":""},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard","title":"<code>gentropy.dataset.l2g_gold_standard.L2GGoldStandard</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>L2G gold standard dataset.</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>@dataclass\nclass L2GGoldStandard(Dataset):\n    \"\"\"L2G gold standard dataset.\"\"\"\n\n    INTERACTION_THRESHOLD = 0.7\n    GS_POSITIVE_LABEL = \"positive\"\n    GS_NEGATIVE_LABEL = \"negative\"\n\n    @classmethod\n    def from_otg_curation(\n        cls: type[L2GGoldStandard],\n        gold_standard_curation: DataFrame,\n        study_locus_overlap: StudyLocusOverlap,\n        variant_index: VariantIndex,\n        interactions: DataFrame,\n    ) -&gt; L2GGoldStandard:\n        \"\"\"Initialise L2GGoldStandard from source dataset.\n\n        Args:\n            gold_standard_curation (DataFrame): Gold standard curation dataframe, extracted from\n            study_locus_overlap (StudyLocusOverlap): Study locus overlap dataset to remove duplicated loci\n            variant_index (VariantIndex): Dataset to bring distance between a variant and a gene's footprint\n            interactions (DataFrame): Gene-gene interactions dataset to remove negative cases where the gene interacts with a positive gene\n\n        Returns:\n            L2GGoldStandard: L2G Gold Standard dataset\n        \"\"\"\n        from gentropy.datasource.open_targets.l2g_gold_standard import (\n            OpenTargetsL2GGoldStandard,\n        )\n\n        interactions_df = cls.process_gene_interactions(interactions)\n\n        return (\n            OpenTargetsL2GGoldStandard.as_l2g_gold_standard(\n                gold_standard_curation, variant_index\n            )\n            .filter_unique_associations(study_locus_overlap)\n            .remove_false_negatives(interactions_df)\n        )\n\n    @classmethod\n    def get_schema(cls: type[L2GGoldStandard]) -&gt; StructType:\n        \"\"\"Provides the schema for the L2GGoldStandard dataset.\n\n        Returns:\n            StructType: Spark schema for the L2GGoldStandard dataset\n        \"\"\"\n        return parse_spark_schema(\"l2g_gold_standard.json\")\n\n    @classmethod\n    def process_gene_interactions(\n        cls: type[L2GGoldStandard], interactions: DataFrame\n    ) -&gt; DataFrame:\n        \"\"\"Extract top scoring gene-gene interaction from the interactions dataset of the Platform.\n\n        Args:\n            interactions (DataFrame): Gene-gene interactions dataset from the Open Targets Platform\n\n        Returns:\n            DataFrame: Top scoring gene-gene interaction per pair of genes\n\n        Examples:\n            &gt;&gt;&gt; interactions = spark.createDataFrame([(\"gene1\", \"gene2\", 0.8), (\"gene1\", \"gene2\", 0.5), (\"gene2\", \"gene3\", 0.7)], [\"targetA\", \"targetB\", \"scoring\"])\n            &gt;&gt;&gt; L2GGoldStandard.process_gene_interactions(interactions).show()\n            +-------+-------+-----+\n            |geneIdA|geneIdB|score|\n            +-------+-------+-----+\n            |  gene1|  gene2|  0.8|\n            |  gene2|  gene3|  0.7|\n            +-------+-------+-----+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return get_record_with_maximum_value(\n            interactions,\n            [\"targetA\", \"targetB\"],\n            \"scoring\",\n        ).selectExpr(\n            \"targetA as geneIdA\",\n            \"targetB as geneIdB\",\n            \"scoring as score\",\n        )\n\n    def build_feature_matrix(\n        self: L2GGoldStandard,\n        full_feature_matrix: L2GFeatureMatrix,\n        credible_set: StudyLocus,\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Return a feature matrix for study loci in the gold standard.\n\n        Args:\n            full_feature_matrix (L2GFeatureMatrix): Feature matrix for all study loci to join on\n            credible_set (StudyLocus): Full credible sets to annotate the feature matrix with variant and study IDs and perform the join\n\n        Returns:\n            L2GFeatureMatrix: Feature matrix for study loci in the gold standard\n        \"\"\"\n        from gentropy.dataset.l2g_feature_matrix import L2GFeatureMatrix\n\n        return L2GFeatureMatrix(\n            _df=full_feature_matrix._df.join(\n                credible_set.df.select(\"studyLocusId\", \"variantId\", \"studyId\"),\n                \"studyLocusId\",\n                \"left\",\n            )\n            .join(\n                f.broadcast(self.df.drop(\"studyLocusId\", \"sources\")),\n                on=[\"studyId\", \"variantId\", \"geneId\"],\n                how=\"inner\",\n            )\n            .filter(f.col(\"isProteinCoding\") == 1)\n            .drop(\"studyId\", \"variantId\")\n            .distinct(),\n            with_gold_standard=True,\n        ).fill_na()\n\n    def filter_unique_associations(\n        self: L2GGoldStandard,\n        study_locus_overlap: StudyLocusOverlap,\n    ) -&gt; L2GGoldStandard:\n        \"\"\"Refines the gold standard to filter out loci that are not independent.\n\n        Rules:\n        - If two loci point to the same gene, one positive and one negative, and have overlapping variants, we keep the positive one.\n        - If two loci point to the same gene, both positive or negative, and have overlapping variants, we drop one.\n        - If two loci point to different genes, and have overlapping variants, we keep both.\n\n        Args:\n            study_locus_overlap (StudyLocusOverlap): A dataset detailing variants that overlap between StudyLocus.\n\n        Returns:\n            L2GGoldStandard: L2GGoldStandard updated to exclude false negatives and redundant positives.\n        \"\"\"\n        squared_overlaps = study_locus_overlap._convert_to_square_matrix()\n        unique_associations = (\n            self.df.alias(\"left\")\n            # identify all the study loci that point to the same gene\n            .withColumn(\n                \"sl_same_gene\",\n                f.collect_set(\"studyLocusId\").over(Window.partitionBy(\"geneId\")),\n            )\n            # identify all the study loci that have an overlapping variant\n            .join(\n                squared_overlaps.df.alias(\"right\"),\n                (f.col(\"left.studyLocusId\") == f.col(\"right.leftStudyLocusId\"))\n                &amp; (f.col(\"left.variantId\") == f.col(\"right.tagVariantId\")),\n                \"left\",\n            )\n            .withColumn(\n                \"overlaps\",\n                f.when(f.col(\"right.tagVariantId\").isNotNull(), f.lit(True)).otherwise(\n                    f.lit(False)\n                ),\n            )\n            # drop redundant rows: where the variantid overlaps and the gene is \"explained\" by more than one study locus\n            .filter(~((f.size(\"sl_same_gene\") &gt; 1) &amp; (f.col(\"overlaps\") == 1)))\n            .select(*self.df.columns)\n        )\n        return L2GGoldStandard(_df=unique_associations, _schema=self.get_schema())\n\n    def remove_false_negatives(\n        self: L2GGoldStandard,\n        interactions_df: DataFrame,\n    ) -&gt; L2GGoldStandard:\n        \"\"\"Refines the gold standard to remove negative gold standard instances where the gene interacts with a positive gene.\n\n        Args:\n            interactions_df (DataFrame): Top scoring gene-gene interaction per pair of genes\n\n        Returns:\n            L2GGoldStandard: A refined set of locus-to-gene associations with increased reliability, having excluded loci that were likely false negatives due to gene-gene interaction confounding.\n        \"\"\"\n        squared_interactions = interactions_df.unionByName(\n            interactions_df.selectExpr(\n                \"geneIdB as geneIdA\", \"geneIdA as geneIdB\", \"score\"\n            )\n        ).filter(f.col(\"score\") &gt; self.INTERACTION_THRESHOLD)\n        df = (\n            self.df.alias(\"left\")\n            .join(\n                # bring gene partners\n                squared_interactions.alias(\"right\"),\n                f.col(\"left.geneId\") == f.col(\"right.geneIdA\"),\n                \"left\",\n            )\n            .withColumnRenamed(\"geneIdB\", \"interactorGeneId\")\n            .join(\n                # bring gold standard status for gene partners\n                self.df.selectExpr(\n                    \"geneId as interactorGeneId\",\n                    \"goldStandardSet as interactorGeneIdGoldStandardSet\",\n                ),\n                \"interactorGeneId\",\n                \"left\",\n            )\n            # remove self-interactions\n            .filter(\n                (f.col(\"geneId\") != f.col(\"interactorGeneId\"))\n                | (f.col(\"interactorGeneId\").isNull())\n            )\n            # remove false negatives\n            .filter(\n                # drop rows where the GS gene is negative but the interactor is a GS positive\n                ~(f.col(\"goldStandardSet\") == \"negative\")\n                &amp; (f.col(\"interactorGeneIdGoldStandardSet\") == \"positive\")\n                |\n                # keep rows where the gene does not interact\n                (f.col(\"interactorGeneId\").isNull())\n            )\n            .select(*self.df.columns)\n            .distinct()\n        )\n        return L2GGoldStandard(_df=df, _schema=self.get_schema())\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.build_feature_matrix","title":"<code>build_feature_matrix(full_feature_matrix: L2GFeatureMatrix, credible_set: StudyLocus) -&gt; L2GFeatureMatrix</code>","text":"<p>Return a feature matrix for study loci in the gold standard.</p> <p>Parameters:</p> Name Type Description Default <code>full_feature_matrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix for all study loci to join on</p> required <code>credible_set</code> <code>StudyLocus</code> <p>Full credible sets to annotate the feature matrix with variant and study IDs and perform the join</p> required <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix for study loci in the gold standard</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>def build_feature_matrix(\n    self: L2GGoldStandard,\n    full_feature_matrix: L2GFeatureMatrix,\n    credible_set: StudyLocus,\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Return a feature matrix for study loci in the gold standard.\n\n    Args:\n        full_feature_matrix (L2GFeatureMatrix): Feature matrix for all study loci to join on\n        credible_set (StudyLocus): Full credible sets to annotate the feature matrix with variant and study IDs and perform the join\n\n    Returns:\n        L2GFeatureMatrix: Feature matrix for study loci in the gold standard\n    \"\"\"\n    from gentropy.dataset.l2g_feature_matrix import L2GFeatureMatrix\n\n    return L2GFeatureMatrix(\n        _df=full_feature_matrix._df.join(\n            credible_set.df.select(\"studyLocusId\", \"variantId\", \"studyId\"),\n            \"studyLocusId\",\n            \"left\",\n        )\n        .join(\n            f.broadcast(self.df.drop(\"studyLocusId\", \"sources\")),\n            on=[\"studyId\", \"variantId\", \"geneId\"],\n            how=\"inner\",\n        )\n        .filter(f.col(\"isProteinCoding\") == 1)\n        .drop(\"studyId\", \"variantId\")\n        .distinct(),\n        with_gold_standard=True,\n    ).fill_na()\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.filter_unique_associations","title":"<code>filter_unique_associations(study_locus_overlap: StudyLocusOverlap) -&gt; L2GGoldStandard</code>","text":"<p>Refines the gold standard to filter out loci that are not independent.</p> <p>Rules: - If two loci point to the same gene, one positive and one negative, and have overlapping variants, we keep the positive one. - If two loci point to the same gene, both positive or negative, and have overlapping variants, we drop one. - If two loci point to different genes, and have overlapping variants, we keep both.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus_overlap</code> <code>StudyLocusOverlap</code> <p>A dataset detailing variants that overlap between StudyLocus.</p> required <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>L2GGoldStandard updated to exclude false negatives and redundant positives.</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>def filter_unique_associations(\n    self: L2GGoldStandard,\n    study_locus_overlap: StudyLocusOverlap,\n) -&gt; L2GGoldStandard:\n    \"\"\"Refines the gold standard to filter out loci that are not independent.\n\n    Rules:\n    - If two loci point to the same gene, one positive and one negative, and have overlapping variants, we keep the positive one.\n    - If two loci point to the same gene, both positive or negative, and have overlapping variants, we drop one.\n    - If two loci point to different genes, and have overlapping variants, we keep both.\n\n    Args:\n        study_locus_overlap (StudyLocusOverlap): A dataset detailing variants that overlap between StudyLocus.\n\n    Returns:\n        L2GGoldStandard: L2GGoldStandard updated to exclude false negatives and redundant positives.\n    \"\"\"\n    squared_overlaps = study_locus_overlap._convert_to_square_matrix()\n    unique_associations = (\n        self.df.alias(\"left\")\n        # identify all the study loci that point to the same gene\n        .withColumn(\n            \"sl_same_gene\",\n            f.collect_set(\"studyLocusId\").over(Window.partitionBy(\"geneId\")),\n        )\n        # identify all the study loci that have an overlapping variant\n        .join(\n            squared_overlaps.df.alias(\"right\"),\n            (f.col(\"left.studyLocusId\") == f.col(\"right.leftStudyLocusId\"))\n            &amp; (f.col(\"left.variantId\") == f.col(\"right.tagVariantId\")),\n            \"left\",\n        )\n        .withColumn(\n            \"overlaps\",\n            f.when(f.col(\"right.tagVariantId\").isNotNull(), f.lit(True)).otherwise(\n                f.lit(False)\n            ),\n        )\n        # drop redundant rows: where the variantid overlaps and the gene is \"explained\" by more than one study locus\n        .filter(~((f.size(\"sl_same_gene\") &gt; 1) &amp; (f.col(\"overlaps\") == 1)))\n        .select(*self.df.columns)\n    )\n    return L2GGoldStandard(_df=unique_associations, _schema=self.get_schema())\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.from_otg_curation","title":"<code>from_otg_curation(gold_standard_curation: DataFrame, study_locus_overlap: StudyLocusOverlap, variant_index: VariantIndex, interactions: DataFrame) -&gt; L2GGoldStandard</code>  <code>classmethod</code>","text":"<p>Initialise L2GGoldStandard from source dataset.</p> <p>Parameters:</p> Name Type Description Default <code>gold_standard_curation</code> <code>DataFrame</code> <p>Gold standard curation dataframe, extracted from</p> required <code>study_locus_overlap</code> <code>StudyLocusOverlap</code> <p>Study locus overlap dataset to remove duplicated loci</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Dataset to bring distance between a variant and a gene's footprint</p> required <code>interactions</code> <code>DataFrame</code> <p>Gene-gene interactions dataset to remove negative cases where the gene interacts with a positive gene</p> required <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>L2G Gold Standard dataset</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef from_otg_curation(\n    cls: type[L2GGoldStandard],\n    gold_standard_curation: DataFrame,\n    study_locus_overlap: StudyLocusOverlap,\n    variant_index: VariantIndex,\n    interactions: DataFrame,\n) -&gt; L2GGoldStandard:\n    \"\"\"Initialise L2GGoldStandard from source dataset.\n\n    Args:\n        gold_standard_curation (DataFrame): Gold standard curation dataframe, extracted from\n        study_locus_overlap (StudyLocusOverlap): Study locus overlap dataset to remove duplicated loci\n        variant_index (VariantIndex): Dataset to bring distance between a variant and a gene's footprint\n        interactions (DataFrame): Gene-gene interactions dataset to remove negative cases where the gene interacts with a positive gene\n\n    Returns:\n        L2GGoldStandard: L2G Gold Standard dataset\n    \"\"\"\n    from gentropy.datasource.open_targets.l2g_gold_standard import (\n        OpenTargetsL2GGoldStandard,\n    )\n\n    interactions_df = cls.process_gene_interactions(interactions)\n\n    return (\n        OpenTargetsL2GGoldStandard.as_l2g_gold_standard(\n            gold_standard_curation, variant_index\n        )\n        .filter_unique_associations(study_locus_overlap)\n        .remove_false_negatives(interactions_df)\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the L2GGoldStandard dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Spark schema for the L2GGoldStandard dataset</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[L2GGoldStandard]) -&gt; StructType:\n    \"\"\"Provides the schema for the L2GGoldStandard dataset.\n\n    Returns:\n        StructType: Spark schema for the L2GGoldStandard dataset\n    \"\"\"\n    return parse_spark_schema(\"l2g_gold_standard.json\")\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.process_gene_interactions","title":"<code>process_gene_interactions(interactions: DataFrame) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Extract top scoring gene-gene interaction from the interactions dataset of the Platform.</p> <p>Parameters:</p> Name Type Description Default <code>interactions</code> <code>DataFrame</code> <p>Gene-gene interactions dataset from the Open Targets Platform</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Top scoring gene-gene interaction per pair of genes</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; interactions = spark.createDataFrame([(\"gene1\", \"gene2\", 0.8), (\"gene1\", \"gene2\", 0.5), (\"gene2\", \"gene3\", 0.7)], [\"targetA\", \"targetB\", \"scoring\"])\n&gt;&gt;&gt; L2GGoldStandard.process_gene_interactions(interactions).show()\n+-------+-------+-----+\n|geneIdA|geneIdB|score|\n+-------+-------+-----+\n|  gene1|  gene2|  0.8|\n|  gene2|  gene3|  0.7|\n+-------+-------+-----+\n</code></pre> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef process_gene_interactions(\n    cls: type[L2GGoldStandard], interactions: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Extract top scoring gene-gene interaction from the interactions dataset of the Platform.\n\n    Args:\n        interactions (DataFrame): Gene-gene interactions dataset from the Open Targets Platform\n\n    Returns:\n        DataFrame: Top scoring gene-gene interaction per pair of genes\n\n    Examples:\n        &gt;&gt;&gt; interactions = spark.createDataFrame([(\"gene1\", \"gene2\", 0.8), (\"gene1\", \"gene2\", 0.5), (\"gene2\", \"gene3\", 0.7)], [\"targetA\", \"targetB\", \"scoring\"])\n        &gt;&gt;&gt; L2GGoldStandard.process_gene_interactions(interactions).show()\n        +-------+-------+-----+\n        |geneIdA|geneIdB|score|\n        +-------+-------+-----+\n        |  gene1|  gene2|  0.8|\n        |  gene2|  gene3|  0.7|\n        +-------+-------+-----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return get_record_with_maximum_value(\n        interactions,\n        [\"targetA\", \"targetB\"],\n        \"scoring\",\n    ).selectExpr(\n        \"targetA as geneIdA\",\n        \"targetB as geneIdB\",\n        \"scoring as score\",\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#gentropy.dataset.l2g_gold_standard.L2GGoldStandard.remove_false_negatives","title":"<code>remove_false_negatives(interactions_df: DataFrame) -&gt; L2GGoldStandard</code>","text":"<p>Refines the gold standard to remove negative gold standard instances where the gene interacts with a positive gene.</p> <p>Parameters:</p> Name Type Description Default <code>interactions_df</code> <code>DataFrame</code> <p>Top scoring gene-gene interaction per pair of genes</p> required <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>A refined set of locus-to-gene associations with increased reliability, having excluded loci that were likely false negatives due to gene-gene interaction confounding.</p> Source code in <code>src/gentropy/dataset/l2g_gold_standard.py</code> <pre><code>def remove_false_negatives(\n    self: L2GGoldStandard,\n    interactions_df: DataFrame,\n) -&gt; L2GGoldStandard:\n    \"\"\"Refines the gold standard to remove negative gold standard instances where the gene interacts with a positive gene.\n\n    Args:\n        interactions_df (DataFrame): Top scoring gene-gene interaction per pair of genes\n\n    Returns:\n        L2GGoldStandard: A refined set of locus-to-gene associations with increased reliability, having excluded loci that were likely false negatives due to gene-gene interaction confounding.\n    \"\"\"\n    squared_interactions = interactions_df.unionByName(\n        interactions_df.selectExpr(\n            \"geneIdB as geneIdA\", \"geneIdA as geneIdB\", \"score\"\n        )\n    ).filter(f.col(\"score\") &gt; self.INTERACTION_THRESHOLD)\n    df = (\n        self.df.alias(\"left\")\n        .join(\n            # bring gene partners\n            squared_interactions.alias(\"right\"),\n            f.col(\"left.geneId\") == f.col(\"right.geneIdA\"),\n            \"left\",\n        )\n        .withColumnRenamed(\"geneIdB\", \"interactorGeneId\")\n        .join(\n            # bring gold standard status for gene partners\n            self.df.selectExpr(\n                \"geneId as interactorGeneId\",\n                \"goldStandardSet as interactorGeneIdGoldStandardSet\",\n            ),\n            \"interactorGeneId\",\n            \"left\",\n        )\n        # remove self-interactions\n        .filter(\n            (f.col(\"geneId\") != f.col(\"interactorGeneId\"))\n            | (f.col(\"interactorGeneId\").isNull())\n        )\n        # remove false negatives\n        .filter(\n            # drop rows where the GS gene is negative but the interactor is a GS positive\n            ~(f.col(\"goldStandardSet\") == \"negative\")\n            &amp; (f.col(\"interactorGeneIdGoldStandardSet\") == \"positive\")\n            |\n            # keep rows where the gene does not interact\n            (f.col(\"interactorGeneId\").isNull())\n        )\n        .select(*self.df.columns)\n        .distinct()\n    )\n    return L2GGoldStandard(_df=df, _schema=self.get_schema())\n</code></pre>"},{"location":"python_api/datasets/l2g_gold_standard/#schema","title":"Schema","text":"<pre><code>root\n |-- studyLocusId: string (nullable = false)\n |-- variantId: string (nullable = false)\n |-- studyId: string (nullable = false)\n |-- geneId: string (nullable = false)\n |-- traitFromSourceMappedId: string (nullable = true)\n |-- goldStandardSet: string (nullable = false)\n |-- sources: array (nullable = true)\n |    |-- element: string (containsNull = true)\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/","title":"L2G Prediction","text":""},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction","title":"<code>gentropy.dataset.l2g_prediction.L2GPrediction</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset that contains the Locus to Gene predictions.</p> <p>It is the result of applying the L2G model on a feature matrix, which contains all the study/locus pairs and their functional annotations. The score column informs the confidence of the prediction that a gene is causal to an association.</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>@dataclass\nclass L2GPrediction(Dataset):\n    \"\"\"Dataset that contains the Locus to Gene predictions.\n\n    It is the result of applying the L2G model on a feature matrix, which contains all\n    the study/locus pairs and their functional annotations. The score column informs the\n    confidence of the prediction that a gene is causal to an association.\n    \"\"\"\n\n    model: LocusToGeneModel | None = field(default=None, repr=False)\n\n    @classmethod\n    def get_schema(cls: type[L2GPrediction]) -&gt; StructType:\n        \"\"\"Provides the schema for the L2GPrediction dataset.\n\n        Returns:\n            StructType: Schema for the L2GPrediction dataset\n        \"\"\"\n        return parse_spark_schema(\"l2g_predictions.json\")\n\n    @classmethod\n    def from_credible_set(\n        cls: type[L2GPrediction],\n        session: Session,\n        credible_set: StudyLocus,\n        feature_matrix: L2GFeatureMatrix,\n        model_path: str | None,\n        features_list: list[str] | None = None,\n        hf_token: str | None = None,\n        hf_model_version: str | None = None,\n        download_from_hub: bool = True,\n    ) -&gt; L2GPrediction:\n        \"\"\"Extract L2G predictions for a set of credible sets derived from GWAS.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            credible_set (StudyLocus): Dataset containing credible sets from GWAS only\n            feature_matrix (L2GFeatureMatrix): Dataset containing all credible sets and their annotations\n            model_path (str | None): Path to the model file. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).\n            features_list (list[str] | None): Default list of features the model uses. Only used if the model is not downloaded from the Hub. CAUTION: This default list can differ from the actual list the model was trained on.\n            hf_token (str | None): Hugging Face token to download the model from the Hub. Only required if the model is private.\n            hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n            download_from_hub (bool): Whether to download the model from the Hugging Face Hub. Defaults to True.\n\n        Returns:\n            L2GPrediction: L2G scores for a set of credible sets.\n\n        Raises:\n            AttributeError: If `features_list` is not provided and the model is not downloaded from the Hub.\n        \"\"\"\n        # Load the model\n        if download_from_hub:\n            # Model ID defaults to \"opentargets/locus_to_gene\" and it assumes the name of the classifier is \"classifier.skops\".\n            model_id = model_path or \"opentargets/locus_to_gene\"\n            l2g_model = LocusToGeneModel.load_from_hub(\n                session, model_id, hf_model_version, hf_token\n            )\n        elif model_path:\n            if not features_list:\n                raise AttributeError(\n                    \"features_list is required if the model is not downloaded from the Hub\"\n                )\n            l2g_model = LocusToGeneModel.load_from_disk(\n                session, path=model_path, features_list=features_list\n            )\n\n        # Prepare data\n        fm = (\n            L2GFeatureMatrix(\n                _df=(\n                    credible_set.df.filter(f.col(\"studyType\") == \"gwas\")\n                    .select(\"studyLocusId\")\n                    .join(feature_matrix._df, \"studyLocusId\")\n                    .filter(f.col(\"isProteinCoding\") == 1)\n                ),\n            )\n            .fill_na()\n            .select_features(l2g_model.features_list)\n        )\n        return l2g_model.predict(fm, session)\n\n    def to_disease_target_evidence(\n        self: L2GPrediction,\n        study_locus: StudyLocus,\n        study_index: StudyIndex,\n        l2g_threshold: float = 0.05,\n    ) -&gt; DataFrame:\n        \"\"\"Convert locus to gene predictions to disease target evidence.\n\n        Args:\n            study_locus (StudyLocus): Study locus dataset\n            study_index (StudyIndex): Study index dataset\n            l2g_threshold (float): Threshold to consider a gene as a target. Defaults to 0.05.\n\n        Returns:\n            DataFrame: Disease target evidence\n\n        Raises:\n            ValueError: if `diseaseIds` column is missing.\n        \"\"\"\n        datasource_id = \"gwas_credible_sets\"\n        datatype_id = \"genetic_association\"\n\n        # A set of optional columns need to be in the input datasets:\n        if \"diseaseIds\" not in study_index.df.columns:\n            raise ValueError(\n                \"DiseaseIds column has to be in the study index to generate disase/target evidence.\"\n            )\n\n        # `pubmedId` and `publicationDate` are optional columns in the study index, so we need to make sure they're there:\n        for optional_column in [\"publicationDate\", \"pubmedId\"]:\n            if optional_column not in study_index.df.columns:\n                study_index = StudyIndex(\n                    study_index.df.withColumn(\n                        optional_column, f.lit(None).cast(StringType())\n                    )\n                )\n\n        return (\n            self.df.filter(f.col(\"score\") &gt;= l2g_threshold)\n            .join(\n                study_locus.df.select(\"studyLocusId\", \"studyId\"),\n                on=\"studyLocusId\",\n                how=\"inner\",\n            )\n            .join(\n                study_index.df.select(\n                    \"studyId\",\n                    \"diseaseIds\",\n                    f.when(\n                        f.col(\"publicationDate\").rlike(r\"\\d{4}-\\d{2}-\\d{2}\"),\n                        f.col(\"publicationDate\"),\n                    ).alias(\"curationDate\"),\n                    # Only store pubmed id if provided from source:\n                    f.when(\n                        f.col(\"pubmedId\").isNotNull(), f.array(f.col(\"pubmedId\"))\n                    ).alias(\"literature\"),\n                ),\n                on=\"studyId\",\n                how=\"inner\",\n            )\n            .select(\n                f.lit(datatype_id).alias(\"datatypeId\"),\n                f.lit(datasource_id).alias(\"datasourceId\"),\n                f.col(\"geneId\").alias(\"targetFromSourceId\"),\n                f.explode(f.col(\"diseaseIds\")).alias(\"diseaseFromSourceMappedId\"),\n                f.col(\"score\").alias(\"resourceScore\"),\n                \"curationDate\",\n                \"studyLocusId\",\n                \"literature\",\n            )\n        )\n\n    def explain(\n        self: L2GPrediction, feature_matrix: L2GFeatureMatrix | None = None\n    ) -&gt; L2GPrediction:\n        \"\"\"Extract Shapley values for the L2G predictions and add them as a map in an additional column.\n\n        Args:\n            feature_matrix (L2GFeatureMatrix | None): Feature matrix in case the predictions are missing the feature annotation. If None, the features are fetched from the dataset.\n\n        Returns:\n            L2GPrediction: L2GPrediction object with additional column containing feature name to Shapley value mappings\n\n        Raises:\n            ValueError: If the model is not set or If feature matrix is not provided and the predictions do not have features\n        \"\"\"\n        # Fetch features if they are not present:\n        if \"features\" not in self.df.columns:\n            if feature_matrix is None:\n                raise ValueError(\n                    \"Feature matrix is required to explain the L2G predictions\"\n                )\n            self.add_features(feature_matrix)\n\n        if self.model is None:\n            raise ValueError(\"Model not set, explainer cannot be created\")\n\n        # Format and pivot the dataframe to pass them before calculating shapley values\n        pdf = pivot_df(\n            df=self.df.withColumn(\"feature\", f.explode(\"features\")).select(\n                \"studyLocusId\",\n                \"geneId\",\n                \"score\",\n                f.col(\"feature.name\").alias(\"feature_name\"),\n                f.col(\"feature.value\").alias(\"feature_value\"),\n            ),\n            pivot_col=\"feature_name\",\n            value_col=\"feature_value\",\n            grouping_cols=[f.col(\"studyLocusId\"), f.col(\"geneId\"), f.col(\"score\")],\n        ).toPandas()\n        pdf = pdf.rename(\n            # trim the suffix that is added after pivoting the df\n            columns={\n                col: col.replace(\"_feature_value\", \"\")\n                for col in pdf.columns\n                if col.endswith(\"_feature_value\")\n            }\n        )\n\n        features_list = self.model.features_list  # The matrix needs to present the features in the same order that the model was trained on)\n        base_value, shap_values = L2GPrediction._explain(\n            model=self.model,\n            pdf=pdf.filter(items=features_list),\n        )\n        for i, feature in enumerate(features_list):\n            pdf[f\"shap_{feature}\"] = [row[i] for row in shap_values]\n\n        spark_session = self.df.sparkSession\n        return L2GPrediction(\n            _df=(\n                spark_session.createDataFrame(pdf.to_dict(orient=\"records\"))\n                .withColumn(\n                    \"features\",\n                    f.array(\n                        *(\n                            f.struct(\n                                f.lit(feature).alias(\"name\"),\n                                f.col(feature).cast(\"float\").alias(\"value\"),\n                                f.col(f\"shap_{feature}\")\n                                .cast(\"float\")\n                                .alias(\"shapValue\"),\n                            )\n                            for feature in features_list\n                        )\n                    ),\n                )\n                .withColumn(\"shapBaseValue\", f.lit(base_value).cast(\"float\"))\n                .select(*L2GPrediction.get_schema().names)\n            ),\n            _schema=self.get_schema(),\n            model=self.model,\n        )\n\n    @staticmethod\n    def _explain(\n        model: LocusToGeneModel, pdf: pd_dataframe\n    ) -&gt; tuple[float, list[list[float]]]:\n        \"\"\"Calculate SHAP values. Output is in probability form (approximated from the log odds ratios).\n\n        Args:\n            model (LocusToGeneModel): L2G model\n            pdf (pd_dataframe): Pandas dataframe containing the feature matrix in the same order that the model was trained on\n\n        Returns:\n            tuple[float, list[list[float]]]: A tuple containing:\n                - base_value (float): Base value of the model\n                - shap_values (list[list[float]]): SHAP values for prediction\n\n        Raises:\n            AttributeError: If model.training_data is not set, seed dataset to get shapley values cannot be created.\n        \"\"\"\n        if not model.training_data:\n            raise AttributeError(\n                \"`model.training_data` is missing, seed dataset to get shapley values cannot be created.\"\n            )\n        background_data = (\n            model.training_data._df.select(*model.features_list)\n            .toPandas()\n            .sample(n=1_000)\n        )\n        explainer = shap.TreeExplainer(\n            model.model,\n            data=background_data,\n            model_output=\"probability\",\n        )\n        if pdf.shape[0] &gt;= 10_000:\n            logging.warning(\n                \"Calculating SHAP values for more than 10,000 rows. This may take a while...\"\n            )\n        shap_values = explainer.shap_values(\n            pdf.to_numpy(),\n            check_additivity=False,\n        )\n        base_value = explainer.expected_value\n        return (base_value, shap_values)\n\n    def add_features(\n        self: L2GPrediction,\n        feature_matrix: L2GFeatureMatrix,\n    ) -&gt; L2GPrediction:\n        \"\"\"Add features used to extract the L2G predictions.\n\n        Args:\n            feature_matrix (L2GFeatureMatrix): Feature matrix dataset\n\n        Returns:\n            L2GPrediction: L2G predictions with additional column `features`\n\n        Raises:\n            ValueError: If model is not set, feature list won't be available\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not set, feature annotation cannot be created.\")\n        # Testing if `features` column already exists:\n        if \"features\" in self.df.columns:\n            self.df = self.df.drop(\"features\")\n\n        features_list = self.model.features_list\n        feature_expressions = [\n            f.struct(f.lit(col).alias(\"name\"), f.col(col).alias(\"value\"))\n            for col in features_list\n        ]\n        self.df = self.df.join(\n            feature_matrix._df.select(*features_list, \"studyLocusId\", \"geneId\"),\n            on=[\"studyLocusId\", \"geneId\"],\n            how=\"left\",\n        ).select(\n            \"studyLocusId\",\n            \"geneId\",\n            \"score\",\n            f.array(*feature_expressions).alias(\"features\"),\n        )\n        return self\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.add_features","title":"<code>add_features(feature_matrix: L2GFeatureMatrix) -&gt; L2GPrediction</code>","text":"<p>Add features used to extract the L2G predictions.</p> <p>Parameters:</p> Name Type Description Default <code>feature_matrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix dataset</p> required <p>Returns:</p> Name Type Description <code>L2GPrediction</code> <code>L2GPrediction</code> <p>L2G predictions with additional column <code>features</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model is not set, feature list won't be available</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>def add_features(\n    self: L2GPrediction,\n    feature_matrix: L2GFeatureMatrix,\n) -&gt; L2GPrediction:\n    \"\"\"Add features used to extract the L2G predictions.\n\n    Args:\n        feature_matrix (L2GFeatureMatrix): Feature matrix dataset\n\n    Returns:\n        L2GPrediction: L2G predictions with additional column `features`\n\n    Raises:\n        ValueError: If model is not set, feature list won't be available\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model not set, feature annotation cannot be created.\")\n    # Testing if `features` column already exists:\n    if \"features\" in self.df.columns:\n        self.df = self.df.drop(\"features\")\n\n    features_list = self.model.features_list\n    feature_expressions = [\n        f.struct(f.lit(col).alias(\"name\"), f.col(col).alias(\"value\"))\n        for col in features_list\n    ]\n    self.df = self.df.join(\n        feature_matrix._df.select(*features_list, \"studyLocusId\", \"geneId\"),\n        on=[\"studyLocusId\", \"geneId\"],\n        how=\"left\",\n    ).select(\n        \"studyLocusId\",\n        \"geneId\",\n        \"score\",\n        f.array(*feature_expressions).alias(\"features\"),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.explain","title":"<code>explain(feature_matrix: L2GFeatureMatrix | None = None) -&gt; L2GPrediction</code>","text":"<p>Extract Shapley values for the L2G predictions and add them as a map in an additional column.</p> <p>Parameters:</p> Name Type Description Default <code>feature_matrix</code> <code>L2GFeatureMatrix | None</code> <p>Feature matrix in case the predictions are missing the feature annotation. If None, the features are fetched from the dataset.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>L2GPrediction</code> <code>L2GPrediction</code> <p>L2GPrediction object with additional column containing feature name to Shapley value mappings</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not set or If feature matrix is not provided and the predictions do not have features</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>def explain(\n    self: L2GPrediction, feature_matrix: L2GFeatureMatrix | None = None\n) -&gt; L2GPrediction:\n    \"\"\"Extract Shapley values for the L2G predictions and add them as a map in an additional column.\n\n    Args:\n        feature_matrix (L2GFeatureMatrix | None): Feature matrix in case the predictions are missing the feature annotation. If None, the features are fetched from the dataset.\n\n    Returns:\n        L2GPrediction: L2GPrediction object with additional column containing feature name to Shapley value mappings\n\n    Raises:\n        ValueError: If the model is not set or If feature matrix is not provided and the predictions do not have features\n    \"\"\"\n    # Fetch features if they are not present:\n    if \"features\" not in self.df.columns:\n        if feature_matrix is None:\n            raise ValueError(\n                \"Feature matrix is required to explain the L2G predictions\"\n            )\n        self.add_features(feature_matrix)\n\n    if self.model is None:\n        raise ValueError(\"Model not set, explainer cannot be created\")\n\n    # Format and pivot the dataframe to pass them before calculating shapley values\n    pdf = pivot_df(\n        df=self.df.withColumn(\"feature\", f.explode(\"features\")).select(\n            \"studyLocusId\",\n            \"geneId\",\n            \"score\",\n            f.col(\"feature.name\").alias(\"feature_name\"),\n            f.col(\"feature.value\").alias(\"feature_value\"),\n        ),\n        pivot_col=\"feature_name\",\n        value_col=\"feature_value\",\n        grouping_cols=[f.col(\"studyLocusId\"), f.col(\"geneId\"), f.col(\"score\")],\n    ).toPandas()\n    pdf = pdf.rename(\n        # trim the suffix that is added after pivoting the df\n        columns={\n            col: col.replace(\"_feature_value\", \"\")\n            for col in pdf.columns\n            if col.endswith(\"_feature_value\")\n        }\n    )\n\n    features_list = self.model.features_list  # The matrix needs to present the features in the same order that the model was trained on)\n    base_value, shap_values = L2GPrediction._explain(\n        model=self.model,\n        pdf=pdf.filter(items=features_list),\n    )\n    for i, feature in enumerate(features_list):\n        pdf[f\"shap_{feature}\"] = [row[i] for row in shap_values]\n\n    spark_session = self.df.sparkSession\n    return L2GPrediction(\n        _df=(\n            spark_session.createDataFrame(pdf.to_dict(orient=\"records\"))\n            .withColumn(\n                \"features\",\n                f.array(\n                    *(\n                        f.struct(\n                            f.lit(feature).alias(\"name\"),\n                            f.col(feature).cast(\"float\").alias(\"value\"),\n                            f.col(f\"shap_{feature}\")\n                            .cast(\"float\")\n                            .alias(\"shapValue\"),\n                        )\n                        for feature in features_list\n                    )\n                ),\n            )\n            .withColumn(\"shapBaseValue\", f.lit(base_value).cast(\"float\"))\n            .select(*L2GPrediction.get_schema().names)\n        ),\n        _schema=self.get_schema(),\n        model=self.model,\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.from_credible_set","title":"<code>from_credible_set(session: Session, credible_set: StudyLocus, feature_matrix: L2GFeatureMatrix, model_path: str | None, features_list: list[str] | None = None, hf_token: str | None = None, hf_model_version: str | None = None, download_from_hub: bool = True) -&gt; L2GPrediction</code>  <code>classmethod</code>","text":"<p>Extract L2G predictions for a set of credible sets derived from GWAS.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>credible_set</code> <code>StudyLocus</code> <p>Dataset containing credible sets from GWAS only</p> required <code>feature_matrix</code> <code>L2GFeatureMatrix</code> <p>Dataset containing all credible sets and their annotations</p> required <code>model_path</code> <code>str | None</code> <p>Path to the model file. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).</p> required <code>features_list</code> <code>list[str] | None</code> <p>Default list of features the model uses. Only used if the model is not downloaded from the Hub. CAUTION: This default list can differ from the actual list the model was trained on.</p> <code>None</code> <code>hf_token</code> <code>str | None</code> <p>Hugging Face token to download the model from the Hub. Only required if the model is private.</p> <code>None</code> <code>hf_model_version</code> <code>str | None</code> <p>Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.</p> <code>None</code> <code>download_from_hub</code> <code>bool</code> <p>Whether to download the model from the Hugging Face Hub. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>L2GPrediction</code> <code>L2GPrediction</code> <p>L2G scores for a set of credible sets.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If <code>features_list</code> is not provided and the model is not downloaded from the Hub.</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>@classmethod\ndef from_credible_set(\n    cls: type[L2GPrediction],\n    session: Session,\n    credible_set: StudyLocus,\n    feature_matrix: L2GFeatureMatrix,\n    model_path: str | None,\n    features_list: list[str] | None = None,\n    hf_token: str | None = None,\n    hf_model_version: str | None = None,\n    download_from_hub: bool = True,\n) -&gt; L2GPrediction:\n    \"\"\"Extract L2G predictions for a set of credible sets derived from GWAS.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        credible_set (StudyLocus): Dataset containing credible sets from GWAS only\n        feature_matrix (L2GFeatureMatrix): Dataset containing all credible sets and their annotations\n        model_path (str | None): Path to the model file. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).\n        features_list (list[str] | None): Default list of features the model uses. Only used if the model is not downloaded from the Hub. CAUTION: This default list can differ from the actual list the model was trained on.\n        hf_token (str | None): Hugging Face token to download the model from the Hub. Only required if the model is private.\n        hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n        download_from_hub (bool): Whether to download the model from the Hugging Face Hub. Defaults to True.\n\n    Returns:\n        L2GPrediction: L2G scores for a set of credible sets.\n\n    Raises:\n        AttributeError: If `features_list` is not provided and the model is not downloaded from the Hub.\n    \"\"\"\n    # Load the model\n    if download_from_hub:\n        # Model ID defaults to \"opentargets/locus_to_gene\" and it assumes the name of the classifier is \"classifier.skops\".\n        model_id = model_path or \"opentargets/locus_to_gene\"\n        l2g_model = LocusToGeneModel.load_from_hub(\n            session, model_id, hf_model_version, hf_token\n        )\n    elif model_path:\n        if not features_list:\n            raise AttributeError(\n                \"features_list is required if the model is not downloaded from the Hub\"\n            )\n        l2g_model = LocusToGeneModel.load_from_disk(\n            session, path=model_path, features_list=features_list\n        )\n\n    # Prepare data\n    fm = (\n        L2GFeatureMatrix(\n            _df=(\n                credible_set.df.filter(f.col(\"studyType\") == \"gwas\")\n                .select(\"studyLocusId\")\n                .join(feature_matrix._df, \"studyLocusId\")\n                .filter(f.col(\"isProteinCoding\") == 1)\n            ),\n        )\n        .fill_na()\n        .select_features(l2g_model.features_list)\n    )\n    return l2g_model.predict(fm, session)\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the L2GPrediction dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the L2GPrediction dataset</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[L2GPrediction]) -&gt; StructType:\n    \"\"\"Provides the schema for the L2GPrediction dataset.\n\n    Returns:\n        StructType: Schema for the L2GPrediction dataset\n    \"\"\"\n    return parse_spark_schema(\"l2g_predictions.json\")\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#gentropy.dataset.l2g_prediction.L2GPrediction.to_disease_target_evidence","title":"<code>to_disease_target_evidence(study_locus: StudyLocus, study_index: StudyIndex, l2g_threshold: float = 0.05) -&gt; DataFrame</code>","text":"<p>Convert locus to gene predictions to disease target evidence.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus</code> <code>StudyLocus</code> <p>Study locus dataset</p> required <code>study_index</code> <code>StudyIndex</code> <p>Study index dataset</p> required <code>l2g_threshold</code> <code>float</code> <p>Threshold to consider a gene as a target. Defaults to 0.05.</p> <code>0.05</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Disease target evidence</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>diseaseIds</code> column is missing.</p> Source code in <code>src/gentropy/dataset/l2g_prediction.py</code> <pre><code>def to_disease_target_evidence(\n    self: L2GPrediction,\n    study_locus: StudyLocus,\n    study_index: StudyIndex,\n    l2g_threshold: float = 0.05,\n) -&gt; DataFrame:\n    \"\"\"Convert locus to gene predictions to disease target evidence.\n\n    Args:\n        study_locus (StudyLocus): Study locus dataset\n        study_index (StudyIndex): Study index dataset\n        l2g_threshold (float): Threshold to consider a gene as a target. Defaults to 0.05.\n\n    Returns:\n        DataFrame: Disease target evidence\n\n    Raises:\n        ValueError: if `diseaseIds` column is missing.\n    \"\"\"\n    datasource_id = \"gwas_credible_sets\"\n    datatype_id = \"genetic_association\"\n\n    # A set of optional columns need to be in the input datasets:\n    if \"diseaseIds\" not in study_index.df.columns:\n        raise ValueError(\n            \"DiseaseIds column has to be in the study index to generate disase/target evidence.\"\n        )\n\n    # `pubmedId` and `publicationDate` are optional columns in the study index, so we need to make sure they're there:\n    for optional_column in [\"publicationDate\", \"pubmedId\"]:\n        if optional_column not in study_index.df.columns:\n            study_index = StudyIndex(\n                study_index.df.withColumn(\n                    optional_column, f.lit(None).cast(StringType())\n                )\n            )\n\n    return (\n        self.df.filter(f.col(\"score\") &gt;= l2g_threshold)\n        .join(\n            study_locus.df.select(\"studyLocusId\", \"studyId\"),\n            on=\"studyLocusId\",\n            how=\"inner\",\n        )\n        .join(\n            study_index.df.select(\n                \"studyId\",\n                \"diseaseIds\",\n                f.when(\n                    f.col(\"publicationDate\").rlike(r\"\\d{4}-\\d{2}-\\d{2}\"),\n                    f.col(\"publicationDate\"),\n                ).alias(\"curationDate\"),\n                # Only store pubmed id if provided from source:\n                f.when(\n                    f.col(\"pubmedId\").isNotNull(), f.array(f.col(\"pubmedId\"))\n                ).alias(\"literature\"),\n            ),\n            on=\"studyId\",\n            how=\"inner\",\n        )\n        .select(\n            f.lit(datatype_id).alias(\"datatypeId\"),\n            f.lit(datasource_id).alias(\"datasourceId\"),\n            f.col(\"geneId\").alias(\"targetFromSourceId\"),\n            f.explode(f.col(\"diseaseIds\")).alias(\"diseaseFromSourceMappedId\"),\n            f.col(\"score\").alias(\"resourceScore\"),\n            \"curationDate\",\n            \"studyLocusId\",\n            \"literature\",\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_prediction/#schema","title":"Schema","text":"<pre><code>root\n |-- studyLocusId: string (nullable = false)\n |-- geneId: string (nullable = false)\n |-- score: double (nullable = false)\n |-- features: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- name: string (nullable = false)\n |    |    |-- value: float (nullable = false)\n |    |    |-- shapValue: float (nullable = true)\n |-- shapBaseValue: float (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/ld_index/","title":"LD Index","text":""},{"location":"python_api/datasets/ld_index/#gentropy.dataset.ld_index.LDIndex","title":"<code>gentropy.dataset.ld_index.LDIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset containing linkage disequilibrium information between variants.</p> Source code in <code>src/gentropy/dataset/ld_index.py</code> <pre><code>@dataclass\nclass LDIndex(Dataset):\n    \"\"\"Dataset containing linkage disequilibrium information between variants.\"\"\"\n\n    @classmethod\n    def get_schema(cls: type[LDIndex]) -&gt; StructType:\n        \"\"\"Provides the schema for the LDIndex dataset.\n\n        Returns:\n            StructType: Schema for the LDIndex dataset\n        \"\"\"\n        return parse_spark_schema(\"ld_index.json\")\n</code></pre>"},{"location":"python_api/datasets/ld_index/#gentropy.dataset.ld_index.LDIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the LDIndex dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the LDIndex dataset</p> Source code in <code>src/gentropy/dataset/ld_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[LDIndex]) -&gt; StructType:\n    \"\"\"Provides the schema for the LDIndex dataset.\n\n    Returns:\n        StructType: Schema for the LDIndex dataset\n    \"\"\"\n    return parse_spark_schema(\"ld_index.json\")\n</code></pre>"},{"location":"python_api/datasets/ld_index/#schema","title":"Schema","text":"<pre><code>root\n |-- variantId: string (nullable = false)\n |-- chromosome: string (nullable = false)\n |-- ldSet: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- tagVariantId: string (nullable = false)\n |    |    |-- rValues: array (nullable = false)\n |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |-- population: string (nullable = false)\n |    |    |    |    |-- r: double (nullable = false)\n</code></pre>"},{"location":"python_api/datasets/study_index/","title":"Study Index","text":""},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex","title":"<code>gentropy.dataset.study_index.StudyIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Study index dataset.</p> <p>A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@dataclass\nclass StudyIndex(Dataset):\n    \"\"\"Study index dataset.\n\n    A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.\n    \"\"\"\n\n    VALID_TYPES = [\n        \"gwas\",\n        \"eqtl\",\n        \"pqtl\",\n        \"sqtl\",\n        \"tuqtl\",\n        \"sceqtl\",\n        \"scpqtl\",\n        \"scsqtl\",\n        \"sctuqtl\",\n    ]\n\n    @staticmethod\n    def _aggregate_samples_by_ancestry(merged: Column, ancestry: Column) -&gt; Column:\n        \"\"\"Aggregate sample counts by ancestry in a list of struct colmns.\n\n        Args:\n            merged (Column): A column representing merged data (list of structs).\n            ancestry (Column): The `ancestry` parameter is a column that represents the ancestry of each\n                sample. (a struct)\n\n        Returns:\n            Column: the modified \"merged\" column after aggregating the samples by ancestry.\n        \"\"\"\n        # Iterating over the list of ancestries and adding the sample size if label matches:\n        return f.transform(\n            merged,\n            lambda a: f.when(\n                a.ancestry == ancestry.ancestry,\n                f.struct(\n                    a.ancestry.alias(\"ancestry\"),\n                    (a.sampleSize + ancestry.sampleSize).alias(\"sampleSize\"),\n                ),\n            ).otherwise(a),\n        )\n\n    @staticmethod\n    def _map_ancestries_to_ld_population(gwas_ancestry_label: Column) -&gt; Column:\n        \"\"\"Normalise ancestry column from GWAS studies into reference LD panel based on a pre-defined map.\n\n        This function assumes all possible ancestry categories have a corresponding\n        LD panel in the LD index. It is very important to have the ancestry labels\n        moved to the LD panel map.\n\n        Args:\n            gwas_ancestry_label (Column): A struct column with ancestry label like Finnish,\n                European, African etc. and the corresponding sample size.\n\n        Returns:\n            Column: Struct column with the mapped LD population label and the sample size.\n        \"\"\"\n        # Loading ancestry label to LD population label:\n\n        pkg = pkg_resources.files(data).joinpath(\"gwas_population_2_LD_panel_map.json\")\n        with pkg.open(encoding=\"utf-8\") as file:\n            json_dict = json.load(file)\n            json_dict = cast(dict[str, str], json_dict)\n\n        map_expr = f.create_map(*[f.lit(x) for x in chain(*json_dict.items())])\n\n        return f.struct(\n            map_expr[gwas_ancestry_label.ancestry].alias(\"ancestry\"),\n            gwas_ancestry_label.sampleSize.alias(\"sampleSize\"),\n        )\n\n    @classmethod\n    def get_schema(cls: type[StudyIndex]) -&gt; StructType:\n        \"\"\"Provide the schema for the StudyIndex dataset.\n\n        Returns:\n            StructType: The schema of the StudyIndex dataset.\n        \"\"\"\n        return parse_spark_schema(\"study_index.json\")\n\n    @classmethod\n    def get_QC_column_name(cls: type[StudyIndex]) -&gt; str:\n        \"\"\"Return the name of the quality control column.\n\n        Returns:\n            str: The name of the quality control column.\n        \"\"\"\n        return \"qualityControls\"\n\n    @classmethod\n    def get_QC_mappings(cls: type[StudyIndex]) -&gt; dict[str, str]:\n        \"\"\"Quality control flag to QC column category mappings.\n\n        Returns:\n            dict[str, str]: Mapping between flag name and QC column category value.\n        \"\"\"\n        return {member.name: member.value for member in StudyQualityCheck}\n\n    @classmethod\n    def aggregate_and_map_ancestries(\n        cls: type[StudyIndex], discovery_samples: Column\n    ) -&gt; Column:\n        \"\"\"Map ancestries to populations in the LD reference and calculate relative sample size.\n\n        Args:\n            discovery_samples (Column): A list of struct column. Has an `ancestry` column and a `sampleSize` columns\n\n        Returns:\n            Column: A list of struct with mapped LD population and their relative sample size.\n        \"\"\"\n        # Map ancestry categories to population labels of the LD index:\n        mapped_ancestries = f.transform(\n            discovery_samples, cls._map_ancestries_to_ld_population\n        )\n\n        # Aggregate sample sizes belonging to the same LD population:\n        aggregated_counts = f.aggregate(\n            mapped_ancestries,\n            f.array_distinct(\n                f.transform(\n                    mapped_ancestries,\n                    lambda x: f.struct(\n                        x.ancestry.alias(\"ancestry\"), f.lit(0.0).alias(\"sampleSize\")\n                    ),\n                )\n            ),\n            cls._aggregate_samples_by_ancestry,\n        )\n        # Getting total sample count:\n        total_sample_count = f.aggregate(\n            aggregated_counts, f.lit(0.0), lambda total, pop: total + pop.sampleSize\n        ).alias(\"sampleSize\")\n        # Calculating relative sample size for each LD population:\n        return f.transform(\n            aggregated_counts,\n            lambda ld_population: f.struct(\n                ld_population.ancestry.alias(\"ldPopulation\"),\n                (ld_population.sampleSize / total_sample_count).alias(\n                    \"relativeSampleSize\"\n                ),\n            ),\n        )\n\n    def study_type_lut(self: StudyIndex) -&gt; DataFrame:\n        \"\"\"Return a lookup table of study type.\n\n        Returns:\n            DataFrame: A dataframe containing `studyId` and `studyType` columns.\n        \"\"\"\n        return self.df.select(\"studyId\", \"studyType\")\n\n    def is_qtl(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column with true values for QTL studies.\n\n        Returns:\n            Column: True if the study is a QTL study.\n        \"\"\"\n        return self.df.studyType.endswith(\"qtl\")\n\n    def is_gwas(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column with true values for GWAS studies.\n\n        Returns:\n            Column: True if the study is a GWAS study.\n        \"\"\"\n        return self.df.studyType == \"gwas\"\n\n    def has_mapped_trait(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column indicating if a study has mapped disease.\n\n        Returns:\n            Column: True if the study has mapped disease.\n        \"\"\"\n        return f.size(self.df.traitFromSourceMappedIds) &gt; 0\n\n    def is_quality_flagged(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column indicating if a study is flagged due to quality issues.\n\n        Returns:\n            Column: True if the study is flagged.\n        \"\"\"\n        # Testing for the presence of the qualityControls column:\n        if \"qualityControls\" not in self.df.columns:\n            return f.lit(False)\n        else:\n            return f.size(self.df[\"qualityControls\"]) != 0\n\n    def has_summarystats(self: StudyIndex) -&gt; Column:\n        \"\"\"Return a boolean column indicating if a study has harmonized summary statistics.\n\n        Returns:\n            Column: True if the study has harmonized summary statistics.\n        \"\"\"\n        return self.df.hasSumstats\n\n    def validate_unique_study_id(self: StudyIndex) -&gt; StudyIndex:\n        \"\"\"Validating the uniqueness of study identifiers and flagging duplicated studies.\n\n        Returns:\n            StudyIndex: with flagged duplicated studies.\n        \"\"\"\n        return StudyIndex(\n            _df=self.df.withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    self.flag_duplicates(f.col(\"studyId\")),\n                    StudyQualityCheck.DUPLICATED_STUDY,\n                ),\n            ),\n            _schema=StudyIndex.get_schema(),\n        )\n\n    def _normalise_disease(\n        self: StudyIndex,\n        source_disease_column_name: str,\n        disease_column_name: str,\n        disease_map: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"Normalising diseases in the study index.\n\n        Given a reference disease map (containing all potential EFO ids with the corresponding reference disease ids),\n        this function maps all EFO ids in the study index to the reference disease ids.\n\n        Args:\n            source_disease_column_name (str): The column name of the disease column to validate.\n            disease_column_name (str): The resulting disease column name that contains the validated ids.\n            disease_map (DataFrame): Reference dataframe with diseases\n\n        Returns:\n            DataFrame: where the newly added diseaseIds column will contain the validated EFO identifiers.\n        \"\"\"\n        return (\n            self.df\n            # Only validating studies with diseases:\n            .filter(f.size(f.col(source_disease_column_name)) &gt; 0)\n            # Explode disease column:\n            .select(\n                \"studyId\",\n                \"studyType\",\n                f.explode_outer(source_disease_column_name).alias(\"efo\"),\n            )\n            # Join disease map:\n            .join(disease_map, on=\"efo\", how=\"left\")\n            .groupBy(\"studyId\")\n            .agg(\n                f.collect_set(f.col(\"diseaseId\")).alias(disease_column_name),\n            )\n        )\n\n    def validate_disease(self: StudyIndex, disease_map: DataFrame) -&gt; StudyIndex:\n        \"\"\"Validate diseases in the study index dataset.\n\n        Args:\n            disease_map (DataFrame): a dataframe with two columns (efo, diseaseId).\n\n        Returns:\n            StudyIndex: where gwas studies are flagged where no valid disease id could be found.\n        \"\"\"\n        # Because the disease ids are not mandatory fields of the schema, we skip vaildation if these columns are not present:\n        if (\"traitFromSourceMappedIds\" not in self.df.columns) or (\n            \"backgroundTraitFromSourceMappedIds\" not in self.df.columns\n        ):\n            return self\n\n        # Disease Column names:\n        foreground_disease_column = \"diseaseIds\"\n        background_disease_column = \"backgroundDiseaseIds\"\n\n        # If diseaseId in schema, we need to drop it:\n        drop_columns = [\n            column\n            for column in self.df.columns\n            if column in [foreground_disease_column, background_disease_column]\n        ]\n\n        if len(drop_columns) &gt; 0:\n            self.df = self.df.drop(*drop_columns)\n\n        # Normalise disease:\n        normalised_disease = self._normalise_disease(\n            \"traitFromSourceMappedIds\", foreground_disease_column, disease_map\n        )\n        normalised_background_disease = self._normalise_disease(\n            \"backgroundTraitFromSourceMappedIds\", background_disease_column, disease_map\n        )\n\n        return StudyIndex(\n            _df=(\n                self.df.join(normalised_disease, on=\"studyId\", how=\"left\")\n                .join(normalised_background_disease, on=\"studyId\", how=\"left\")\n                # Updating disease columns:\n                .withColumn(\n                    foreground_disease_column,\n                    f.when(\n                        f.col(foreground_disease_column).isNull(), f.array()\n                    ).otherwise(f.col(foreground_disease_column)),\n                )\n                .withColumn(\n                    background_disease_column,\n                    f.when(\n                        f.col(background_disease_column).isNull(), f.array()\n                    ).otherwise(f.col(background_disease_column)),\n                )\n                # Flagging gwas studies where no valid disease is avilable:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyIndex.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        # Flagging all gwas studies with no normalised disease:\n                        (f.size(f.col(foreground_disease_column)) == 0)\n                        &amp; (f.col(\"studyType\") == \"gwas\"),\n                        StudyQualityCheck.UNRESOLVED_DISEASE,\n                    ),\n                )\n                # Added to avoid Spark optimisation (see: https://github.com/opentargets/issues/issues/3906#issuecomment-2949299965)\n                .persist()\n            ),\n            _schema=StudyIndex.get_schema(),\n        )\n\n    def validate_study_type(self: StudyIndex) -&gt; StudyIndex:\n        \"\"\"Validating study type and flag unsupported types.\n\n        Returns:\n            StudyIndex: with flagged studies with unsupported type.\n        \"\"\"\n        validated_df = (\n            self.df\n            # Flagging unsupported study types:\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.when(\n                        (f.col(\"studyType\") == \"gwas\")\n                        | f.col(\"studyType\").endswith(\"qtl\"),\n                        False,\n                    ).otherwise(True),\n                    StudyQualityCheck.UNKNOWN_STUDY_TYPE,\n                ),\n            )\n        )\n        return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n\n    def validate_target(self: StudyIndex, target_index: TargetIndex) -&gt; StudyIndex:\n        \"\"\"Validating gene identifiers in the study index against the provided target index.\n\n        Args:\n            target_index (TargetIndex): target index containing the reference gene identifiers (Ensembl gene identifiers).\n\n        Returns:\n            StudyIndex: with flagged studies if geneId could not be validated.\n        \"\"\"\n        gene_set = target_index.df.select(\n            f.col(\"id\").alias(\"geneId\"), f.lit(True).alias(\"isIdFound\")\n        )\n\n        # As the geneId is not a mandatory field of study index, we return if the column is not there:\n        if \"geneId\" not in self.df.columns:\n            return self\n\n        validated_df = (\n            self.df.join(gene_set, on=\"geneId\", how=\"left\")\n            .withColumn(\n                \"isIdFound\",\n                f.when(\n                    (f.col(\"studyType\") != \"gwas\") &amp; f.col(\"isIdFound\").isNull(),\n                    f.lit(False),\n                ).otherwise(f.lit(True)),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~f.col(\"isIdFound\"),\n                    StudyQualityCheck.UNRESOLVED_TARGET,\n                ),\n            )\n            .drop(\"isIdFound\")\n        )\n\n        return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n\n    def validate_biosample(\n        self: StudyIndex, biosample_index: BiosampleIndex\n    ) -&gt; StudyIndex:\n        \"\"\"Validating biosample identifiers in the study index against the provided biosample index.\n\n        Args:\n            biosample_index (BiosampleIndex): Biosample index containing a reference of biosample identifiers e.g. cell types, tissues, cell lines, etc.\n\n        Returns:\n            StudyIndex: where non-gwas studies are flagged if biosampleIndex could not be validated.\n        \"\"\"\n        biosample_set = biosample_index.df.select(\n            \"biosampleId\", f.lit(True).alias(\"isIdFound\")\n        )\n\n        # If biosampleId in df, we need to drop it:\n        if \"biosampleId\" in self.df.columns:\n            self.df = self.df.drop(\"biosampleId\")\n\n        # As the biosampleFromSourceId is not a mandatory field of study index, we return if the column is not there:\n        if \"biosampleFromSourceId\" not in self.df.columns:\n            return self\n\n        validated_df = (\n            self.df.join(\n                biosample_set,\n                self.df.biosampleFromSourceId == biosample_set.biosampleId,\n                how=\"left\",\n            )\n            .withColumn(\n                \"isIdFound\",\n                f.when(\n                    (f.col(\"studyType\") != \"gwas\") &amp; (f.col(\"isIdFound\").isNull()),\n                    f.lit(False),\n                ).otherwise(f.lit(True)),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~f.col(\"isIdFound\"),\n                    StudyQualityCheck.UNKNOWN_BIOSAMPLE,\n                ),\n            )\n            .drop(\"isIdFound\")\n        )\n\n        return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n\n    def annotate_sumstats_qc(\n        self: StudyIndex,\n        sumstats_qc: SummaryStatisticsQC,\n        threshold_mean_beta: float = 0.05,\n        threshold_mean_diff_pz: float = 0.05,\n        threshold_se_diff_pz: float = 0.05,\n        threshold_min_gc_lambda: float = 0.7,\n        threshold_max_gc_lambda: float = 2.5,\n        threshold_min_n_variants: int = 2_000_000,\n    ) -&gt; StudyIndex:\n        \"\"\"Annotate summary stats QC information.\n\n        Args:\n            sumstats_qc (SummaryStatisticsQC): Dataset containing summary statistics-based quality controls.\n            threshold_mean_beta (float): Threshold for mean beta check. Defaults to 0.05.\n            threshold_mean_diff_pz (float): Threshold for mean diff PZ check. Defaults to 0.05.\n            threshold_se_diff_pz (float): Threshold for SE diff PZ check. Defaults to 0.05.\n            threshold_min_gc_lambda (float): Minimum threshold for GC lambda check. Defaults to 0.7.\n            threshold_max_gc_lambda (float): Maximum threshold for GC lambda check. Defaults to 2.5.\n            threshold_min_n_variants (int): Minimum number of variants for SuSiE check. Defaults to 2_000_000.\n\n        Returns:\n            StudyIndex: Updated study index with QC information\n        \"\"\"\n        # convert all columns in sumstats_qc dataframe in array of structs grouped by studyId\n        qc_check_cols = [c for c in sumstats_qc.df.columns if c != \"studyId\"]\n\n        studies = self.df\n\n        # Prepare the QC flags format for the study index:\n        melted_qc = convert_from_wide_to_long(\n            sumstats_qc.df,\n            id_vars=[\"studyId\"],\n            value_vars=qc_check_cols,\n            var_name=\"QCCheckName\",\n            value_name=\"QCCheckValue\",\n        )\n        qc_struct = f.struct(f.col(\"QCCheckName\"), f.col(\"QCCheckValue\"))\n        qc_df = (\n            melted_qc.groupBy(\"studyId\")\n            .agg(f.collect_list(qc_struct).alias(\"sumstatQCValues\"))\n            .select(\"studyId\", \"sumstatQCValues\")\n            .withColumn(\"hasSumstats\", f.lit(True))\n        )\n        extract_qc_value: Callable[[str], Column] = lambda x: filter_array_struct(\n            \"sumstatQCValues\", \"QCCheckName\", x, \"QCCheckValue\"\n        )\n\n        df = (\n            studies.drop(\"sumstatQCValues\", \"hasSumstats\")\n            .join(qc_df, how=\"left\", on=\"studyId\")\n            .withColumn(\"hasSumstats\", f.coalesce(f.col(\"hasSumstats\"), f.lit(False)))\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~f.col(\"hasSumstats\"),\n                    StudyQualityCheck.SUMSTATS_NOT_AVAILABLE,\n                ),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~(f.abs(extract_qc_value(\"mean_beta\")) &lt;= threshold_mean_beta),\n                    StudyQualityCheck.FAILED_MEAN_BETA_CHECK,\n                ),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~(\n                        (\n                            f.abs(extract_qc_value(\"mean_diff_pz\"))\n                            &lt;= threshold_mean_diff_pz\n                        )\n                        &amp; (extract_qc_value(\"se_diff_pz\") &lt;= threshold_se_diff_pz)\n                    ),\n                    StudyQualityCheck.FAILED_PZ_CHECK,\n                ),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~(\n                        (extract_qc_value(\"gc_lambda\") &lt;= threshold_max_gc_lambda)\n                        &amp; (extract_qc_value(\"gc_lambda\") &gt;= threshold_min_gc_lambda)\n                    ),\n                    StudyQualityCheck.FAILED_GC_LAMBDA_CHECK,\n                ),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    extract_qc_value(\"n_variants\") &lt; threshold_min_n_variants,\n                    StudyQualityCheck.SMALL_NUMBER_OF_SNPS,\n                ),\n            )\n        )\n\n        # Annotate study index with QC information:\n        return StudyIndex(\n            _df=df,\n            _schema=StudyIndex.get_schema(),\n        )\n\n    def validate_analysis_flags(self: StudyIndex) -&gt; StudyIndex:\n        \"\"\"Validating analysis flags in the study index dataset.\n\n        This method checks the analysis flags and reports the the outcome of predicates to the quality control column.\n\n        Returns:\n            StudyIndex: with qualityControls column flagged based on analysisFlags.\n        \"\"\"\n        predicate = f.array_contains(\n            \"analysisFlags\", StudyAnalysisFlag.CASE_CASE_STUDY.value\n        )\n        df = self.df.withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                predicate,\n                StudyQualityCheck.CASE_CASE_STUDY_DESIGN,\n            ),\n        )\n        return StudyIndex(_df=df, _schema=StudyIndex.get_schema())\n\n    def deconvolute_studies(self: StudyIndex) -&gt; StudyIndex:\n        \"\"\"Deconvolute the study index dataset.\n\n        When ingesting the study index dataset, the same studyId might be ingested from more than one source.\n        In such cases, the data needs to be merged and the quality control flags need to be combined.\n\n        Returns:\n            StudyIndex: Deconvoluted study index dataset.\n        \"\"\"\n        # Windowing by study ID assume random order, but this is OK, because we are not selecting rows by a specific order.\n        study_id_window = Window.partitionBy(\"studyId\").orderBy(f.rand())\n\n        # For certain aggregation, the full window is needed to be considered:\n        full_study_id_window = study_id_window.orderBy(\"studyId\").rangeBetween(\n            Window.unboundedPreceding, Window.unboundedFollowing\n        )\n\n        # Temporary columns to drop at the end:\n        columns_to_drop = [\"keepTopHit\", \"mostGranular\", \"rank\"]\n\n        return StudyIndex(\n            _df=(\n                self.df\n                # Initialising quality controls column, if not present:\n                .withColumn(\n                    \"qualityControls\",\n                    f.when(\n                        f.col(\"qualityControls\").isNull(),\n                        f.array().cast(ArrayType(StringType())),\n                    ).otherwise(f.col(\"qualityControls\")),\n                )\n                # Keeping top hit studies unless the same study is available from a summmary statistics source:\n                # This value will be set for all rows for the same `studyId`:\n                .withColumn(\n                    \"keepTopHit\",\n                    f.when(\n                        f.array_contains(\n                            f.collect_set(f.col(\"hasSumstats\")).over(\n                                full_study_id_window\n                            ),\n                            True,\n                        ),\n                        f.lit(False),\n                    ).otherwise(True),\n                )\n                # For studies without summary statistics, we remove the \"Not curated by Open Targets\" flag:\n                .withColumn(\n                    \"qualityControls\",\n                    f.when(\n                        ~f.col(\"hasSumstats\"),\n                        f.array_remove(\n                            f.col(\"qualityControls\"),\n                            StudyQualityCheck.NO_OT_CURATION.value,\n                        ),\n                    ).otherwise(f.col(\"qualityControls\")),\n                )\n                # If top hits are not kept, we remove the \"sumstats not available\" flag from all QC lists:\n                .withColumn(\n                    \"qualityControls\",\n                    f.when(\n                        ~f.col(\"keepTopHit\"),\n                        f.array_remove(\n                            f.col(\"qualityControls\"),\n                            StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value,\n                        ),\n                    ).otherwise(f.col(\"qualityControls\")),\n                )\n                # Then propagate quality checks for all sources of the same study:\n                .withColumn(\n                    \"qualityControls\",\n                    f.array_distinct(\n                        f.flatten(\n                            f.collect_set(\"qualityControls\").over(full_study_id_window)\n                        )\n                    ),\n                )\n                # Propagating sumstatQCValues -&gt; map, cannot be flatten:\n                .withColumn(\n                    \"sumstatQCValues\",\n                    f.first(\"sumstatQCValues\", ignorenulls=True).over(\n                        full_study_id_window\n                    ),\n                )\n                # Propagating analysisFlags:\n                .withColumn(\n                    \"analysisFlags\",\n                    f.flatten(\n                        f.collect_list(\"analysisFlags\").over(full_study_id_window)\n                    ),\n                )\n                # Propagating hasSumstatsFlag - if no flag, leave null:\n                .withColumn(\n                    \"hasSumstats\",\n                    f.when(\n                        # There's a true:\n                        f.array_contains(\n                            f.collect_set(\"hasSumstats\").over(full_study_id_window),\n                            True,\n                        ),\n                        f.lit(True),\n                    ).when(\n                        # There's a false:\n                        f.array_contains(\n                            f.collect_set(\"hasSumstats\").over(full_study_id_window),\n                            False,\n                        ),\n                        f.lit(False),\n                    ),\n                )\n                # Propagating disease: when different sets of diseases available for the same study,\n                # we pick the shortest list, becasuse we assume, that is the most accurate disease assignment:\n                .withColumn(\n                    \"mostGranular\",\n                    f.size(f.col(\"traitFromSourceMappedIds\"))\n                    == f.min(f.size(f.col(\"traitFromSourceMappedIds\"))).over(\n                        full_study_id_window\n                    ),\n                )\n                # Remove less granular disease mappings:\n                .withColumn(\n                    \"traitFromSourceMappedIds\",\n                    f.when(f.col(\"mostGranular\"), f.col(\"traitFromSourceMappedIds\")),\n                )\n                # Propagate mapped disease:\n                .withColumn(\n                    \"traitFromSourceMappedIds\",\n                    f.last(f.col(\"traitFromSourceMappedIds\"), True).over(\n                        full_study_id_window\n                    ),\n                )\n                # Repeating these steps for the `traitFromSource` column:\n                .withColumn(\n                    \"traitFromSource\",\n                    f.when(f.col(\"mostGranular\"), f.col(\"traitFromSource\")),\n                )\n                # Propagate disease:\n                .withColumn(\n                    \"traitFromSource\",\n                    f.last(f.col(\"traitFromSource\"), True).over(full_study_id_window),\n                )\n                # Distinct study types are joined together into a string. So, if there's ambiguite, the study will be flagged when the study type is validated:\n                .withColumn(\n                    \"studyType\",\n                    f.concat_ws(\n                        \",\", f.collect_set(\"studyType\").over(full_study_id_window)\n                    ),\n                )\n                # At this point, all studies in one window is expected to be identical. Let's just pick one:\n                .withColumn(\"rank\", f.row_number().over(study_id_window))\n                .filter(f.col(\"rank\") == 1)\n                .drop(*columns_to_drop)\n                # Added to avoid Spark optimisation (see: https://github.com/opentargets/issues/issues/3906#issuecomment-2949299965)\n                .persist()\n            ),\n            _schema=StudyIndex.get_schema(),\n        )\n\n    def get_summary_statistics_paths(self: StudyIndex) -&gt; list[str]:\n        \"\"\"Get summary statistics paths from study index.\n\n        Returns:\n            list[str]: List of summary statistics file paths.\n        \"\"\"\n        paths_df = self.df.filter(self.has_summarystats()).select(\n            f.col(\"summarystatsLocation\")\n        )\n        paths = [row[\"summarystatsLocation\"] for row in paths_df.collect()]\n        return paths\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.aggregate_and_map_ancestries","title":"<code>aggregate_and_map_ancestries(discovery_samples: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Map ancestries to populations in the LD reference and calculate relative sample size.</p> <p>Parameters:</p> Name Type Description Default <code>discovery_samples</code> <code>Column</code> <p>A list of struct column. Has an <code>ancestry</code> column and a <code>sampleSize</code> columns</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A list of struct with mapped LD population and their relative sample size.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@classmethod\ndef aggregate_and_map_ancestries(\n    cls: type[StudyIndex], discovery_samples: Column\n) -&gt; Column:\n    \"\"\"Map ancestries to populations in the LD reference and calculate relative sample size.\n\n    Args:\n        discovery_samples (Column): A list of struct column. Has an `ancestry` column and a `sampleSize` columns\n\n    Returns:\n        Column: A list of struct with mapped LD population and their relative sample size.\n    \"\"\"\n    # Map ancestry categories to population labels of the LD index:\n    mapped_ancestries = f.transform(\n        discovery_samples, cls._map_ancestries_to_ld_population\n    )\n\n    # Aggregate sample sizes belonging to the same LD population:\n    aggregated_counts = f.aggregate(\n        mapped_ancestries,\n        f.array_distinct(\n            f.transform(\n                mapped_ancestries,\n                lambda x: f.struct(\n                    x.ancestry.alias(\"ancestry\"), f.lit(0.0).alias(\"sampleSize\")\n                ),\n            )\n        ),\n        cls._aggregate_samples_by_ancestry,\n    )\n    # Getting total sample count:\n    total_sample_count = f.aggregate(\n        aggregated_counts, f.lit(0.0), lambda total, pop: total + pop.sampleSize\n    ).alias(\"sampleSize\")\n    # Calculating relative sample size for each LD population:\n    return f.transform(\n        aggregated_counts,\n        lambda ld_population: f.struct(\n            ld_population.ancestry.alias(\"ldPopulation\"),\n            (ld_population.sampleSize / total_sample_count).alias(\n                \"relativeSampleSize\"\n            ),\n        ),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.annotate_sumstats_qc","title":"<code>annotate_sumstats_qc(sumstats_qc: SummaryStatisticsQC, threshold_mean_beta: float = 0.05, threshold_mean_diff_pz: float = 0.05, threshold_se_diff_pz: float = 0.05, threshold_min_gc_lambda: float = 0.7, threshold_max_gc_lambda: float = 2.5, threshold_min_n_variants: int = 2000000) -&gt; StudyIndex</code>","text":"<p>Annotate summary stats QC information.</p> <p>Parameters:</p> Name Type Description Default <code>sumstats_qc</code> <code>SummaryStatisticsQC</code> <p>Dataset containing summary statistics-based quality controls.</p> required <code>threshold_mean_beta</code> <code>float</code> <p>Threshold for mean beta check. Defaults to 0.05.</p> <code>0.05</code> <code>threshold_mean_diff_pz</code> <code>float</code> <p>Threshold for mean diff PZ check. Defaults to 0.05.</p> <code>0.05</code> <code>threshold_se_diff_pz</code> <code>float</code> <p>Threshold for SE diff PZ check. Defaults to 0.05.</p> <code>0.05</code> <code>threshold_min_gc_lambda</code> <code>float</code> <p>Minimum threshold for GC lambda check. Defaults to 0.7.</p> <code>0.7</code> <code>threshold_max_gc_lambda</code> <code>float</code> <p>Maximum threshold for GC lambda check. Defaults to 2.5.</p> <code>2.5</code> <code>threshold_min_n_variants</code> <code>int</code> <p>Minimum number of variants for SuSiE check. Defaults to 2_000_000.</p> <code>2000000</code> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Updated study index with QC information</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def annotate_sumstats_qc(\n    self: StudyIndex,\n    sumstats_qc: SummaryStatisticsQC,\n    threshold_mean_beta: float = 0.05,\n    threshold_mean_diff_pz: float = 0.05,\n    threshold_se_diff_pz: float = 0.05,\n    threshold_min_gc_lambda: float = 0.7,\n    threshold_max_gc_lambda: float = 2.5,\n    threshold_min_n_variants: int = 2_000_000,\n) -&gt; StudyIndex:\n    \"\"\"Annotate summary stats QC information.\n\n    Args:\n        sumstats_qc (SummaryStatisticsQC): Dataset containing summary statistics-based quality controls.\n        threshold_mean_beta (float): Threshold for mean beta check. Defaults to 0.05.\n        threshold_mean_diff_pz (float): Threshold for mean diff PZ check. Defaults to 0.05.\n        threshold_se_diff_pz (float): Threshold for SE diff PZ check. Defaults to 0.05.\n        threshold_min_gc_lambda (float): Minimum threshold for GC lambda check. Defaults to 0.7.\n        threshold_max_gc_lambda (float): Maximum threshold for GC lambda check. Defaults to 2.5.\n        threshold_min_n_variants (int): Minimum number of variants for SuSiE check. Defaults to 2_000_000.\n\n    Returns:\n        StudyIndex: Updated study index with QC information\n    \"\"\"\n    # convert all columns in sumstats_qc dataframe in array of structs grouped by studyId\n    qc_check_cols = [c for c in sumstats_qc.df.columns if c != \"studyId\"]\n\n    studies = self.df\n\n    # Prepare the QC flags format for the study index:\n    melted_qc = convert_from_wide_to_long(\n        sumstats_qc.df,\n        id_vars=[\"studyId\"],\n        value_vars=qc_check_cols,\n        var_name=\"QCCheckName\",\n        value_name=\"QCCheckValue\",\n    )\n    qc_struct = f.struct(f.col(\"QCCheckName\"), f.col(\"QCCheckValue\"))\n    qc_df = (\n        melted_qc.groupBy(\"studyId\")\n        .agg(f.collect_list(qc_struct).alias(\"sumstatQCValues\"))\n        .select(\"studyId\", \"sumstatQCValues\")\n        .withColumn(\"hasSumstats\", f.lit(True))\n    )\n    extract_qc_value: Callable[[str], Column] = lambda x: filter_array_struct(\n        \"sumstatQCValues\", \"QCCheckName\", x, \"QCCheckValue\"\n    )\n\n    df = (\n        studies.drop(\"sumstatQCValues\", \"hasSumstats\")\n        .join(qc_df, how=\"left\", on=\"studyId\")\n        .withColumn(\"hasSumstats\", f.coalesce(f.col(\"hasSumstats\"), f.lit(False)))\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~f.col(\"hasSumstats\"),\n                StudyQualityCheck.SUMSTATS_NOT_AVAILABLE,\n            ),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~(f.abs(extract_qc_value(\"mean_beta\")) &lt;= threshold_mean_beta),\n                StudyQualityCheck.FAILED_MEAN_BETA_CHECK,\n            ),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~(\n                    (\n                        f.abs(extract_qc_value(\"mean_diff_pz\"))\n                        &lt;= threshold_mean_diff_pz\n                    )\n                    &amp; (extract_qc_value(\"se_diff_pz\") &lt;= threshold_se_diff_pz)\n                ),\n                StudyQualityCheck.FAILED_PZ_CHECK,\n            ),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~(\n                    (extract_qc_value(\"gc_lambda\") &lt;= threshold_max_gc_lambda)\n                    &amp; (extract_qc_value(\"gc_lambda\") &gt;= threshold_min_gc_lambda)\n                ),\n                StudyQualityCheck.FAILED_GC_LAMBDA_CHECK,\n            ),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                extract_qc_value(\"n_variants\") &lt; threshold_min_n_variants,\n                StudyQualityCheck.SMALL_NUMBER_OF_SNPS,\n            ),\n        )\n    )\n\n    # Annotate study index with QC information:\n    return StudyIndex(\n        _df=df,\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.deconvolute_studies","title":"<code>deconvolute_studies() -&gt; StudyIndex</code>","text":"<p>Deconvolute the study index dataset.</p> <p>When ingesting the study index dataset, the same studyId might be ingested from more than one source. In such cases, the data needs to be merged and the quality control flags need to be combined.</p> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Deconvoluted study index dataset.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def deconvolute_studies(self: StudyIndex) -&gt; StudyIndex:\n    \"\"\"Deconvolute the study index dataset.\n\n    When ingesting the study index dataset, the same studyId might be ingested from more than one source.\n    In such cases, the data needs to be merged and the quality control flags need to be combined.\n\n    Returns:\n        StudyIndex: Deconvoluted study index dataset.\n    \"\"\"\n    # Windowing by study ID assume random order, but this is OK, because we are not selecting rows by a specific order.\n    study_id_window = Window.partitionBy(\"studyId\").orderBy(f.rand())\n\n    # For certain aggregation, the full window is needed to be considered:\n    full_study_id_window = study_id_window.orderBy(\"studyId\").rangeBetween(\n        Window.unboundedPreceding, Window.unboundedFollowing\n    )\n\n    # Temporary columns to drop at the end:\n    columns_to_drop = [\"keepTopHit\", \"mostGranular\", \"rank\"]\n\n    return StudyIndex(\n        _df=(\n            self.df\n            # Initialising quality controls column, if not present:\n            .withColumn(\n                \"qualityControls\",\n                f.when(\n                    f.col(\"qualityControls\").isNull(),\n                    f.array().cast(ArrayType(StringType())),\n                ).otherwise(f.col(\"qualityControls\")),\n            )\n            # Keeping top hit studies unless the same study is available from a summmary statistics source:\n            # This value will be set for all rows for the same `studyId`:\n            .withColumn(\n                \"keepTopHit\",\n                f.when(\n                    f.array_contains(\n                        f.collect_set(f.col(\"hasSumstats\")).over(\n                            full_study_id_window\n                        ),\n                        True,\n                    ),\n                    f.lit(False),\n                ).otherwise(True),\n            )\n            # For studies without summary statistics, we remove the \"Not curated by Open Targets\" flag:\n            .withColumn(\n                \"qualityControls\",\n                f.when(\n                    ~f.col(\"hasSumstats\"),\n                    f.array_remove(\n                        f.col(\"qualityControls\"),\n                        StudyQualityCheck.NO_OT_CURATION.value,\n                    ),\n                ).otherwise(f.col(\"qualityControls\")),\n            )\n            # If top hits are not kept, we remove the \"sumstats not available\" flag from all QC lists:\n            .withColumn(\n                \"qualityControls\",\n                f.when(\n                    ~f.col(\"keepTopHit\"),\n                    f.array_remove(\n                        f.col(\"qualityControls\"),\n                        StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value,\n                    ),\n                ).otherwise(f.col(\"qualityControls\")),\n            )\n            # Then propagate quality checks for all sources of the same study:\n            .withColumn(\n                \"qualityControls\",\n                f.array_distinct(\n                    f.flatten(\n                        f.collect_set(\"qualityControls\").over(full_study_id_window)\n                    )\n                ),\n            )\n            # Propagating sumstatQCValues -&gt; map, cannot be flatten:\n            .withColumn(\n                \"sumstatQCValues\",\n                f.first(\"sumstatQCValues\", ignorenulls=True).over(\n                    full_study_id_window\n                ),\n            )\n            # Propagating analysisFlags:\n            .withColumn(\n                \"analysisFlags\",\n                f.flatten(\n                    f.collect_list(\"analysisFlags\").over(full_study_id_window)\n                ),\n            )\n            # Propagating hasSumstatsFlag - if no flag, leave null:\n            .withColumn(\n                \"hasSumstats\",\n                f.when(\n                    # There's a true:\n                    f.array_contains(\n                        f.collect_set(\"hasSumstats\").over(full_study_id_window),\n                        True,\n                    ),\n                    f.lit(True),\n                ).when(\n                    # There's a false:\n                    f.array_contains(\n                        f.collect_set(\"hasSumstats\").over(full_study_id_window),\n                        False,\n                    ),\n                    f.lit(False),\n                ),\n            )\n            # Propagating disease: when different sets of diseases available for the same study,\n            # we pick the shortest list, becasuse we assume, that is the most accurate disease assignment:\n            .withColumn(\n                \"mostGranular\",\n                f.size(f.col(\"traitFromSourceMappedIds\"))\n                == f.min(f.size(f.col(\"traitFromSourceMappedIds\"))).over(\n                    full_study_id_window\n                ),\n            )\n            # Remove less granular disease mappings:\n            .withColumn(\n                \"traitFromSourceMappedIds\",\n                f.when(f.col(\"mostGranular\"), f.col(\"traitFromSourceMappedIds\")),\n            )\n            # Propagate mapped disease:\n            .withColumn(\n                \"traitFromSourceMappedIds\",\n                f.last(f.col(\"traitFromSourceMappedIds\"), True).over(\n                    full_study_id_window\n                ),\n            )\n            # Repeating these steps for the `traitFromSource` column:\n            .withColumn(\n                \"traitFromSource\",\n                f.when(f.col(\"mostGranular\"), f.col(\"traitFromSource\")),\n            )\n            # Propagate disease:\n            .withColumn(\n                \"traitFromSource\",\n                f.last(f.col(\"traitFromSource\"), True).over(full_study_id_window),\n            )\n            # Distinct study types are joined together into a string. So, if there's ambiguite, the study will be flagged when the study type is validated:\n            .withColumn(\n                \"studyType\",\n                f.concat_ws(\n                    \",\", f.collect_set(\"studyType\").over(full_study_id_window)\n                ),\n            )\n            # At this point, all studies in one window is expected to be identical. Let's just pick one:\n            .withColumn(\"rank\", f.row_number().over(study_id_window))\n            .filter(f.col(\"rank\") == 1)\n            .drop(*columns_to_drop)\n            # Added to avoid Spark optimisation (see: https://github.com/opentargets/issues/issues/3906#issuecomment-2949299965)\n            .persist()\n        ),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.get_QC_column_name","title":"<code>get_QC_column_name() -&gt; str</code>  <code>classmethod</code>","text":"<p>Return the name of the quality control column.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the quality control column.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@classmethod\ndef get_QC_column_name(cls: type[StudyIndex]) -&gt; str:\n    \"\"\"Return the name of the quality control column.\n\n    Returns:\n        str: The name of the quality control column.\n    \"\"\"\n    return \"qualityControls\"\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.get_QC_mappings","title":"<code>get_QC_mappings() -&gt; dict[str, str]</code>  <code>classmethod</code>","text":"<p>Quality control flag to QC column category mappings.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: Mapping between flag name and QC column category value.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@classmethod\ndef get_QC_mappings(cls: type[StudyIndex]) -&gt; dict[str, str]:\n    \"\"\"Quality control flag to QC column category mappings.\n\n    Returns:\n        dict[str, str]: Mapping between flag name and QC column category value.\n    \"\"\"\n    return {member.name: member.value for member in StudyQualityCheck}\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provide the schema for the StudyIndex dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>The schema of the StudyIndex dataset.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[StudyIndex]) -&gt; StructType:\n    \"\"\"Provide the schema for the StudyIndex dataset.\n\n    Returns:\n        StructType: The schema of the StudyIndex dataset.\n    \"\"\"\n    return parse_spark_schema(\"study_index.json\")\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.get_summary_statistics_paths","title":"<code>get_summary_statistics_paths() -&gt; list[str]</code>","text":"<p>Get summary statistics paths from study index.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of summary statistics file paths.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def get_summary_statistics_paths(self: StudyIndex) -&gt; list[str]:\n    \"\"\"Get summary statistics paths from study index.\n\n    Returns:\n        list[str]: List of summary statistics file paths.\n    \"\"\"\n    paths_df = self.df.filter(self.has_summarystats()).select(\n        f.col(\"summarystatsLocation\")\n    )\n    paths = [row[\"summarystatsLocation\"] for row in paths_df.collect()]\n    return paths\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.has_mapped_trait","title":"<code>has_mapped_trait() -&gt; Column</code>","text":"<p>Return a boolean column indicating if a study has mapped disease.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study has mapped disease.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def has_mapped_trait(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column indicating if a study has mapped disease.\n\n    Returns:\n        Column: True if the study has mapped disease.\n    \"\"\"\n    return f.size(self.df.traitFromSourceMappedIds) &gt; 0\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.has_summarystats","title":"<code>has_summarystats() -&gt; Column</code>","text":"<p>Return a boolean column indicating if a study has harmonized summary statistics.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study has harmonized summary statistics.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def has_summarystats(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column indicating if a study has harmonized summary statistics.\n\n    Returns:\n        Column: True if the study has harmonized summary statistics.\n    \"\"\"\n    return self.df.hasSumstats\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.is_gwas","title":"<code>is_gwas() -&gt; Column</code>","text":"<p>Return a boolean column with true values for GWAS studies.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study is a GWAS study.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def is_gwas(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column with true values for GWAS studies.\n\n    Returns:\n        Column: True if the study is a GWAS study.\n    \"\"\"\n    return self.df.studyType == \"gwas\"\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.is_qtl","title":"<code>is_qtl() -&gt; Column</code>","text":"<p>Return a boolean column with true values for QTL studies.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study is a QTL study.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def is_qtl(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column with true values for QTL studies.\n\n    Returns:\n        Column: True if the study is a QTL study.\n    \"\"\"\n    return self.df.studyType.endswith(\"qtl\")\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.is_quality_flagged","title":"<code>is_quality_flagged() -&gt; Column</code>","text":"<p>Return a boolean column indicating if a study is flagged due to quality issues.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>True if the study is flagged.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def is_quality_flagged(self: StudyIndex) -&gt; Column:\n    \"\"\"Return a boolean column indicating if a study is flagged due to quality issues.\n\n    Returns:\n        Column: True if the study is flagged.\n    \"\"\"\n    # Testing for the presence of the qualityControls column:\n    if \"qualityControls\" not in self.df.columns:\n        return f.lit(False)\n    else:\n        return f.size(self.df[\"qualityControls\"]) != 0\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.study_type_lut","title":"<code>study_type_lut() -&gt; DataFrame</code>","text":"<p>Return a lookup table of study type.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing <code>studyId</code> and <code>studyType</code> columns.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def study_type_lut(self: StudyIndex) -&gt; DataFrame:\n    \"\"\"Return a lookup table of study type.\n\n    Returns:\n        DataFrame: A dataframe containing `studyId` and `studyType` columns.\n    \"\"\"\n    return self.df.select(\"studyId\", \"studyType\")\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_analysis_flags","title":"<code>validate_analysis_flags() -&gt; StudyIndex</code>","text":"<p>Validating analysis flags in the study index dataset.</p> <p>This method checks the analysis flags and reports the the outcome of predicates to the quality control column.</p> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>with qualityControls column flagged based on analysisFlags.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_analysis_flags(self: StudyIndex) -&gt; StudyIndex:\n    \"\"\"Validating analysis flags in the study index dataset.\n\n    This method checks the analysis flags and reports the the outcome of predicates to the quality control column.\n\n    Returns:\n        StudyIndex: with qualityControls column flagged based on analysisFlags.\n    \"\"\"\n    predicate = f.array_contains(\n        \"analysisFlags\", StudyAnalysisFlag.CASE_CASE_STUDY.value\n    )\n    df = self.df.withColumn(\n        \"qualityControls\",\n        StudyIndex.update_quality_flag(\n            f.col(\"qualityControls\"),\n            predicate,\n            StudyQualityCheck.CASE_CASE_STUDY_DESIGN,\n        ),\n    )\n    return StudyIndex(_df=df, _schema=StudyIndex.get_schema())\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_biosample","title":"<code>validate_biosample(biosample_index: BiosampleIndex) -&gt; StudyIndex</code>","text":"<p>Validating biosample identifiers in the study index against the provided biosample index.</p> <p>Parameters:</p> Name Type Description Default <code>biosample_index</code> <code>BiosampleIndex</code> <p>Biosample index containing a reference of biosample identifiers e.g. cell types, tissues, cell lines, etc.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>where non-gwas studies are flagged if biosampleIndex could not be validated.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_biosample(\n    self: StudyIndex, biosample_index: BiosampleIndex\n) -&gt; StudyIndex:\n    \"\"\"Validating biosample identifiers in the study index against the provided biosample index.\n\n    Args:\n        biosample_index (BiosampleIndex): Biosample index containing a reference of biosample identifiers e.g. cell types, tissues, cell lines, etc.\n\n    Returns:\n        StudyIndex: where non-gwas studies are flagged if biosampleIndex could not be validated.\n    \"\"\"\n    biosample_set = biosample_index.df.select(\n        \"biosampleId\", f.lit(True).alias(\"isIdFound\")\n    )\n\n    # If biosampleId in df, we need to drop it:\n    if \"biosampleId\" in self.df.columns:\n        self.df = self.df.drop(\"biosampleId\")\n\n    # As the biosampleFromSourceId is not a mandatory field of study index, we return if the column is not there:\n    if \"biosampleFromSourceId\" not in self.df.columns:\n        return self\n\n    validated_df = (\n        self.df.join(\n            biosample_set,\n            self.df.biosampleFromSourceId == biosample_set.biosampleId,\n            how=\"left\",\n        )\n        .withColumn(\n            \"isIdFound\",\n            f.when(\n                (f.col(\"studyType\") != \"gwas\") &amp; (f.col(\"isIdFound\").isNull()),\n                f.lit(False),\n            ).otherwise(f.lit(True)),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~f.col(\"isIdFound\"),\n                StudyQualityCheck.UNKNOWN_BIOSAMPLE,\n            ),\n        )\n        .drop(\"isIdFound\")\n    )\n\n    return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_disease","title":"<code>validate_disease(disease_map: DataFrame) -&gt; StudyIndex</code>","text":"<p>Validate diseases in the study index dataset.</p> <p>Parameters:</p> Name Type Description Default <code>disease_map</code> <code>DataFrame</code> <p>a dataframe with two columns (efo, diseaseId).</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>where gwas studies are flagged where no valid disease id could be found.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_disease(self: StudyIndex, disease_map: DataFrame) -&gt; StudyIndex:\n    \"\"\"Validate diseases in the study index dataset.\n\n    Args:\n        disease_map (DataFrame): a dataframe with two columns (efo, diseaseId).\n\n    Returns:\n        StudyIndex: where gwas studies are flagged where no valid disease id could be found.\n    \"\"\"\n    # Because the disease ids are not mandatory fields of the schema, we skip vaildation if these columns are not present:\n    if (\"traitFromSourceMappedIds\" not in self.df.columns) or (\n        \"backgroundTraitFromSourceMappedIds\" not in self.df.columns\n    ):\n        return self\n\n    # Disease Column names:\n    foreground_disease_column = \"diseaseIds\"\n    background_disease_column = \"backgroundDiseaseIds\"\n\n    # If diseaseId in schema, we need to drop it:\n    drop_columns = [\n        column\n        for column in self.df.columns\n        if column in [foreground_disease_column, background_disease_column]\n    ]\n\n    if len(drop_columns) &gt; 0:\n        self.df = self.df.drop(*drop_columns)\n\n    # Normalise disease:\n    normalised_disease = self._normalise_disease(\n        \"traitFromSourceMappedIds\", foreground_disease_column, disease_map\n    )\n    normalised_background_disease = self._normalise_disease(\n        \"backgroundTraitFromSourceMappedIds\", background_disease_column, disease_map\n    )\n\n    return StudyIndex(\n        _df=(\n            self.df.join(normalised_disease, on=\"studyId\", how=\"left\")\n            .join(normalised_background_disease, on=\"studyId\", how=\"left\")\n            # Updating disease columns:\n            .withColumn(\n                foreground_disease_column,\n                f.when(\n                    f.col(foreground_disease_column).isNull(), f.array()\n                ).otherwise(f.col(foreground_disease_column)),\n            )\n            .withColumn(\n                background_disease_column,\n                f.when(\n                    f.col(background_disease_column).isNull(), f.array()\n                ).otherwise(f.col(background_disease_column)),\n            )\n            # Flagging gwas studies where no valid disease is avilable:\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    # Flagging all gwas studies with no normalised disease:\n                    (f.size(f.col(foreground_disease_column)) == 0)\n                    &amp; (f.col(\"studyType\") == \"gwas\"),\n                    StudyQualityCheck.UNRESOLVED_DISEASE,\n                ),\n            )\n            # Added to avoid Spark optimisation (see: https://github.com/opentargets/issues/issues/3906#issuecomment-2949299965)\n            .persist()\n        ),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_study_type","title":"<code>validate_study_type() -&gt; StudyIndex</code>","text":"<p>Validating study type and flag unsupported types.</p> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>with flagged studies with unsupported type.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_study_type(self: StudyIndex) -&gt; StudyIndex:\n    \"\"\"Validating study type and flag unsupported types.\n\n    Returns:\n        StudyIndex: with flagged studies with unsupported type.\n    \"\"\"\n    validated_df = (\n        self.df\n        # Flagging unsupported study types:\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.when(\n                    (f.col(\"studyType\") == \"gwas\")\n                    | f.col(\"studyType\").endswith(\"qtl\"),\n                    False,\n                ).otherwise(True),\n                StudyQualityCheck.UNKNOWN_STUDY_TYPE,\n            ),\n        )\n    )\n    return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_target","title":"<code>validate_target(target_index: TargetIndex) -&gt; StudyIndex</code>","text":"<p>Validating gene identifiers in the study index against the provided target index.</p> <p>Parameters:</p> Name Type Description Default <code>target_index</code> <code>TargetIndex</code> <p>target index containing the reference gene identifiers (Ensembl gene identifiers).</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>with flagged studies if geneId could not be validated.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_target(self: StudyIndex, target_index: TargetIndex) -&gt; StudyIndex:\n    \"\"\"Validating gene identifiers in the study index against the provided target index.\n\n    Args:\n        target_index (TargetIndex): target index containing the reference gene identifiers (Ensembl gene identifiers).\n\n    Returns:\n        StudyIndex: with flagged studies if geneId could not be validated.\n    \"\"\"\n    gene_set = target_index.df.select(\n        f.col(\"id\").alias(\"geneId\"), f.lit(True).alias(\"isIdFound\")\n    )\n\n    # As the geneId is not a mandatory field of study index, we return if the column is not there:\n    if \"geneId\" not in self.df.columns:\n        return self\n\n    validated_df = (\n        self.df.join(gene_set, on=\"geneId\", how=\"left\")\n        .withColumn(\n            \"isIdFound\",\n            f.when(\n                (f.col(\"studyType\") != \"gwas\") &amp; f.col(\"isIdFound\").isNull(),\n                f.lit(False),\n            ).otherwise(f.lit(True)),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~f.col(\"isIdFound\"),\n                StudyQualityCheck.UNRESOLVED_TARGET,\n            ),\n        )\n        .drop(\"isIdFound\")\n    )\n\n    return StudyIndex(_df=validated_df, _schema=StudyIndex.get_schema())\n</code></pre>"},{"location":"python_api/datasets/study_index/#gentropy.dataset.study_index.StudyIndex.validate_unique_study_id","title":"<code>validate_unique_study_id() -&gt; StudyIndex</code>","text":"<p>Validating the uniqueness of study identifiers and flagging duplicated studies.</p> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>with flagged duplicated studies.</p> Source code in <code>src/gentropy/dataset/study_index.py</code> <pre><code>def validate_unique_study_id(self: StudyIndex) -&gt; StudyIndex:\n    \"\"\"Validating the uniqueness of study identifiers and flagging duplicated studies.\n\n    Returns:\n        StudyIndex: with flagged duplicated studies.\n    \"\"\"\n    return StudyIndex(\n        _df=self.df.withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                self.flag_duplicates(f.col(\"studyId\")),\n                StudyQualityCheck.DUPLICATED_STUDY,\n            ),\n        ),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_index/#schema","title":"Schema","text":"<pre><code>root\n |-- studyId: string (nullable = false)\n |-- projectId: string (nullable = false)\n |-- studyType: string (nullable = false)\n |-- traitFromSource: string (nullable = true)\n |-- traitFromSourceMappedIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- diseaseIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- geneId: string (nullable = true)\n |-- biosampleFromSourceId: string (nullable = true)\n |-- biosampleId: string (nullable = true)\n |-- pubmedId: string (nullable = true)\n |-- publicationTitle: string (nullable = true)\n |-- publicationFirstAuthor: string (nullable = true)\n |-- publicationDate: string (nullable = true)\n |-- publicationJournal: string (nullable = true)\n |-- backgroundTraitFromSourceMappedIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- backgroundDiseaseIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- initialSampleSize: string (nullable = true)\n |-- nCases: integer (nullable = true)\n |-- nControls: integer (nullable = true)\n |-- nSamples: integer (nullable = true)\n |-- cohorts: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- ldPopulationStructure: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- ldPopulation: string (nullable = true)\n |    |    |-- relativeSampleSize: double (nullable = true)\n |-- discoverySamples: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- sampleSize: integer (nullable = true)\n |    |    |-- ancestry: string (nullable = true)\n |-- replicationSamples: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- sampleSize: integer (nullable = true)\n |    |    |-- ancestry: string (nullable = true)\n |-- qualityControls: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- analysisFlags: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- summarystatsLocation: string (nullable = true)\n |-- hasSumstats: boolean (nullable = true)\n |-- condition: string (nullable = true)\n |-- sumstatQCValues: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- QCCheckName: string (nullable = false)\n |    |    |-- QCCheckValue: float (nullable = false)\n</code></pre>"},{"location":"python_api/datasets/study_locus/","title":"Study Locus","text":""},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus","title":"<code>gentropy.dataset.study_locus.StudyLocus</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Study-Locus dataset.</p> <p>This dataset captures associations between study/traits and a genetic loci as provided by finemapping methods.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@dataclass\nclass StudyLocus(Dataset):\n    \"\"\"Study-Locus dataset.\n\n    This dataset captures associations between study/traits and a genetic loci as provided by finemapping methods.\n    \"\"\"\n\n    def validate_study(self: StudyLocus, study_index: StudyIndex) -&gt; StudyLocus:\n        \"\"\"Flagging study loci if the corresponding study has issues.\n\n        There are two different potential flags:\n        - flagged study: flagging locus if the study has quality control flags.\n        - study with summary statistics for top hit: flagging locus if the study has available summary statistics.\n        - missing study: flagging locus if the study was not found in the reference study index.\n\n        Args:\n            study_index (StudyIndex): Study index to resolve study types.\n\n        Returns:\n            StudyLocus: Updated study locus with quality control flags.\n        \"\"\"\n        # Quality controls is not a mandatory field in the study index schema, so we have to be ready to handle it:\n        qc_select_expression = (\n            f.col(\"qualityControls\")\n            if \"qualityControls\" in study_index.df.columns\n            else f.lit(None).cast(StringType())\n        )\n\n        # The study Id of the study index needs to be kept, because we would not know which study was in the index after the left join:\n        study_flags = study_index.df.select(\n            f.col(\"studyId\").alias(\"study_studyId\"),\n            qc_select_expression.alias(\"study_qualityControls\"),\n        )\n\n        return StudyLocus(\n            _df=(\n                self.df.join(\n                    study_flags, f.col(\"studyId\") == f.col(\"study_studyId\"), \"left\"\n                )\n                # Flagging loci with flagged studies - without propagating the actual flags:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyLocus.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        f.size(f.col(\"study_qualityControls\")) &gt; 0,\n                        StudyLocusQualityCheck.FLAGGED_STUDY,\n                    ),\n                )\n                # Flagging top-hits, where the study has available summary statistics:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyLocus.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        # Condition is true, if the study has summary statistics available and the locus is a top hit:\n                        f.array_contains(\n                            f.col(\"qualityControls\"),\n                            StudyLocusQualityCheck.TOP_HIT.value,\n                        )\n                        &amp; ~f.array_contains(\n                            f.col(\"study_qualityControls\"),\n                            StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value,\n                        ),\n                        StudyLocusQualityCheck.TOP_HIT_AND_SUMMARY_STATS,\n                    ),\n                )\n                # Flagging loci where no studies were found:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyLocus.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        f.col(\"study_studyId\").isNull(),\n                        StudyLocusQualityCheck.MISSING_STUDY,\n                    ),\n                )\n                .drop(\"study_studyId\", \"study_qualityControls\")\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def annotate_study_type(self: StudyLocus, study_index: StudyIndex) -&gt; StudyLocus:\n        \"\"\"Gets study type from study index and adds it to study locus.\n\n        Args:\n            study_index (StudyIndex): Study index to get study type.\n\n        Returns:\n            StudyLocus: Updated study locus with study type.\n        \"\"\"\n        return StudyLocus(\n            _df=(\n                self.df.drop(\"studyType\").join(\n                    study_index.study_type_lut(), on=\"studyId\", how=\"left\"\n                )\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def validate_chromosome_label(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Flagging study loci, where chromosome is coded not as 1:22, X, Y, Xy and MT.\n\n        Returns:\n            StudyLocus: Updated study locus with quality control flags.\n        \"\"\"\n        # QC column might not be present in the variant index schema, so we have to be ready to handle it:\n        qc_select_expression = (\n            f.col(\"qualityControls\")\n            if \"qualityControls\" in self.df.columns\n            else f.lit(None).cast(ArrayType(StringType()))\n        )\n        valid_chromosomes = [str(i) for i in range(1, 23)] + [\"X\", \"Y\", \"XY\", \"MT\"]\n\n        return StudyLocus(\n            _df=(\n                self.df.withColumn(\n                    \"qualityControls\",\n                    self.update_quality_flag(\n                        qc_select_expression,\n                        ~f.col(\"chromosome\").isin(valid_chromosomes),\n                        StudyLocusQualityCheck.INVALID_CHROMOSOME,\n                    ),\n                )\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def validate_variant_identifiers(\n        self: StudyLocus, variant_index: VariantIndex\n    ) -&gt; StudyLocus:\n        \"\"\"Flagging study loci, where tagging variant identifiers are not found in variant index.\n\n        Args:\n            variant_index (VariantIndex): Variant index to resolve variant identifiers.\n\n        Returns:\n            StudyLocus: Updated study locus with quality control flags.\n        \"\"\"\n        # QC column might not be present in the variant index schema, so we have to be ready to handle it:\n        qc_select_expression = (\n            f.col(\"qualityControls\")\n            if \"qualityControls\" in self.df.columns\n            else f.lit(None).cast(ArrayType(StringType()))\n        )\n\n        # Find out which study loci have variants not in the variant index:\n        flag = (\n            self.df\n            # Exploding locus:\n            .select(\"studyLocusId\", f.explode(\"locus\").alias(\"locus\"))\n            .select(\"studyLocusId\", \"locus.variantId\")\n            # Join with variant index variants:\n            .join(\n                variant_index.df.select(\n                    \"variantId\", f.lit(True).alias(\"inVariantIndex\")\n                ),\n                on=\"variantId\",\n                how=\"left\",\n            )\n            # Flagging variants not in the variant index:\n            .withColumn(\"inVariantIndex\", f.col(\"inVariantIndex\").isNotNull())\n            # Flagging study loci with ANY variants not in the variant index:\n            .groupBy(\"studyLocusId\")\n            .agg(f.collect_set(\"inVariantIndex\").alias(\"inVariantIndex\"))\n            .select(\n                \"studyLocusId\",\n                f.array_contains(\"inVariantIndex\", False).alias(\"toFlag\"),\n            )\n        )\n\n        return StudyLocus(\n            _df=(\n                self.df.join(flag, on=\"studyLocusId\", how=\"left\")\n                .withColumn(\n                    \"qualityControls\",\n                    self.update_quality_flag(\n                        qc_select_expression,\n                        f.col(\"toFlag\"),\n                        StudyLocusQualityCheck.INVALID_VARIANT_IDENTIFIER,\n                    ),\n                )\n                .drop(\"toFlag\")\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def validate_lead_pvalue(self: StudyLocus, pvalue_cutoff: float) -&gt; StudyLocus:\n        \"\"\"Flag associations below significant threshold.\n\n        Args:\n            pvalue_cutoff (float): association p-value cut-off\n\n        Returns:\n            StudyLocus: Updated study locus with quality control flags.\n        \"\"\"\n        df = self.df\n        qc_colname = StudyLocus.get_QC_column_name()\n        if qc_colname not in self.df.columns:\n            df = self.df.withColumn(\n                qc_colname,\n                create_empty_column_if_not_exists(\n                    qc_colname,\n                    get_struct_field_schema(StudyLocus.get_schema(), qc_colname),\n                ),\n            )\n        return StudyLocus(\n            _df=(\n                df.withColumn(\n                    qc_colname,\n                    # Because this QC might already run on the dataset, the unique set of flags is generated:\n                    f.array_distinct(\n                        self._qc_subsignificant_associations(\n                            f.col(\"qualityControls\"),\n                            f.col(\"pValueMantissa\"),\n                            f.col(\"pValueExponent\"),\n                            pvalue_cutoff,\n                        )\n                    ),\n                )\n            ),\n            _schema=self.get_schema(),\n        )\n\n    def validate_unique_study_locus_id(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Validating the uniqueness of study-locus identifiers and flagging duplicated studyloci.\n\n        Returns:\n            StudyLocus: with flagged duplicated studies.\n        \"\"\"\n        return StudyLocus(\n            _df=self.df.withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    self.flag_duplicates(f.col(\"studyLocusId\")),\n                    StudyLocusQualityCheck.DUPLICATED_STUDYLOCUS_ID,\n                ),\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    @staticmethod\n    def _qc_subsignificant_associations(\n        quality_controls_column: Column,\n        p_value_mantissa: Column,\n        p_value_exponent: Column,\n        pvalue_cutoff: float,\n    ) -&gt; Column:\n        \"\"\"Flag associations below significant threshold.\n\n        Args:\n            quality_controls_column (Column): QC column\n            p_value_mantissa (Column): P-value mantissa column\n            p_value_exponent (Column): P-value exponent column\n            pvalue_cutoff (float): association p-value cut-off\n\n        Returns:\n            Column: Updated QC column with flag.\n\n        Examples:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [{'qc': None, 'p_value_mantissa': 1, 'p_value_exponent': -7}, {'qc': None, 'p_value_mantissa': 1, 'p_value_exponent': -8}, {'qc': None, 'p_value_mantissa': 5, 'p_value_exponent': -8}, {'qc': None, 'p_value_mantissa': 1, 'p_value_exponent': -9}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, t.StructType([t.StructField('qc', t.ArrayType(t.StringType()), True), t.StructField('p_value_mantissa', t.IntegerType()), t.StructField('p_value_exponent', t.IntegerType())]))\n            &gt;&gt;&gt; df.withColumn('qc', StudyLocus._qc_subsignificant_associations(f.col(\"qc\"), f.col(\"p_value_mantissa\"), f.col(\"p_value_exponent\"), 5e-8)).show(truncate = False)\n            +------------------------+----------------+----------------+\n            |qc                      |p_value_mantissa|p_value_exponent|\n            +------------------------+----------------+----------------+\n            |[Subsignificant p-value]|1               |-7              |\n            |[]                      |1               |-8              |\n            |[]                      |5               |-8              |\n            |[]                      |1               |-9              |\n            +------------------------+----------------+----------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            quality_controls_column,\n            neglogpval_from_pvalue(p_value_mantissa, p_value_exponent)\n            &lt; f.lit(-np.log10(pvalue_cutoff)),\n            StudyLocusQualityCheck.SUBSIGNIFICANT_FLAG,\n        )\n\n    def qc_abnormal_pips(\n        self: StudyLocus,\n        sum_pips_lower_threshold: float = 0.99,\n        # Set slightly above 1 to account for floating point errors\n        sum_pips_upper_threshold: float = 1.0001,\n    ) -&gt; StudyLocus:\n        \"\"\"Filter study-locus by sum of posterior inclusion probabilities to ensure that the sum of PIPs is within a given range.\n\n        Args:\n            sum_pips_lower_threshold (float): Lower threshold for the sum of PIPs.\n            sum_pips_upper_threshold (float): Upper threshold for the sum of PIPs.\n\n        Returns:\n            StudyLocus: Filtered study-locus dataset.\n        \"\"\"\n        # QC column might not be present so we have to be ready to handle it:\n        qc_select_expression = (\n            f.col(\"qualityControls\")\n            if \"qualityControls\" in self.df.columns\n            else f.lit(None).cast(ArrayType(StringType()))\n        )\n\n        flag = self.df.withColumn(\n            \"sumPosteriorProbability\",\n            f.aggregate(\n                f.col(\"locus\"),\n                f.lit(0.0),\n                lambda acc, x: acc + x[\"posteriorProbability\"],\n            ),\n        ).withColumn(\n            \"pipOutOfRange\",\n            f.when(\n                (f.col(\"sumPosteriorProbability\") &lt; sum_pips_lower_threshold)\n                | (f.col(\"sumPosteriorProbability\") &gt; sum_pips_upper_threshold),\n                True,\n            ).otherwise(False),\n        )\n\n        return StudyLocus(\n            _df=(\n                flag\n                # Flagging loci with failed studies:\n                .withColumn(\n                    \"qualityControls\",\n                    self.update_quality_flag(\n                        qc_select_expression,\n                        f.col(\"pipOutOfRange\"),\n                        StudyLocusQualityCheck.ABNORMAL_PIPS,\n                    ),\n                ).drop(\"sumPosteriorProbability\", \"pipOutOfRange\")\n            ),\n            _schema=self.get_schema(),\n        )\n\n    @staticmethod\n    def _overlapping_peaks(\n        credset_to_overlap: DataFrame,\n        restrict_right_studies: list[str] | None = None,\n        gwas_v_qtl_overlap_only: bool = False,\n    ) -&gt; DataFrame:\n        \"\"\"Calculate overlapping signals (study-locus) between GWAS-GWAS and GWAS-Molecular trait.\n\n        Args:\n            credset_to_overlap (DataFrame): DataFrame containing at least `studyLocusId`, `studyType`, `chromosome` and `tagVariantId` columns.\n            restrict_right_studies (list[str] | None): List of studyIds to restrict finding overlaps on the right side. Default is None.\n            gwas_v_qtl_overlap_only (bool): When True, finds overlaps with just molecular-QTLs on the right-hand side. Default is False.\n\n        Returns:\n            DataFrame: containing `leftStudyLocusId`, `rightStudyLocusId` and `chromosome` columns.\n        \"\"\"\n        # Reduce columns to the minimum to reduce the size of the dataframe\n        credset_to_overlap = credset_to_overlap.select(\n            \"studyLocusId\",\n            \"studyId\",\n            \"studyType\",\n            \"chromosome\",\n            \"region\",\n            \"tagVariantId\",\n        )\n        # Define join condition\n        # If gwas_v_qtl_overlap_only is True, finds only gwas vs molQTL overlaps.\n        if gwas_v_qtl_overlap_only:\n            join_condition = [\n                f.col(\"left.chromosome\") == f.col(\"right.chromosome\"),\n                f.col(\"left.tagVariantId\") == f.col(\"right.tagVariantId\"),\n                f.col(\"left.studyType\") == f.lit(\"gwas\"),\n                f.col(\"right.studyType\") != f.lit(\"gwas\"),\n            ]\n        # If restrict_right_studies is not empty, restrict finding overlaps to those studies\n        elif restrict_right_studies is not None:\n            join_condition = [\n                f.col(\"left.chromosome\") == f.col(\"right.chromosome\"),\n                f.col(\"left.tagVariantId\") == f.col(\"right.tagVariantId\"),\n                f.col(\"left.studyType\") == f.lit(\"gwas\"),\n                (f.col(\"right.studyId\").isin(restrict_right_studies))\n                &amp; (~f.col(\"left.studyId\").isin(restrict_right_studies))\n                &amp; (f.col(\"left.studyLocusId\") != f.col(\"right.studyLocusId\")),\n            ]\n        else:\n            join_condition = [\n                f.col(\"left.chromosome\") == f.col(\"right.chromosome\"),\n                f.col(\"left.tagVariantId\") == f.col(\"right.tagVariantId\"),\n                (f.col(\"right.studyType\") != \"gwas\")\n                | (f.col(\"left.studyLocusId\") &gt; f.col(\"right.studyLocusId\")),\n                f.col(\"left.studyType\") == f.lit(\"gwas\"),\n            ]\n\n        return (\n            credset_to_overlap.alias(\"left\")\n            # Self join with complex condition.\n            .join(\n                credset_to_overlap.alias(\"right\"),\n                on=join_condition,\n                how=\"inner\",\n            )\n            .select(\n                f.col(\"left.studyLocusId\").alias(\"leftStudyLocusId\"),\n                f.col(\"right.studyLocusId\").alias(\"rightStudyLocusId\"),\n                f.col(\"right.studyType\").alias(\"rightStudyType\"),\n                f.col(\"left.chromosome\").alias(\"chromosome\"),\n            )\n            .distinct()\n            .repartition(\"chromosome\")\n            .persist()\n        )\n\n    @staticmethod\n    def _align_overlapping_tags(\n        loci_to_overlap: DataFrame, peak_overlaps: DataFrame\n    ) -&gt; StudyLocusOverlap:\n        \"\"\"Align overlapping tags in pairs of overlapping study-locus, keeping all tags in both loci.\n\n        Args:\n            loci_to_overlap (DataFrame): containing `studyLocusId`, `studyType`, `chromosome`, `tagVariantId`, `logBF` and `posteriorProbability` columns.\n            peak_overlaps (DataFrame): containing `leftStudyLocusId`, `rightStudyLocusId` and `chromosome` columns.\n\n        Returns:\n            StudyLocusOverlap: Pairs of overlapping study-locus with aligned tags.\n        \"\"\"\n        # Complete information about all tags in the left study-locus of the overlap\n        stats_cols = [\n            \"logBF\",\n            \"posteriorProbability\",\n            \"beta\",\n            \"pValueMantissa\",\n            \"pValueExponent\",\n        ]\n        overlapping_left = loci_to_overlap.select(\n            f.col(\"chromosome\"),\n            f.col(\"tagVariantId\"),\n            f.col(\"studyLocusId\").alias(\"leftStudyLocusId\"),\n            *[f.col(col).alias(f\"left_{col}\") for col in stats_cols],\n        ).join(peak_overlaps, on=[\"chromosome\", \"leftStudyLocusId\"], how=\"inner\")\n\n        # Complete information about all tags in the right study-locus of the overlap\n        overlapping_right = loci_to_overlap.select(\n            f.col(\"chromosome\"),\n            f.col(\"tagVariantId\"),\n            f.col(\"studyLocusId\").alias(\"rightStudyLocusId\"),\n            *[f.col(col).alias(f\"right_{col}\") for col in stats_cols],\n        ).join(peak_overlaps, on=[\"chromosome\", \"rightStudyLocusId\"], how=\"inner\")\n\n        # Include information about all tag variants in both study-locus aligned by tag variant id\n        overlaps = overlapping_left.join(\n            overlapping_right,\n            on=[\n                \"chromosome\",\n                \"rightStudyLocusId\",\n                \"leftStudyLocusId\",\n                \"tagVariantId\",\n                \"rightStudyType\",\n            ],\n            how=\"outer\",\n        ).select(\n            \"leftStudyLocusId\",\n            \"rightStudyLocusId\",\n            \"rightStudyType\",\n            \"chromosome\",\n            \"tagVariantId\",\n            f.struct(\n                *[f\"left_{e}\" for e in stats_cols] + [f\"right_{e}\" for e in stats_cols]\n            ).alias(\"statistics\"),\n        )\n        return StudyLocusOverlap(\n            _df=overlaps,\n            _schema=StudyLocusOverlap.get_schema(),\n        )\n\n    @staticmethod\n    def assign_study_locus_id(uniqueness_defining_columns: list[str]) -&gt; Column:\n        \"\"\"Hashes the provided columns to extract a consistent studyLocusId.\n\n        Args:\n            uniqueness_defining_columns (list[str]): list of columns defining uniqueness\n\n        Returns:\n            Column: column with a study locus ID\n\n        Examples:\n            &gt;&gt;&gt; df = spark.createDataFrame([(\"GCST000001\", \"1_1000_A_C\", \"SuSiE-inf\"), (\"GCST000002\", \"1_1000_A_C\", \"pics\")]).toDF(\"studyId\", \"variantId\", \"finemappingMethod\")\n            &gt;&gt;&gt; df.withColumn(\"study_locus_id\", StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\", \"finemappingMethod\"])).show(truncate=False)\n            +----------+----------+-----------------+--------------------------------+\n            |studyId   |variantId |finemappingMethod|study_locus_id                  |\n            +----------+----------+-----------------+--------------------------------+\n            |GCST000001|1_1000_A_C|SuSiE-inf        |109804fe1e20c94231a31bafd71b566e|\n            |GCST000002|1_1000_A_C|pics             |de310be4558e0482c9cc359c97d37773|\n            +----------+----------+-----------------+--------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return Dataset.generate_identifier(uniqueness_defining_columns).alias(\n            \"studyLocusId\"\n        )\n\n    @classmethod\n    def calculate_credible_set_log10bf(\n        cls: type[StudyLocus], logbfs: Column, num_variants_region: int = 500\n    ) -&gt; Column:\n        \"\"\"Calculate Bayes factor for the entire credible set. The Bayes factor is calculated as the logsumexp of the logBF values of the variants in the locus.\n\n        Args:\n            logbfs (Column): Array column with the logBF values of the variants in the locus.\n            num_variants_region (int): Number of variants in the region for calculation of priors. Default: 500.\n\n        Returns:\n            Column: log10 Bayes factor for the entire credible set.\n\n        Examples:\n            &gt;&gt;&gt; spark.createDataFrame([([1.0, 0.5, 0.25, 0.0],)]).toDF(\"logBF\").select(f.round(StudyLocus.calculate_credible_set_log10bf(f.col(\"logBF\"), 4), 7).alias(\"credibleSetlog10BF\")).show()\n            +------------------+\n            |credibleSetlog10BF|\n            +------------------+\n            |         0.2208288|\n            +------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # log10=log/log(10)=log*0.43429448190325176\n        logsumexp_udf = f.udf(\n            lambda x: (\n                get_logsum(x + np.log(1 / num_variants_region)) * 0.43429448190325176\n            ),\n            FloatType(),\n        )\n        return logsumexp_udf(logbfs).cast(\"double\").alias(\"credibleSetlog10BF\")\n\n    @classmethod\n    def get_schema(cls: type[StudyLocus]) -&gt; StructType:\n        \"\"\"Provides the schema for the StudyLocus dataset.\n\n        Returns:\n            StructType: schema for the StudyLocus dataset.\n        \"\"\"\n        return parse_spark_schema(\"study_locus.json\")\n\n    @classmethod\n    def get_QC_column_name(cls: type[StudyLocus]) -&gt; str:\n        \"\"\"Quality control column.\n\n        Returns:\n            str: Name of the quality control column.\n        \"\"\"\n        return \"qualityControls\"\n\n    @classmethod\n    def get_QC_mappings(cls: type[StudyLocus]) -&gt; dict[str, str]:\n        \"\"\"Quality control flag to QC column category mappings.\n\n        Returns:\n            dict[str, str]: Mapping between flag name and QC column category value.\n        \"\"\"\n        return {member.name: member.value for member in StudyLocusQualityCheck}\n\n    def flag_trans_qtls(\n        self: StudyLocus,\n        study_index: StudyIndex,\n        target_index: TargetIndex,\n        trans_threshold: int = 5_000_000,\n    ) -&gt; StudyLocus:\n        \"\"\"Flagging transQTL credible sets based on genomic location of the measured gene.\n\n        Process:\n        0. Make sure that the `isTransQtl` column does not exist (remove if exists)\n        1. Enrich study-locus dataset with geneId based on study metadata. (only QTL studies are considered)\n        2. Enrich with transcription start site and chromosome of the studied gegne.\n        3. Flagging any tagging variant of QTL credible sets, if chromosome is different from the gene or distance is above the threshold.\n        4. Propagate flags to credible sets where any tags are considered as trans.\n        5. Return study locus object with annotation stored in 'isTransQtl` boolean column, where gwas credible sets will be `null`\n\n        Args:\n            study_index (StudyIndex): study index to extract identifier of the measured gene\n            target_index (TargetIndex): target index bringing TSS and chromosome of the measured gene\n            trans_threshold (int): Distance above which the QTL is considered trans. Default: 5_000_000bp\n\n        Returns:\n            StudyLocus: new column added indicating if the QTL credibles sets are trans.\n        \"\"\"\n        # As the `geneId` column in the study index is optional, we have to test for that:\n        if \"geneId\" not in study_index.df.columns:\n            return self\n\n        # We have to remove the column `isTransQtl` to ensure the column is not duplicated\n        # The duplication can happen when one reads the StudyLocus from parquet with\n        # predefined schema that already contains the `isTransQtl` column.\n        if \"isTransQtl\" in self.df.columns:\n            self.df = self.df.drop(\"isTransQtl\")\n\n        # Process study index:\n        processed_studies = (\n            study_index.df\n            # Dropping gwas studies. This ensures that only QTLs will have \"isTrans\" annotation:\n            .filter(f.col(\"studyType\") != \"gwas\").select(\n                \"studyId\", \"geneId\", \"projectId\"\n            )\n        )\n\n        # Process study locus:\n        processed_credible_set = (\n            self.df\n            # Exploding locus to test all tag variants:\n            .withColumn(\"locus\", f.explode(\"locus\")).select(\n                \"studyLocusId\",\n                \"studyId\",\n                f.split(\"locus.variantId\", \"_\")[0].alias(\"chromosome\"),\n                f.split(\"locus.variantId\", \"_\")[1].cast(LongType()).alias(\"position\"),\n            )\n        )\n\n        # Process target index:\n        processed_targets = target_index.df.select(\n            f.col(\"id\").alias(\"geneId\"),\n            f.col(\"tss\"),\n            f.col(\"genomicLocation.chromosome\").alias(\"geneChromosome\"),\n        )\n\n        # Pool datasets:\n        joined_data = (\n            processed_credible_set\n            # Join processed studies:\n            .join(processed_studies, on=\"studyId\", how=\"inner\")\n            # Join processed targets:\n            .join(processed_targets, on=\"geneId\", how=\"left\")\n            # Assign True/False for QTL studies:\n            .withColumn(\n                \"isTagTrans\",\n                # The QTL signal is considered trans if the locus is on a different chromosome than the measured gene.\n                # OR the distance from the gene's transcription start site is &gt; threshold.\n                f.when(\n                    (f.col(\"chromosome\") != f.col(\"geneChromosome\"))\n                    | (f.abs(f.col(\"tss\") - f.col(\"position\")) &gt; trans_threshold),\n                    f.lit(True),\n                ).otherwise(f.lit(False)),\n            )\n            .groupby(\"studyLocusId\")\n            .agg(\n                # If all tagging variants of the locus is in trans position, the QTL is considered trans:\n                f.when(\n                    f.array_contains(f.collect_set(\"isTagTrans\"), f.lit(False)), False\n                )\n                .otherwise(f.lit(True))\n                .alias(\"isTransQtl\")\n            )\n        )\n        # Adding new column, where the value is null for gwas loci:\n        return StudyLocus(self.df.join(joined_data, on=\"studyLocusId\", how=\"left\"))\n\n    def filter_credible_set(\n        self: StudyLocus,\n        credible_interval: CredibleInterval,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotate and filter study-locus tag variants based on given credible interval.\n\n        Args:\n            credible_interval (CredibleInterval): Credible interval to filter for.\n\n        Returns:\n            StudyLocus: Filtered study-locus dataset.\n        \"\"\"\n        return StudyLocus(\n            _df=self.annotate_credible_sets().df.withColumn(\n                \"locus\",\n                f.filter(\n                    f.col(\"locus\"),\n                    lambda tag: (tag[credible_interval.value]),\n                ),\n            ),\n            _schema=self._schema,\n        )\n\n    @staticmethod\n    def filter_ld_set(ld_set: Column, r2_threshold: float) -&gt; Column:\n        \"\"\"Filter the LD set by a given R2 threshold.\n\n        Args:\n            ld_set (Column): LD set\n            r2_threshold (float): R2 threshold to filter the LD set on\n\n        Returns:\n            Column: Filtered LD index\n        \"\"\"\n        return f.when(\n            ld_set.isNotNull(),\n            f.filter(\n                ld_set,\n                lambda tag: tag[\"r2Overall\"] &gt;= r2_threshold,\n            ),\n        )\n\n    def find_overlaps(\n        self: StudyLocus,\n        restrict_right_studies: list[str] | None = None,\n        gwas_v_qtl_overlap_only: bool = False,\n    ) -&gt; StudyLocusOverlap:\n        \"\"\"Calculate overlapping study-locus.\n\n        Find overlapping study-locus that share at least one tagging variant. By default all GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always\n        appearing on the right side of the overlap. The user can restrict computation of overlaps to a list of studyIds on the right-hand side or\n        can restrict the overlaps to just GWAS vs Molecular traits.\n\n        Args:\n            restrict_right_studies (list[str] | None): List of studyIds to restrict finding overlaps to on the right-side. Default is None.\n            gwas_v_qtl_overlap_only (bool): If True, finds overlaps between all GWAS loci vs molecular-QTLs. Default is False.\n\n        Returns:\n            StudyLocusOverlap: Pairs of overlapping study-locus with aligned tags.\n        \"\"\"\n        loci_to_overlap = (\n            self.df.filter(f.col(\"studyType\").isNotNull())\n            .withColumn(\"locus\", f.explode(\"locus\"))\n            .select(\n                \"studyLocusId\",\n                \"studyId\",\n                \"studyType\",\n                \"chromosome\",\n                \"region\",\n                f.col(\"locus.variantId\").alias(\"tagVariantId\"),\n                f.col(\"locus.logBF\").alias(\"logBF\"),\n                f.col(\"locus.posteriorProbability\").alias(\"posteriorProbability\"),\n                f.col(\"locus.pValueMantissa\").alias(\"pValueMantissa\"),\n                f.col(\"locus.pValueExponent\").alias(\"pValueExponent\"),\n                f.col(\"locus.beta\").alias(\"beta\"),\n            )\n            .persist()\n        )\n\n        # overlapping study-locus\n        peak_overlaps = self._overlapping_peaks(\n            loci_to_overlap,\n            restrict_right_studies=restrict_right_studies,\n            gwas_v_qtl_overlap_only=gwas_v_qtl_overlap_only,\n        )\n\n        # study-locus overlap by aligning overlapping variants\n        return self._align_overlapping_tags(loci_to_overlap, peak_overlaps)\n\n    def unique_variants_in_locus(self: StudyLocus) -&gt; DataFrame:\n        \"\"\"All unique variants collected in a `StudyLocus` dataframe.\n\n        Returns:\n            DataFrame: A dataframe containing `variantId` and `chromosome` columns.\n        \"\"\"\n        return (\n            self.df.withColumn(\n                \"variantId\",\n                # Joint array of variants in that studylocus. Locus can be null\n                f.explode(\n                    f.array_union(\n                        f.array(f.col(\"variantId\")),\n                        f.coalesce(f.col(\"locus.variantId\"), f.array()),\n                    )\n                ),\n            )\n            .select(\n                \"variantId\", f.split(f.col(\"variantId\"), \"_\")[0].alias(\"chromosome\")\n            )\n            .distinct()\n        )\n\n    def neglog_pvalue(self: StudyLocus) -&gt; Column:\n        \"\"\"Returns the negative log p-value.\n\n        Returns:\n            Column: Negative log p-value\n        \"\"\"\n        return neglogpval_from_pvalue(\n            self.df.pValueMantissa,\n            self.df.pValueExponent,\n        )\n\n    def build_feature_matrix(\n        self: StudyLocus,\n        features_list: list[str],\n        features_input_loader: L2GFeatureInputLoader,\n        append_null_features: bool = False,\n    ) -&gt; L2GFeatureMatrix:\n        \"\"\"Returns the feature matrix for a StudyLocus.\n\n        Args:\n            features_list (list[str]): List of features to include in the feature matrix.\n            features_input_loader (L2GFeatureInputLoader): Feature input loader to use.\n            append_null_features (bool): If True, appends null features to the feature matrix. Default is False. Usefull with small datasets that may have all null features, that are anyway required by the model.\n\n        Returns:\n            L2GFeatureMatrix: Feature matrix for this study-locus.\n        \"\"\"\n        from gentropy.dataset.l2g_feature_matrix import L2GFeatureMatrix\n\n        if append_null_features:\n            feature_matrix = (\n                L2GFeatureMatrix.from_features_list(\n                    self,\n                    features_list,\n                    features_input_loader,\n                )\n                .append_null_features(features_list)\n                .fill_na()\n            )\n        else:\n            feature_matrix = L2GFeatureMatrix.from_features_list(\n                self,\n                features_list,\n                features_input_loader,\n            ).fill_na()\n\n        return feature_matrix\n\n    def annotate_credible_sets(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Annotate study-locus dataset with credible set flags.\n\n        Sorts the array in the `locus` column elements by their `posteriorProbability` values in descending order and adds\n        `is95CredibleSet` and `is99CredibleSet` fields to the elements, indicating which are the tagging variants whose cumulative sum\n        of their `posteriorProbability` values is below 0.95 and 0.99, respectively.\n\n        Returns:\n            StudyLocus: including annotation on `is95CredibleSet` and `is99CredibleSet`.\n\n        Raises:\n            ValueError: If `locus` column is not available.\n        \"\"\"\n        if \"locus\" not in self.df.columns:\n            raise ValueError(\"Locus column not available.\")\n\n        self.df = self.df.withColumn(\n            # Sort credible set by posterior probability in descending order\n            \"locus\",\n            f.when(\n                f.col(\"locus\").isNotNull() &amp; (f.size(f.col(\"locus\")) &gt; 0),\n                order_array_of_structs_by_field(\"locus\", \"posteriorProbability\"),\n            ),\n        ).withColumn(\n            # Calculate array of cumulative sums of posterior probabilities to determine which variants are in the 95% and 99% credible sets\n            # and zip the cumulative sums array with the credible set array to add the flags\n            \"locus\",\n            f.when(\n                f.col(\"locus\").isNotNull() &amp; (f.size(f.col(\"locus\")) &gt; 0),\n                f.zip_with(\n                    f.col(\"locus\"),\n                    f.transform(\n                        f.sequence(f.lit(1), f.size(f.col(\"locus\"))),\n                        lambda index: f.aggregate(\n                            f.slice(\n                                # By using `index - 1` we introduce a value of `0.0` in the cumulative sums array. to ensure that the last variant\n                                # that exceeds the 0.95 threshold is included in the cumulative sum, as its probability is necessary to satisfy the threshold.\n                                f.col(\"locus.posteriorProbability\"),\n                                1,\n                                index - 1,\n                            ),\n                            f.lit(0.0),\n                            lambda acc, el: acc + el,\n                        ),\n                    ),\n                    lambda struct_e, acc: struct_e.withField(\n                        CredibleInterval.IS95.value, (acc &lt; 0.95) &amp; acc.isNotNull()\n                    ).withField(\n                        CredibleInterval.IS99.value, (acc &lt; 0.99) &amp; acc.isNotNull()\n                    ),\n                ),\n            ),\n        )\n        return self\n\n    def annotate_locus_statistics(\n        self: StudyLocus,\n        summary_statistics: SummaryStatistics,\n        collect_locus_distance: int,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotates study locus with summary statistics in the specified distance around the position.\n\n        Args:\n            summary_statistics (SummaryStatistics): Summary statistics to be used for annotation.\n            collect_locus_distance (int): distance from variant defining window for inclusion of variants in locus.\n\n        Returns:\n            StudyLocus: Study locus annotated with summary statistics in `locus` column. If no statistics are found, the `locus` column will be empty.\n        \"\"\"\n        # The clumps will be used several times (persisting)\n        self.df.persist()\n        # Renaming columns:\n        sumstats_renamed = summary_statistics.df.selectExpr(\n            *[f\"{col} as tag_{col}\" for col in summary_statistics.df.columns]\n        ).alias(\"sumstat\")\n\n        locus_df = (\n            sumstats_renamed\n            # Joining the two datasets together:\n            .join(\n                f.broadcast(\n                    self.df.alias(\"clumped\").select(\n                        \"position\", \"chromosome\", \"studyId\", \"studyLocusId\"\n                    )\n                ),\n                on=[\n                    (f.col(\"sumstat.tag_studyId\") == f.col(\"clumped.studyId\"))\n                    &amp; (f.col(\"sumstat.tag_chromosome\") == f.col(\"clumped.chromosome\"))\n                    &amp; (\n                        f.col(\"sumstat.tag_position\")\n                        &gt;= (f.col(\"clumped.position\") - collect_locus_distance)\n                    )\n                    &amp; (\n                        f.col(\"sumstat.tag_position\")\n                        &lt;= (f.col(\"clumped.position\") + collect_locus_distance)\n                    )\n                ],\n                how=\"inner\",\n            )\n            .withColumn(\n                \"locus\",\n                f.struct(\n                    f.col(\"tag_variantId\").alias(\"variantId\"),\n                    f.col(\"tag_beta\").alias(\"beta\"),\n                    f.col(\"tag_pValueMantissa\").alias(\"pValueMantissa\"),\n                    f.col(\"tag_pValueExponent\").alias(\"pValueExponent\"),\n                    f.col(\"tag_standardError\").alias(\"standardError\"),\n                ),\n            )\n            .groupBy(\"studyLocusId\")\n            .agg(\n                f.collect_list(f.col(\"locus\")).alias(\"locus\"),\n            )\n        )\n\n        self.df = self.df.drop(\"locus\").join(\n            locus_df,\n            on=\"studyLocusId\",\n            how=\"left\",\n        )\n\n        return self\n\n    def annotate_ld(\n        self: StudyLocus,\n        study_index: StudyIndex,\n        ld_index: LDIndex,\n        r2_threshold: float = 0.0,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotate LD information to study-locus.\n\n        Args:\n            study_index (StudyIndex): Study index to resolve ancestries.\n            ld_index (LDIndex): LD index to resolve LD information.\n            r2_threshold (float): R2 threshold to filter the LD index. Default is 0.0.\n\n        Returns:\n            StudyLocus: Study locus annotated with ld information from LD index.\n        \"\"\"\n        from gentropy.method.ld import LDAnnotator\n\n        return LDAnnotator.ld_annotate(self, study_index, ld_index, r2_threshold)\n\n    def clump(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Perform LD clumping of the studyLocus.\n\n        Evaluates whether a lead variant is linked to a tag (with lowest p-value) in the same studyLocus dataset.\n\n        Returns:\n            StudyLocus: with empty credible sets for linked variants and QC flag.\n        \"\"\"\n        clumped_df = (\n            self.df.withColumn(\n                \"is_lead_linked\",\n                LDclumping._is_lead_linked(\n                    self.df.studyId,\n                    self.df.chromosome,\n                    self.df.variantId,\n                    self.df.pValueExponent,\n                    self.df.pValueMantissa,\n                    self.df.ldSet,\n                ),\n            )\n            .withColumn(\n                \"ldSet\",\n                f.when(f.col(\"is_lead_linked\"), f.array()).otherwise(f.col(\"ldSet\")),\n            )\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.col(\"is_lead_linked\"),\n                    StudyLocusQualityCheck.LD_CLUMPED,\n                ),\n            )\n            .drop(\"is_lead_linked\")\n        )\n        return StudyLocus(\n            _df=clumped_df,\n            _schema=self.get_schema(),\n        )\n\n    def exclude_region(\n        self: StudyLocus, region: GenomicRegion, exclude_overlap: bool = False\n    ) -&gt; StudyLocus:\n        \"\"\"Exclude a region from the StudyLocus dataset.\n\n        Args:\n            region (GenomicRegion): genomic region object.\n            exclude_overlap (bool): If True, excludes StudyLocus windows with any overlap with the region.\n\n        Returns:\n            StudyLocus: filtered StudyLocus object.\n        \"\"\"\n        if exclude_overlap:\n            filter_condition = ~(\n                (f.col(\"chromosome\") == region.chromosome)\n                &amp; (\n                    (f.col(\"locusStart\") &lt;= region.end)\n                    &amp; (f.col(\"locusEnd\") &gt;= region.start)\n                )\n            )\n        else:\n            filter_condition = ~(\n                (f.col(\"chromosome\") == region.chromosome)\n                &amp; (\n                    (f.col(\"position\") &gt;= region.start)\n                    &amp; (f.col(\"position\") &lt;= region.end)\n                )\n            )\n\n        return StudyLocus(\n            _df=self.df.filter(filter_condition),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    def qc_MHC_region(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Adds qualityControl flag when lead overlaps with MHC region.\n\n        Returns:\n            StudyLocus: including qualityControl flag if in MHC region.\n        \"\"\"\n        region = GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC)\n        self.df = self.df.withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                (\n                    (f.col(\"chromosome\") == region.chromosome)\n                    &amp; (\n                        (f.col(\"position\") &lt;= region.end)\n                        &amp; (f.col(\"position\") &gt;= region.start)\n                    )\n                ),\n                StudyLocusQualityCheck.IN_MHC,\n            ),\n        )\n        return self\n\n    def qc_redundant_top_hits_from_PICS(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Flag associations from top hits when the study contains other PICS associations from summary statistics.\n\n        This flag can be useful to identify top hits that should be explained by other associations in the study derived from the summary statistics.\n\n        Returns:\n            StudyLocus: Updated study locus with redundant top hits flagged.\n        \"\"\"\n        studies_with_pics_sumstats = (\n            self.df.filter(f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n            # Returns True if the study contains any PICS associations from summary statistics\n            .withColumn(\n                \"hasPicsSumstats\",\n                ~f.array_contains(\n                    \"qualityControls\", StudyLocusQualityCheck.TOP_HIT.value\n                ),\n            )\n            .groupBy(\"studyId\")\n            .agg(f.max(f.col(\"hasPicsSumstats\")).alias(\"studiesWithPicsSumstats\"))\n        )\n\n        return StudyLocus(\n            _df=self.df.join(studies_with_pics_sumstats, on=\"studyId\", how=\"left\")\n            .withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.array_contains(\n                        \"qualityControls\", StudyLocusQualityCheck.TOP_HIT.value\n                    )\n                    &amp; f.col(\"studiesWithPicsSumstats\"),\n                    StudyLocusQualityCheck.REDUNDANT_PICS_TOP_HIT,\n                ),\n            )\n            .drop(\"studiesWithPicsSumstats\"),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    def qc_explained_by_SuSiE(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Flag associations that are explained by SuSiE associations.\n\n        Credible sets overlapping in the same region as a SuSiE credible set are flagged as explained by SuSiE.\n\n        Returns:\n            StudyLocus: Updated study locus with SuSiE explained flags.\n        \"\"\"\n        # unique study-regions covered by SuSie credible sets\n        susie_study_regions = (\n            self.filter(\n                f.col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n                )\n            )\n            .df.select(\n                \"studyId\",\n                \"chromosome\",\n                \"locusStart\",\n                \"locusEnd\",\n                f.lit(True).alias(\"inSuSiE\"),\n            )\n            .distinct()\n        )\n\n        # non SuSiE credible sets (studyLocusId) overlapping in any variant with SuSiE locus\n        redundant_study_locus = (\n            self.filter(\n                ~f.col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n                )\n            )\n            .df.withColumn(\"l\", f.explode(\"locus\"))\n            .select(\n                \"studyLocusId\",\n                \"studyId\",\n                \"chromosome\",\n                f.split(f.col(\"l.variantId\"), \"_\")[1].alias(\"tag_position\"),\n            )\n            .alias(\"study_locus\")\n            .join(\n                susie_study_regions.alias(\"regions\"),\n                how=\"inner\",\n                on=[\n                    (f.col(\"study_locus.chromosome\") == f.col(\"regions.chromosome\"))\n                    &amp; (f.col(\"study_locus.studyId\") == f.col(\"regions.studyId\"))\n                    &amp; (f.col(\"study_locus.tag_position\") &gt;= f.col(\"regions.locusStart\"))\n                    &amp; (f.col(\"study_locus.tag_position\") &lt;= f.col(\"regions.locusEnd\"))\n                ],\n            )\n            .select(\"studyLocusId\", \"inSuSiE\")\n            .distinct()\n        )\n\n        return StudyLocus(\n            _df=(\n                self.df.join(redundant_study_locus, on=\"studyLocusId\", how=\"left\")\n                .withColumn(\n                    \"qualityControls\",\n                    self.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        # credible set in SuSiE overlapping region\n                        f.col(\"inSuSiE\")\n                        # credible set not based on SuSiE\n                        &amp; (\n                            ~f.col(\"finemappingMethod\").isin(\n                                FinemappingMethod.SUSIE.value,\n                                FinemappingMethod.SUSIE_INF.value,\n                            )\n                        ),\n                        StudyLocusQualityCheck.EXPLAINED_BY_SUSIE,\n                    ),\n                )\n                .drop(\"inSuSiE\")\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    def _qc_no_population(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Flag associations where the study doesn't have population information to resolve LD.\n\n        Returns:\n            StudyLocus: Updated study locus.\n        \"\"\"\n        # If the tested column is not present, return self unchanged:\n        if \"ldPopulationStructure\" not in self.df.columns:\n            return self\n\n        self.df = self.df.withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.col(\"ldPopulationStructure\").isNull(),\n                StudyLocusQualityCheck.NO_POPULATION,\n            ),\n        )\n        return self\n\n    def annotate_locus_statistics_boundaries(\n        self: StudyLocus,\n        summary_statistics: SummaryStatistics,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotates study locus with summary statistics in the specified boundaries - locusStart and locusEnd.\n\n        Args:\n            summary_statistics (SummaryStatistics): Summary statistics to be used for annotation.\n\n        Returns:\n            StudyLocus: Study locus annotated with summary statistics in `locus` column. If no statistics are found, the `locus` column will be empty.\n        \"\"\"\n        # The clumps will be used several times (persisting)\n        self.df.persist()\n        # Renaming columns:\n        sumstats_renamed = summary_statistics.df.selectExpr(\n            *[f\"{col} as tag_{col}\" for col in summary_statistics.df.columns]\n        ).alias(\"sumstat\")\n\n        locus_df = (\n            sumstats_renamed\n            # Joining the two datasets together:\n            .join(\n                f.broadcast(\n                    self.df.alias(\"clumped\").select(\n                        \"position\",\n                        \"chromosome\",\n                        \"studyId\",\n                        \"studyLocusId\",\n                        \"locusStart\",\n                        \"locusEnd\",\n                    )\n                ),\n                on=[\n                    (f.col(\"sumstat.tag_studyId\") == f.col(\"clumped.studyId\"))\n                    &amp; (f.col(\"sumstat.tag_chromosome\") == f.col(\"clumped.chromosome\"))\n                    &amp; (f.col(\"sumstat.tag_position\") &gt;= (f.col(\"clumped.locusStart\")))\n                    &amp; (f.col(\"sumstat.tag_position\") &lt;= (f.col(\"clumped.locusEnd\")))\n                ],\n                how=\"inner\",\n            )\n            .withColumn(\n                \"locus\",\n                f.struct(\n                    f.col(\"tag_variantId\").alias(\"variantId\"),\n                    f.col(\"tag_beta\").alias(\"beta\"),\n                    f.col(\"tag_pValueMantissa\").alias(\"pValueMantissa\"),\n                    f.col(\"tag_pValueExponent\").alias(\"pValueExponent\"),\n                    f.col(\"tag_standardError\").alias(\"standardError\"),\n                ),\n            )\n            .groupBy(\"studyLocusId\")\n            .agg(\n                f.collect_list(f.col(\"locus\")).alias(\"locus\"),\n            )\n        )\n\n        self.df = self.df.drop(\"locus\").join(\n            locus_df,\n            on=\"studyLocusId\",\n            how=\"left\",\n        )\n\n        return self\n\n    def window_based_clumping(\n        self: StudyLocus,\n        window_size: int = WindowBasedClumpingStepConfig().distance,\n    ) -&gt; StudyLocus:\n        \"\"\"Clump study locus by window size.\n\n        Args:\n            window_size (int): Window size for clumping.\n\n        Returns:\n            StudyLocus: Clumped study locus, where clumped associations are flagged.\n        \"\"\"\n        from gentropy.method.window_based_clumping import WindowBasedClumping\n\n        return WindowBasedClumping.clump(self, window_size)\n\n    def assign_confidence(self: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Assign confidence to study locus.\n\n        Returns:\n            StudyLocus: Study locus with confidence assigned.\n        \"\"\"\n        # Return self if the required columns are not in the dataframe:\n        if (\n            \"qualityControls\" not in self.df.columns\n            or \"finemappingMethod\" not in self.df.columns\n        ):\n            return self\n\n        # Assign confidence based on the presence of quality controls\n        df = self.df.withColumn(\n            \"confidence\",\n            f.when(\n                (\n                    f.col(\"finemappingMethod\").isin(\n                        FinemappingMethod.SUSIE.value,\n                        FinemappingMethod.SUSIE_INF.value,\n                    )\n                )\n                &amp; (\n                    ~f.array_contains(\n                        f.col(\"qualityControls\"),\n                        StudyLocusQualityCheck.OUT_OF_SAMPLE_LD.value,\n                    )\n                ),\n                CredibleSetConfidenceClasses.FINEMAPPED_IN_SAMPLE_LD.value,\n            )\n            .when(\n                (\n                    f.col(\"finemappingMethod\").isin(\n                        FinemappingMethod.SUSIE.value,\n                        FinemappingMethod.SUSIE_INF.value,\n                    )\n                )\n                &amp; (\n                    f.array_contains(\n                        f.col(\"qualityControls\"),\n                        StudyLocusQualityCheck.OUT_OF_SAMPLE_LD.value,\n                    )\n                ),\n                CredibleSetConfidenceClasses.FINEMAPPED_OUT_OF_SAMPLE_LD.value,\n            )\n            .when(\n                (f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n                &amp; (\n                    ~f.array_contains(\n                        f.col(\"qualityControls\"), StudyLocusQualityCheck.TOP_HIT.value\n                    )\n                ),\n                CredibleSetConfidenceClasses.PICSED_SUMMARY_STATS.value,\n            )\n            .when(\n                (f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n                &amp; (\n                    f.array_contains(\n                        f.col(\"qualityControls\"), StudyLocusQualityCheck.TOP_HIT.value\n                    )\n                ),\n                CredibleSetConfidenceClasses.PICSED_TOP_HIT.value,\n            )\n            .otherwise(CredibleSetConfidenceClasses.UNKNOWN.value),\n        )\n\n        return StudyLocus(\n            _df=df,\n            _schema=self.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_credible_sets","title":"<code>annotate_credible_sets() -&gt; StudyLocus</code>","text":"<p>Annotate study-locus dataset with credible set flags.</p> <p>Sorts the array in the <code>locus</code> column elements by their <code>posteriorProbability</code> values in descending order and adds <code>is95CredibleSet</code> and <code>is99CredibleSet</code> fields to the elements, indicating which are the tagging variants whose cumulative sum of their <code>posteriorProbability</code> values is below 0.95 and 0.99, respectively.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>including annotation on <code>is95CredibleSet</code> and <code>is99CredibleSet</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>locus</code> column is not available.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_credible_sets(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Annotate study-locus dataset with credible set flags.\n\n    Sorts the array in the `locus` column elements by their `posteriorProbability` values in descending order and adds\n    `is95CredibleSet` and `is99CredibleSet` fields to the elements, indicating which are the tagging variants whose cumulative sum\n    of their `posteriorProbability` values is below 0.95 and 0.99, respectively.\n\n    Returns:\n        StudyLocus: including annotation on `is95CredibleSet` and `is99CredibleSet`.\n\n    Raises:\n        ValueError: If `locus` column is not available.\n    \"\"\"\n    if \"locus\" not in self.df.columns:\n        raise ValueError(\"Locus column not available.\")\n\n    self.df = self.df.withColumn(\n        # Sort credible set by posterior probability in descending order\n        \"locus\",\n        f.when(\n            f.col(\"locus\").isNotNull() &amp; (f.size(f.col(\"locus\")) &gt; 0),\n            order_array_of_structs_by_field(\"locus\", \"posteriorProbability\"),\n        ),\n    ).withColumn(\n        # Calculate array of cumulative sums of posterior probabilities to determine which variants are in the 95% and 99% credible sets\n        # and zip the cumulative sums array with the credible set array to add the flags\n        \"locus\",\n        f.when(\n            f.col(\"locus\").isNotNull() &amp; (f.size(f.col(\"locus\")) &gt; 0),\n            f.zip_with(\n                f.col(\"locus\"),\n                f.transform(\n                    f.sequence(f.lit(1), f.size(f.col(\"locus\"))),\n                    lambda index: f.aggregate(\n                        f.slice(\n                            # By using `index - 1` we introduce a value of `0.0` in the cumulative sums array. to ensure that the last variant\n                            # that exceeds the 0.95 threshold is included in the cumulative sum, as its probability is necessary to satisfy the threshold.\n                            f.col(\"locus.posteriorProbability\"),\n                            1,\n                            index - 1,\n                        ),\n                        f.lit(0.0),\n                        lambda acc, el: acc + el,\n                    ),\n                ),\n                lambda struct_e, acc: struct_e.withField(\n                    CredibleInterval.IS95.value, (acc &lt; 0.95) &amp; acc.isNotNull()\n                ).withField(\n                    CredibleInterval.IS99.value, (acc &lt; 0.99) &amp; acc.isNotNull()\n                ),\n            ),\n        ),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_ld","title":"<code>annotate_ld(study_index: StudyIndex, ld_index: LDIndex, r2_threshold: float = 0.0) -&gt; StudyLocus</code>","text":"<p>Annotate LD information to study-locus.</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>Study index to resolve ancestries.</p> required <code>ld_index</code> <code>LDIndex</code> <p>LD index to resolve LD information.</p> required <code>r2_threshold</code> <code>float</code> <p>R2 threshold to filter the LD index. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus annotated with ld information from LD index.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_ld(\n    self: StudyLocus,\n    study_index: StudyIndex,\n    ld_index: LDIndex,\n    r2_threshold: float = 0.0,\n) -&gt; StudyLocus:\n    \"\"\"Annotate LD information to study-locus.\n\n    Args:\n        study_index (StudyIndex): Study index to resolve ancestries.\n        ld_index (LDIndex): LD index to resolve LD information.\n        r2_threshold (float): R2 threshold to filter the LD index. Default is 0.0.\n\n    Returns:\n        StudyLocus: Study locus annotated with ld information from LD index.\n    \"\"\"\n    from gentropy.method.ld import LDAnnotator\n\n    return LDAnnotator.ld_annotate(self, study_index, ld_index, r2_threshold)\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_locus_statistics","title":"<code>annotate_locus_statistics(summary_statistics: SummaryStatistics, collect_locus_distance: int) -&gt; StudyLocus</code>","text":"<p>Annotates study locus with summary statistics in the specified distance around the position.</p> <p>Parameters:</p> Name Type Description Default <code>summary_statistics</code> <code>SummaryStatistics</code> <p>Summary statistics to be used for annotation.</p> required <code>collect_locus_distance</code> <code>int</code> <p>distance from variant defining window for inclusion of variants in locus.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus annotated with summary statistics in <code>locus</code> column. If no statistics are found, the <code>locus</code> column will be empty.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_locus_statistics(\n    self: StudyLocus,\n    summary_statistics: SummaryStatistics,\n    collect_locus_distance: int,\n) -&gt; StudyLocus:\n    \"\"\"Annotates study locus with summary statistics in the specified distance around the position.\n\n    Args:\n        summary_statistics (SummaryStatistics): Summary statistics to be used for annotation.\n        collect_locus_distance (int): distance from variant defining window for inclusion of variants in locus.\n\n    Returns:\n        StudyLocus: Study locus annotated with summary statistics in `locus` column. If no statistics are found, the `locus` column will be empty.\n    \"\"\"\n    # The clumps will be used several times (persisting)\n    self.df.persist()\n    # Renaming columns:\n    sumstats_renamed = summary_statistics.df.selectExpr(\n        *[f\"{col} as tag_{col}\" for col in summary_statistics.df.columns]\n    ).alias(\"sumstat\")\n\n    locus_df = (\n        sumstats_renamed\n        # Joining the two datasets together:\n        .join(\n            f.broadcast(\n                self.df.alias(\"clumped\").select(\n                    \"position\", \"chromosome\", \"studyId\", \"studyLocusId\"\n                )\n            ),\n            on=[\n                (f.col(\"sumstat.tag_studyId\") == f.col(\"clumped.studyId\"))\n                &amp; (f.col(\"sumstat.tag_chromosome\") == f.col(\"clumped.chromosome\"))\n                &amp; (\n                    f.col(\"sumstat.tag_position\")\n                    &gt;= (f.col(\"clumped.position\") - collect_locus_distance)\n                )\n                &amp; (\n                    f.col(\"sumstat.tag_position\")\n                    &lt;= (f.col(\"clumped.position\") + collect_locus_distance)\n                )\n            ],\n            how=\"inner\",\n        )\n        .withColumn(\n            \"locus\",\n            f.struct(\n                f.col(\"tag_variantId\").alias(\"variantId\"),\n                f.col(\"tag_beta\").alias(\"beta\"),\n                f.col(\"tag_pValueMantissa\").alias(\"pValueMantissa\"),\n                f.col(\"tag_pValueExponent\").alias(\"pValueExponent\"),\n                f.col(\"tag_standardError\").alias(\"standardError\"),\n            ),\n        )\n        .groupBy(\"studyLocusId\")\n        .agg(\n            f.collect_list(f.col(\"locus\")).alias(\"locus\"),\n        )\n    )\n\n    self.df = self.df.drop(\"locus\").join(\n        locus_df,\n        on=\"studyLocusId\",\n        how=\"left\",\n    )\n\n    return self\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_locus_statistics_boundaries","title":"<code>annotate_locus_statistics_boundaries(summary_statistics: SummaryStatistics) -&gt; StudyLocus</code>","text":"<p>Annotates study locus with summary statistics in the specified boundaries - locusStart and locusEnd.</p> <p>Parameters:</p> Name Type Description Default <code>summary_statistics</code> <code>SummaryStatistics</code> <p>Summary statistics to be used for annotation.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus annotated with summary statistics in <code>locus</code> column. If no statistics are found, the <code>locus</code> column will be empty.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_locus_statistics_boundaries(\n    self: StudyLocus,\n    summary_statistics: SummaryStatistics,\n) -&gt; StudyLocus:\n    \"\"\"Annotates study locus with summary statistics in the specified boundaries - locusStart and locusEnd.\n\n    Args:\n        summary_statistics (SummaryStatistics): Summary statistics to be used for annotation.\n\n    Returns:\n        StudyLocus: Study locus annotated with summary statistics in `locus` column. If no statistics are found, the `locus` column will be empty.\n    \"\"\"\n    # The clumps will be used several times (persisting)\n    self.df.persist()\n    # Renaming columns:\n    sumstats_renamed = summary_statistics.df.selectExpr(\n        *[f\"{col} as tag_{col}\" for col in summary_statistics.df.columns]\n    ).alias(\"sumstat\")\n\n    locus_df = (\n        sumstats_renamed\n        # Joining the two datasets together:\n        .join(\n            f.broadcast(\n                self.df.alias(\"clumped\").select(\n                    \"position\",\n                    \"chromosome\",\n                    \"studyId\",\n                    \"studyLocusId\",\n                    \"locusStart\",\n                    \"locusEnd\",\n                )\n            ),\n            on=[\n                (f.col(\"sumstat.tag_studyId\") == f.col(\"clumped.studyId\"))\n                &amp; (f.col(\"sumstat.tag_chromosome\") == f.col(\"clumped.chromosome\"))\n                &amp; (f.col(\"sumstat.tag_position\") &gt;= (f.col(\"clumped.locusStart\")))\n                &amp; (f.col(\"sumstat.tag_position\") &lt;= (f.col(\"clumped.locusEnd\")))\n            ],\n            how=\"inner\",\n        )\n        .withColumn(\n            \"locus\",\n            f.struct(\n                f.col(\"tag_variantId\").alias(\"variantId\"),\n                f.col(\"tag_beta\").alias(\"beta\"),\n                f.col(\"tag_pValueMantissa\").alias(\"pValueMantissa\"),\n                f.col(\"tag_pValueExponent\").alias(\"pValueExponent\"),\n                f.col(\"tag_standardError\").alias(\"standardError\"),\n            ),\n        )\n        .groupBy(\"studyLocusId\")\n        .agg(\n            f.collect_list(f.col(\"locus\")).alias(\"locus\"),\n        )\n    )\n\n    self.df = self.df.drop(\"locus\").join(\n        locus_df,\n        on=\"studyLocusId\",\n        how=\"left\",\n    )\n\n    return self\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.annotate_study_type","title":"<code>annotate_study_type(study_index: StudyIndex) -&gt; StudyLocus</code>","text":"<p>Gets study type from study index and adds it to study locus.</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>Study index to get study type.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with study type.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def annotate_study_type(self: StudyLocus, study_index: StudyIndex) -&gt; StudyLocus:\n    \"\"\"Gets study type from study index and adds it to study locus.\n\n    Args:\n        study_index (StudyIndex): Study index to get study type.\n\n    Returns:\n        StudyLocus: Updated study locus with study type.\n    \"\"\"\n    return StudyLocus(\n        _df=(\n            self.df.drop(\"studyType\").join(\n                study_index.study_type_lut(), on=\"studyId\", how=\"left\"\n            )\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.assign_confidence","title":"<code>assign_confidence() -&gt; StudyLocus</code>","text":"<p>Assign confidence to study locus.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus with confidence assigned.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def assign_confidence(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Assign confidence to study locus.\n\n    Returns:\n        StudyLocus: Study locus with confidence assigned.\n    \"\"\"\n    # Return self if the required columns are not in the dataframe:\n    if (\n        \"qualityControls\" not in self.df.columns\n        or \"finemappingMethod\" not in self.df.columns\n    ):\n        return self\n\n    # Assign confidence based on the presence of quality controls\n    df = self.df.withColumn(\n        \"confidence\",\n        f.when(\n            (\n                f.col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value,\n                    FinemappingMethod.SUSIE_INF.value,\n                )\n            )\n            &amp; (\n                ~f.array_contains(\n                    f.col(\"qualityControls\"),\n                    StudyLocusQualityCheck.OUT_OF_SAMPLE_LD.value,\n                )\n            ),\n            CredibleSetConfidenceClasses.FINEMAPPED_IN_SAMPLE_LD.value,\n        )\n        .when(\n            (\n                f.col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value,\n                    FinemappingMethod.SUSIE_INF.value,\n                )\n            )\n            &amp; (\n                f.array_contains(\n                    f.col(\"qualityControls\"),\n                    StudyLocusQualityCheck.OUT_OF_SAMPLE_LD.value,\n                )\n            ),\n            CredibleSetConfidenceClasses.FINEMAPPED_OUT_OF_SAMPLE_LD.value,\n        )\n        .when(\n            (f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n            &amp; (\n                ~f.array_contains(\n                    f.col(\"qualityControls\"), StudyLocusQualityCheck.TOP_HIT.value\n                )\n            ),\n            CredibleSetConfidenceClasses.PICSED_SUMMARY_STATS.value,\n        )\n        .when(\n            (f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n            &amp; (\n                f.array_contains(\n                    f.col(\"qualityControls\"), StudyLocusQualityCheck.TOP_HIT.value\n                )\n            ),\n            CredibleSetConfidenceClasses.PICSED_TOP_HIT.value,\n        )\n        .otherwise(CredibleSetConfidenceClasses.UNKNOWN.value),\n    )\n\n    return StudyLocus(\n        _df=df,\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.assign_study_locus_id","title":"<code>assign_study_locus_id(uniqueness_defining_columns: list[str]) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Hashes the provided columns to extract a consistent studyLocusId.</p> <p>Parameters:</p> Name Type Description Default <code>uniqueness_defining_columns</code> <code>list[str]</code> <p>list of columns defining uniqueness</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>column with a study locus ID</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"GCST000001\", \"1_1000_A_C\", \"SuSiE-inf\"), (\"GCST000002\", \"1_1000_A_C\", \"pics\")]).toDF(\"studyId\", \"variantId\", \"finemappingMethod\")\n&gt;&gt;&gt; df.withColumn(\"study_locus_id\", StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\", \"finemappingMethod\"])).show(truncate=False)\n+----------+----------+-----------------+--------------------------------+\n|studyId   |variantId |finemappingMethod|study_locus_id                  |\n+----------+----------+-----------------+--------------------------------+\n|GCST000001|1_1000_A_C|SuSiE-inf        |109804fe1e20c94231a31bafd71b566e|\n|GCST000002|1_1000_A_C|pics             |de310be4558e0482c9cc359c97d37773|\n+----------+----------+-----------------+--------------------------------+\n</code></pre> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@staticmethod\ndef assign_study_locus_id(uniqueness_defining_columns: list[str]) -&gt; Column:\n    \"\"\"Hashes the provided columns to extract a consistent studyLocusId.\n\n    Args:\n        uniqueness_defining_columns (list[str]): list of columns defining uniqueness\n\n    Returns:\n        Column: column with a study locus ID\n\n    Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"GCST000001\", \"1_1000_A_C\", \"SuSiE-inf\"), (\"GCST000002\", \"1_1000_A_C\", \"pics\")]).toDF(\"studyId\", \"variantId\", \"finemappingMethod\")\n        &gt;&gt;&gt; df.withColumn(\"study_locus_id\", StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\", \"finemappingMethod\"])).show(truncate=False)\n        +----------+----------+-----------------+--------------------------------+\n        |studyId   |variantId |finemappingMethod|study_locus_id                  |\n        +----------+----------+-----------------+--------------------------------+\n        |GCST000001|1_1000_A_C|SuSiE-inf        |109804fe1e20c94231a31bafd71b566e|\n        |GCST000002|1_1000_A_C|pics             |de310be4558e0482c9cc359c97d37773|\n        +----------+----------+-----------------+--------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return Dataset.generate_identifier(uniqueness_defining_columns).alias(\n        \"studyLocusId\"\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.build_feature_matrix","title":"<code>build_feature_matrix(features_list: list[str], features_input_loader: L2GFeatureInputLoader, append_null_features: bool = False) -&gt; L2GFeatureMatrix</code>","text":"<p>Returns the feature matrix for a StudyLocus.</p> <p>Parameters:</p> Name Type Description Default <code>features_list</code> <code>list[str]</code> <p>List of features to include in the feature matrix.</p> required <code>features_input_loader</code> <code>L2GFeatureInputLoader</code> <p>Feature input loader to use.</p> required <code>append_null_features</code> <code>bool</code> <p>If True, appends null features to the feature matrix. Default is False. Usefull with small datasets that may have all null features, that are anyway required by the model.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>L2GFeatureMatrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix for this study-locus.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def build_feature_matrix(\n    self: StudyLocus,\n    features_list: list[str],\n    features_input_loader: L2GFeatureInputLoader,\n    append_null_features: bool = False,\n) -&gt; L2GFeatureMatrix:\n    \"\"\"Returns the feature matrix for a StudyLocus.\n\n    Args:\n        features_list (list[str]): List of features to include in the feature matrix.\n        features_input_loader (L2GFeatureInputLoader): Feature input loader to use.\n        append_null_features (bool): If True, appends null features to the feature matrix. Default is False. Usefull with small datasets that may have all null features, that are anyway required by the model.\n\n    Returns:\n        L2GFeatureMatrix: Feature matrix for this study-locus.\n    \"\"\"\n    from gentropy.dataset.l2g_feature_matrix import L2GFeatureMatrix\n\n    if append_null_features:\n        feature_matrix = (\n            L2GFeatureMatrix.from_features_list(\n                self,\n                features_list,\n                features_input_loader,\n            )\n            .append_null_features(features_list)\n            .fill_na()\n        )\n    else:\n        feature_matrix = L2GFeatureMatrix.from_features_list(\n            self,\n            features_list,\n            features_input_loader,\n        ).fill_na()\n\n    return feature_matrix\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.calculate_credible_set_log10bf","title":"<code>calculate_credible_set_log10bf(logbfs: Column, num_variants_region: int = 500) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Calculate Bayes factor for the entire credible set. The Bayes factor is calculated as the logsumexp of the logBF values of the variants in the locus.</p> <p>Parameters:</p> Name Type Description Default <code>logbfs</code> <code>Column</code> <p>Array column with the logBF values of the variants in the locus.</p> required <code>num_variants_region</code> <code>int</code> <p>Number of variants in the region for calculation of priors. Default: 500.</p> <code>500</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>log10 Bayes factor for the entire credible set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark.createDataFrame([([1.0, 0.5, 0.25, 0.0],)]).toDF(\"logBF\").select(f.round(StudyLocus.calculate_credible_set_log10bf(f.col(\"logBF\"), 4), 7).alias(\"credibleSetlog10BF\")).show()\n+------------------+\n|credibleSetlog10BF|\n+------------------+\n|         0.2208288|\n+------------------+\n</code></pre> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@classmethod\ndef calculate_credible_set_log10bf(\n    cls: type[StudyLocus], logbfs: Column, num_variants_region: int = 500\n) -&gt; Column:\n    \"\"\"Calculate Bayes factor for the entire credible set. The Bayes factor is calculated as the logsumexp of the logBF values of the variants in the locus.\n\n    Args:\n        logbfs (Column): Array column with the logBF values of the variants in the locus.\n        num_variants_region (int): Number of variants in the region for calculation of priors. Default: 500.\n\n    Returns:\n        Column: log10 Bayes factor for the entire credible set.\n\n    Examples:\n        &gt;&gt;&gt; spark.createDataFrame([([1.0, 0.5, 0.25, 0.0],)]).toDF(\"logBF\").select(f.round(StudyLocus.calculate_credible_set_log10bf(f.col(\"logBF\"), 4), 7).alias(\"credibleSetlog10BF\")).show()\n        +------------------+\n        |credibleSetlog10BF|\n        +------------------+\n        |         0.2208288|\n        +------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    # log10=log/log(10)=log*0.43429448190325176\n    logsumexp_udf = f.udf(\n        lambda x: (\n            get_logsum(x + np.log(1 / num_variants_region)) * 0.43429448190325176\n        ),\n        FloatType(),\n    )\n    return logsumexp_udf(logbfs).cast(\"double\").alias(\"credibleSetlog10BF\")\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.clump","title":"<code>clump() -&gt; StudyLocus</code>","text":"<p>Perform LD clumping of the studyLocus.</p> <p>Evaluates whether a lead variant is linked to a tag (with lowest p-value) in the same studyLocus dataset.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>with empty credible sets for linked variants and QC flag.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def clump(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Perform LD clumping of the studyLocus.\n\n    Evaluates whether a lead variant is linked to a tag (with lowest p-value) in the same studyLocus dataset.\n\n    Returns:\n        StudyLocus: with empty credible sets for linked variants and QC flag.\n    \"\"\"\n    clumped_df = (\n        self.df.withColumn(\n            \"is_lead_linked\",\n            LDclumping._is_lead_linked(\n                self.df.studyId,\n                self.df.chromosome,\n                self.df.variantId,\n                self.df.pValueExponent,\n                self.df.pValueMantissa,\n                self.df.ldSet,\n            ),\n        )\n        .withColumn(\n            \"ldSet\",\n            f.when(f.col(\"is_lead_linked\"), f.array()).otherwise(f.col(\"ldSet\")),\n        )\n        .withColumn(\n            \"qualityControls\",\n            StudyLocus.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.col(\"is_lead_linked\"),\n                StudyLocusQualityCheck.LD_CLUMPED,\n            ),\n        )\n        .drop(\"is_lead_linked\")\n    )\n    return StudyLocus(\n        _df=clumped_df,\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.exclude_region","title":"<code>exclude_region(region: GenomicRegion, exclude_overlap: bool = False) -&gt; StudyLocus</code>","text":"<p>Exclude a region from the StudyLocus dataset.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>GenomicRegion</code> <p>genomic region object.</p> required <code>exclude_overlap</code> <code>bool</code> <p>If True, excludes StudyLocus windows with any overlap with the region.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>filtered StudyLocus object.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def exclude_region(\n    self: StudyLocus, region: GenomicRegion, exclude_overlap: bool = False\n) -&gt; StudyLocus:\n    \"\"\"Exclude a region from the StudyLocus dataset.\n\n    Args:\n        region (GenomicRegion): genomic region object.\n        exclude_overlap (bool): If True, excludes StudyLocus windows with any overlap with the region.\n\n    Returns:\n        StudyLocus: filtered StudyLocus object.\n    \"\"\"\n    if exclude_overlap:\n        filter_condition = ~(\n            (f.col(\"chromosome\") == region.chromosome)\n            &amp; (\n                (f.col(\"locusStart\") &lt;= region.end)\n                &amp; (f.col(\"locusEnd\") &gt;= region.start)\n            )\n        )\n    else:\n        filter_condition = ~(\n            (f.col(\"chromosome\") == region.chromosome)\n            &amp; (\n                (f.col(\"position\") &gt;= region.start)\n                &amp; (f.col(\"position\") &lt;= region.end)\n            )\n        )\n\n    return StudyLocus(\n        _df=self.df.filter(filter_condition),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.filter_credible_set","title":"<code>filter_credible_set(credible_interval: CredibleInterval) -&gt; StudyLocus</code>","text":"<p>Annotate and filter study-locus tag variants based on given credible interval.</p> <p>Parameters:</p> Name Type Description Default <code>credible_interval</code> <code>CredibleInterval</code> <p>Credible interval to filter for.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Filtered study-locus dataset.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def filter_credible_set(\n    self: StudyLocus,\n    credible_interval: CredibleInterval,\n) -&gt; StudyLocus:\n    \"\"\"Annotate and filter study-locus tag variants based on given credible interval.\n\n    Args:\n        credible_interval (CredibleInterval): Credible interval to filter for.\n\n    Returns:\n        StudyLocus: Filtered study-locus dataset.\n    \"\"\"\n    return StudyLocus(\n        _df=self.annotate_credible_sets().df.withColumn(\n            \"locus\",\n            f.filter(\n                f.col(\"locus\"),\n                lambda tag: (tag[credible_interval.value]),\n            ),\n        ),\n        _schema=self._schema,\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.filter_ld_set","title":"<code>filter_ld_set(ld_set: Column, r2_threshold: float) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Filter the LD set by a given R2 threshold.</p> <p>Parameters:</p> Name Type Description Default <code>ld_set</code> <code>Column</code> <p>LD set</p> required <code>r2_threshold</code> <code>float</code> <p>R2 threshold to filter the LD set on</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Filtered LD index</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@staticmethod\ndef filter_ld_set(ld_set: Column, r2_threshold: float) -&gt; Column:\n    \"\"\"Filter the LD set by a given R2 threshold.\n\n    Args:\n        ld_set (Column): LD set\n        r2_threshold (float): R2 threshold to filter the LD set on\n\n    Returns:\n        Column: Filtered LD index\n    \"\"\"\n    return f.when(\n        ld_set.isNotNull(),\n        f.filter(\n            ld_set,\n            lambda tag: tag[\"r2Overall\"] &gt;= r2_threshold,\n        ),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.find_overlaps","title":"<code>find_overlaps(restrict_right_studies: list[str] | None = None, gwas_v_qtl_overlap_only: bool = False) -&gt; StudyLocusOverlap</code>","text":"<p>Calculate overlapping study-locus.</p> <p>Find overlapping study-locus that share at least one tagging variant. By default all GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side of the overlap. The user can restrict computation of overlaps to a list of studyIds on the right-hand side or can restrict the overlaps to just GWAS vs Molecular traits.</p> <p>Parameters:</p> Name Type Description Default <code>restrict_right_studies</code> <code>list[str] | None</code> <p>List of studyIds to restrict finding overlaps to on the right-side. Default is None.</p> <code>None</code> <code>gwas_v_qtl_overlap_only</code> <code>bool</code> <p>If True, finds overlaps between all GWAS loci vs molecular-QTLs. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>StudyLocusOverlap</code> <code>StudyLocusOverlap</code> <p>Pairs of overlapping study-locus with aligned tags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def find_overlaps(\n    self: StudyLocus,\n    restrict_right_studies: list[str] | None = None,\n    gwas_v_qtl_overlap_only: bool = False,\n) -&gt; StudyLocusOverlap:\n    \"\"\"Calculate overlapping study-locus.\n\n    Find overlapping study-locus that share at least one tagging variant. By default all GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always\n    appearing on the right side of the overlap. The user can restrict computation of overlaps to a list of studyIds on the right-hand side or\n    can restrict the overlaps to just GWAS vs Molecular traits.\n\n    Args:\n        restrict_right_studies (list[str] | None): List of studyIds to restrict finding overlaps to on the right-side. Default is None.\n        gwas_v_qtl_overlap_only (bool): If True, finds overlaps between all GWAS loci vs molecular-QTLs. Default is False.\n\n    Returns:\n        StudyLocusOverlap: Pairs of overlapping study-locus with aligned tags.\n    \"\"\"\n    loci_to_overlap = (\n        self.df.filter(f.col(\"studyType\").isNotNull())\n        .withColumn(\"locus\", f.explode(\"locus\"))\n        .select(\n            \"studyLocusId\",\n            \"studyId\",\n            \"studyType\",\n            \"chromosome\",\n            \"region\",\n            f.col(\"locus.variantId\").alias(\"tagVariantId\"),\n            f.col(\"locus.logBF\").alias(\"logBF\"),\n            f.col(\"locus.posteriorProbability\").alias(\"posteriorProbability\"),\n            f.col(\"locus.pValueMantissa\").alias(\"pValueMantissa\"),\n            f.col(\"locus.pValueExponent\").alias(\"pValueExponent\"),\n            f.col(\"locus.beta\").alias(\"beta\"),\n        )\n        .persist()\n    )\n\n    # overlapping study-locus\n    peak_overlaps = self._overlapping_peaks(\n        loci_to_overlap,\n        restrict_right_studies=restrict_right_studies,\n        gwas_v_qtl_overlap_only=gwas_v_qtl_overlap_only,\n    )\n\n    # study-locus overlap by aligning overlapping variants\n    return self._align_overlapping_tags(loci_to_overlap, peak_overlaps)\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.flag_trans_qtls","title":"<code>flag_trans_qtls(study_index: StudyIndex, target_index: TargetIndex, trans_threshold: int = 5000000) -&gt; StudyLocus</code>","text":"<p>Flagging transQTL credible sets based on genomic location of the measured gene.</p> <p>Process: 0. Make sure that the <code>isTransQtl</code> column does not exist (remove if exists) 1. Enrich study-locus dataset with geneId based on study metadata. (only QTL studies are considered) 2. Enrich with transcription start site and chromosome of the studied gegne. 3. Flagging any tagging variant of QTL credible sets, if chromosome is different from the gene or distance is above the threshold. 4. Propagate flags to credible sets where any tags are considered as trans. 5. Return study locus object with annotation stored in 'isTransQtl<code>boolean column, where gwas credible sets will be</code>null`</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>study index to extract identifier of the measured gene</p> required <code>target_index</code> <code>TargetIndex</code> <p>target index bringing TSS and chromosome of the measured gene</p> required <code>trans_threshold</code> <code>int</code> <p>Distance above which the QTL is considered trans. Default: 5_000_000bp</p> <code>5000000</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>new column added indicating if the QTL credibles sets are trans.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def flag_trans_qtls(\n    self: StudyLocus,\n    study_index: StudyIndex,\n    target_index: TargetIndex,\n    trans_threshold: int = 5_000_000,\n) -&gt; StudyLocus:\n    \"\"\"Flagging transQTL credible sets based on genomic location of the measured gene.\n\n    Process:\n    0. Make sure that the `isTransQtl` column does not exist (remove if exists)\n    1. Enrich study-locus dataset with geneId based on study metadata. (only QTL studies are considered)\n    2. Enrich with transcription start site and chromosome of the studied gegne.\n    3. Flagging any tagging variant of QTL credible sets, if chromosome is different from the gene or distance is above the threshold.\n    4. Propagate flags to credible sets where any tags are considered as trans.\n    5. Return study locus object with annotation stored in 'isTransQtl` boolean column, where gwas credible sets will be `null`\n\n    Args:\n        study_index (StudyIndex): study index to extract identifier of the measured gene\n        target_index (TargetIndex): target index bringing TSS and chromosome of the measured gene\n        trans_threshold (int): Distance above which the QTL is considered trans. Default: 5_000_000bp\n\n    Returns:\n        StudyLocus: new column added indicating if the QTL credibles sets are trans.\n    \"\"\"\n    # As the `geneId` column in the study index is optional, we have to test for that:\n    if \"geneId\" not in study_index.df.columns:\n        return self\n\n    # We have to remove the column `isTransQtl` to ensure the column is not duplicated\n    # The duplication can happen when one reads the StudyLocus from parquet with\n    # predefined schema that already contains the `isTransQtl` column.\n    if \"isTransQtl\" in self.df.columns:\n        self.df = self.df.drop(\"isTransQtl\")\n\n    # Process study index:\n    processed_studies = (\n        study_index.df\n        # Dropping gwas studies. This ensures that only QTLs will have \"isTrans\" annotation:\n        .filter(f.col(\"studyType\") != \"gwas\").select(\n            \"studyId\", \"geneId\", \"projectId\"\n        )\n    )\n\n    # Process study locus:\n    processed_credible_set = (\n        self.df\n        # Exploding locus to test all tag variants:\n        .withColumn(\"locus\", f.explode(\"locus\")).select(\n            \"studyLocusId\",\n            \"studyId\",\n            f.split(\"locus.variantId\", \"_\")[0].alias(\"chromosome\"),\n            f.split(\"locus.variantId\", \"_\")[1].cast(LongType()).alias(\"position\"),\n        )\n    )\n\n    # Process target index:\n    processed_targets = target_index.df.select(\n        f.col(\"id\").alias(\"geneId\"),\n        f.col(\"tss\"),\n        f.col(\"genomicLocation.chromosome\").alias(\"geneChromosome\"),\n    )\n\n    # Pool datasets:\n    joined_data = (\n        processed_credible_set\n        # Join processed studies:\n        .join(processed_studies, on=\"studyId\", how=\"inner\")\n        # Join processed targets:\n        .join(processed_targets, on=\"geneId\", how=\"left\")\n        # Assign True/False for QTL studies:\n        .withColumn(\n            \"isTagTrans\",\n            # The QTL signal is considered trans if the locus is on a different chromosome than the measured gene.\n            # OR the distance from the gene's transcription start site is &gt; threshold.\n            f.when(\n                (f.col(\"chromosome\") != f.col(\"geneChromosome\"))\n                | (f.abs(f.col(\"tss\") - f.col(\"position\")) &gt; trans_threshold),\n                f.lit(True),\n            ).otherwise(f.lit(False)),\n        )\n        .groupby(\"studyLocusId\")\n        .agg(\n            # If all tagging variants of the locus is in trans position, the QTL is considered trans:\n            f.when(\n                f.array_contains(f.collect_set(\"isTagTrans\"), f.lit(False)), False\n            )\n            .otherwise(f.lit(True))\n            .alias(\"isTransQtl\")\n        )\n    )\n    # Adding new column, where the value is null for gwas loci:\n    return StudyLocus(self.df.join(joined_data, on=\"studyLocusId\", how=\"left\"))\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.get_QC_column_name","title":"<code>get_QC_column_name() -&gt; str</code>  <code>classmethod</code>","text":"<p>Quality control column.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Name of the quality control column.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@classmethod\ndef get_QC_column_name(cls: type[StudyLocus]) -&gt; str:\n    \"\"\"Quality control column.\n\n    Returns:\n        str: Name of the quality control column.\n    \"\"\"\n    return \"qualityControls\"\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.get_QC_mappings","title":"<code>get_QC_mappings() -&gt; dict[str, str]</code>  <code>classmethod</code>","text":"<p>Quality control flag to QC column category mappings.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: Mapping between flag name and QC column category value.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@classmethod\ndef get_QC_mappings(cls: type[StudyLocus]) -&gt; dict[str, str]:\n    \"\"\"Quality control flag to QC column category mappings.\n\n    Returns:\n        dict[str, str]: Mapping between flag name and QC column category value.\n    \"\"\"\n    return {member.name: member.value for member in StudyLocusQualityCheck}\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the StudyLocus dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>schema for the StudyLocus dataset.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[StudyLocus]) -&gt; StructType:\n    \"\"\"Provides the schema for the StudyLocus dataset.\n\n    Returns:\n        StructType: schema for the StudyLocus dataset.\n    \"\"\"\n    return parse_spark_schema(\"study_locus.json\")\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.neglog_pvalue","title":"<code>neglog_pvalue() -&gt; Column</code>","text":"<p>Returns the negative log p-value.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Negative log p-value</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def neglog_pvalue(self: StudyLocus) -&gt; Column:\n    \"\"\"Returns the negative log p-value.\n\n    Returns:\n        Column: Negative log p-value\n    \"\"\"\n    return neglogpval_from_pvalue(\n        self.df.pValueMantissa,\n        self.df.pValueExponent,\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.qc_MHC_region","title":"<code>qc_MHC_region() -&gt; StudyLocus</code>","text":"<p>Adds qualityControl flag when lead overlaps with MHC region.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>including qualityControl flag if in MHC region.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def qc_MHC_region(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Adds qualityControl flag when lead overlaps with MHC region.\n\n    Returns:\n        StudyLocus: including qualityControl flag if in MHC region.\n    \"\"\"\n    region = GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC)\n    self.df = self.df.withColumn(\n        \"qualityControls\",\n        self.update_quality_flag(\n            f.col(\"qualityControls\"),\n            (\n                (f.col(\"chromosome\") == region.chromosome)\n                &amp; (\n                    (f.col(\"position\") &lt;= region.end)\n                    &amp; (f.col(\"position\") &gt;= region.start)\n                )\n            ),\n            StudyLocusQualityCheck.IN_MHC,\n        ),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.qc_abnormal_pips","title":"<code>qc_abnormal_pips(sum_pips_lower_threshold: float = 0.99, sum_pips_upper_threshold: float = 1.0001) -&gt; StudyLocus</code>","text":"<p>Filter study-locus by sum of posterior inclusion probabilities to ensure that the sum of PIPs is within a given range.</p> <p>Parameters:</p> Name Type Description Default <code>sum_pips_lower_threshold</code> <code>float</code> <p>Lower threshold for the sum of PIPs.</p> <code>0.99</code> <code>sum_pips_upper_threshold</code> <code>float</code> <p>Upper threshold for the sum of PIPs.</p> <code>1.0001</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Filtered study-locus dataset.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def qc_abnormal_pips(\n    self: StudyLocus,\n    sum_pips_lower_threshold: float = 0.99,\n    # Set slightly above 1 to account for floating point errors\n    sum_pips_upper_threshold: float = 1.0001,\n) -&gt; StudyLocus:\n    \"\"\"Filter study-locus by sum of posterior inclusion probabilities to ensure that the sum of PIPs is within a given range.\n\n    Args:\n        sum_pips_lower_threshold (float): Lower threshold for the sum of PIPs.\n        sum_pips_upper_threshold (float): Upper threshold for the sum of PIPs.\n\n    Returns:\n        StudyLocus: Filtered study-locus dataset.\n    \"\"\"\n    # QC column might not be present so we have to be ready to handle it:\n    qc_select_expression = (\n        f.col(\"qualityControls\")\n        if \"qualityControls\" in self.df.columns\n        else f.lit(None).cast(ArrayType(StringType()))\n    )\n\n    flag = self.df.withColumn(\n        \"sumPosteriorProbability\",\n        f.aggregate(\n            f.col(\"locus\"),\n            f.lit(0.0),\n            lambda acc, x: acc + x[\"posteriorProbability\"],\n        ),\n    ).withColumn(\n        \"pipOutOfRange\",\n        f.when(\n            (f.col(\"sumPosteriorProbability\") &lt; sum_pips_lower_threshold)\n            | (f.col(\"sumPosteriorProbability\") &gt; sum_pips_upper_threshold),\n            True,\n        ).otherwise(False),\n    )\n\n    return StudyLocus(\n        _df=(\n            flag\n            # Flagging loci with failed studies:\n            .withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    qc_select_expression,\n                    f.col(\"pipOutOfRange\"),\n                    StudyLocusQualityCheck.ABNORMAL_PIPS,\n                ),\n            ).drop(\"sumPosteriorProbability\", \"pipOutOfRange\")\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.qc_explained_by_SuSiE","title":"<code>qc_explained_by_SuSiE() -&gt; StudyLocus</code>","text":"<p>Flag associations that are explained by SuSiE associations.</p> <p>Credible sets overlapping in the same region as a SuSiE credible set are flagged as explained by SuSiE.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with SuSiE explained flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def qc_explained_by_SuSiE(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Flag associations that are explained by SuSiE associations.\n\n    Credible sets overlapping in the same region as a SuSiE credible set are flagged as explained by SuSiE.\n\n    Returns:\n        StudyLocus: Updated study locus with SuSiE explained flags.\n    \"\"\"\n    # unique study-regions covered by SuSie credible sets\n    susie_study_regions = (\n        self.filter(\n            f.col(\"finemappingMethod\").isin(\n                FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n            )\n        )\n        .df.select(\n            \"studyId\",\n            \"chromosome\",\n            \"locusStart\",\n            \"locusEnd\",\n            f.lit(True).alias(\"inSuSiE\"),\n        )\n        .distinct()\n    )\n\n    # non SuSiE credible sets (studyLocusId) overlapping in any variant with SuSiE locus\n    redundant_study_locus = (\n        self.filter(\n            ~f.col(\"finemappingMethod\").isin(\n                FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n            )\n        )\n        .df.withColumn(\"l\", f.explode(\"locus\"))\n        .select(\n            \"studyLocusId\",\n            \"studyId\",\n            \"chromosome\",\n            f.split(f.col(\"l.variantId\"), \"_\")[1].alias(\"tag_position\"),\n        )\n        .alias(\"study_locus\")\n        .join(\n            susie_study_regions.alias(\"regions\"),\n            how=\"inner\",\n            on=[\n                (f.col(\"study_locus.chromosome\") == f.col(\"regions.chromosome\"))\n                &amp; (f.col(\"study_locus.studyId\") == f.col(\"regions.studyId\"))\n                &amp; (f.col(\"study_locus.tag_position\") &gt;= f.col(\"regions.locusStart\"))\n                &amp; (f.col(\"study_locus.tag_position\") &lt;= f.col(\"regions.locusEnd\"))\n            ],\n        )\n        .select(\"studyLocusId\", \"inSuSiE\")\n        .distinct()\n    )\n\n    return StudyLocus(\n        _df=(\n            self.df.join(redundant_study_locus, on=\"studyLocusId\", how=\"left\")\n            .withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    # credible set in SuSiE overlapping region\n                    f.col(\"inSuSiE\")\n                    # credible set not based on SuSiE\n                    &amp; (\n                        ~f.col(\"finemappingMethod\").isin(\n                            FinemappingMethod.SUSIE.value,\n                            FinemappingMethod.SUSIE_INF.value,\n                        )\n                    ),\n                    StudyLocusQualityCheck.EXPLAINED_BY_SUSIE,\n                ),\n            )\n            .drop(\"inSuSiE\")\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.qc_redundant_top_hits_from_PICS","title":"<code>qc_redundant_top_hits_from_PICS() -&gt; StudyLocus</code>","text":"<p>Flag associations from top hits when the study contains other PICS associations from summary statistics.</p> <p>This flag can be useful to identify top hits that should be explained by other associations in the study derived from the summary statistics.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with redundant top hits flagged.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def qc_redundant_top_hits_from_PICS(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Flag associations from top hits when the study contains other PICS associations from summary statistics.\n\n    This flag can be useful to identify top hits that should be explained by other associations in the study derived from the summary statistics.\n\n    Returns:\n        StudyLocus: Updated study locus with redundant top hits flagged.\n    \"\"\"\n    studies_with_pics_sumstats = (\n        self.df.filter(f.col(\"finemappingMethod\") == FinemappingMethod.PICS.value)\n        # Returns True if the study contains any PICS associations from summary statistics\n        .withColumn(\n            \"hasPicsSumstats\",\n            ~f.array_contains(\n                \"qualityControls\", StudyLocusQualityCheck.TOP_HIT.value\n            ),\n        )\n        .groupBy(\"studyId\")\n        .agg(f.max(f.col(\"hasPicsSumstats\")).alias(\"studiesWithPicsSumstats\"))\n    )\n\n    return StudyLocus(\n        _df=self.df.join(studies_with_pics_sumstats, on=\"studyId\", how=\"left\")\n        .withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.array_contains(\n                    \"qualityControls\", StudyLocusQualityCheck.TOP_HIT.value\n                )\n                &amp; f.col(\"studiesWithPicsSumstats\"),\n                StudyLocusQualityCheck.REDUNDANT_PICS_TOP_HIT,\n            ),\n        )\n        .drop(\"studiesWithPicsSumstats\"),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.unique_variants_in_locus","title":"<code>unique_variants_in_locus() -&gt; DataFrame</code>","text":"<p>All unique variants collected in a <code>StudyLocus</code> dataframe.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing <code>variantId</code> and <code>chromosome</code> columns.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def unique_variants_in_locus(self: StudyLocus) -&gt; DataFrame:\n    \"\"\"All unique variants collected in a `StudyLocus` dataframe.\n\n    Returns:\n        DataFrame: A dataframe containing `variantId` and `chromosome` columns.\n    \"\"\"\n    return (\n        self.df.withColumn(\n            \"variantId\",\n            # Joint array of variants in that studylocus. Locus can be null\n            f.explode(\n                f.array_union(\n                    f.array(f.col(\"variantId\")),\n                    f.coalesce(f.col(\"locus.variantId\"), f.array()),\n                )\n            ),\n        )\n        .select(\n            \"variantId\", f.split(f.col(\"variantId\"), \"_\")[0].alias(\"chromosome\")\n        )\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_chromosome_label","title":"<code>validate_chromosome_label() -&gt; StudyLocus</code>","text":"<p>Flagging study loci, where chromosome is coded not as 1:22, X, Y, Xy and MT.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with quality control flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_chromosome_label(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Flagging study loci, where chromosome is coded not as 1:22, X, Y, Xy and MT.\n\n    Returns:\n        StudyLocus: Updated study locus with quality control flags.\n    \"\"\"\n    # QC column might not be present in the variant index schema, so we have to be ready to handle it:\n    qc_select_expression = (\n        f.col(\"qualityControls\")\n        if \"qualityControls\" in self.df.columns\n        else f.lit(None).cast(ArrayType(StringType()))\n    )\n    valid_chromosomes = [str(i) for i in range(1, 23)] + [\"X\", \"Y\", \"XY\", \"MT\"]\n\n    return StudyLocus(\n        _df=(\n            self.df.withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    qc_select_expression,\n                    ~f.col(\"chromosome\").isin(valid_chromosomes),\n                    StudyLocusQualityCheck.INVALID_CHROMOSOME,\n                ),\n            )\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_lead_pvalue","title":"<code>validate_lead_pvalue(pvalue_cutoff: float) -&gt; StudyLocus</code>","text":"<p>Flag associations below significant threshold.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue_cutoff</code> <code>float</code> <p>association p-value cut-off</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with quality control flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_lead_pvalue(self: StudyLocus, pvalue_cutoff: float) -&gt; StudyLocus:\n    \"\"\"Flag associations below significant threshold.\n\n    Args:\n        pvalue_cutoff (float): association p-value cut-off\n\n    Returns:\n        StudyLocus: Updated study locus with quality control flags.\n    \"\"\"\n    df = self.df\n    qc_colname = StudyLocus.get_QC_column_name()\n    if qc_colname not in self.df.columns:\n        df = self.df.withColumn(\n            qc_colname,\n            create_empty_column_if_not_exists(\n                qc_colname,\n                get_struct_field_schema(StudyLocus.get_schema(), qc_colname),\n            ),\n        )\n    return StudyLocus(\n        _df=(\n            df.withColumn(\n                qc_colname,\n                # Because this QC might already run on the dataset, the unique set of flags is generated:\n                f.array_distinct(\n                    self._qc_subsignificant_associations(\n                        f.col(\"qualityControls\"),\n                        f.col(\"pValueMantissa\"),\n                        f.col(\"pValueExponent\"),\n                        pvalue_cutoff,\n                    )\n                ),\n            )\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_study","title":"<code>validate_study(study_index: StudyIndex) -&gt; StudyLocus</code>","text":"<p>Flagging study loci if the corresponding study has issues.</p> <p>There are two different potential flags: - flagged study: flagging locus if the study has quality control flags. - study with summary statistics for top hit: flagging locus if the study has available summary statistics. - missing study: flagging locus if the study was not found in the reference study index.</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>Study index to resolve study types.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with quality control flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_study(self: StudyLocus, study_index: StudyIndex) -&gt; StudyLocus:\n    \"\"\"Flagging study loci if the corresponding study has issues.\n\n    There are two different potential flags:\n    - flagged study: flagging locus if the study has quality control flags.\n    - study with summary statistics for top hit: flagging locus if the study has available summary statistics.\n    - missing study: flagging locus if the study was not found in the reference study index.\n\n    Args:\n        study_index (StudyIndex): Study index to resolve study types.\n\n    Returns:\n        StudyLocus: Updated study locus with quality control flags.\n    \"\"\"\n    # Quality controls is not a mandatory field in the study index schema, so we have to be ready to handle it:\n    qc_select_expression = (\n        f.col(\"qualityControls\")\n        if \"qualityControls\" in study_index.df.columns\n        else f.lit(None).cast(StringType())\n    )\n\n    # The study Id of the study index needs to be kept, because we would not know which study was in the index after the left join:\n    study_flags = study_index.df.select(\n        f.col(\"studyId\").alias(\"study_studyId\"),\n        qc_select_expression.alias(\"study_qualityControls\"),\n    )\n\n    return StudyLocus(\n        _df=(\n            self.df.join(\n                study_flags, f.col(\"studyId\") == f.col(\"study_studyId\"), \"left\"\n            )\n            # Flagging loci with flagged studies - without propagating the actual flags:\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.size(f.col(\"study_qualityControls\")) &gt; 0,\n                    StudyLocusQualityCheck.FLAGGED_STUDY,\n                ),\n            )\n            # Flagging top-hits, where the study has available summary statistics:\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    # Condition is true, if the study has summary statistics available and the locus is a top hit:\n                    f.array_contains(\n                        f.col(\"qualityControls\"),\n                        StudyLocusQualityCheck.TOP_HIT.value,\n                    )\n                    &amp; ~f.array_contains(\n                        f.col(\"study_qualityControls\"),\n                        StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value,\n                    ),\n                    StudyLocusQualityCheck.TOP_HIT_AND_SUMMARY_STATS,\n                ),\n            )\n            # Flagging loci where no studies were found:\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.col(\"study_studyId\").isNull(),\n                    StudyLocusQualityCheck.MISSING_STUDY,\n                ),\n            )\n            .drop(\"study_studyId\", \"study_qualityControls\")\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_unique_study_locus_id","title":"<code>validate_unique_study_locus_id() -&gt; StudyLocus</code>","text":"<p>Validating the uniqueness of study-locus identifiers and flagging duplicated studyloci.</p> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>with flagged duplicated studies.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_unique_study_locus_id(self: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Validating the uniqueness of study-locus identifiers and flagging duplicated studyloci.\n\n    Returns:\n        StudyLocus: with flagged duplicated studies.\n    \"\"\"\n    return StudyLocus(\n        _df=self.df.withColumn(\n            \"qualityControls\",\n            self.update_quality_flag(\n                f.col(\"qualityControls\"),\n                self.flag_duplicates(f.col(\"studyLocusId\")),\n                StudyLocusQualityCheck.DUPLICATED_STUDYLOCUS_ID,\n            ),\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.validate_variant_identifiers","title":"<code>validate_variant_identifiers(variant_index: VariantIndex) -&gt; StudyLocus</code>","text":"<p>Flagging study loci, where tagging variant identifiers are not found in variant index.</p> <p>Parameters:</p> Name Type Description Default <code>variant_index</code> <code>VariantIndex</code> <p>Variant index to resolve variant identifiers.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Updated study locus with quality control flags.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def validate_variant_identifiers(\n    self: StudyLocus, variant_index: VariantIndex\n) -&gt; StudyLocus:\n    \"\"\"Flagging study loci, where tagging variant identifiers are not found in variant index.\n\n    Args:\n        variant_index (VariantIndex): Variant index to resolve variant identifiers.\n\n    Returns:\n        StudyLocus: Updated study locus with quality control flags.\n    \"\"\"\n    # QC column might not be present in the variant index schema, so we have to be ready to handle it:\n    qc_select_expression = (\n        f.col(\"qualityControls\")\n        if \"qualityControls\" in self.df.columns\n        else f.lit(None).cast(ArrayType(StringType()))\n    )\n\n    # Find out which study loci have variants not in the variant index:\n    flag = (\n        self.df\n        # Exploding locus:\n        .select(\"studyLocusId\", f.explode(\"locus\").alias(\"locus\"))\n        .select(\"studyLocusId\", \"locus.variantId\")\n        # Join with variant index variants:\n        .join(\n            variant_index.df.select(\n                \"variantId\", f.lit(True).alias(\"inVariantIndex\")\n            ),\n            on=\"variantId\",\n            how=\"left\",\n        )\n        # Flagging variants not in the variant index:\n        .withColumn(\"inVariantIndex\", f.col(\"inVariantIndex\").isNotNull())\n        # Flagging study loci with ANY variants not in the variant index:\n        .groupBy(\"studyLocusId\")\n        .agg(f.collect_set(\"inVariantIndex\").alias(\"inVariantIndex\"))\n        .select(\n            \"studyLocusId\",\n            f.array_contains(\"inVariantIndex\", False).alias(\"toFlag\"),\n        )\n    )\n\n    return StudyLocus(\n        _df=(\n            self.df.join(flag, on=\"studyLocusId\", how=\"left\")\n            .withColumn(\n                \"qualityControls\",\n                self.update_quality_flag(\n                    qc_select_expression,\n                    f.col(\"toFlag\"),\n                    StudyLocusQualityCheck.INVALID_VARIANT_IDENTIFIER,\n                ),\n            )\n            .drop(\"toFlag\")\n        ),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocus.window_based_clumping","title":"<code>window_based_clumping(window_size: int = WindowBasedClumpingStepConfig().distance) -&gt; StudyLocus</code>","text":"<p>Clump study locus by window size.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Window size for clumping.</p> <code>distance</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Clumped study locus, where clumped associations are flagged.</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>def window_based_clumping(\n    self: StudyLocus,\n    window_size: int = WindowBasedClumpingStepConfig().distance,\n) -&gt; StudyLocus:\n    \"\"\"Clump study locus by window size.\n\n    Args:\n        window_size (int): Window size for clumping.\n\n    Returns:\n        StudyLocus: Clumped study locus, where clumped associations are flagged.\n    \"\"\"\n    from gentropy.method.window_based_clumping import WindowBasedClumping\n\n    return WindowBasedClumping.clump(self, window_size)\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.FinemappingMethod","title":"<code>gentropy.dataset.study_locus.FinemappingMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Finemapping method enum.</p> <p>Attributes:</p> Name Type Description <code>PICS</code> <code>str</code> <p>PICS</p> <code>SUSIE</code> <code>str</code> <p>SuSiE method</p> <code>SUSIE_INF</code> <code>str</code> <p>SuSiE-inf method implemented in <code>gentropy</code></p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>class FinemappingMethod(Enum):\n    \"\"\"Finemapping method enum.\n\n    Attributes:\n        PICS (str): PICS\n        SUSIE (str): SuSiE method\n        SUSIE_INF (str): SuSiE-inf method implemented in `gentropy`\n    \"\"\"\n\n    PICS = \"PICS\"\n    SUSIE = \"SuSie\"\n    SUSIE_INF = \"SuSiE-inf\"\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.StudyLocusQualityCheck","title":"<code>gentropy.dataset.study_locus.StudyLocusQualityCheck</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Study-Locus quality control options listing concerns on the quality of the association.</p> <p>Attributes:</p> Name Type Description <code>SUBSIGNIFICANT_FLAG</code> <code>str</code> <p>p-value below significance threshold</p> <code>NO_GENOMIC_LOCATION_FLAG</code> <code>str</code> <p>Incomplete genomic mapping</p> <code>COMPOSITE_FLAG</code> <code>str</code> <p>Composite association due to variant x variant interactions</p> <code>INCONSISTENCY_FLAG</code> <code>str</code> <p>Inconsistencies in the reported variants</p> <code>NON_MAPPED_VARIANT_FLAG</code> <code>str</code> <p>Variant not mapped to GnomAd</p> <code>PALINDROMIC_ALLELE_FLAG</code> <code>str</code> <p>Alleles are palindromic - cannot harmonize</p> <code>AMBIGUOUS_STUDY</code> <code>str</code> <p>Association with ambiguous study</p> <code>UNRESOLVED_LD</code> <code>str</code> <p>Variant not found in LD reference</p> <code>LD_CLUMPED</code> <code>str</code> <p>Explained by a more significant variant in high LD</p> <code>WINDOW_CLUMPED</code> <code>str</code> <p>Explained by a more significant variant in the same window</p> <code>NO_POPULATION</code> <code>str</code> <p>Study does not have population annotation to resolve LD</p> <code>FLAGGED_STUDY</code> <code>str</code> <p>Study has quality control flag(s)</p> <code>MISSING_STUDY</code> <code>str</code> <p>Flagging study loci if the study is not found in the study index as a reference</p> <code>DUPLICATED_STUDYLOCUS_ID</code> <code>str</code> <p>Study-locus identifier is not unique</p> <code>INVALID_VARIANT_IDENTIFIER</code> <code>str</code> <p>Flagging study loci where identifier of any tagging variant was not found in the variant index</p> <code>TOP_HIT</code> <code>str</code> <p>Study locus from curated top hit</p> <code>IN_MHC</code> <code>str</code> <p>Flagging study loci in the MHC region</p> <code>REDUNDANT_PICS_TOP_HIT</code> <code>str</code> <p>Flagging study loci in studies with PICS results from summary statistics</p> <code>EXPLAINED_BY_SUSIE</code> <code>str</code> <p>Study locus in region explained by a SuSiE credible set</p> <code>ABNORMAL_PIPS</code> <code>str</code> <p>Flagging study loci with a sum of PIPs that are not in [0.99,1]</p> <code>OUT_OF_SAMPLE_LD</code> <code>str</code> <p>Study locus finemapped without in-sample LD reference</p> <code>INVALID_CHROMOSOME</code> <code>str</code> <p>Chromosome not in 1:22, X, Y, XY or MT</p> <code>TOP_HIT_AND_SUMMARY_STATS</code> <code>str</code> <p>Curated top hit is flagged because summary statistics are available for study</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>class StudyLocusQualityCheck(Enum):\n    \"\"\"Study-Locus quality control options listing concerns on the quality of the association.\n\n    Attributes:\n        SUBSIGNIFICANT_FLAG (str): p-value below significance threshold\n        NO_GENOMIC_LOCATION_FLAG (str): Incomplete genomic mapping\n        COMPOSITE_FLAG (str): Composite association due to variant x variant interactions\n        INCONSISTENCY_FLAG (str): Inconsistencies in the reported variants\n        NON_MAPPED_VARIANT_FLAG (str): Variant not mapped to GnomAd\n        PALINDROMIC_ALLELE_FLAG (str): Alleles are palindromic - cannot harmonize\n        AMBIGUOUS_STUDY (str): Association with ambiguous study\n        UNRESOLVED_LD (str): Variant not found in LD reference\n        LD_CLUMPED (str): Explained by a more significant variant in high LD\n        WINDOW_CLUMPED (str): Explained by a more significant variant in the same window\n        NO_POPULATION (str): Study does not have population annotation to resolve LD\n        FLAGGED_STUDY (str): Study has quality control flag(s)\n        MISSING_STUDY (str): Flagging study loci if the study is not found in the study index as a reference\n        DUPLICATED_STUDYLOCUS_ID (str): Study-locus identifier is not unique\n        INVALID_VARIANT_IDENTIFIER (str): Flagging study loci where identifier of any tagging variant was not found in the variant index\n        TOP_HIT (str): Study locus from curated top hit\n        IN_MHC (str): Flagging study loci in the MHC region\n        REDUNDANT_PICS_TOP_HIT (str): Flagging study loci in studies with PICS results from summary statistics\n        EXPLAINED_BY_SUSIE (str): Study locus in region explained by a SuSiE credible set\n        ABNORMAL_PIPS (str): Flagging study loci with a sum of PIPs that are not in [0.99,1]\n        OUT_OF_SAMPLE_LD (str): Study locus finemapped without in-sample LD reference\n        INVALID_CHROMOSOME (str): Chromosome not in 1:22, X, Y, XY or MT\n        TOP_HIT_AND_SUMMARY_STATS (str): Curated top hit is flagged because summary statistics are available for study\n    \"\"\"\n\n    SUBSIGNIFICANT_FLAG = \"Subsignificant p-value\"\n    NO_GENOMIC_LOCATION_FLAG = \"Incomplete genomic mapping\"\n    COMPOSITE_FLAG = \"Composite association\"\n    INCONSISTENCY_FLAG = \"Variant inconsistency\"\n    NON_MAPPED_VARIANT_FLAG = \"No mapping in GnomAd\"\n    PALINDROMIC_ALLELE_FLAG = \"Palindrome alleles - cannot harmonize\"\n    AMBIGUOUS_STUDY = \"Association with ambiguous study\"\n    UNRESOLVED_LD = \"Variant not found in LD reference\"\n    LD_CLUMPED = \"Explained by a more significant variant in high LD\"\n    WINDOW_CLUMPED = \"Explained by a more significant variant in the same window\"\n    NO_POPULATION = \"Study does not have population annotation to resolve LD\"\n    FLAGGED_STUDY = \"Study has quality control flag(s)\"\n    MISSING_STUDY = \"Study not found in the study index\"\n    DUPLICATED_STUDYLOCUS_ID = \"Non-unique study locus identifier\"\n    INVALID_VARIANT_IDENTIFIER = (\n        \"Some variant identifiers of this locus were not found in variant index\"\n    )\n    IN_MHC = \"MHC region\"\n    REDUNDANT_PICS_TOP_HIT = (\n        \"PICS results from summary statistics available for this same study\"\n    )\n    TOP_HIT = \"Study locus from curated top hit\"\n    EXPLAINED_BY_SUSIE = \"Study locus in region explained by a SuSiE credible set\"\n    OUT_OF_SAMPLE_LD = \"Study locus finemapped without in-sample LD reference\"\n    ABNORMAL_PIPS = (\n        \"Study locus with a sum of PIPs that not in the expected range [0.95,1]\"\n    )\n    INVALID_CHROMOSOME = \"Chromosome not in 1:22, X, Y, XY or MT\"\n    TOP_HIT_AND_SUMMARY_STATS = (\n        \"Curated top hit is flagged because summary statistics are available for study\"\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus/#gentropy.dataset.study_locus.CredibleInterval","title":"<code>gentropy.dataset.study_locus.CredibleInterval</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Credible interval enum.</p> <p>Interval within which an unobserved parameter value falls with a particular probability.</p> <p>Attributes:</p> Name Type Description <code>IS95</code> <code>str</code> <p>95% credible interval</p> <code>IS99</code> <code>str</code> <p>99% credible interval</p> Source code in <code>src/gentropy/dataset/study_locus.py</code> <pre><code>class CredibleInterval(Enum):\n    \"\"\"Credible interval enum.\n\n    Interval within which an unobserved parameter value falls with a particular probability.\n\n    Attributes:\n        IS95 (str): 95% credible interval\n        IS99 (str): 99% credible interval\n    \"\"\"\n\n    IS95 = \"is95CredibleSet\"\n    IS99 = \"is99CredibleSet\"\n</code></pre>"},{"location":"python_api/datasets/study_locus/#schema","title":"Schema","text":"<pre><code>root\n |-- studyLocusId: string (nullable = false)\n |-- studyType: string (nullable = true)\n |-- variantId: string (nullable = false)\n |-- chromosome: string (nullable = true)\n |-- position: integer (nullable = true)\n |-- region: string (nullable = true)\n |-- studyId: string (nullable = false)\n |-- beta: double (nullable = true)\n |-- zScore: double (nullable = true)\n |-- pValueMantissa: float (nullable = true)\n |-- pValueExponent: integer (nullable = true)\n |-- effectAlleleFrequencyFromSource: float (nullable = true)\n |-- standardError: double (nullable = true)\n |-- subStudyDescription: string (nullable = true)\n |-- qualityControls: array (nullable = true)\n |    |-- element: string (containsNull = false)\n |-- finemappingMethod: string (nullable = true)\n |-- credibleSetIndex: integer (nullable = true)\n |-- credibleSetlog10BF: double (nullable = true)\n |-- purityMeanR2: double (nullable = true)\n |-- purityMinR2: double (nullable = true)\n |-- locusStart: integer (nullable = true)\n |-- locusEnd: integer (nullable = true)\n |-- sampleSize: integer (nullable = true)\n |-- ldSet: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- tagVariantId: string (nullable = true)\n |    |    |-- r2Overall: double (nullable = true)\n |-- locus: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- is95CredibleSet: boolean (nullable = true)\n |    |    |-- is99CredibleSet: boolean (nullable = true)\n |    |    |-- logBF: double (nullable = true)\n |    |    |-- posteriorProbability: double (nullable = true)\n |    |    |-- variantId: string (nullable = true)\n |    |    |-- pValueMantissa: float (nullable = true)\n |    |    |-- pValueExponent: integer (nullable = true)\n |    |    |-- beta: double (nullable = true)\n |    |    |-- standardError: double (nullable = true)\n |    |    |-- r2Overall: double (nullable = true)\n |-- confidence: string (nullable = true)\n |-- isTransQtl: boolean (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/","title":"Study Locus Overlap","text":""},{"location":"python_api/datasets/study_locus_overlap/#gentropy.dataset.study_locus_overlap.StudyLocusOverlap","title":"<code>gentropy.dataset.study_locus_overlap.StudyLocusOverlap</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Study-Locus overlap.</p> <p>This dataset captures pairs of overlapping <code>StudyLocus</code>: that is associations whose credible sets share at least one tagging variant.</p> <p>Note</p> <p>This is a helpful dataset for other downstream analyses, such as colocalisation. This dataset will contain the overlapping signals between studyLocus associations once they have been clumped and fine-mapped.</p> Source code in <code>src/gentropy/dataset/study_locus_overlap.py</code> <pre><code>@dataclass\nclass StudyLocusOverlap(Dataset):\n    \"\"\"Study-Locus overlap.\n\n    This dataset captures pairs of overlapping `StudyLocus`: that is associations whose credible sets share at least one tagging variant.\n\n    !!! note\n\n        This is a helpful dataset for other downstream analyses, such as colocalisation. This dataset will contain the overlapping signals between studyLocus associations once they have been clumped and fine-mapped.\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[StudyLocusOverlap]) -&gt; StructType:\n        \"\"\"Provides the schema for the StudyLocusOverlap dataset.\n\n        Returns:\n            StructType: Schema for the StudyLocusOverlap dataset\n        \"\"\"\n        return parse_spark_schema(\"study_locus_overlap.json\")\n\n    @classmethod\n    def from_associations(\n        cls: type[StudyLocusOverlap], study_locus: StudyLocus\n    ) -&gt; StudyLocusOverlap:\n        \"\"\"Find the overlapping signals in a particular set of associations (StudyLocus dataset).\n\n        Args:\n            study_locus (StudyLocus): Study-locus associations to find the overlapping signals\n\n        Returns:\n            StudyLocusOverlap: Study-locus overlap dataset\n        \"\"\"\n        return study_locus.find_overlaps()\n\n\n    def calculate_beta_ratio(self: StudyLocusOverlap) -&gt; DataFrame:\n        \"\"\"Calculate the beta ratio for the overlapping signals.\n\n        Returns:\n            DataFrame: A dataframe containing left and right loci IDs, chromosome\n            and the average sign of the beta ratio\n        \"\"\"\n        return (\n            # Unpack statistics column:\n            self.df.select(\"*\", \"statistics.*\")\n            .drop(\"statistics\")\n            # Drop any rows where the beta is null or zero\n            .filter(\n                f.col(\"left_beta\").isNotNull() &amp;\n                f.col(\"right_beta\").isNotNull() &amp;\n                (f.col(\"left_beta\") != 0) &amp;\n                (f.col(\"right_beta\") != 0)\n            )\n            # Calculate the beta ratio and get the sign, then calculate the average sign across all variants in the locus\n            .withColumn(\n                \"betaRatioSign\",\n                f.signum(f.col(\"left_beta\") / f.col(\"right_beta\"))\n            )\n            # Aggregate beta signs:\n            .groupBy(\"leftStudyLocusId\",\"rightStudyLocusId\",\"chromosome\")\n            .agg(\n                f.avg(\"betaRatioSign\").alias(\"betaRatioSignAverage\")\n            )\n        )\n\n    def _convert_to_square_matrix(self: StudyLocusOverlap) -&gt; StudyLocusOverlap:\n        \"\"\"Convert the dataset to a square matrix.\n\n        Returns:\n            StudyLocusOverlap: Square matrix of the dataset\n        \"\"\"\n        return StudyLocusOverlap(\n            _df=self.df.unionByName(\n                self.df.selectExpr(\n                    \"leftStudyLocusId as rightStudyLocusId\",\n                    \"rightStudyLocusId as leftStudyLocusId\",\n                    \"rightStudyType\",\n                    \"tagVariantId\",\n                    \"chromosome\",\n                    \"statistics\",\n                )\n            ).distinct(),\n            _schema=self.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/#gentropy.dataset.study_locus_overlap.StudyLocusOverlap.calculate_beta_ratio","title":"<code>calculate_beta_ratio() -&gt; DataFrame</code>","text":"<p>Calculate the beta ratio for the overlapping signals.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing left and right loci IDs, chromosome</p> <code>DataFrame</code> <p>and the average sign of the beta ratio</p> Source code in <code>src/gentropy/dataset/study_locus_overlap.py</code> <pre><code>def calculate_beta_ratio(self: StudyLocusOverlap) -&gt; DataFrame:\n    \"\"\"Calculate the beta ratio for the overlapping signals.\n\n    Returns:\n        DataFrame: A dataframe containing left and right loci IDs, chromosome\n        and the average sign of the beta ratio\n    \"\"\"\n    return (\n        # Unpack statistics column:\n        self.df.select(\"*\", \"statistics.*\")\n        .drop(\"statistics\")\n        # Drop any rows where the beta is null or zero\n        .filter(\n            f.col(\"left_beta\").isNotNull() &amp;\n            f.col(\"right_beta\").isNotNull() &amp;\n            (f.col(\"left_beta\") != 0) &amp;\n            (f.col(\"right_beta\") != 0)\n        )\n        # Calculate the beta ratio and get the sign, then calculate the average sign across all variants in the locus\n        .withColumn(\n            \"betaRatioSign\",\n            f.signum(f.col(\"left_beta\") / f.col(\"right_beta\"))\n        )\n        # Aggregate beta signs:\n        .groupBy(\"leftStudyLocusId\",\"rightStudyLocusId\",\"chromosome\")\n        .agg(\n            f.avg(\"betaRatioSign\").alias(\"betaRatioSignAverage\")\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/#gentropy.dataset.study_locus_overlap.StudyLocusOverlap.from_associations","title":"<code>from_associations(study_locus: StudyLocus) -&gt; StudyLocusOverlap</code>  <code>classmethod</code>","text":"<p>Find the overlapping signals in a particular set of associations (StudyLocus dataset).</p> <p>Parameters:</p> Name Type Description Default <code>study_locus</code> <code>StudyLocus</code> <p>Study-locus associations to find the overlapping signals</p> required <p>Returns:</p> Name Type Description <code>StudyLocusOverlap</code> <code>StudyLocusOverlap</code> <p>Study-locus overlap dataset</p> Source code in <code>src/gentropy/dataset/study_locus_overlap.py</code> <pre><code>@classmethod\ndef from_associations(\n    cls: type[StudyLocusOverlap], study_locus: StudyLocus\n) -&gt; StudyLocusOverlap:\n    \"\"\"Find the overlapping signals in a particular set of associations (StudyLocus dataset).\n\n    Args:\n        study_locus (StudyLocus): Study-locus associations to find the overlapping signals\n\n    Returns:\n        StudyLocusOverlap: Study-locus overlap dataset\n    \"\"\"\n    return study_locus.find_overlaps()\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/#gentropy.dataset.study_locus_overlap.StudyLocusOverlap.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the StudyLocusOverlap dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the StudyLocusOverlap dataset</p> Source code in <code>src/gentropy/dataset/study_locus_overlap.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[StudyLocusOverlap]) -&gt; StructType:\n    \"\"\"Provides the schema for the StudyLocusOverlap dataset.\n\n    Returns:\n        StructType: Schema for the StudyLocusOverlap dataset\n    \"\"\"\n    return parse_spark_schema(\"study_locus_overlap.json\")\n</code></pre>"},{"location":"python_api/datasets/study_locus_overlap/#schema","title":"Schema","text":"<pre><code>root\n |-- leftStudyLocusId: string (nullable = false)\n |-- rightStudyLocusId: string (nullable = false)\n |-- rightStudyType: string (nullable = false)\n |-- chromosome: string (nullable = true)\n |-- tagVariantId: string (nullable = false)\n |-- statistics: struct (nullable = true)\n |    |-- left_pValueMantissa: float (nullable = true)\n |    |-- left_pValueExponent: integer (nullable = true)\n |    |-- right_pValueMantissa: float (nullable = true)\n |    |-- right_pValueExponent: integer (nullable = true)\n |    |-- left_beta: double (nullable = true)\n |    |-- right_beta: double (nullable = true)\n |    |-- left_logBF: double (nullable = true)\n |    |-- right_logBF: double (nullable = true)\n |    |-- left_posteriorProbability: double (nullable = true)\n |    |-- right_posteriorProbability: double (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/","title":"Summary Statistics","text":""},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics","title":"<code>gentropy.dataset.summary_statistics.SummaryStatistics</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Summary Statistics dataset.</p> <p>A summary statistics dataset contains all single point statistics resulting from a GWAS.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>@dataclass\nclass SummaryStatistics(Dataset):\n    \"\"\"Summary Statistics dataset.\n\n    A summary statistics dataset contains all single point statistics resulting from a GWAS.\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[SummaryStatistics]) -&gt; StructType:\n        \"\"\"Provides the schema for the SummaryStatistics dataset.\n\n        Returns:\n            StructType: Schema for the SummaryStatistics dataset\n        \"\"\"\n        return parse_spark_schema(\"summary_statistics.json\")\n\n    def pvalue_filter(self: SummaryStatistics, pvalue: float) -&gt; SummaryStatistics:\n        \"\"\"Filter summary statistics based on the provided p-value threshold.\n\n        Args:\n            pvalue (float): upper limit of the p-value to be filtered upon.\n\n        Returns:\n            SummaryStatistics: summary statistics object containing single point associations with p-values at least as significant as the provided threshold.\n        \"\"\"\n        # Converting p-value to mantissa and exponent:\n        (mantissa, exponent) = split_pvalue(pvalue)\n\n        # Applying filter:\n        df = self._df.filter(\n            (f.col(\"pValueExponent\") &lt; exponent)\n            | (\n                (f.col(\"pValueExponent\") == exponent)\n                &amp; (f.col(\"pValueMantissa\") &lt;= mantissa)\n            )\n        )\n        return SummaryStatistics(_df=df, _schema=self._schema)\n\n    def window_based_clumping(\n        self: SummaryStatistics,\n        distance: int = WindowBasedClumpingStepConfig().distance,\n        gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance,\n    ) -&gt; StudyLocus:\n        \"\"\"Generate study-locus from summary statistics using window-based clumping.\n\n        For more info, see [`WindowBasedClumping`][gentropy.method.window_based_clumping.WindowBasedClumping]\n\n        Args:\n            distance (int): Distance in base pairs to be used for clumping. Defaults to 500_000.\n            gwas_significance (float, optional): GWAS significance threshold. Defaults to 5e-8.\n\n        Returns:\n            StudyLocus: Clumped study-locus optionally containing variants based on window.\n            Check WindowBasedClumpingStepConfig object for default values.\n        \"\"\"\n        from gentropy.method.window_based_clumping import WindowBasedClumping\n\n        return WindowBasedClumping.clump(\n            # Before clumping, we filter the summary statistics by p-value:\n            self.pvalue_filter(gwas_significance),\n            distance=distance,\n            # After applying the clumping, we filter the clumped loci by the flag:\n        ).valid_rows([\"WINDOW_CLUMPED\"])\n\n    def locus_breaker_clumping(\n        self: SummaryStatistics,\n        baseline_pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_baseline_pvalue,\n        distance_cutoff: int = LocusBreakerClumpingConfig.lbc_distance_cutoff,\n        pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_pvalue_threshold,\n        flanking_distance: int = LocusBreakerClumpingConfig.lbc_flanking_distance,\n    ) -&gt; StudyLocus:\n        \"\"\"Generate study-locus from summary statistics using locus-breaker clumping method with locus boundaries.\n\n        For more info, see [`locus_breaker`][gentropy.method.locus_breaker_clumping.LocusBreakerClumping]\n\n        Args:\n            baseline_pvalue_cutoff (float, optional): Baseline significance we consider for the locus.\n            distance_cutoff (int, optional): Distance in base pairs to be used for clumping.\n            pvalue_cutoff (float, optional): GWAS significance threshold.\n            flanking_distance (int, optional): Flank distance in base pairs to be used for clumping.\n\n        Returns:\n            StudyLocus: Clumped study-locus optionally containing variants based on window.\n            Check LocusBreakerClumpingConfig object for default values.\n        \"\"\"\n        from gentropy.method.locus_breaker_clumping import LocusBreakerClumping\n\n        return LocusBreakerClumping.locus_breaker(\n            self,\n            baseline_pvalue_cutoff,\n            distance_cutoff,\n            pvalue_cutoff,\n            flanking_distance,\n        )\n\n    def exclude_region(\n        self: SummaryStatistics, region: GenomicRegion\n    ) -&gt; SummaryStatistics:\n        \"\"\"Exclude a region from the summary stats dataset.\n\n        Args:\n            region (GenomicRegion): Genomic region to be excluded.\n\n        Returns:\n            SummaryStatistics: filtered summary statistics.\n        \"\"\"\n        return SummaryStatistics(\n            _df=(\n                self.df.filter(\n                    ~(\n                        (f.col(\"chromosome\") == region.chromosome)\n                        &amp; (\n                            (f.col(\"position\") &gt;= region.start)\n                            &amp; (f.col(\"position\") &lt;= region.end)\n                        )\n                    )\n                )\n            ),\n            _schema=SummaryStatistics.get_schema(),\n        )\n\n    def sanity_filter(self: SummaryStatistics) -&gt; SummaryStatistics:\n        \"\"\"The function filters the summary statistics by sanity filters.\n\n        The function filters the summary statistics by the following filters:\n            - The p-value should be less than 1.\n            - The pValueMantissa should be greater than 0.\n            - The beta should not be equal 0.\n            - The p-value, beta and se should not be NaN.\n            - The se should be positive.\n            - The beta and se should not be infinite.\n\n        Returns:\n            SummaryStatistics: The filtered summary statistics.\n        \"\"\"\n        gwas_df = self._df\n        gwas_df = gwas_df.dropna(\n            subset=[\"beta\", \"standardError\", \"pValueMantissa\", \"pValueExponent\"]\n        )\n        gwas_df = gwas_df.filter((f.col(\"beta\") != 0) &amp; (f.col(\"standardError\") &gt; 0))\n        gwas_df = gwas_df.filter(\n            (f.col(\"pValueMantissa\") * 10 ** f.col(\"pValueExponent\") &lt; 1)\n            &amp; (f.col(\"pValueMantissa\") &gt; 0)\n        )\n        cols = [\"beta\", \"standardError\"]\n        summary_stats = SummaryStatistics(\n            _df=gwas_df,\n            _schema=SummaryStatistics.get_schema(),\n        ).drop_infinity_values(*cols)\n\n        return summary_stats\n\n    def limit_to_studies(\n        self, session: Session, study_index: StudyIndex\n    ) -&gt; SummaryStatistics:\n        \"\"\"Limit summary statistics to those present in the study index.\n\n        Args:\n            session (Session): Session object.\n            study_index (StudyIndex): Study index object.\n\n        Returns:\n            SummaryStatistics: Filtered summary statistics object.\n        \"\"\"\n        studies = study_index.df.select(\"studyId\").distinct().collect()\n        study_ids = [row.studyId for row in studies]\n        session.logger.info(f\"Expecting N={len(study_ids)} studies.\")\n\n        count_pre = self.df.select(\"studyId\").distinct().count()\n        session.logger.info(f\"Found N={count_pre} distinct summary statistics.\")\n\n        session.logger.info(\"Filtering summary statistics to match the study index.\")\n        filtered_sumstats = self.df.filter(f.col(\"studyId\").isin(study_ids))\n\n        count_post = filtered_sumstats.select(\"studyId\").distinct().count()\n        session.logger.info(f\"N={count_post} studies match the study index.\")\n\n        if count_pre != count_post:\n            session.logger.warning(\n                f\"Dropping N={count_pre - count_post} studies that are not in the study index.\"\n            )\n        return SummaryStatistics(_df=filtered_sumstats)\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.exclude_region","title":"<code>exclude_region(region: GenomicRegion) -&gt; SummaryStatistics</code>","text":"<p>Exclude a region from the summary stats dataset.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>GenomicRegion</code> <p>Genomic region to be excluded.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>filtered summary statistics.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def exclude_region(\n    self: SummaryStatistics, region: GenomicRegion\n) -&gt; SummaryStatistics:\n    \"\"\"Exclude a region from the summary stats dataset.\n\n    Args:\n        region (GenomicRegion): Genomic region to be excluded.\n\n    Returns:\n        SummaryStatistics: filtered summary statistics.\n    \"\"\"\n    return SummaryStatistics(\n        _df=(\n            self.df.filter(\n                ~(\n                    (f.col(\"chromosome\") == region.chromosome)\n                    &amp; (\n                        (f.col(\"position\") &gt;= region.start)\n                        &amp; (f.col(\"position\") &lt;= region.end)\n                    )\n                )\n            )\n        ),\n        _schema=SummaryStatistics.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the SummaryStatistics dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the SummaryStatistics dataset</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[SummaryStatistics]) -&gt; StructType:\n    \"\"\"Provides the schema for the SummaryStatistics dataset.\n\n    Returns:\n        StructType: Schema for the SummaryStatistics dataset\n    \"\"\"\n    return parse_spark_schema(\"summary_statistics.json\")\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.limit_to_studies","title":"<code>limit_to_studies(session: Session, study_index: StudyIndex) -&gt; SummaryStatistics</code>","text":"<p>Limit summary statistics to those present in the study index.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>study_index</code> <code>StudyIndex</code> <p>Study index object.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>Filtered summary statistics object.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def limit_to_studies(\n    self, session: Session, study_index: StudyIndex\n) -&gt; SummaryStatistics:\n    \"\"\"Limit summary statistics to those present in the study index.\n\n    Args:\n        session (Session): Session object.\n        study_index (StudyIndex): Study index object.\n\n    Returns:\n        SummaryStatistics: Filtered summary statistics object.\n    \"\"\"\n    studies = study_index.df.select(\"studyId\").distinct().collect()\n    study_ids = [row.studyId for row in studies]\n    session.logger.info(f\"Expecting N={len(study_ids)} studies.\")\n\n    count_pre = self.df.select(\"studyId\").distinct().count()\n    session.logger.info(f\"Found N={count_pre} distinct summary statistics.\")\n\n    session.logger.info(\"Filtering summary statistics to match the study index.\")\n    filtered_sumstats = self.df.filter(f.col(\"studyId\").isin(study_ids))\n\n    count_post = filtered_sumstats.select(\"studyId\").distinct().count()\n    session.logger.info(f\"N={count_post} studies match the study index.\")\n\n    if count_pre != count_post:\n        session.logger.warning(\n            f\"Dropping N={count_pre - count_post} studies that are not in the study index.\"\n        )\n    return SummaryStatistics(_df=filtered_sumstats)\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.locus_breaker_clumping","title":"<code>locus_breaker_clumping(baseline_pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_baseline_pvalue, distance_cutoff: int = LocusBreakerClumpingConfig.lbc_distance_cutoff, pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_pvalue_threshold, flanking_distance: int = LocusBreakerClumpingConfig.lbc_flanking_distance) -&gt; StudyLocus</code>","text":"<p>Generate study-locus from summary statistics using locus-breaker clumping method with locus boundaries.</p> <p>For more info, see <code>locus_breaker</code></p> <p>Parameters:</p> Name Type Description Default <code>baseline_pvalue_cutoff</code> <code>float</code> <p>Baseline significance we consider for the locus.</p> <code>lbc_baseline_pvalue</code> <code>distance_cutoff</code> <code>int</code> <p>Distance in base pairs to be used for clumping.</p> <code>lbc_distance_cutoff</code> <code>pvalue_cutoff</code> <code>float</code> <p>GWAS significance threshold.</p> <code>lbc_pvalue_threshold</code> <code>flanking_distance</code> <code>int</code> <p>Flank distance in base pairs to be used for clumping.</p> <code>lbc_flanking_distance</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Clumped study-locus optionally containing variants based on window.</p> <code>StudyLocus</code> <p>Check LocusBreakerClumpingConfig object for default values.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def locus_breaker_clumping(\n    self: SummaryStatistics,\n    baseline_pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_baseline_pvalue,\n    distance_cutoff: int = LocusBreakerClumpingConfig.lbc_distance_cutoff,\n    pvalue_cutoff: float = LocusBreakerClumpingConfig.lbc_pvalue_threshold,\n    flanking_distance: int = LocusBreakerClumpingConfig.lbc_flanking_distance,\n) -&gt; StudyLocus:\n    \"\"\"Generate study-locus from summary statistics using locus-breaker clumping method with locus boundaries.\n\n    For more info, see [`locus_breaker`][gentropy.method.locus_breaker_clumping.LocusBreakerClumping]\n\n    Args:\n        baseline_pvalue_cutoff (float, optional): Baseline significance we consider for the locus.\n        distance_cutoff (int, optional): Distance in base pairs to be used for clumping.\n        pvalue_cutoff (float, optional): GWAS significance threshold.\n        flanking_distance (int, optional): Flank distance in base pairs to be used for clumping.\n\n    Returns:\n        StudyLocus: Clumped study-locus optionally containing variants based on window.\n        Check LocusBreakerClumpingConfig object for default values.\n    \"\"\"\n    from gentropy.method.locus_breaker_clumping import LocusBreakerClumping\n\n    return LocusBreakerClumping.locus_breaker(\n        self,\n        baseline_pvalue_cutoff,\n        distance_cutoff,\n        pvalue_cutoff,\n        flanking_distance,\n    )\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.pvalue_filter","title":"<code>pvalue_filter(pvalue: float) -&gt; SummaryStatistics</code>","text":"<p>Filter summary statistics based on the provided p-value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue</code> <code>float</code> <p>upper limit of the p-value to be filtered upon.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>summary statistics object containing single point associations with p-values at least as significant as the provided threshold.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def pvalue_filter(self: SummaryStatistics, pvalue: float) -&gt; SummaryStatistics:\n    \"\"\"Filter summary statistics based on the provided p-value threshold.\n\n    Args:\n        pvalue (float): upper limit of the p-value to be filtered upon.\n\n    Returns:\n        SummaryStatistics: summary statistics object containing single point associations with p-values at least as significant as the provided threshold.\n    \"\"\"\n    # Converting p-value to mantissa and exponent:\n    (mantissa, exponent) = split_pvalue(pvalue)\n\n    # Applying filter:\n    df = self._df.filter(\n        (f.col(\"pValueExponent\") &lt; exponent)\n        | (\n            (f.col(\"pValueExponent\") == exponent)\n            &amp; (f.col(\"pValueMantissa\") &lt;= mantissa)\n        )\n    )\n    return SummaryStatistics(_df=df, _schema=self._schema)\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.sanity_filter","title":"<code>sanity_filter() -&gt; SummaryStatistics</code>","text":"<p>The function filters the summary statistics by sanity filters.</p> The function filters the summary statistics by the following filters <ul> <li>The p-value should be less than 1.</li> <li>The pValueMantissa should be greater than 0.</li> <li>The beta should not be equal 0.</li> <li>The p-value, beta and se should not be NaN.</li> <li>The se should be positive.</li> <li>The beta and se should not be infinite.</li> </ul> <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>The filtered summary statistics.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def sanity_filter(self: SummaryStatistics) -&gt; SummaryStatistics:\n    \"\"\"The function filters the summary statistics by sanity filters.\n\n    The function filters the summary statistics by the following filters:\n        - The p-value should be less than 1.\n        - The pValueMantissa should be greater than 0.\n        - The beta should not be equal 0.\n        - The p-value, beta and se should not be NaN.\n        - The se should be positive.\n        - The beta and se should not be infinite.\n\n    Returns:\n        SummaryStatistics: The filtered summary statistics.\n    \"\"\"\n    gwas_df = self._df\n    gwas_df = gwas_df.dropna(\n        subset=[\"beta\", \"standardError\", \"pValueMantissa\", \"pValueExponent\"]\n    )\n    gwas_df = gwas_df.filter((f.col(\"beta\") != 0) &amp; (f.col(\"standardError\") &gt; 0))\n    gwas_df = gwas_df.filter(\n        (f.col(\"pValueMantissa\") * 10 ** f.col(\"pValueExponent\") &lt; 1)\n        &amp; (f.col(\"pValueMantissa\") &gt; 0)\n    )\n    cols = [\"beta\", \"standardError\"]\n    summary_stats = SummaryStatistics(\n        _df=gwas_df,\n        _schema=SummaryStatistics.get_schema(),\n    ).drop_infinity_values(*cols)\n\n    return summary_stats\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#gentropy.dataset.summary_statistics.SummaryStatistics.window_based_clumping","title":"<code>window_based_clumping(distance: int = WindowBasedClumpingStepConfig().distance, gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance) -&gt; StudyLocus</code>","text":"<p>Generate study-locus from summary statistics using window-based clumping.</p> <p>For more info, see <code>WindowBasedClumping</code></p> <p>Parameters:</p> Name Type Description Default <code>distance</code> <code>int</code> <p>Distance in base pairs to be used for clumping. Defaults to 500_000.</p> <code>distance</code> <code>gwas_significance</code> <code>float</code> <p>GWAS significance threshold. Defaults to 5e-8.</p> <code>gwas_significance</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Clumped study-locus optionally containing variants based on window.</p> <code>StudyLocus</code> <p>Check WindowBasedClumpingStepConfig object for default values.</p> Source code in <code>src/gentropy/dataset/summary_statistics.py</code> <pre><code>def window_based_clumping(\n    self: SummaryStatistics,\n    distance: int = WindowBasedClumpingStepConfig().distance,\n    gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance,\n) -&gt; StudyLocus:\n    \"\"\"Generate study-locus from summary statistics using window-based clumping.\n\n    For more info, see [`WindowBasedClumping`][gentropy.method.window_based_clumping.WindowBasedClumping]\n\n    Args:\n        distance (int): Distance in base pairs to be used for clumping. Defaults to 500_000.\n        gwas_significance (float, optional): GWAS significance threshold. Defaults to 5e-8.\n\n    Returns:\n        StudyLocus: Clumped study-locus optionally containing variants based on window.\n        Check WindowBasedClumpingStepConfig object for default values.\n    \"\"\"\n    from gentropy.method.window_based_clumping import WindowBasedClumping\n\n    return WindowBasedClumping.clump(\n        # Before clumping, we filter the summary statistics by p-value:\n        self.pvalue_filter(gwas_significance),\n        distance=distance,\n        # After applying the clumping, we filter the clumped loci by the flag:\n    ).valid_rows([\"WINDOW_CLUMPED\"])\n</code></pre>"},{"location":"python_api/datasets/summary_statistics/#schema","title":"Schema","text":"<pre><code>root\n |-- studyId: string (nullable = false)\n |-- variantId: string (nullable = false)\n |-- chromosome: string (nullable = false)\n |-- position: integer (nullable = false)\n |-- beta: double (nullable = false)\n |-- sampleSize: integer (nullable = true)\n |-- pValueMantissa: float (nullable = false)\n |-- pValueExponent: integer (nullable = false)\n |-- effectAlleleFrequencyFromSource: float (nullable = true)\n |-- standardError: double (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/summary_statistics_qc/","title":"Summary Statistics Quality Control","text":""},{"location":"python_api/datasets/summary_statistics_qc/#gentropy.dataset.summary_statistics_qc.SummaryStatisticsQC","title":"<code>gentropy.dataset.summary_statistics_qc.SummaryStatisticsQC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Summary Statistics Quality Controls dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"S1\", 0.45, 6.78, 8.47, 0.55, 2, 1), (\"S2\", 0.26, -2.15, 4.38, 0.04, 2, 0)]\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema=SummaryStatisticsQC.get_schema())\n&gt;&gt;&gt; qc = SummaryStatisticsQC(_df=df)\n&gt;&gt;&gt; isinstance(qc, SummaryStatisticsQC)\nTrue\n&gt;&gt;&gt; qc.df.show()\n+-------+---------+------------+----------+---------+----------+--------------+\n|studyId|mean_beta|mean_diff_pz|se_diff_pz|gc_lambda|n_variants|n_variants_sig|\n+-------+---------+------------+----------+---------+----------+--------------+\n|     S1|     0.45|        6.78|      8.47|     0.55|         2|             1|\n|     S2|     0.26|       -2.15|      4.38|     0.04|         2|             0|\n+-------+---------+------------+----------+---------+----------+--------------+\n</code></pre> Source code in <code>src/gentropy/dataset/summary_statistics_qc.py</code> <pre><code>@dataclass\nclass SummaryStatisticsQC(Dataset):\n    \"\"\"Summary Statistics Quality Controls dataset.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"S1\", 0.45, 6.78, 8.47, 0.55, 2, 1), (\"S2\", 0.26, -2.15, 4.38, 0.04, 2, 0)]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema=SummaryStatisticsQC.get_schema())\n        &gt;&gt;&gt; qc = SummaryStatisticsQC(_df=df)\n        &gt;&gt;&gt; isinstance(qc, SummaryStatisticsQC)\n        True\n        &gt;&gt;&gt; qc.df.show()\n        +-------+---------+------------+----------+---------+----------+--------------+\n        |studyId|mean_beta|mean_diff_pz|se_diff_pz|gc_lambda|n_variants|n_variants_sig|\n        +-------+---------+------------+----------+---------+----------+--------------+\n        |     S1|     0.45|        6.78|      8.47|     0.55|         2|             1|\n        |     S2|     0.26|       -2.15|      4.38|     0.04|         2|             0|\n        +-------+---------+------------+----------+---------+----------+--------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[SummaryStatisticsQC]) -&gt; StructType:\n        \"\"\"Provide the schema for the SummaryStatisticsQC dataset.\n\n        Returns:\n            StructType: The schema for the SummaryStatisticsQC dataset.\n        \"\"\"\n        return parse_spark_schema(\"summary_statistics_qc.json\")\n\n    @classmethod\n    def from_summary_statistics(\n        cls: type[SummaryStatisticsQC],\n        gwas: SummaryStatistics,\n        pval_threshold: float = 1e-8,\n    ) -&gt; SummaryStatisticsQC:\n        \"\"\"The function calculates the quality control metrics for the summary statistics.\n\n        Args:\n            gwas (SummaryStatistics): The instance of the SummaryStatistics class.\n            pval_threshold (float): The p-value threshold for the QC. Default is 1e-8.\n\n        Returns:\n            SummaryStatisticsQC: Dataset with quality control metrics for the summary statistics.\n\n        Examples:\n            &gt;&gt;&gt; from pyspark.sql import functions as f\n            &gt;&gt;&gt; s = 'studyId STRING, variantId STRING, chromosome STRING, position INT, beta DOUBLE, standardError DOUBLE, pValueMantissa FLOAT, pValueExponent INTEGER'\n            &gt;&gt;&gt; v1 = [(\"S1\", \"1_10000_A_T\", \"1\", 10000, 1.0, 0.2, 9.9, -20), (\"S1\", \"X_10001_C_T\", \"X\", 10001, -0.1, 0.2, 1.0, -1)]\n            &gt;&gt;&gt; v2 = [(\"S2\", \"1_10001_C_T\", \"1\", 10001, 0.028, 0.2, 1.0, -1), (\"S2\", \"1_10002_G_C\", \"1\", 10002, 0.5, 0.1, 1.0, -1)]\n            &gt;&gt;&gt; df = spark.createDataFrame(v1 + v2, s)\n            &gt;&gt;&gt; df.show()\n            +-------+-----------+----------+--------+-----+-------------+--------------+--------------+\n            |studyId|  variantId|chromosome|position| beta|standardError|pValueMantissa|pValueExponent|\n            +-------+-----------+----------+--------+-----+-------------+--------------+--------------+\n            |     S1|1_10000_A_T|         1|   10000|  1.0|          0.2|           9.9|           -20|\n            |     S1|X_10001_C_T|         X|   10001| -0.1|          0.2|           1.0|            -1|\n            |     S2|1_10001_C_T|         1|   10001|0.028|          0.2|           1.0|            -1|\n            |     S2|1_10002_G_C|         1|   10002|  0.5|          0.1|           1.0|            -1|\n            +-------+-----------+----------+--------+-----+-------------+--------------+--------------+\n            &lt;BLANKLINE&gt;\n\n            ** This method outputs one value per study, mean beta, mean diff pz, se diff pz, gc lambda, n variants and n variants sig**\n\n            &gt;&gt;&gt; stats = SummaryStatistics(df)\n            &gt;&gt;&gt; qc = SummaryStatisticsQC.from_summary_statistics(stats)\n            &gt;&gt;&gt; isinstance(qc, SummaryStatisticsQC)\n            True\n            &gt;&gt;&gt; mean_beta = f.round(\"mean_beta\", 2).alias(\"mean_beta\")\n            &gt;&gt;&gt; mean_diff_pz = f.round(\"mean_diff_pz\", 2).alias(\"mean_diff_pz\")\n            &gt;&gt;&gt; se_diff_pz = f.round(\"se_diff_pz\", 2).alias(\"se_diff_pz\")\n            &gt;&gt;&gt; gc_lambda = f.round(\"gc_lambda\", 2).alias(\"gc_lambda\")\n            &gt;&gt;&gt; qc.df.select('studyId', mean_beta, mean_diff_pz, se_diff_pz, gc_lambda, 'n_variants', 'n_variants_sig').show()\n            +-------+---------+------------+----------+---------+----------+--------------+\n            |studyId|mean_beta|mean_diff_pz|se_diff_pz|gc_lambda|n_variants|n_variants_sig|\n            +-------+---------+------------+----------+---------+----------+--------------+\n            |     S1|     0.45|        6.78|      8.47|     0.55|         2|             1|\n            |     S2|     0.26|       -2.15|      4.38|     0.04|         2|             0|\n            +-------+---------+------------+----------+---------+----------+--------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        n_variants: Callable[[DataFrame], DataFrame] = lambda df: number_of_variants(\n            df, pval_threshold\n        )\n        QC_TESTS = [\n            QCTest([\"mean_beta\"], mean_beta_check),\n            QCTest([\"mean_diff_pz\", \"se_diff_pz\"], p_z_test),\n            QCTest([\"gc_lambda\"], gc_lambda_check),\n            QCTest([\"n_variants\", \"n_variants_sig\"], n_variants),\n        ]\n\n        qc = reduce(\n            lambda qc1, qc2: qc1.join(qc2, on=\"studyId\", how=\"outer\"),\n            [test.call_test(gwas.df) for test in QC_TESTS],\n        )\n\n        return cls(_df=qc)\n</code></pre>"},{"location":"python_api/datasets/summary_statistics_qc/#gentropy.dataset.summary_statistics_qc.SummaryStatisticsQC.from_summary_statistics","title":"<code>from_summary_statistics(gwas: SummaryStatistics, pval_threshold: float = 1e-08) -&gt; SummaryStatisticsQC</code>  <code>classmethod</code>","text":"<p>The function calculates the quality control metrics for the summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>gwas</code> <code>SummaryStatistics</code> <p>The instance of the SummaryStatistics class.</p> required <code>pval_threshold</code> <code>float</code> <p>The p-value threshold for the QC. Default is 1e-8.</p> <code>1e-08</code> <p>Returns:</p> Name Type Description <code>SummaryStatisticsQC</code> <code>SummaryStatisticsQC</code> <p>Dataset with quality control metrics for the summary statistics.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; s = 'studyId STRING, variantId STRING, chromosome STRING, position INT, beta DOUBLE, standardError DOUBLE, pValueMantissa FLOAT, pValueExponent INTEGER'\n&gt;&gt;&gt; v1 = [(\"S1\", \"1_10000_A_T\", \"1\", 10000, 1.0, 0.2, 9.9, -20), (\"S1\", \"X_10001_C_T\", \"X\", 10001, -0.1, 0.2, 1.0, -1)]\n&gt;&gt;&gt; v2 = [(\"S2\", \"1_10001_C_T\", \"1\", 10001, 0.028, 0.2, 1.0, -1), (\"S2\", \"1_10002_G_C\", \"1\", 10002, 0.5, 0.1, 1.0, -1)]\n&gt;&gt;&gt; df = spark.createDataFrame(v1 + v2, s)\n&gt;&gt;&gt; df.show()\n+-------+-----------+----------+--------+-----+-------------+--------------+--------------+\n|studyId|  variantId|chromosome|position| beta|standardError|pValueMantissa|pValueExponent|\n+-------+-----------+----------+--------+-----+-------------+--------------+--------------+\n|     S1|1_10000_A_T|         1|   10000|  1.0|          0.2|           9.9|           -20|\n|     S1|X_10001_C_T|         X|   10001| -0.1|          0.2|           1.0|            -1|\n|     S2|1_10001_C_T|         1|   10001|0.028|          0.2|           1.0|            -1|\n|     S2|1_10002_G_C|         1|   10002|  0.5|          0.1|           1.0|            -1|\n+-------+-----------+----------+--------+-----+-------------+--------------+--------------+\n</code></pre> <p>** This method outputs one value per study, mean beta, mean diff pz, se diff pz, gc lambda, n variants and n variants sig**</p> <pre><code>&gt;&gt;&gt; stats = SummaryStatistics(df)\n&gt;&gt;&gt; qc = SummaryStatisticsQC.from_summary_statistics(stats)\n&gt;&gt;&gt; isinstance(qc, SummaryStatisticsQC)\nTrue\n&gt;&gt;&gt; mean_beta = f.round(\"mean_beta\", 2).alias(\"mean_beta\")\n&gt;&gt;&gt; mean_diff_pz = f.round(\"mean_diff_pz\", 2).alias(\"mean_diff_pz\")\n&gt;&gt;&gt; se_diff_pz = f.round(\"se_diff_pz\", 2).alias(\"se_diff_pz\")\n&gt;&gt;&gt; gc_lambda = f.round(\"gc_lambda\", 2).alias(\"gc_lambda\")\n&gt;&gt;&gt; qc.df.select('studyId', mean_beta, mean_diff_pz, se_diff_pz, gc_lambda, 'n_variants', 'n_variants_sig').show()\n+-------+---------+------------+----------+---------+----------+--------------+\n|studyId|mean_beta|mean_diff_pz|se_diff_pz|gc_lambda|n_variants|n_variants_sig|\n+-------+---------+------------+----------+---------+----------+--------------+\n|     S1|     0.45|        6.78|      8.47|     0.55|         2|             1|\n|     S2|     0.26|       -2.15|      4.38|     0.04|         2|             0|\n+-------+---------+------------+----------+---------+----------+--------------+\n</code></pre> Source code in <code>src/gentropy/dataset/summary_statistics_qc.py</code> <pre><code>@classmethod\ndef from_summary_statistics(\n    cls: type[SummaryStatisticsQC],\n    gwas: SummaryStatistics,\n    pval_threshold: float = 1e-8,\n) -&gt; SummaryStatisticsQC:\n    \"\"\"The function calculates the quality control metrics for the summary statistics.\n\n    Args:\n        gwas (SummaryStatistics): The instance of the SummaryStatistics class.\n        pval_threshold (float): The p-value threshold for the QC. Default is 1e-8.\n\n    Returns:\n        SummaryStatisticsQC: Dataset with quality control metrics for the summary statistics.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import functions as f\n        &gt;&gt;&gt; s = 'studyId STRING, variantId STRING, chromosome STRING, position INT, beta DOUBLE, standardError DOUBLE, pValueMantissa FLOAT, pValueExponent INTEGER'\n        &gt;&gt;&gt; v1 = [(\"S1\", \"1_10000_A_T\", \"1\", 10000, 1.0, 0.2, 9.9, -20), (\"S1\", \"X_10001_C_T\", \"X\", 10001, -0.1, 0.2, 1.0, -1)]\n        &gt;&gt;&gt; v2 = [(\"S2\", \"1_10001_C_T\", \"1\", 10001, 0.028, 0.2, 1.0, -1), (\"S2\", \"1_10002_G_C\", \"1\", 10002, 0.5, 0.1, 1.0, -1)]\n        &gt;&gt;&gt; df = spark.createDataFrame(v1 + v2, s)\n        &gt;&gt;&gt; df.show()\n        +-------+-----------+----------+--------+-----+-------------+--------------+--------------+\n        |studyId|  variantId|chromosome|position| beta|standardError|pValueMantissa|pValueExponent|\n        +-------+-----------+----------+--------+-----+-------------+--------------+--------------+\n        |     S1|1_10000_A_T|         1|   10000|  1.0|          0.2|           9.9|           -20|\n        |     S1|X_10001_C_T|         X|   10001| -0.1|          0.2|           1.0|            -1|\n        |     S2|1_10001_C_T|         1|   10001|0.028|          0.2|           1.0|            -1|\n        |     S2|1_10002_G_C|         1|   10002|  0.5|          0.1|           1.0|            -1|\n        +-------+-----------+----------+--------+-----+-------------+--------------+--------------+\n        &lt;BLANKLINE&gt;\n\n        ** This method outputs one value per study, mean beta, mean diff pz, se diff pz, gc lambda, n variants and n variants sig**\n\n        &gt;&gt;&gt; stats = SummaryStatistics(df)\n        &gt;&gt;&gt; qc = SummaryStatisticsQC.from_summary_statistics(stats)\n        &gt;&gt;&gt; isinstance(qc, SummaryStatisticsQC)\n        True\n        &gt;&gt;&gt; mean_beta = f.round(\"mean_beta\", 2).alias(\"mean_beta\")\n        &gt;&gt;&gt; mean_diff_pz = f.round(\"mean_diff_pz\", 2).alias(\"mean_diff_pz\")\n        &gt;&gt;&gt; se_diff_pz = f.round(\"se_diff_pz\", 2).alias(\"se_diff_pz\")\n        &gt;&gt;&gt; gc_lambda = f.round(\"gc_lambda\", 2).alias(\"gc_lambda\")\n        &gt;&gt;&gt; qc.df.select('studyId', mean_beta, mean_diff_pz, se_diff_pz, gc_lambda, 'n_variants', 'n_variants_sig').show()\n        +-------+---------+------------+----------+---------+----------+--------------+\n        |studyId|mean_beta|mean_diff_pz|se_diff_pz|gc_lambda|n_variants|n_variants_sig|\n        +-------+---------+------------+----------+---------+----------+--------------+\n        |     S1|     0.45|        6.78|      8.47|     0.55|         2|             1|\n        |     S2|     0.26|       -2.15|      4.38|     0.04|         2|             0|\n        +-------+---------+------------+----------+---------+----------+--------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    n_variants: Callable[[DataFrame], DataFrame] = lambda df: number_of_variants(\n        df, pval_threshold\n    )\n    QC_TESTS = [\n        QCTest([\"mean_beta\"], mean_beta_check),\n        QCTest([\"mean_diff_pz\", \"se_diff_pz\"], p_z_test),\n        QCTest([\"gc_lambda\"], gc_lambda_check),\n        QCTest([\"n_variants\", \"n_variants_sig\"], n_variants),\n    ]\n\n    qc = reduce(\n        lambda qc1, qc2: qc1.join(qc2, on=\"studyId\", how=\"outer\"),\n        [test.call_test(gwas.df) for test in QC_TESTS],\n    )\n\n    return cls(_df=qc)\n</code></pre>"},{"location":"python_api/datasets/summary_statistics_qc/#gentropy.dataset.summary_statistics_qc.SummaryStatisticsQC.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provide the schema for the SummaryStatisticsQC dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>The schema for the SummaryStatisticsQC dataset.</p> Source code in <code>src/gentropy/dataset/summary_statistics_qc.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[SummaryStatisticsQC]) -&gt; StructType:\n    \"\"\"Provide the schema for the SummaryStatisticsQC dataset.\n\n    Returns:\n        StructType: The schema for the SummaryStatisticsQC dataset.\n    \"\"\"\n    return parse_spark_schema(\"summary_statistics_qc.json\")\n</code></pre>"},{"location":"python_api/datasets/summary_statistics_qc/#schema","title":"Schema","text":"<pre><code>root\n |-- studyId: string (nullable = true)\n |-- mean_beta: double (nullable = true)\n |-- mean_diff_pz: double (nullable = true)\n |-- se_diff_pz: double (nullable = true)\n |-- gc_lambda: double (nullable = true)\n |-- n_variants: long (nullable = true)\n |-- n_variants_sig: long (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/target_index/","title":"Target Index","text":""},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex","title":"<code>gentropy.dataset.target_index.TargetIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Target index dataset.</p> <p>Gene-based annotation.</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>@dataclass\nclass TargetIndex(Dataset):\n    \"\"\"Target index dataset.\n\n    Gene-based annotation.\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[TargetIndex]) -&gt; StructType:\n        \"\"\"Provides the schema for the TargetIndex dataset.\n\n        Returns:\n            StructType: Schema for the TargetIndex dataset\n        \"\"\"\n        return parse_spark_schema(\"target_index.json\")\n\n    def filter_by_biotypes(self: TargetIndex, biotypes: list[str]) -&gt; TargetIndex:\n        \"\"\"Filter by approved biotypes.\n\n        Args:\n            biotypes (list[str]): List of Ensembl biotypes to keep.\n\n        Returns:\n            TargetIndex: Target index dataset filtered by biotypes.\n        \"\"\"\n        self.df = self._df.filter(f.col(\"biotype\").isin(biotypes))\n        return self\n\n    def locations_lut(self: TargetIndex) -&gt; DataFrame:\n        \"\"\"Gene location information.\n\n        Returns:\n            DataFrame: Gene LUT including genomic location information.\n        \"\"\"\n        return self.df.select(\n            f.col(\"id\").alias(\"geneId\"),\n            f.col(\"genomicLocation.chromosome\").alias(\"chromosome\"),\n            f.col(\"genomicLocation.start\").alias(\"start\"),\n            f.col(\"genomicLocation.end\").alias(\"end\"),\n            f.col(\"genomicLocation.strand\").alias(\"strand\"),\n            \"tss\",\n        )\n\n    def symbols_lut(self: TargetIndex) -&gt; DataFrame:\n        \"\"\"Gene symbol lookup table.\n\n        Pre-processess gene/target dataset to create lookup table of gene symbols, including\n        obsoleted gene symbols.\n\n        Returns:\n            DataFrame: Gene LUT for symbol mapping containing `geneId` and `geneSymbol` columns.\n        \"\"\"\n        return self.df.select(\n            f.explode(\n                f.array_union(f.array(\"approvedSymbol\"), f.col(\"obsoleteSymbols.label\"))\n            ).alias(\"geneSymbol\"),\n            f.col(\"id\").alias(\"geneId\"),\n            f.col(\"genomicLocation.chromosome\").alias(\"chromosome\"),\n            \"tss\",\n        )\n</code></pre>"},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex.filter_by_biotypes","title":"<code>filter_by_biotypes(biotypes: list[str]) -&gt; TargetIndex</code>","text":"<p>Filter by approved biotypes.</p> <p>Parameters:</p> Name Type Description Default <code>biotypes</code> <code>list[str]</code> <p>List of Ensembl biotypes to keep.</p> required <p>Returns:</p> Name Type Description <code>TargetIndex</code> <code>TargetIndex</code> <p>Target index dataset filtered by biotypes.</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>def filter_by_biotypes(self: TargetIndex, biotypes: list[str]) -&gt; TargetIndex:\n    \"\"\"Filter by approved biotypes.\n\n    Args:\n        biotypes (list[str]): List of Ensembl biotypes to keep.\n\n    Returns:\n        TargetIndex: Target index dataset filtered by biotypes.\n    \"\"\"\n    self.df = self._df.filter(f.col(\"biotype\").isin(biotypes))\n    return self\n</code></pre>"},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the TargetIndex dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the TargetIndex dataset</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[TargetIndex]) -&gt; StructType:\n    \"\"\"Provides the schema for the TargetIndex dataset.\n\n    Returns:\n        StructType: Schema for the TargetIndex dataset\n    \"\"\"\n    return parse_spark_schema(\"target_index.json\")\n</code></pre>"},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex.locations_lut","title":"<code>locations_lut() -&gt; DataFrame</code>","text":"<p>Gene location information.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Gene LUT including genomic location information.</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>def locations_lut(self: TargetIndex) -&gt; DataFrame:\n    \"\"\"Gene location information.\n\n    Returns:\n        DataFrame: Gene LUT including genomic location information.\n    \"\"\"\n    return self.df.select(\n        f.col(\"id\").alias(\"geneId\"),\n        f.col(\"genomicLocation.chromosome\").alias(\"chromosome\"),\n        f.col(\"genomicLocation.start\").alias(\"start\"),\n        f.col(\"genomicLocation.end\").alias(\"end\"),\n        f.col(\"genomicLocation.strand\").alias(\"strand\"),\n        \"tss\",\n    )\n</code></pre>"},{"location":"python_api/datasets/target_index/#gentropy.dataset.target_index.TargetIndex.symbols_lut","title":"<code>symbols_lut() -&gt; DataFrame</code>","text":"<p>Gene symbol lookup table.</p> <p>Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Gene LUT for symbol mapping containing <code>geneId</code> and <code>geneSymbol</code> columns.</p> Source code in <code>src/gentropy/dataset/target_index.py</code> <pre><code>def symbols_lut(self: TargetIndex) -&gt; DataFrame:\n    \"\"\"Gene symbol lookup table.\n\n    Pre-processess gene/target dataset to create lookup table of gene symbols, including\n    obsoleted gene symbols.\n\n    Returns:\n        DataFrame: Gene LUT for symbol mapping containing `geneId` and `geneSymbol` columns.\n    \"\"\"\n    return self.df.select(\n        f.explode(\n            f.array_union(f.array(\"approvedSymbol\"), f.col(\"obsoleteSymbols.label\"))\n        ).alias(\"geneSymbol\"),\n        f.col(\"id\").alias(\"geneId\"),\n        f.col(\"genomicLocation.chromosome\").alias(\"chromosome\"),\n        \"tss\",\n    )\n</code></pre>"},{"location":"python_api/datasets/target_index/#schema","title":"Schema","text":"<pre><code>root\n |-- id: string (nullable = false)\n |-- approvedSymbol: string (nullable = true)\n |-- biotype: string (nullable = true)\n |-- transcriptIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- canonicalTranscript: struct (nullable = true)\n |    |-- id: string (nullable = true)\n |    |-- chromosome: string (nullable = true)\n |    |-- start: long (nullable = true)\n |    |-- end: long (nullable = true)\n |    |-- strand: string (nullable = true)\n |-- canonicalExons: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- genomicLocation: struct (nullable = true)\n |    |-- chromosome: string (nullable = true)\n |    |-- start: long (nullable = true)\n |    |-- end: long (nullable = true)\n |    |-- strand: integer (nullable = true)\n |-- alternativeGenes: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- approvedName: string (nullable = true)\n |-- go: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |    |    |-- evidence: string (nullable = true)\n |    |    |-- aspect: string (nullable = true)\n |    |    |-- geneProduct: string (nullable = true)\n |    |    |-- ecoId: string (nullable = true)\n |-- hallmarks: struct (nullable = true)\n |    |-- attributes: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- pmid: long (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- attribute_name: string (nullable = true)\n |    |-- cancerHallmarks: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- pmid: long (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- impact: string (nullable = true)\n |    |    |    |-- label: string (nullable = true)\n |-- synonyms: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- symbolSynonyms: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- nameSynonyms: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- functionDescriptions: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- subcellularLocations: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- location: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |    |    |-- termSL: string (nullable = true)\n |    |    |-- labelSL: string (nullable = true)\n |-- targetClass: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- level: string (nullable = true)\n |-- obsoleteSymbols: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- obsoleteNames: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- label: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- constraint: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- constraintType: string (nullable = true)\n |    |    |-- score: float (nullable = true)\n |    |    |-- exp: float (nullable = true)\n |    |    |-- obs: integer (nullable = true)\n |    |    |-- oe: float (nullable = true)\n |    |    |-- oeLower: float (nullable = true)\n |    |    |-- oeUpper: float (nullable = true)\n |    |    |-- upperRank: integer (nullable = true)\n |    |    |-- upperBin: integer (nullable = true)\n |    |    |-- upperBin6: integer (nullable = true)\n |-- tep: struct (nullable = true)\n |    |-- targetFromSourceId: string (nullable = true)\n |    |-- description: string (nullable = true)\n |    |-- therapeuticArea: string (nullable = true)\n |    |-- url: string (nullable = true)\n |-- proteinIds: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- dbXrefs: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- chemicalProbes: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- control: string (nullable = true)\n |    |    |-- drugId: string (nullable = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- isHighQuality: boolean (nullable = true)\n |    |    |-- mechanismOfAction: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- origin: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- probeMinerScore: long (nullable = true)\n |    |    |-- probesDrugsScore: long (nullable = true)\n |    |    |-- scoreInCells: long (nullable = true)\n |    |    |-- scoreInOrganisms: long (nullable = true)\n |    |    |-- targetFromSourceId: string (nullable = true)\n |    |    |-- urls: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- niceName: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |-- homologues: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- speciesId: string (nullable = true)\n |    |    |-- speciesName: string (nullable = true)\n |    |    |-- homologyType: string (nullable = true)\n |    |    |-- targetGeneId: string (nullable = true)\n |    |    |-- isHighConfidence: string (nullable = true)\n |    |    |-- targetGeneSymbol: string (nullable = true)\n |    |    |-- queryPercentageIdentity: double (nullable = true)\n |    |    |-- targetPercentageIdentity: double (nullable = true)\n |    |    |-- priority: integer (nullable = true)\n |-- tractability: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- modality: string (nullable = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- value: boolean (nullable = true)\n |-- safetyLiabilities: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- event: string (nullable = true)\n |    |    |-- eventId: string (nullable = true)\n |    |    |-- effects: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- direction: string (nullable = true)\n |    |    |    |    |-- dosing: string (nullable = true)\n |    |    |-- biosamples: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- cellFormat: string (nullable = true)\n |    |    |    |    |-- cellLabel: string (nullable = true)\n |    |    |    |    |-- tissueId: string (nullable = true)\n |    |    |    |    |-- tissueLabel: string (nullable = true)\n |    |    |-- datasource: string (nullable = true)\n |    |    |-- literature: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- studies: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |-- pathways: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- pathwayId: string (nullable = true)\n |    |    |-- pathway: string (nullable = true)\n |    |    |-- topLevelTerm: string (nullable = true)\n |-- tss: long (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/variant_direction/","title":"Variant Direction","text":""},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.Direction","title":"<code>gentropy.dataset.variant_direction.Direction</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>Allele direction.</p> <p>Attributes:</p> Name Type Description <code>DIRECT</code> <code>int</code> <p>Direct allele direction (e.g., A/G). Defaults to 1.</p> <code>FLIPPED</code> <code>int</code> <p>Flipped allele direction (e.g., G/A). Defaults to -1.</p> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>class Direction(int, Enum):\n    \"\"\"Allele direction.\n\n    Attributes:\n        DIRECT (int): Direct allele direction (e.g., A/G). Defaults to 1.\n        FLIPPED (int): Flipped allele direction (e.g., G/A). Defaults to -1.\n    \"\"\"\n\n    DIRECT = 1\n    FLIPPED = -1\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.Strand","title":"<code>gentropy.dataset.variant_direction.Strand</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>Strand orientation.</p> <p>Attributes:</p> Name Type Description <code>FORWARD</code> <code>int</code> <p>Forward strand. Defaults to 1.</p> <code>REVERSE</code> <code>int</code> <p>Reverse strand. Defaults to -1.</p> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>class Strand(int, Enum):\n    \"\"\"Strand orientation.\n\n    Attributes:\n        FORWARD (int): Forward strand. Defaults to 1.\n        REVERSE (int): Reverse strand. Defaults to -1.\n    \"\"\"\n\n    FORWARD = 1\n    REVERSE = -1\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantType","title":"<code>gentropy.dataset.variant_direction.VariantType</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>Variant types based on length of reference and alternate alleles.</p> <p>Attributes:</p> Name Type Description <code>SNP</code> <code>int</code> <p>Single Nucleotide Polymorphism. Defaults to 1.</p> <code>INS</code> <code>int</code> <p>Insertion. Defaults to 2.</p> <code>DEL</code> <code>int</code> <p>Deletion. Defaults to 3.</p> <code>MNP</code> <code>int</code> <p>Multi-Nucleotide Polymorphism. Defaults to 4.</p> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>class VariantType(int, Enum):\n    \"\"\"Variant types based on length of reference and alternate alleles.\n\n    Attributes:\n        SNP (int): Single Nucleotide Polymorphism. Defaults to 1.\n        INS (int): Insertion. Defaults to 2.\n        DEL (int): Deletion. Defaults to 3.\n        MNP (int): Multi-Nucleotide Polymorphism. Defaults to 4.\n    \"\"\"\n\n    SNP = 1\n    INS = 2\n    DEL = 3\n    MNP = 4\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantDirection","title":"<code>gentropy.dataset.variant_direction.VariantDirection</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset used for aligning allele directionality between different datasets.</p> <p>This dataset is useful for flipping alleles to match reference datasets.</p> <p>This dataset expends each variant into 4 entries to account for</p> <ul> <li>Different directions (<code>FORWARD</code> and <code>FLIPPED</code>) - e.g. A/G and G/A</li> <li>Different strands (<code>FORWARD</code> and <code>REVERSE</code>) - e.g. A/G and T/C</li> </ul> <p>Each entry contains the combination of both, meaning that for each input variant there will be 4 entries in this dataset. For strand ambiguous variants the <code>FORWARD</code> and <code>FLIPPED</code> entries will be identical to the <code>REVERSE</code> and <code>FLIPPED</code> entries, so we keep only one copy of them.</p> <p>Additionally this dataset annotates:</p> <ul> <li>ambiguous strand variants</li> <li>type of variant (SNP, INS, DEL, MNP)</li> <li>original allele frequencies from the source dataset</li> <li>original variant id from the source dataset</li> </ul> Joining with other datasets <p>To compare two datasets, you need to ensure that both datasets are joined on the <code>variantId</code> that is a combination of <code>chromosome</code>, <code>position</code>, <code>reference allele</code> and <code>alternate allele</code>.</p> Building the dataset <p>The easiest way to create this dataset (have a complete variant space) is to build it from a VariantIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"1\", 100, \"A\", \"G\", [(\"nfe_adj\", 0.1), (\"fin_adj\", 0.2)]), (\"1\", 100, \"T\", \"A\", [(\"nfe_adj\", 0.1), (\"fin_adj\", 0.2)])]\n&gt;&gt;&gt; schema = \"chromosome STRING, position INT, referenceAllele STRING, alternateAllele STRING, alleleFrequencies ARRAY&lt;STRUCT&lt;populationName: STRING, alleleFrequency: DOUBLE&gt;&gt;\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema).withColumn(\"variantId\",\n... f.concat_ws(\"_\", \"chromosome\", \"position\", \"referenceAllele\", \"alternateAllele\"))\n&gt;&gt;&gt; variant_index = VariantIndex(_df=df)\n&gt;&gt;&gt; variant_direction = VariantDirection.from_variant_index(variant_index)\n&gt;&gt;&gt; variant_direction.df.show(truncate=False)\n+----------+-----------------+----+---------+---------+------+-----------------+--------------------------------+\n|chromosome|originalVariantId|type|variantId|direction|strand|isStrandAmbiguous|originalAlleleFrequencies       |\n+----------+-----------------+----+---------+---------+------+-----------------+--------------------------------+\n|1         |1_100_A_G        |1   |1_100_A_G|1        |1     |false            |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n|1         |1_100_A_G        |1   |1_100_G_A|-1       |1     |false            |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n|1         |1_100_A_G        |1   |1_100_T_C|1        |-1    |false            |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n|1         |1_100_A_G        |1   |1_100_C_T|-1       |-1    |false            |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n|1         |1_100_T_A        |1   |1_100_T_A|1        |1     |true             |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n|1         |1_100_T_A        |1   |1_100_A_T|-1       |1     |true             |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n+----------+-----------------+----+---------+---------+------+-----------------+--------------------------------+\n</code></pre> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>@dataclass\nclass VariantDirection(Dataset):\n    \"\"\"Dataset used for aligning allele directionality between different datasets.\n\n    This dataset is useful for flipping alleles to match reference datasets.\n\n    This dataset expends each variant into 4 entries to account for\n\n    * Different directions (`FORWARD` and `FLIPPED`) - e.g. A/G and G/A\n    * Different strands (`FORWARD` and `REVERSE`) - e.g. A/G and T/C\n\n    Each entry contains the combination of both, meaning that for each input variant\n    there will be 4 entries in this dataset. For strand ambiguous variants the\n    `FORWARD` and `FLIPPED` entries will be identical to the `REVERSE` and `FLIPPED` entries,\n    so we keep only one copy of them.\n\n    Additionally this dataset annotates:\n\n    * ambiguous strand variants\n    * type of variant (SNP, INS, DEL, MNP)\n    * original allele frequencies from the source dataset\n    * original variant id from the source dataset\n\n    ??? tip \"Joining with other datasets\"\n        To compare two datasets, you need to ensure that both datasets are joined on the `variantId` that\n        is a combination of `chromosome`, `position`, `reference allele` and `alternate allele`.\n\n    ??? note \"Building the dataset\"\n        The easiest way to create this dataset (have a complete variant space) is to build it\n        from a **VariantIndex**.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"1\", 100, \"A\", \"G\", [(\"nfe_adj\", 0.1), (\"fin_adj\", 0.2)]), (\"1\", 100, \"T\", \"A\", [(\"nfe_adj\", 0.1), (\"fin_adj\", 0.2)])]\n        &gt;&gt;&gt; schema = \"chromosome STRING, position INT, referenceAllele STRING, alternateAllele STRING, alleleFrequencies ARRAY&lt;STRUCT&lt;populationName: STRING, alleleFrequency: DOUBLE&gt;&gt;\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema).withColumn(\"variantId\",\n        ... f.concat_ws(\"_\", \"chromosome\", \"position\", \"referenceAllele\", \"alternateAllele\"))\n        &gt;&gt;&gt; variant_index = VariantIndex(_df=df)\n        &gt;&gt;&gt; variant_direction = VariantDirection.from_variant_index(variant_index)\n        &gt;&gt;&gt; variant_direction.df.show(truncate=False)\n        +----------+-----------------+----+---------+---------+------+-----------------+--------------------------------+\n        |chromosome|originalVariantId|type|variantId|direction|strand|isStrandAmbiguous|originalAlleleFrequencies       |\n        +----------+-----------------+----+---------+---------+------+-----------------+--------------------------------+\n        |1         |1_100_A_G        |1   |1_100_A_G|1        |1     |false            |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n        |1         |1_100_A_G        |1   |1_100_G_A|-1       |1     |false            |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n        |1         |1_100_A_G        |1   |1_100_T_C|1        |-1    |false            |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n        |1         |1_100_A_G        |1   |1_100_C_T|-1       |-1    |false            |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n        |1         |1_100_T_A        |1   |1_100_T_A|1        |1     |true             |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n        |1         |1_100_T_A        |1   |1_100_A_T|-1       |1     |true             |[{nfe_adj, 0.1}, {fin_adj, 0.2}]|\n        +----------+-----------------+----+---------+---------+------+-----------------+--------------------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n\n    @classmethod\n    def get_schema(cls: type[VariantDirection]) -&gt; t.StructType:\n        \"\"\"Provides the schema for the variant index dataset.\n\n        Returns:\n            t.StructType: Schema for the VariantIndex dataset\n        \"\"\"\n        return parse_spark_schema(\"variant_direction.json\")\n\n    @classmethod\n    def is_strand_ambiguous(cls, ref: Column, alt: Column) -&gt; Column:\n        \"\"\"Check if the variant is strand ambiguous.\n\n        Args:\n            ref (Column): Reference allele column.\n            alt (Column): Alternate allele column.\n\n        Returns:\n            Column: Boolean column indicating if the variant is palindromic.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"A\", \"T\"), (\"C\", \"G\"), (\"A\", \"G\"), (\"AC\", \"GT\"), (\"AT\", \"TA\"), (\"A\", \"AT\")]\n            &gt;&gt;&gt; schema = \"ref STRING, alt STRING\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.withColumn(\"isStrandAmbiguous\", VariantDirection.is_strand_ambiguous(f.col(\"ref\"), f.col(\"alt\"))).show()\n            +---+---+-----------------+\n            |ref|alt|isStrandAmbiguous|\n            +---+---+-----------------+\n            |  A|  T|             true|\n            |  C|  G|             true|\n            |  A|  G|            false|\n            | AC| GT|             true|\n            | AT| TA|            false|\n            |  A| AT|            false|\n            +---+---+-----------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        ref_len = f.length(ref)\n        alt_len = f.length(alt)\n        ref = f.upper(ref)\n        alt = f.upper(alt)\n        return f.when(\n            (ref_len == alt_len) &amp; (cls.reverse(cls.complement(alt)) == ref), True\n        ).otherwise(False)\n\n    @staticmethod\n    def reverse(allele: Column) -&gt; Column:\n        \"\"\"Reverse the allele string.\n\n        Args:\n            allele (Column): Allele column.\n\n        Returns:\n            Column: Reversed allele column.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"A\",), (\"AT\",), (\"GTC\",)]\n            &gt;&gt;&gt; schema = \"allele STRING\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.withColumn(\"reversed\", VariantDirection.reverse(f.col(\"allele\"))).show()\n            +------+--------+\n            |allele|reversed|\n            +------+--------+\n            |     A|       A|\n            |    AT|      TA|\n            |   GTC|     CTG|\n            +------+--------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return f.reverse(allele)\n\n    @staticmethod\n    def complement(allele: Column) -&gt; Column:\n        \"\"\"Complement the allele string.\n\n        Args:\n            allele (Column): Allele column.\n\n        Returns:\n            Column: Complemented allele column.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"A\",), (\"C\",), (\"G\",), (\"T\",), (\"AT\",), (\"GTC\",)]\n            &gt;&gt;&gt; schema = \"allele STRING\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.withColumn(\"complemented\", VariantDirection.complement(f.col(\"allele\"))).show()\n            +------+------------+\n            |allele|complemented|\n            +------+------------+\n            |     A|           T|\n            |     C|           G|\n            |     G|           C|\n            |     T|           A|\n            |    AT|          TA|\n            |   GTC|         CAG|\n            +------+------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return f.translate(f.upper(allele), \"ACGT\", \"TGCA\")\n\n    @classmethod\n    def variant_type(cls, ref: Column, alt: Column) -&gt; Column:\n        \"\"\"Get the variant type.\n\n        Args:\n            ref (Column): Reference allele column.\n            alt (Column): Alternate allele column.\n\n        Returns:\n            Column: Variant type column.\n\n        Note:\n            Variant type coding follows VariantType enum:\n            - 1: SNP (Single Nucleotide Polymorphism)\n            - 2: INS (Insertion)\n            - 3: DEL (Deletion)\n            - 4: MNP (Multi-Nucleotide Polymorphism)\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"A\", \"G\"), (\"A\", \"AT\"), (\"AT\", \"A\"), (\"AT\", \"GC\")]\n            &gt;&gt;&gt; schema = \"ref STRING, alt STRING\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.withColumn(\"type\", VariantDirection.variant_type(f.col(\"ref\"), f.col(\"alt\"))).show()\n            +---+---+----+\n            |ref|alt|type|\n            +---+---+----+\n            |  A|  G|   1|\n            |  A| AT|   2|\n            | AT|  A|   3|\n            | AT| GC|   4|\n            +---+---+----+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        ref = f.upper(ref)\n        alt = f.upper(alt)\n        expr = (\n            f.when((f.length(alt) &gt; f.length(ref)), f.lit(VariantType.INS.value))\n            .when((f.length(alt) &lt; f.length(ref)), f.lit(VariantType.DEL.value))\n            .when(\n                ((f.length(alt) == 1) &amp; (f.length(ref) == 1)),\n                f.lit(VariantType.SNP.value),\n            )\n            .otherwise(f.lit(VariantType.MNP.value))\n        )\n        return expr.cast(t.ByteType())\n\n    @classmethod\n    def alleles(\n        cls, chrom: Column, pos: Column, ref: Column, alt: Column, af: Column\n    ) -&gt; Column:\n        \"\"\"Get the alleles of the variant.\n\n        Args:\n            chrom (Column): Chromosome column.\n            pos (Column): Position column.\n            ref (Column): Reference allele column.\n            alt (Column): Alternate allele column.\n            af (Column): Allele frequencies column.\n\n        Returns:\n            Column: Array of structs with variantId, direction, strand, isStrandAmbiguous, alleleFrequencies.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"1\", 100, \"A\", \"G\", [(\"nfe_adj\", 0.1),]), (\"1\", 100, \"T\", \"A\", [(\"nfe_adj\", 0.1),])]\n            &gt;&gt;&gt; schema = \"chrom STRING, pos INT, ref STRING, alt STRING, af ARRAY&lt;STRUCT&lt;populationName: STRING, alleleFrequency: DOUBLE&gt;&gt;\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df = df.withColumn(\"alleles\", VariantDirection.alleles(f.col(\"chrom\"), f.col(\"pos\"), f.col(\"ref\"), f.col(\"alt\"), f.col(\"af\"))).select(\"alleles\")\n            &gt;&gt;&gt; df.select(f.explode(\"alleles\").alias(\"allele\")).select(\"allele.*\").show(truncate=False)\n            +---------+---------+------+-----------------+-------------------------+\n            |variantId|direction|strand|isStrandAmbiguous|originalAlleleFrequencies|\n            +---------+---------+------+-----------------+-------------------------+\n            |1_100_A_G|1        |1     |false            |[{nfe_adj, 0.1}]         |\n            |1_100_G_A|-1       |1     |false            |[{nfe_adj, 0.1}]         |\n            |1_100_T_C|1        |-1    |false            |[{nfe_adj, 0.1}]         |\n            |1_100_C_T|-1       |-1    |false            |[{nfe_adj, 0.1}]         |\n            |1_100_T_A|1        |1     |true             |[{nfe_adj, 0.1}]         |\n            |1_100_A_T|-1       |1     |true             |[{nfe_adj, 0.1}]         |\n            +---------+---------+------+-----------------+-------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        ref = f.upper(ref)\n        alt = f.upper(alt)\n        forward_direct = cls.variant_id(chrom, pos, ref, alt)\n        forward_flipped = cls.variant_id(chrom, pos, alt, ref)\n        reverse_direct = cls.variant_id(\n            chrom,\n            pos,\n            cls.reverse(cls.complement(ref)),\n            cls.reverse(cls.complement(alt)),\n        )\n        reverse_flipped = cls.variant_id(\n            chrom,\n            pos,\n            cls.reverse(cls.complement(alt)),\n            cls.reverse(cls.complement(ref)),\n        )\n\n        return f.when(\n            ~cls.is_strand_ambiguous(ref, alt),\n            f.array(\n                f.struct(\n                    forward_direct.alias(\"variantId\"),\n                    f.lit(Direction.DIRECT.value).cast(t.ByteType()).alias(\"direction\"),\n                    f.lit(Strand.FORWARD.value).cast(t.ByteType()).alias(\"strand\"),\n                    f.lit(False).alias(\"isStrandAmbiguous\"),\n                    af.alias(\"originalAlleleFrequencies\"),\n                ),\n                f.struct(\n                    forward_flipped.alias(\"variantId\"),\n                    f.lit(Direction.FLIPPED.value)\n                    .cast(t.ByteType())\n                    .alias(\"direction\"),\n                    f.lit(Strand.FORWARD.value).cast(t.ByteType()).alias(\"strand\"),\n                    f.lit(False).alias(\"isStrandAmbiguous\"),\n                    af.alias(\"originalAlleleFrequencies\"),\n                ),\n                f.struct(\n                    reverse_direct.alias(\"variantId\"),\n                    f.lit(Direction.DIRECT.value).cast(t.ByteType()).alias(\"direction\"),\n                    f.lit(Strand.REVERSE.value).cast(t.ByteType()).alias(\"strand\"),\n                    f.lit(False).alias(\"isStrandAmbiguous\"),\n                    af.alias(\"originalAlleleFrequencies\"),\n                ),\n                f.struct(\n                    reverse_flipped.alias(\"variantId\"),\n                    f.lit(Direction.FLIPPED.value)\n                    .cast(t.ByteType())\n                    .alias(\"direction\"),\n                    f.lit(Strand.REVERSE.value).cast(t.ByteType()).alias(\"strand\"),\n                    f.lit(False).alias(\"isStrandAmbiguous\"),\n                    af.alias(\"originalAlleleFrequencies\"),\n                ),\n            ),\n        ).otherwise(\n            f.array(\n                f.struct(\n                    forward_direct.alias(\"variantId\"),\n                    f.lit(Direction.DIRECT.value).cast(t.ByteType()).alias(\"direction\"),\n                    f.lit(Strand.FORWARD.value).cast(t.ByteType()).alias(\"strand\"),\n                    f.lit(True).alias(\"isStrandAmbiguous\"),\n                    af.alias(\"originalAlleleFrequencies\"),\n                ),\n                f.struct(\n                    forward_flipped.alias(\"variantId\"),\n                    f.lit(Direction.FLIPPED.value)\n                    .cast(t.ByteType())\n                    .alias(\"direction\"),\n                    f.lit(Strand.FORWARD.value).cast(t.ByteType()).alias(\"strand\"),\n                    f.lit(True).alias(\"isStrandAmbiguous\"),\n                    af.alias(\"originalAlleleFrequencies\"),\n                ),\n            )\n        )\n\n    @classmethod\n    def variant_id(cls, chrom: Column, pos: Column, ref: Column, alt: Column) -&gt; Column:\n        \"\"\"Get the variant id.\n\n        Args:\n            chrom (Column): Chromosome column.\n            pos (Column): Position column.\n            ref (Column): Reference allele column.\n            alt (Column): Alternate allele column.\n\n        Returns:\n            Column: Variant ID column in the format \"chrom_pos_ref_alt\".\n        \"\"\"\n        ref = f.upper(ref)\n        alt = f.upper(alt)\n        return f.concat_ws(\"_\", chrom, pos, ref, alt)\n\n    @classmethod\n    def from_variant_index(cls, variant_index: VariantIndex) -&gt; VariantDirection:\n        \"\"\"Prepare the variant direction DataFrame with DIRECT and FLIPPED entries.\n\n        Args:\n            variant_index (VariantIndex): Variant index dataset.\n\n        Returns:\n            VariantDirection: Variant direction dataset.\n        \"\"\"\n        lut = variant_index.df.select(\n            f.col(\"chromosome\"),\n            f.col(\"variantId\").alias(\"originalVariantId\"),\n            cls.variant_type(f.col(\"referenceAllele\"), f.col(\"alternateAllele\")).alias(\n                \"type\"\n            ),\n            f.explode(\n                cls.alleles(\n                    f.col(\"chromosome\"),\n                    f.col(\"position\"),\n                    f.col(\"referenceAllele\"),\n                    f.col(\"alternateAllele\"),\n                    f.col(\"alleleFrequencies\"),\n                ).alias(\"alleles\")\n            ).alias(\"allele\"),\n        ).select(\n            f.col(\"chromosome\"),\n            f.col(\"originalVariantId\"),\n            f.col(\"type\"),\n            f.col(\"allele.variantId\").alias(\"variantId\"),\n            f.col(\"allele.direction\").alias(\"direction\"),\n            f.col(\"allele.strand\").alias(\"strand\"),\n            f.col(\"allele.isStrandAmbiguous\").alias(\"isStrandAmbiguous\"),\n            f.col(\"allele.originalAlleleFrequencies\").alias(\n                \"originalAlleleFrequencies\"\n            ),\n        )\n\n        return VariantDirection(_df=lut)\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantDirection.alleles","title":"<code>alleles(chrom: Column, pos: Column, ref: Column, alt: Column, af: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Get the alleles of the variant.</p> <p>Parameters:</p> Name Type Description Default <code>chrom</code> <code>Column</code> <p>Chromosome column.</p> required <code>pos</code> <code>Column</code> <p>Position column.</p> required <code>ref</code> <code>Column</code> <p>Reference allele column.</p> required <code>alt</code> <code>Column</code> <p>Alternate allele column.</p> required <code>af</code> <code>Column</code> <p>Allele frequencies column.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Array of structs with variantId, direction, strand, isStrandAmbiguous, alleleFrequencies.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"1\", 100, \"A\", \"G\", [(\"nfe_adj\", 0.1),]), (\"1\", 100, \"T\", \"A\", [(\"nfe_adj\", 0.1),])]\n&gt;&gt;&gt; schema = \"chrom STRING, pos INT, ref STRING, alt STRING, af ARRAY&lt;STRUCT&lt;populationName: STRING, alleleFrequency: DOUBLE&gt;&gt;\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df = df.withColumn(\"alleles\", VariantDirection.alleles(f.col(\"chrom\"), f.col(\"pos\"), f.col(\"ref\"), f.col(\"alt\"), f.col(\"af\"))).select(\"alleles\")\n&gt;&gt;&gt; df.select(f.explode(\"alleles\").alias(\"allele\")).select(\"allele.*\").show(truncate=False)\n+---------+---------+------+-----------------+-------------------------+\n|variantId|direction|strand|isStrandAmbiguous|originalAlleleFrequencies|\n+---------+---------+------+-----------------+-------------------------+\n|1_100_A_G|1        |1     |false            |[{nfe_adj, 0.1}]         |\n|1_100_G_A|-1       |1     |false            |[{nfe_adj, 0.1}]         |\n|1_100_T_C|1        |-1    |false            |[{nfe_adj, 0.1}]         |\n|1_100_C_T|-1       |-1    |false            |[{nfe_adj, 0.1}]         |\n|1_100_T_A|1        |1     |true             |[{nfe_adj, 0.1}]         |\n|1_100_A_T|-1       |1     |true             |[{nfe_adj, 0.1}]         |\n+---------+---------+------+-----------------+-------------------------+\n</code></pre> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>@classmethod\ndef alleles(\n    cls, chrom: Column, pos: Column, ref: Column, alt: Column, af: Column\n) -&gt; Column:\n    \"\"\"Get the alleles of the variant.\n\n    Args:\n        chrom (Column): Chromosome column.\n        pos (Column): Position column.\n        ref (Column): Reference allele column.\n        alt (Column): Alternate allele column.\n        af (Column): Allele frequencies column.\n\n    Returns:\n        Column: Array of structs with variantId, direction, strand, isStrandAmbiguous, alleleFrequencies.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"1\", 100, \"A\", \"G\", [(\"nfe_adj\", 0.1),]), (\"1\", 100, \"T\", \"A\", [(\"nfe_adj\", 0.1),])]\n        &gt;&gt;&gt; schema = \"chrom STRING, pos INT, ref STRING, alt STRING, af ARRAY&lt;STRUCT&lt;populationName: STRING, alleleFrequency: DOUBLE&gt;&gt;\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df = df.withColumn(\"alleles\", VariantDirection.alleles(f.col(\"chrom\"), f.col(\"pos\"), f.col(\"ref\"), f.col(\"alt\"), f.col(\"af\"))).select(\"alleles\")\n        &gt;&gt;&gt; df.select(f.explode(\"alleles\").alias(\"allele\")).select(\"allele.*\").show(truncate=False)\n        +---------+---------+------+-----------------+-------------------------+\n        |variantId|direction|strand|isStrandAmbiguous|originalAlleleFrequencies|\n        +---------+---------+------+-----------------+-------------------------+\n        |1_100_A_G|1        |1     |false            |[{nfe_adj, 0.1}]         |\n        |1_100_G_A|-1       |1     |false            |[{nfe_adj, 0.1}]         |\n        |1_100_T_C|1        |-1    |false            |[{nfe_adj, 0.1}]         |\n        |1_100_C_T|-1       |-1    |false            |[{nfe_adj, 0.1}]         |\n        |1_100_T_A|1        |1     |true             |[{nfe_adj, 0.1}]         |\n        |1_100_A_T|-1       |1     |true             |[{nfe_adj, 0.1}]         |\n        +---------+---------+------+-----------------+-------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    ref = f.upper(ref)\n    alt = f.upper(alt)\n    forward_direct = cls.variant_id(chrom, pos, ref, alt)\n    forward_flipped = cls.variant_id(chrom, pos, alt, ref)\n    reverse_direct = cls.variant_id(\n        chrom,\n        pos,\n        cls.reverse(cls.complement(ref)),\n        cls.reverse(cls.complement(alt)),\n    )\n    reverse_flipped = cls.variant_id(\n        chrom,\n        pos,\n        cls.reverse(cls.complement(alt)),\n        cls.reverse(cls.complement(ref)),\n    )\n\n    return f.when(\n        ~cls.is_strand_ambiguous(ref, alt),\n        f.array(\n            f.struct(\n                forward_direct.alias(\"variantId\"),\n                f.lit(Direction.DIRECT.value).cast(t.ByteType()).alias(\"direction\"),\n                f.lit(Strand.FORWARD.value).cast(t.ByteType()).alias(\"strand\"),\n                f.lit(False).alias(\"isStrandAmbiguous\"),\n                af.alias(\"originalAlleleFrequencies\"),\n            ),\n            f.struct(\n                forward_flipped.alias(\"variantId\"),\n                f.lit(Direction.FLIPPED.value)\n                .cast(t.ByteType())\n                .alias(\"direction\"),\n                f.lit(Strand.FORWARD.value).cast(t.ByteType()).alias(\"strand\"),\n                f.lit(False).alias(\"isStrandAmbiguous\"),\n                af.alias(\"originalAlleleFrequencies\"),\n            ),\n            f.struct(\n                reverse_direct.alias(\"variantId\"),\n                f.lit(Direction.DIRECT.value).cast(t.ByteType()).alias(\"direction\"),\n                f.lit(Strand.REVERSE.value).cast(t.ByteType()).alias(\"strand\"),\n                f.lit(False).alias(\"isStrandAmbiguous\"),\n                af.alias(\"originalAlleleFrequencies\"),\n            ),\n            f.struct(\n                reverse_flipped.alias(\"variantId\"),\n                f.lit(Direction.FLIPPED.value)\n                .cast(t.ByteType())\n                .alias(\"direction\"),\n                f.lit(Strand.REVERSE.value).cast(t.ByteType()).alias(\"strand\"),\n                f.lit(False).alias(\"isStrandAmbiguous\"),\n                af.alias(\"originalAlleleFrequencies\"),\n            ),\n        ),\n    ).otherwise(\n        f.array(\n            f.struct(\n                forward_direct.alias(\"variantId\"),\n                f.lit(Direction.DIRECT.value).cast(t.ByteType()).alias(\"direction\"),\n                f.lit(Strand.FORWARD.value).cast(t.ByteType()).alias(\"strand\"),\n                f.lit(True).alias(\"isStrandAmbiguous\"),\n                af.alias(\"originalAlleleFrequencies\"),\n            ),\n            f.struct(\n                forward_flipped.alias(\"variantId\"),\n                f.lit(Direction.FLIPPED.value)\n                .cast(t.ByteType())\n                .alias(\"direction\"),\n                f.lit(Strand.FORWARD.value).cast(t.ByteType()).alias(\"strand\"),\n                f.lit(True).alias(\"isStrandAmbiguous\"),\n                af.alias(\"originalAlleleFrequencies\"),\n            ),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantDirection.complement","title":"<code>complement(allele: Column) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Complement the allele string.</p> <p>Parameters:</p> Name Type Description Default <code>allele</code> <code>Column</code> <p>Allele column.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Complemented allele column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"A\",), (\"C\",), (\"G\",), (\"T\",), (\"AT\",), (\"GTC\",)]\n&gt;&gt;&gt; schema = \"allele STRING\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.withColumn(\"complemented\", VariantDirection.complement(f.col(\"allele\"))).show()\n+------+------------+\n|allele|complemented|\n+------+------------+\n|     A|           T|\n|     C|           G|\n|     G|           C|\n|     T|           A|\n|    AT|          TA|\n|   GTC|         CAG|\n+------+------------+\n</code></pre> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>@staticmethod\ndef complement(allele: Column) -&gt; Column:\n    \"\"\"Complement the allele string.\n\n    Args:\n        allele (Column): Allele column.\n\n    Returns:\n        Column: Complemented allele column.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"A\",), (\"C\",), (\"G\",), (\"T\",), (\"AT\",), (\"GTC\",)]\n        &gt;&gt;&gt; schema = \"allele STRING\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.withColumn(\"complemented\", VariantDirection.complement(f.col(\"allele\"))).show()\n        +------+------------+\n        |allele|complemented|\n        +------+------------+\n        |     A|           T|\n        |     C|           G|\n        |     G|           C|\n        |     T|           A|\n        |    AT|          TA|\n        |   GTC|         CAG|\n        +------+------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return f.translate(f.upper(allele), \"ACGT\", \"TGCA\")\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantDirection.from_variant_index","title":"<code>from_variant_index(variant_index: VariantIndex) -&gt; VariantDirection</code>  <code>classmethod</code>","text":"<p>Prepare the variant direction DataFrame with DIRECT and FLIPPED entries.</p> <p>Parameters:</p> Name Type Description Default <code>variant_index</code> <code>VariantIndex</code> <p>Variant index dataset.</p> required <p>Returns:</p> Name Type Description <code>VariantDirection</code> <code>VariantDirection</code> <p>Variant direction dataset.</p> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>@classmethod\ndef from_variant_index(cls, variant_index: VariantIndex) -&gt; VariantDirection:\n    \"\"\"Prepare the variant direction DataFrame with DIRECT and FLIPPED entries.\n\n    Args:\n        variant_index (VariantIndex): Variant index dataset.\n\n    Returns:\n        VariantDirection: Variant direction dataset.\n    \"\"\"\n    lut = variant_index.df.select(\n        f.col(\"chromosome\"),\n        f.col(\"variantId\").alias(\"originalVariantId\"),\n        cls.variant_type(f.col(\"referenceAllele\"), f.col(\"alternateAllele\")).alias(\n            \"type\"\n        ),\n        f.explode(\n            cls.alleles(\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                f.col(\"referenceAllele\"),\n                f.col(\"alternateAllele\"),\n                f.col(\"alleleFrequencies\"),\n            ).alias(\"alleles\")\n        ).alias(\"allele\"),\n    ).select(\n        f.col(\"chromosome\"),\n        f.col(\"originalVariantId\"),\n        f.col(\"type\"),\n        f.col(\"allele.variantId\").alias(\"variantId\"),\n        f.col(\"allele.direction\").alias(\"direction\"),\n        f.col(\"allele.strand\").alias(\"strand\"),\n        f.col(\"allele.isStrandAmbiguous\").alias(\"isStrandAmbiguous\"),\n        f.col(\"allele.originalAlleleFrequencies\").alias(\n            \"originalAlleleFrequencies\"\n        ),\n    )\n\n    return VariantDirection(_df=lut)\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantDirection.get_schema","title":"<code>get_schema() -&gt; t.StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the variant index dataset.</p> <p>Returns:</p> Type Description <code>StructType</code> <p>t.StructType: Schema for the VariantIndex dataset</p> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[VariantDirection]) -&gt; t.StructType:\n    \"\"\"Provides the schema for the variant index dataset.\n\n    Returns:\n        t.StructType: Schema for the VariantIndex dataset\n    \"\"\"\n    return parse_spark_schema(\"variant_direction.json\")\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantDirection.is_strand_ambiguous","title":"<code>is_strand_ambiguous(ref: Column, alt: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Check if the variant is strand ambiguous.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>Column</code> <p>Reference allele column.</p> required <code>alt</code> <code>Column</code> <p>Alternate allele column.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Boolean column indicating if the variant is palindromic.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"A\", \"T\"), (\"C\", \"G\"), (\"A\", \"G\"), (\"AC\", \"GT\"), (\"AT\", \"TA\"), (\"A\", \"AT\")]\n&gt;&gt;&gt; schema = \"ref STRING, alt STRING\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.withColumn(\"isStrandAmbiguous\", VariantDirection.is_strand_ambiguous(f.col(\"ref\"), f.col(\"alt\"))).show()\n+---+---+-----------------+\n|ref|alt|isStrandAmbiguous|\n+---+---+-----------------+\n|  A|  T|             true|\n|  C|  G|             true|\n|  A|  G|            false|\n| AC| GT|             true|\n| AT| TA|            false|\n|  A| AT|            false|\n+---+---+-----------------+\n</code></pre> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>@classmethod\ndef is_strand_ambiguous(cls, ref: Column, alt: Column) -&gt; Column:\n    \"\"\"Check if the variant is strand ambiguous.\n\n    Args:\n        ref (Column): Reference allele column.\n        alt (Column): Alternate allele column.\n\n    Returns:\n        Column: Boolean column indicating if the variant is palindromic.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"A\", \"T\"), (\"C\", \"G\"), (\"A\", \"G\"), (\"AC\", \"GT\"), (\"AT\", \"TA\"), (\"A\", \"AT\")]\n        &gt;&gt;&gt; schema = \"ref STRING, alt STRING\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.withColumn(\"isStrandAmbiguous\", VariantDirection.is_strand_ambiguous(f.col(\"ref\"), f.col(\"alt\"))).show()\n        +---+---+-----------------+\n        |ref|alt|isStrandAmbiguous|\n        +---+---+-----------------+\n        |  A|  T|             true|\n        |  C|  G|             true|\n        |  A|  G|            false|\n        | AC| GT|             true|\n        | AT| TA|            false|\n        |  A| AT|            false|\n        +---+---+-----------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    ref_len = f.length(ref)\n    alt_len = f.length(alt)\n    ref = f.upper(ref)\n    alt = f.upper(alt)\n    return f.when(\n        (ref_len == alt_len) &amp; (cls.reverse(cls.complement(alt)) == ref), True\n    ).otherwise(False)\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantDirection.reverse","title":"<code>reverse(allele: Column) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Reverse the allele string.</p> <p>Parameters:</p> Name Type Description Default <code>allele</code> <code>Column</code> <p>Allele column.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Reversed allele column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"A\",), (\"AT\",), (\"GTC\",)]\n&gt;&gt;&gt; schema = \"allele STRING\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.withColumn(\"reversed\", VariantDirection.reverse(f.col(\"allele\"))).show()\n+------+--------+\n|allele|reversed|\n+------+--------+\n|     A|       A|\n|    AT|      TA|\n|   GTC|     CTG|\n+------+--------+\n</code></pre> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>@staticmethod\ndef reverse(allele: Column) -&gt; Column:\n    \"\"\"Reverse the allele string.\n\n    Args:\n        allele (Column): Allele column.\n\n    Returns:\n        Column: Reversed allele column.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"A\",), (\"AT\",), (\"GTC\",)]\n        &gt;&gt;&gt; schema = \"allele STRING\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.withColumn(\"reversed\", VariantDirection.reverse(f.col(\"allele\"))).show()\n        +------+--------+\n        |allele|reversed|\n        +------+--------+\n        |     A|       A|\n        |    AT|      TA|\n        |   GTC|     CTG|\n        +------+--------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return f.reverse(allele)\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantDirection.variant_id","title":"<code>variant_id(chrom: Column, pos: Column, ref: Column, alt: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Get the variant id.</p> <p>Parameters:</p> Name Type Description Default <code>chrom</code> <code>Column</code> <p>Chromosome column.</p> required <code>pos</code> <code>Column</code> <p>Position column.</p> required <code>ref</code> <code>Column</code> <p>Reference allele column.</p> required <code>alt</code> <code>Column</code> <p>Alternate allele column.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Variant ID column in the format \"chrom_pos_ref_alt\".</p> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>@classmethod\ndef variant_id(cls, chrom: Column, pos: Column, ref: Column, alt: Column) -&gt; Column:\n    \"\"\"Get the variant id.\n\n    Args:\n        chrom (Column): Chromosome column.\n        pos (Column): Position column.\n        ref (Column): Reference allele column.\n        alt (Column): Alternate allele column.\n\n    Returns:\n        Column: Variant ID column in the format \"chrom_pos_ref_alt\".\n    \"\"\"\n    ref = f.upper(ref)\n    alt = f.upper(alt)\n    return f.concat_ws(\"_\", chrom, pos, ref, alt)\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#gentropy.dataset.variant_direction.VariantDirection.variant_type","title":"<code>variant_type(ref: Column, alt: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Get the variant type.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>Column</code> <p>Reference allele column.</p> required <code>alt</code> <code>Column</code> <p>Alternate allele column.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Variant type column.</p> Note <p>Variant type coding follows VariantType enum: - 1: SNP (Single Nucleotide Polymorphism) - 2: INS (Insertion) - 3: DEL (Deletion) - 4: MNP (Multi-Nucleotide Polymorphism)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"A\", \"G\"), (\"A\", \"AT\"), (\"AT\", \"A\"), (\"AT\", \"GC\")]\n&gt;&gt;&gt; schema = \"ref STRING, alt STRING\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.withColumn(\"type\", VariantDirection.variant_type(f.col(\"ref\"), f.col(\"alt\"))).show()\n+---+---+----+\n|ref|alt|type|\n+---+---+----+\n|  A|  G|   1|\n|  A| AT|   2|\n| AT|  A|   3|\n| AT| GC|   4|\n+---+---+----+\n</code></pre> Source code in <code>src/gentropy/dataset/variant_direction.py</code> <pre><code>@classmethod\ndef variant_type(cls, ref: Column, alt: Column) -&gt; Column:\n    \"\"\"Get the variant type.\n\n    Args:\n        ref (Column): Reference allele column.\n        alt (Column): Alternate allele column.\n\n    Returns:\n        Column: Variant type column.\n\n    Note:\n        Variant type coding follows VariantType enum:\n        - 1: SNP (Single Nucleotide Polymorphism)\n        - 2: INS (Insertion)\n        - 3: DEL (Deletion)\n        - 4: MNP (Multi-Nucleotide Polymorphism)\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"A\", \"G\"), (\"A\", \"AT\"), (\"AT\", \"A\"), (\"AT\", \"GC\")]\n        &gt;&gt;&gt; schema = \"ref STRING, alt STRING\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.withColumn(\"type\", VariantDirection.variant_type(f.col(\"ref\"), f.col(\"alt\"))).show()\n        +---+---+----+\n        |ref|alt|type|\n        +---+---+----+\n        |  A|  G|   1|\n        |  A| AT|   2|\n        | AT|  A|   3|\n        | AT| GC|   4|\n        +---+---+----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    ref = f.upper(ref)\n    alt = f.upper(alt)\n    expr = (\n        f.when((f.length(alt) &gt; f.length(ref)), f.lit(VariantType.INS.value))\n        .when((f.length(alt) &lt; f.length(ref)), f.lit(VariantType.DEL.value))\n        .when(\n            ((f.length(alt) == 1) &amp; (f.length(ref) == 1)),\n            f.lit(VariantType.SNP.value),\n        )\n        .otherwise(f.lit(VariantType.MNP.value))\n    )\n    return expr.cast(t.ByteType())\n</code></pre>"},{"location":"python_api/datasets/variant_direction/#schema","title":"Schema","text":"<pre><code>root\n |-- chromosome: string (nullable = true)\n |-- originalVariantId: string (nullable = false)\n |-- type: byte (nullable = false)\n |-- variantId: string (nullable = false)\n |-- direction: byte (nullable = false)\n |-- strand: byte (nullable = false)\n |-- isStrandAmbiguous: boolean (nullable = false)\n |-- originalAlleleFrequencies: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- populationName: string (nullable = true)\n |    |    |-- alleleFrequency: double (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/variant_index/","title":"Variant index","text":""},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex","title":"<code>gentropy.dataset.variant_index.VariantIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for representing variants and methods applied on them.</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>@dataclass\nclass VariantIndex(Dataset):\n    \"\"\"Dataset for representing variants and methods applied on them.\"\"\"\n\n    id_threshold: int = field(default=300)\n\n    def __post_init__(self: VariantIndex) -&gt; None:\n        \"\"\"Forcing the presence of empty arrays even if the schema allows missing values.\n\n        To bring in annotations from other sources, we use the `array_union()` function. However it assumes\n        both columns have arrays (not just the array schema!). If one of the array is null, the union\n        is nullified. This needs to be avoided.\n        \"\"\"\n        # Calling dataset's post init to validate schema:\n        super().__post_init__()\n\n        # Composing a list of expressions to replace nulls with empty arrays if the schema assumes:\n        array_columns = {\n            column.name: f.when(f.col(column.name).isNull(), f.array()).otherwise(\n                f.col(column.name)\n            )\n            for column in self.df.schema\n            if \"ArrayType\" in column.dataType.__str__()\n        }\n\n        # Not returning, but changing the data:\n        self.df = self.df.withColumns(array_columns).withColumn(\n            # Hashing long variant identifiers:\n            \"variantId\",\n            self.hash_long_variant_ids(\n                f.col(\"variantId\"),\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                self.id_threshold,\n            ),\n        )\n\n    @classmethod\n    def get_schema(cls: type[VariantIndex]) -&gt; StructType:\n        \"\"\"Provides the schema for the variant index dataset.\n\n        Returns:\n            StructType: Schema for the VariantIndex dataset\n        \"\"\"\n        return parse_spark_schema(\"variant_index.json\")\n\n    @staticmethod\n    def hash_long_variant_ids(\n        variant_id: Column, chromosome: Column, position: Column, threshold: int\n    ) -&gt; Column:\n        \"\"\"Hash long variant identifiers.\n\n        Args:\n            variant_id (Column): Column containing variant identifiers.\n            chromosome (Column): Chromosome column.\n            position (Column): position column.\n            threshold (int): Above this limit, a hash will be generated.\n\n        Returns:\n            Column: Hashed variant identifiers for long variants.\n\n        Examples:\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame([('v_short', 'x', 23),('v_looooooong', '23', 23), ('no_chrom', None, None), (None, None, None)], ['variantId', 'chromosome', 'position'])\n            ...    .select('variantId', VariantIndex.hash_long_variant_ids(f.col('variantId'), f.col('chromosome'), f.col('position'), 10).alias('hashedVariantId'))\n            ...    .show(truncate=False)\n            ... )\n            +------------+--------------------------------------------+\n            |variantId   |hashedVariantId                             |\n            +------------+--------------------------------------------+\n            |v_short     |v_short                                     |\n            |v_looooooong|OTVAR_23_23_3749d019d645894770c364992ae70a05|\n            |no_chrom    |OTVAR_41acfcd7d4fd523b33600b504914ef25      |\n            |NULL        |NULL                                        |\n            +------------+--------------------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return (\n            # If either the position or the chromosome is missing, we hash the identifier:\n            f.when(\n                chromosome.isNull() | position.isNull(),\n                f.concat(\n                    f.lit(\"OTVAR_\"),\n                    f.md5(variant_id).cast(\"string\"),\n                ),\n            )\n            # If chromosome and position are given, but alleles are too long, create hash:\n            .when(\n                f.length(variant_id) &gt;= threshold,\n                f.concat_ws(\n                    \"_\",\n                    f.lit(\"OTVAR\"),\n                    chromosome,\n                    position,\n                    f.md5(variant_id).cast(\"string\"),\n                ),\n            )\n            # Missing and regular variant identifiers are left unchanged:\n            .otherwise(variant_id)\n        )\n\n    def add_annotation(\n        self: VariantIndex, annotation_source: VariantIndex\n    ) -&gt; VariantIndex:\n        \"\"\"Import annotation from an other variant index dataset.\n\n        At this point the annotation can be extended with extra cross-references,\n        variant effects, allele frequencies, and variant descriptions.\n\n        Args:\n            annotation_source (VariantIndex): Annotation to add to the dataset\n\n        Returns:\n            VariantIndex: VariantIndex dataset with the annotation added\n        \"\"\"\n        # Prefix for renaming columns:\n        prefix = \"annotation_\"\n\n        # Generate select expressions that to merge and import columns from annotation:\n        select_expressions = []\n\n        # Collect columns by iterating over the variant index schema:\n        for schema_field in VariantIndex.get_schema():\n            column = schema_field.name\n\n            # If an annotation column can be found in both datasets:\n            if (column in self.df.columns) and (column in annotation_source.df.columns):\n                # Arrays are merged:\n                if isinstance(schema_field.dataType, t.ArrayType):\n                    fields_order = None\n                    if isinstance(schema_field.dataType.elementType, t.StructType):\n                        # Extract the schema of the array to get the order of the fields:\n                        array_schema = [\n                            schema_field\n                            for schema_field in VariantIndex.get_schema().fields\n                            if schema_field.name == column\n                        ][0].dataType\n                        fields_order = get_nested_struct_schema(\n                            array_schema\n                        ).fieldNames()\n                    select_expressions.append(\n                        safe_array_union(\n                            f.col(column), f.col(f\"{prefix}{column}\"), fields_order\n                        ).alias(column)\n                    )\n                # variantDescription columns are concatenated:\n                elif column == \"variantDescription\":\n                    select_expressions.append(\n                        f.concat_ws(\n                            \" \", f.col(column), f.col(f\"{prefix}{column}\")\n                        ).alias(column)\n                    )\n                # All other non-array columns are coalesced:\n                else:\n                    select_expressions.append(\n                        f.coalesce(f.col(column), f.col(f\"{prefix}{column}\")).alias(\n                            column\n                        )\n                    )\n            # If the column is only found in the annotation dataset rename it:\n            elif column in annotation_source.df.columns:\n                select_expressions.append(f.col(f\"{prefix}{column}\").alias(column))\n            # If the column is only found in the main dataset:\n            elif column in self.df.columns:\n                select_expressions.append(f.col(column))\n            # VariantIndex columns not found in either dataset are ignored.\n\n        # Join the annotation to the dataset:\n        return VariantIndex(\n            _df=(\n                f.broadcast(self.df)\n                .join(\n                    rename_all_columns(annotation_source.df, prefix),\n                    on=[f.col(\"variantId\") == f.col(f\"{prefix}variantId\")],\n                    how=\"left\",\n                )\n                .select(*select_expressions)\n            ),\n            _schema=self.schema,\n        )\n\n    def max_maf(self: VariantIndex) -&gt; Column:\n        \"\"\"Maximum minor allele frequency accross all populations assuming all variants biallelic.\n\n        Returns:\n            Column: Maximum minor allele frequency accross all populations.\n\n        Raises:\n            ValueError: Allele frequencies are not present in the dataset.\n        \"\"\"\n        if \"alleleFrequencies\" not in self.df.columns:\n            raise ValueError(\"Allele frequencies are not present in the dataset.\")\n\n        return f.array_max(\n            f.transform(\n                self.df.alleleFrequencies,\n                lambda af: f.when(\n                    af.alleleFrequency &gt; 0.5, 1 - af.alleleFrequency\n                ).otherwise(af.alleleFrequency),\n            )\n        )\n\n    def filter_by_variant(self: VariantIndex, df: DataFrame) -&gt; VariantIndex:\n        \"\"\"Filter variant annotation dataset by a variant dataframe.\n\n        Args:\n            df (DataFrame): A dataframe of variants.\n\n        Returns:\n            VariantIndex: A filtered variant annotation dataset.\n\n        Raises:\n            AssertionError: When the variant dataframe does not contain eiter `variantId` or `chromosome` column.\n        \"\"\"\n        join_columns = [\"variantId\", \"chromosome\"]\n\n        assert all(\n            col in df.columns for col in join_columns\n        ), \"The variant dataframe must contain the columns 'variantId' and 'chromosome'.\"\n\n        return VariantIndex(\n            _df=self._df.join(\n                f.broadcast(df.select(*join_columns).distinct()),\n                on=join_columns,\n                how=\"inner\",\n            ),\n            _schema=self.schema,\n        )\n\n    def get_distance_to_gene(\n        self: VariantIndex,\n        *,\n        distance_type: str = \"distanceFromTss\",\n        max_distance: int = 500_000,\n    ) -&gt; DataFrame:\n        \"\"\"Extracts variant to gene assignments for variants falling within a window of a gene's TSS or footprint.\n\n        Args:\n            distance_type (str): The type of distance to use. Can be \"distanceFromTss\" or \"distanceFromFootprint\". Defaults to \"distanceFromTss\".\n            max_distance (int): The maximum distance to consider. Defaults to 500_000, the default window size for VEP.\n\n        Returns:\n            DataFrame: A dataframe with the distance between a variant and a gene's TSS or footprint.\n\n        Raises:\n            ValueError: Invalid distance type.\n        \"\"\"\n        if distance_type not in {\"distanceFromTss\", \"distanceFromFootprint\"}:\n            raise ValueError(\n                f\"Invalid distance_type: {distance_type}. Must be 'distanceFromTss' or 'distanceFromFootprint'.\"\n            )\n        df = self.df.select(\n            \"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\")\n        ).select(\"variantId\", \"tc.targetId\", f\"tc.{distance_type}\")\n        if max_distance == 500_000:\n            return df\n        elif max_distance &lt; 500_000:\n            return df.filter(f\"{distance_type} &lt;= {max_distance}\")\n        else:\n            raise ValueError(\n                f\"max_distance must be less than 500_000. Got {max_distance}.\"\n            )\n\n    def annotate_with_amino_acid_consequences(\n        self: VariantIndex, annotation: AminoAcidVariants\n    ) -&gt; VariantIndex:\n        \"\"\"Enriching variant effect assessments with amino-acid derived predicted consequences.\n\n        Args:\n            annotation (AminoAcidVariants): amino-acid level variant consequences.\n\n        Returns:\n            VariantIndex: where amino-acid causing variants are enriched with extra annotation\n        \"\"\"\n        w = Window.partitionBy(\"variantId\").orderBy(f.size(\"variantEffect\").desc())\n\n        return VariantIndex(\n            _df=self.df\n            # Extracting variant consequence on Uniprot and amino-acid changes from the transcripts:\n            .withColumns(\n                {\n                    \"aminoAcidChange\": f.filter(\n                        \"transcriptConsequences\",\n                        lambda vep: vep.aminoAcidChange.isNotNull(),\n                    )[0].aminoAcidChange,\n                    \"uniprotAccession\": f.explode_outer(\n                        f.filter(\n                            \"transcriptConsequences\",\n                            lambda vep: vep.aminoAcidChange.isNotNull(),\n                        )[0].uniprotAccessions\n                    ),\n                }\n            )\n            # Joining with amino-acid predictions:\n            .join(\n                annotation.df.withColumnRenamed(\"variantEffect\", \"annotations\"),\n                on=[\"uniprotAccession\", \"aminoAcidChange\"],\n                how=\"left\",\n            )\n            # Merge predictors:\n            .withColumn(\n                \"variantEffect\",\n                f.when(\n                    f.col(\"annotations\").isNotNull(),\n                    f.array_union(\"variantEffect\", \"annotations\"),\n                ).otherwise(f.col(\"variantEffect\")),\n            )\n            # Dropping unused columns:\n            .drop(\"uniprotAccession\", \"aminoAcidChange\", \"annotations\")\n            # Dropping potentially exploded variant rows:\n            .distinct()\n            .withColumn(\"rank\", f.row_number().over(w))\n            .filter(f.col(\"rank\") == 1)\n            .drop(\"rank\"),\n            _schema=self.get_schema(),\n        )\n\n    def get_loftee(self: VariantIndex) -&gt; DataFrame:\n        \"\"\"Returns a dataframe with a flag indicating whether a variant is predicted to cause loss of function in a gene. The source of this information is the LOFTEE algorithm (https://github.com/konradjk/loftee).\n\n        !!! note, \"This will return a filtered dataframe with only variants that have been annotated by LOFTEE.\"\n\n        Returns:\n            DataFrame: variant to gene assignments from the LOFTEE algorithm\n        \"\"\"\n        return (\n            self.df.select(\"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\"))\n            .filter(f.col(\"tc.lofteePrediction\").isNotNull())\n            .withColumn(\n                \"isHighQualityPlof\",\n                f.when(f.col(\"tc.lofteePrediction\") == \"HC\", True).when(\n                    f.col(\"tc.lofteePrediction\") == \"LC\", False\n                ),\n            )\n            .select(\n                \"variantId\",\n                f.col(\"tc.targetId\"),\n                f.col(\"tc.lofteePrediction\"),\n                \"isHighQualityPlof\",\n            )\n        )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.add_annotation","title":"<code>add_annotation(annotation_source: VariantIndex) -&gt; VariantIndex</code>","text":"<p>Import annotation from an other variant index dataset.</p> <p>At this point the annotation can be extended with extra cross-references, variant effects, allele frequencies, and variant descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_source</code> <code>VariantIndex</code> <p>Annotation to add to the dataset</p> required <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>VariantIndex dataset with the annotation added</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def add_annotation(\n    self: VariantIndex, annotation_source: VariantIndex\n) -&gt; VariantIndex:\n    \"\"\"Import annotation from an other variant index dataset.\n\n    At this point the annotation can be extended with extra cross-references,\n    variant effects, allele frequencies, and variant descriptions.\n\n    Args:\n        annotation_source (VariantIndex): Annotation to add to the dataset\n\n    Returns:\n        VariantIndex: VariantIndex dataset with the annotation added\n    \"\"\"\n    # Prefix for renaming columns:\n    prefix = \"annotation_\"\n\n    # Generate select expressions that to merge and import columns from annotation:\n    select_expressions = []\n\n    # Collect columns by iterating over the variant index schema:\n    for schema_field in VariantIndex.get_schema():\n        column = schema_field.name\n\n        # If an annotation column can be found in both datasets:\n        if (column in self.df.columns) and (column in annotation_source.df.columns):\n            # Arrays are merged:\n            if isinstance(schema_field.dataType, t.ArrayType):\n                fields_order = None\n                if isinstance(schema_field.dataType.elementType, t.StructType):\n                    # Extract the schema of the array to get the order of the fields:\n                    array_schema = [\n                        schema_field\n                        for schema_field in VariantIndex.get_schema().fields\n                        if schema_field.name == column\n                    ][0].dataType\n                    fields_order = get_nested_struct_schema(\n                        array_schema\n                    ).fieldNames()\n                select_expressions.append(\n                    safe_array_union(\n                        f.col(column), f.col(f\"{prefix}{column}\"), fields_order\n                    ).alias(column)\n                )\n            # variantDescription columns are concatenated:\n            elif column == \"variantDescription\":\n                select_expressions.append(\n                    f.concat_ws(\n                        \" \", f.col(column), f.col(f\"{prefix}{column}\")\n                    ).alias(column)\n                )\n            # All other non-array columns are coalesced:\n            else:\n                select_expressions.append(\n                    f.coalesce(f.col(column), f.col(f\"{prefix}{column}\")).alias(\n                        column\n                    )\n                )\n        # If the column is only found in the annotation dataset rename it:\n        elif column in annotation_source.df.columns:\n            select_expressions.append(f.col(f\"{prefix}{column}\").alias(column))\n        # If the column is only found in the main dataset:\n        elif column in self.df.columns:\n            select_expressions.append(f.col(column))\n        # VariantIndex columns not found in either dataset are ignored.\n\n    # Join the annotation to the dataset:\n    return VariantIndex(\n        _df=(\n            f.broadcast(self.df)\n            .join(\n                rename_all_columns(annotation_source.df, prefix),\n                on=[f.col(\"variantId\") == f.col(f\"{prefix}variantId\")],\n                how=\"left\",\n            )\n            .select(*select_expressions)\n        ),\n        _schema=self.schema,\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.annotate_with_amino_acid_consequences","title":"<code>annotate_with_amino_acid_consequences(annotation: AminoAcidVariants) -&gt; VariantIndex</code>","text":"<p>Enriching variant effect assessments with amino-acid derived predicted consequences.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>AminoAcidVariants</code> <p>amino-acid level variant consequences.</p> required <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>where amino-acid causing variants are enriched with extra annotation</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def annotate_with_amino_acid_consequences(\n    self: VariantIndex, annotation: AminoAcidVariants\n) -&gt; VariantIndex:\n    \"\"\"Enriching variant effect assessments with amino-acid derived predicted consequences.\n\n    Args:\n        annotation (AminoAcidVariants): amino-acid level variant consequences.\n\n    Returns:\n        VariantIndex: where amino-acid causing variants are enriched with extra annotation\n    \"\"\"\n    w = Window.partitionBy(\"variantId\").orderBy(f.size(\"variantEffect\").desc())\n\n    return VariantIndex(\n        _df=self.df\n        # Extracting variant consequence on Uniprot and amino-acid changes from the transcripts:\n        .withColumns(\n            {\n                \"aminoAcidChange\": f.filter(\n                    \"transcriptConsequences\",\n                    lambda vep: vep.aminoAcidChange.isNotNull(),\n                )[0].aminoAcidChange,\n                \"uniprotAccession\": f.explode_outer(\n                    f.filter(\n                        \"transcriptConsequences\",\n                        lambda vep: vep.aminoAcidChange.isNotNull(),\n                    )[0].uniprotAccessions\n                ),\n            }\n        )\n        # Joining with amino-acid predictions:\n        .join(\n            annotation.df.withColumnRenamed(\"variantEffect\", \"annotations\"),\n            on=[\"uniprotAccession\", \"aminoAcidChange\"],\n            how=\"left\",\n        )\n        # Merge predictors:\n        .withColumn(\n            \"variantEffect\",\n            f.when(\n                f.col(\"annotations\").isNotNull(),\n                f.array_union(\"variantEffect\", \"annotations\"),\n            ).otherwise(f.col(\"variantEffect\")),\n        )\n        # Dropping unused columns:\n        .drop(\"uniprotAccession\", \"aminoAcidChange\", \"annotations\")\n        # Dropping potentially exploded variant rows:\n        .distinct()\n        .withColumn(\"rank\", f.row_number().over(w))\n        .filter(f.col(\"rank\") == 1)\n        .drop(\"rank\"),\n        _schema=self.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.filter_by_variant","title":"<code>filter_by_variant(df: DataFrame) -&gt; VariantIndex</code>","text":"<p>Filter variant annotation dataset by a variant dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe of variants.</p> required <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>A filtered variant annotation dataset.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>When the variant dataframe does not contain eiter <code>variantId</code> or <code>chromosome</code> column.</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def filter_by_variant(self: VariantIndex, df: DataFrame) -&gt; VariantIndex:\n    \"\"\"Filter variant annotation dataset by a variant dataframe.\n\n    Args:\n        df (DataFrame): A dataframe of variants.\n\n    Returns:\n        VariantIndex: A filtered variant annotation dataset.\n\n    Raises:\n        AssertionError: When the variant dataframe does not contain eiter `variantId` or `chromosome` column.\n    \"\"\"\n    join_columns = [\"variantId\", \"chromosome\"]\n\n    assert all(\n        col in df.columns for col in join_columns\n    ), \"The variant dataframe must contain the columns 'variantId' and 'chromosome'.\"\n\n    return VariantIndex(\n        _df=self._df.join(\n            f.broadcast(df.select(*join_columns).distinct()),\n            on=join_columns,\n            how=\"inner\",\n        ),\n        _schema=self.schema,\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.get_distance_to_gene","title":"<code>get_distance_to_gene(*, distance_type: str = 'distanceFromTss', max_distance: int = 500000) -&gt; DataFrame</code>","text":"<p>Extracts variant to gene assignments for variants falling within a window of a gene's TSS or footprint.</p> <p>Parameters:</p> Name Type Description Default <code>distance_type</code> <code>str</code> <p>The type of distance to use. Can be \"distanceFromTss\" or \"distanceFromFootprint\". Defaults to \"distanceFromTss\".</p> <code>'distanceFromTss'</code> <code>max_distance</code> <code>int</code> <p>The maximum distance to consider. Defaults to 500_000, the default window size for VEP.</p> <code>500000</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the distance between a variant and a gene's TSS or footprint.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Invalid distance type.</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def get_distance_to_gene(\n    self: VariantIndex,\n    *,\n    distance_type: str = \"distanceFromTss\",\n    max_distance: int = 500_000,\n) -&gt; DataFrame:\n    \"\"\"Extracts variant to gene assignments for variants falling within a window of a gene's TSS or footprint.\n\n    Args:\n        distance_type (str): The type of distance to use. Can be \"distanceFromTss\" or \"distanceFromFootprint\". Defaults to \"distanceFromTss\".\n        max_distance (int): The maximum distance to consider. Defaults to 500_000, the default window size for VEP.\n\n    Returns:\n        DataFrame: A dataframe with the distance between a variant and a gene's TSS or footprint.\n\n    Raises:\n        ValueError: Invalid distance type.\n    \"\"\"\n    if distance_type not in {\"distanceFromTss\", \"distanceFromFootprint\"}:\n        raise ValueError(\n            f\"Invalid distance_type: {distance_type}. Must be 'distanceFromTss' or 'distanceFromFootprint'.\"\n        )\n    df = self.df.select(\n        \"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\")\n    ).select(\"variantId\", \"tc.targetId\", f\"tc.{distance_type}\")\n    if max_distance == 500_000:\n        return df\n    elif max_distance &lt; 500_000:\n        return df.filter(f\"{distance_type} &lt;= {max_distance}\")\n    else:\n        raise ValueError(\n            f\"max_distance must be less than 500_000. Got {max_distance}.\"\n        )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.get_loftee","title":"<code>get_loftee() -&gt; DataFrame</code>","text":"<p>Returns a dataframe with a flag indicating whether a variant is predicted to cause loss of function in a gene. The source of this information is the LOFTEE algorithm (https://github.com/konradjk/loftee).</p> <p>!!! note, \"This will return a filtered dataframe with only variants that have been annotated by LOFTEE.\"</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>variant to gene assignments from the LOFTEE algorithm</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def get_loftee(self: VariantIndex) -&gt; DataFrame:\n    \"\"\"Returns a dataframe with a flag indicating whether a variant is predicted to cause loss of function in a gene. The source of this information is the LOFTEE algorithm (https://github.com/konradjk/loftee).\n\n    !!! note, \"This will return a filtered dataframe with only variants that have been annotated by LOFTEE.\"\n\n    Returns:\n        DataFrame: variant to gene assignments from the LOFTEE algorithm\n    \"\"\"\n    return (\n        self.df.select(\"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\"))\n        .filter(f.col(\"tc.lofteePrediction\").isNotNull())\n        .withColumn(\n            \"isHighQualityPlof\",\n            f.when(f.col(\"tc.lofteePrediction\") == \"HC\", True).when(\n                f.col(\"tc.lofteePrediction\") == \"LC\", False\n            ),\n        )\n        .select(\n            \"variantId\",\n            f.col(\"tc.targetId\"),\n            f.col(\"tc.lofteePrediction\"),\n            \"isHighQualityPlof\",\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the variant index dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the VariantIndex dataset</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[VariantIndex]) -&gt; StructType:\n    \"\"\"Provides the schema for the variant index dataset.\n\n    Returns:\n        StructType: Schema for the VariantIndex dataset\n    \"\"\"\n    return parse_spark_schema(\"variant_index.json\")\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.hash_long_variant_ids","title":"<code>hash_long_variant_ids(variant_id: Column, chromosome: Column, position: Column, threshold: int) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Hash long variant identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>variant_id</code> <code>Column</code> <p>Column containing variant identifiers.</p> required <code>chromosome</code> <code>Column</code> <p>Chromosome column.</p> required <code>position</code> <code>Column</code> <p>position column.</p> required <code>threshold</code> <code>int</code> <p>Above this limit, a hash will be generated.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Hashed variant identifiers for long variants.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; (\n...    spark.createDataFrame([('v_short', 'x', 23),('v_looooooong', '23', 23), ('no_chrom', None, None), (None, None, None)], ['variantId', 'chromosome', 'position'])\n...    .select('variantId', VariantIndex.hash_long_variant_ids(f.col('variantId'), f.col('chromosome'), f.col('position'), 10).alias('hashedVariantId'))\n...    .show(truncate=False)\n... )\n+------------+--------------------------------------------+\n|variantId   |hashedVariantId                             |\n+------------+--------------------------------------------+\n|v_short     |v_short                                     |\n|v_looooooong|OTVAR_23_23_3749d019d645894770c364992ae70a05|\n|no_chrom    |OTVAR_41acfcd7d4fd523b33600b504914ef25      |\n|NULL        |NULL                                        |\n+------------+--------------------------------------------+\n</code></pre> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>@staticmethod\ndef hash_long_variant_ids(\n    variant_id: Column, chromosome: Column, position: Column, threshold: int\n) -&gt; Column:\n    \"\"\"Hash long variant identifiers.\n\n    Args:\n        variant_id (Column): Column containing variant identifiers.\n        chromosome (Column): Chromosome column.\n        position (Column): position column.\n        threshold (int): Above this limit, a hash will be generated.\n\n    Returns:\n        Column: Hashed variant identifiers for long variants.\n\n    Examples:\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame([('v_short', 'x', 23),('v_looooooong', '23', 23), ('no_chrom', None, None), (None, None, None)], ['variantId', 'chromosome', 'position'])\n        ...    .select('variantId', VariantIndex.hash_long_variant_ids(f.col('variantId'), f.col('chromosome'), f.col('position'), 10).alias('hashedVariantId'))\n        ...    .show(truncate=False)\n        ... )\n        +------------+--------------------------------------------+\n        |variantId   |hashedVariantId                             |\n        +------------+--------------------------------------------+\n        |v_short     |v_short                                     |\n        |v_looooooong|OTVAR_23_23_3749d019d645894770c364992ae70a05|\n        |no_chrom    |OTVAR_41acfcd7d4fd523b33600b504914ef25      |\n        |NULL        |NULL                                        |\n        +------------+--------------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return (\n        # If either the position or the chromosome is missing, we hash the identifier:\n        f.when(\n            chromosome.isNull() | position.isNull(),\n            f.concat(\n                f.lit(\"OTVAR_\"),\n                f.md5(variant_id).cast(\"string\"),\n            ),\n        )\n        # If chromosome and position are given, but alleles are too long, create hash:\n        .when(\n            f.length(variant_id) &gt;= threshold,\n            f.concat_ws(\n                \"_\",\n                f.lit(\"OTVAR\"),\n                chromosome,\n                position,\n                f.md5(variant_id).cast(\"string\"),\n            ),\n        )\n        # Missing and regular variant identifiers are left unchanged:\n        .otherwise(variant_id)\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#gentropy.dataset.variant_index.VariantIndex.max_maf","title":"<code>max_maf() -&gt; Column</code>","text":"<p>Maximum minor allele frequency accross all populations assuming all variants biallelic.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Maximum minor allele frequency accross all populations.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Allele frequencies are not present in the dataset.</p> Source code in <code>src/gentropy/dataset/variant_index.py</code> <pre><code>def max_maf(self: VariantIndex) -&gt; Column:\n    \"\"\"Maximum minor allele frequency accross all populations assuming all variants biallelic.\n\n    Returns:\n        Column: Maximum minor allele frequency accross all populations.\n\n    Raises:\n        ValueError: Allele frequencies are not present in the dataset.\n    \"\"\"\n    if \"alleleFrequencies\" not in self.df.columns:\n        raise ValueError(\"Allele frequencies are not present in the dataset.\")\n\n    return f.array_max(\n        f.transform(\n            self.df.alleleFrequencies,\n            lambda af: f.when(\n                af.alleleFrequency &gt; 0.5, 1 - af.alleleFrequency\n            ).otherwise(af.alleleFrequency),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/variant_index/#schema","title":"Schema","text":"<pre><code>root\n |-- variantId: string (nullable = false)\n |-- chromosome: string (nullable = false)\n |-- position: integer (nullable = false)\n |-- referenceAllele: string (nullable = false)\n |-- alternateAllele: string (nullable = false)\n |-- variantEffect: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- method: string (nullable = true)\n |    |    |-- assessment: string (nullable = true)\n |    |    |-- score: float (nullable = true)\n |    |    |-- assessmentFlag: string (nullable = true)\n |    |    |-- targetId: string (nullable = true)\n |    |    |-- normalisedScore: double (nullable = true)\n |-- mostSevereConsequenceId: string (nullable = true)\n |-- transcriptConsequences: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- variantFunctionalConsequenceIds: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- aminoAcidChange: string (nullable = true)\n |    |    |-- uniprotAccessions: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- isEnsemblCanonical: boolean (nullable = false)\n |    |    |-- codons: string (nullable = true)\n |    |    |-- distanceFromFootprint: long (nullable = true)\n |    |    |-- distanceFromTss: long (nullable = true)\n |    |    |-- appris: string (nullable = true)\n |    |    |-- maneSelect: string (nullable = true)\n |    |    |-- targetId: string (nullable = true)\n |    |    |-- impact: string (nullable = true)\n |    |    |-- lofteePrediction: string (nullable = true)\n |    |    |-- siftPrediction: float (nullable = true)\n |    |    |-- polyphenPrediction: float (nullable = true)\n |    |    |-- consequenceScore: float (nullable = true)\n |    |    |-- transcriptIndex: integer (nullable = true)\n |    |    |-- approvedSymbol: string (nullable = true)\n |    |    |-- biotype: string (nullable = true)\n |    |    |-- transcriptId: string (nullable = true)\n |-- rsIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- hgvsId: string (nullable = true)\n |-- alleleFrequencies: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- populationName: string (nullable = true)\n |    |    |-- alleleFrequency: double (nullable = true)\n |-- dbXrefs: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- variantDescription: string (nullable = true)\n</code></pre>"},{"location":"python_api/datasets/l2g_features/_l2g_feature/","title":"L2G Feature","text":""},{"location":"python_api/datasets/l2g_features/_l2g_feature/#abstract-class","title":"Abstract Class","text":""},{"location":"python_api/datasets/l2g_features/_l2g_feature/#gentropy.dataset.l2g_features.l2g_feature.L2GFeature","title":"<code>gentropy.dataset.l2g_features.l2g_feature.L2GFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset</code>, <code>ABC</code></p> <p>Locus-to-gene feature dataset that serves as template to generate each of the features that inform about locus to gene assignments.</p> Source code in <code>src/gentropy/dataset/l2g_features/l2g_feature.py</code> <pre><code>@dataclass\nclass L2GFeature(Dataset, ABC):\n    \"\"\"Locus-to-gene feature dataset that serves as template to generate each of the features that inform about locus to gene assignments.\"\"\"\n\n    def __post_init__(\n        self: L2GFeature,\n        feature_dependency_type: Any = None,\n        credible_set: StudyLocus | None = None,\n    ) -&gt; None:\n        \"\"\"Initializes a L2GFeature dataset. Any child class of L2GFeature must implement the `compute` method.\n\n        Args:\n            feature_dependency_type (Any): The dependency that the L2GFeature dataset depends on. Defaults to None.\n            credible_set (StudyLocus | None): The credible set that the L2GFeature dataset is based on. Defaults to None.\n        \"\"\"\n        super().__post_init__()\n        self.feature_dependency_type = feature_dependency_type\n        self.credible_set = credible_set\n\n    @classmethod\n    def get_schema(cls: type[L2GFeature]) -&gt; StructType:\n        \"\"\"Provides the schema for the L2GFeature dataset.\n\n        Returns:\n            StructType: Schema for the L2GFeature dataset\n        \"\"\"\n        return parse_spark_schema(\"l2g_feature.json\")\n\n    @classmethod\n    @abstractmethod\n    def compute(\n        cls: type[L2GFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: Any,\n    ) -&gt; L2GFeature:\n        \"\"\"Computes the L2GFeature dataset.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (Any): The dependency that the L2GFeature class needs to compute the feature\n        Returns:\n            L2GFeature: a L2GFeature dataset\n\n        Raises:\n                NotImplementedError: This method must be implemented in the child classes\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in the child classes\")\n</code></pre>"},{"location":"python_api/datasets/l2g_features/_l2g_feature/#gentropy.dataset.l2g_features.l2g_feature.L2GFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: Any) -&gt; L2GFeature</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Computes the L2GFeature dataset.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>Any</code> <p>The dependency that the L2GFeature class needs to compute the feature</p> required <p>Returns:     L2GFeature: a L2GFeature dataset</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented in the child classes</p> Source code in <code>src/gentropy/dataset/l2g_features/l2g_feature.py</code> <pre><code>@classmethod\n@abstractmethod\ndef compute(\n    cls: type[L2GFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: Any,\n) -&gt; L2GFeature:\n    \"\"\"Computes the L2GFeature dataset.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (Any): The dependency that the L2GFeature class needs to compute the feature\n    Returns:\n        L2GFeature: a L2GFeature dataset\n\n    Raises:\n            NotImplementedError: This method must be implemented in the child classes\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in the child classes\")\n</code></pre>"},{"location":"python_api/datasets/l2g_features/_l2g_feature/#gentropy.dataset.l2g_features.l2g_feature.L2GFeature.get_schema","title":"<code>get_schema() -&gt; StructType</code>  <code>classmethod</code>","text":"<p>Provides the schema for the L2GFeature dataset.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>Schema for the L2GFeature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/l2g_feature.py</code> <pre><code>@classmethod\ndef get_schema(cls: type[L2GFeature]) -&gt; StructType:\n    \"\"\"Provides the schema for the L2GFeature dataset.\n\n    Returns:\n        StructType: Schema for the L2GFeature dataset\n    \"\"\"\n    return parse_spark_schema(\"l2g_feature.json\")\n</code></pre>"},{"location":"python_api/datasets/l2g_features/_l2g_feature/#schema","title":"Schema","text":"<pre><code>root\n |-- studyLocusId: string (nullable = false)\n |-- geneId: string (nullable = false)\n |-- featureName: string (nullable = false)\n |-- featureValue: float (nullable = false)\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/","title":"From colocalisation","text":""},{"location":"python_api/datasets/l2g_features/colocalisation/#list-of-features","title":"List of features","text":""},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all eQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class EQtlColocClppMaximumFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all eQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"eQtlColocClppMaximum\"\n\n    @classmethod\n    def compute(\n        cls: type[EQtlColocClppMaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; EQtlColocClppMaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary with the dependencies required. They are passed as keyword arguments.\n\n        Returns:\n            EQtlColocClppMaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_type = [\"eqtl\", \"sceqtl\"]\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; EQtlColocClppMaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary with the dependencies required. They are passed as keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>EQtlColocClppMaximumFeature</code> <code>EQtlColocClppMaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[EQtlColocClppMaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; EQtlColocClppMaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary with the dependencies required. They are passed as keyword arguments.\n\n    Returns:\n        EQtlColocClppMaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_type = [\"eqtl\", \"sceqtl\"]\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all pQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class PQtlColocClppMaximumFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all pQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"pQtlColocClppMaximum\"\n\n    @classmethod\n    def compute(\n        cls: type[PQtlColocClppMaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; PQtlColocClppMaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            PQtlColocClppMaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_type = \"pqtl\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; PQtlColocClppMaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>PQtlColocClppMaximumFeature</code> <code>PQtlColocClppMaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[PQtlColocClppMaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; PQtlColocClppMaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        PQtlColocClppMaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_type = \"pqtl\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all sQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class SQtlColocClppMaximumFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all sQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"sQtlColocClppMaximum\"\n\n    @classmethod\n    def compute(\n        cls: type[SQtlColocClppMaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; SQtlColocClppMaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            SQtlColocClppMaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_types,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; SQtlColocClppMaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>SQtlColocClppMaximumFeature</code> <code>SQtlColocClppMaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[SQtlColocClppMaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; SQtlColocClppMaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        SQtlColocClppMaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_types,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus, gene) aggregating over all eQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class EQtlColocH4MaximumFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus, gene) aggregating over all eQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"eQtlColocH4Maximum\"\n\n    @classmethod\n    def compute(\n        cls: type[EQtlColocH4MaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; EQtlColocH4MaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            EQtlColocH4MaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_type = [\"eqtl\", \"sceqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; EQtlColocH4MaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>EQtlColocH4MaximumFeature</code> <code>EQtlColocH4MaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[EQtlColocH4MaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; EQtlColocH4MaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        EQtlColocH4MaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_type = [\"eqtl\", \"sceqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus, gene) aggregating over all pQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class PQtlColocH4MaximumFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus, gene) aggregating over all pQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"pQtlColocH4Maximum\"\n\n    @classmethod\n    def compute(\n        cls: type[PQtlColocH4MaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; PQtlColocH4MaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            PQtlColocH4MaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_type = \"pqtl\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; PQtlColocH4MaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>PQtlColocH4MaximumFeature</code> <code>PQtlColocH4MaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[PQtlColocH4MaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; PQtlColocH4MaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        PQtlColocH4MaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_type = \"pqtl\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus, gene) aggregating over all sQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class SQtlColocH4MaximumFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus, gene) aggregating over all sQTLs.\"\"\"\n\n    feature_dependency_type = [Colocalisation, StudyIndex, StudyLocus]\n    feature_name = \"sQtlColocH4Maximum\"\n\n    @classmethod\n    def compute(\n        cls: type[SQtlColocH4MaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; SQtlColocH4MaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            SQtlColocH4MaximumFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_types,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; SQtlColocH4MaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>SQtlColocH4MaximumFeature</code> <code>SQtlColocH4MaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[SQtlColocH4MaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; SQtlColocH4MaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        SQtlColocH4MaximumFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_types,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus) aggregating over all eQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class EQtlColocClppMaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus) aggregating over all eQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"eQtlColocClppMaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[EQtlColocClppMaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; EQtlColocClppMaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary with the dependencies required. They are passed as keyword arguments.\n\n        Returns:\n            EQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_type = [\"eqtl\", \"sceqtl\"]\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocClppMaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; EQtlColocClppMaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary with the dependencies required. They are passed as keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>EQtlColocClppMaximumNeighbourhoodFeature</code> <code>EQtlColocClppMaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[EQtlColocClppMaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; EQtlColocClppMaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary with the dependencies required. They are passed as keyword arguments.\n\n    Returns:\n        EQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_type = [\"eqtl\", \"sceqtl\"]\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all pQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class PQtlColocClppMaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all pQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"pQtlColocClppMaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[PQtlColocClppMaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; PQtlColocClppMaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            PQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_type = \"pqtl\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocClppMaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; PQtlColocClppMaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>PQtlColocClppMaximumNeighbourhoodFeature</code> <code>PQtlColocClppMaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[PQtlColocClppMaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; PQtlColocClppMaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        PQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_type = \"pqtl\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max CLPP for each (study, locus, gene) aggregating over all sQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class SQtlColocClppMaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max CLPP for each (study, locus, gene) aggregating over all sQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"sQtlColocClppMaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[SQtlColocClppMaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; SQtlColocClppMaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            SQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"ECaviar\"\n        colocalisation_metric = \"clpp\"\n        qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_types,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocClppMaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; SQtlColocClppMaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>SQtlColocClppMaximumNeighbourhoodFeature</code> <code>SQtlColocClppMaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[SQtlColocClppMaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; SQtlColocClppMaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        SQtlColocClppMaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"ECaviar\"\n    colocalisation_metric = \"clpp\"\n    qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_types,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus) aggregating over all eQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class EQtlColocH4MaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus) aggregating over all eQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"eQtlColocH4MaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[EQtlColocH4MaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; EQtlColocH4MaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            EQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_type = [\"eqtl\", \"sceqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.EQtlColocH4MaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; EQtlColocH4MaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>EQtlColocH4MaximumNeighbourhoodFeature</code> <code>EQtlColocH4MaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[EQtlColocH4MaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; EQtlColocH4MaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        EQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_type = [\"eqtl\", \"sceqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus) aggregating over all pQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class PQtlColocH4MaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus) aggregating over all pQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"pQtlColocH4MaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[PQtlColocH4MaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; PQtlColocH4MaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            PQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_type = \"pqtl\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.PQtlColocH4MaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; PQtlColocH4MaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>PQtlColocH4MaximumNeighbourhoodFeature</code> <code>PQtlColocH4MaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[PQtlColocH4MaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; PQtlColocH4MaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        PQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_type = \"pqtl\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Max H4 for each (study, locus) aggregating over all sQTLs.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>class SQtlColocH4MaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Max H4 for each (study, locus) aggregating over all sQTLs.\"\"\"\n\n    feature_dependency_type = [\n        Colocalisation,\n        StudyIndex,\n        TargetIndex,\n        StudyLocus,\n        VariantIndex,\n    ]\n    feature_name = \"sQtlColocH4MaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[SQtlColocH4MaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; SQtlColocH4MaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n        Returns:\n            SQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        colocalisation_method = \"Coloc\"\n        colocalisation_metric = \"h4\"\n        qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_colocalisation_feature_logic(\n                    study_loci_to_annotate,\n                    colocalisation_method,\n                    colocalisation_metric,\n                    cls.feature_name,\n                    qtl_types,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.SQtlColocH4MaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; SQtlColocH4MaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset with the colocalisation results</p> required <p>Returns:</p> Name Type Description <code>SQtlColocH4MaximumNeighbourhoodFeature</code> <code>SQtlColocH4MaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[SQtlColocH4MaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; SQtlColocH4MaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset with the colocalisation results\n\n    Returns:\n        SQtlColocH4MaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    colocalisation_method = \"Coloc\"\n    colocalisation_metric = \"h4\"\n    qtl_types = [\"sqtl\", \"tuqtl\", \"scsqtl\", \"sctuqtl\"]\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_colocalisation_feature_logic(\n                study_loci_to_annotate,\n                colocalisation_method,\n                colocalisation_metric,\n                cls.feature_name,\n                qtl_types,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#common-logic","title":"Common logic","text":""},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.common_colocalisation_feature_logic","title":"<code>gentropy.dataset.l2g_features.colocalisation.common_colocalisation_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, colocalisation_method: str, colocalisation_metric: str, feature_name: str, qtl_types: list[str] | str, *, colocalisation: Colocalisation, study_index: StudyIndex, study_locus: StudyLocus) -&gt; DataFrame</code>","text":"<p>Wrapper to call the logic that creates a type of colocalisation features.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>colocalisation_method</code> <code>str</code> <p>The colocalisation method to filter the data by</p> required <code>colocalisation_metric</code> <code>str</code> <p>The colocalisation metric to use</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature to create</p> required <code>qtl_types</code> <code>list[str] | str</code> <p>The types of QTL to filter the data by</p> required <code>colocalisation</code> <code>Colocalisation</code> <p>Dataset with the colocalisation results</p> required <code>study_index</code> <code>StudyIndex</code> <p>Study index to fetch study type and gene</p> required <code>study_locus</code> <code>StudyLocus</code> <p>Study locus to traverse between colocalisation and study index</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature annotation in long format with the columns: studyLocusId, geneId, featureName, featureValue</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>def common_colocalisation_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    colocalisation_method: str,\n    colocalisation_metric: str,\n    feature_name: str,\n    qtl_types: list[str] | str,\n    *,\n    colocalisation: Colocalisation,\n    study_index: StudyIndex,\n    study_locus: StudyLocus,\n) -&gt; DataFrame:\n    \"\"\"Wrapper to call the logic that creates a type of colocalisation features.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        colocalisation_method (str): The colocalisation method to filter the data by\n        colocalisation_metric (str): The colocalisation metric to use\n        feature_name (str): The name of the feature to create\n        qtl_types (list[str] | str): The types of QTL to filter the data by\n        colocalisation (Colocalisation): Dataset with the colocalisation results\n        study_index (StudyIndex): Study index to fetch study type and gene\n        study_locus (StudyLocus): Study locus to traverse between colocalisation and study index\n\n    Returns:\n        DataFrame: Feature annotation in long format with the columns: studyLocusId, geneId, featureName, featureValue\n    \"\"\"\n    joining_cols = (\n        [\"studyLocusId\", \"geneId\"]\n        if isinstance(study_loci_to_annotate, L2GGoldStandard)\n        else [\"studyLocusId\"]\n    )\n    return (\n        study_loci_to_annotate.df.join(\n            # Remove colocalisation with trans QTLs\n            colocalisation.drop_trans_effects(study_locus)\n            # Extract maximum colocalisation probability per region and gene\n            .extract_maximum_coloc_probability_per_region_and_gene(\n                study_locus,\n                study_index,\n                filter_by_colocalisation_method=colocalisation_method,\n                filter_by_qtls=qtl_types,\n            ),\n            on=joining_cols,\n        )\n        .selectExpr(\n            \"studyLocusId\",\n            \"geneId\",\n            f\"{colocalisation_metric} as {feature_name}\",\n        )\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.extend_missing_colocalisation_to_neighbourhood_genes","title":"<code>gentropy.dataset.l2g_features.colocalisation.extend_missing_colocalisation_to_neighbourhood_genes(feature_name: str, local_features: DataFrame, variant_index: VariantIndex, target_index: TargetIndex, study_locus: StudyLocus) -&gt; DataFrame</code>","text":"<p>This function creates an artificial dataset of features that represents the missing colocalisation to the neighbourhood genes.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>The name of the feature to extend</p> required <code>local_features</code> <code>DataFrame</code> <p>The dataframe of features to extend</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Variant index containing all variant/gene relationships</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index to fetch the gene information</p> required <code>study_locus</code> <code>StudyLocus</code> <p>Study locus to traverse between colocalisation and variant index</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Dataframe of features that include genes in the neighbourhood not present in the colocalisation results. For these genes, the feature value is set to 0.</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>def extend_missing_colocalisation_to_neighbourhood_genes(\n    feature_name: str,\n    local_features: DataFrame,\n    variant_index: VariantIndex,\n    target_index: TargetIndex,\n    study_locus: StudyLocus,\n) -&gt; DataFrame:\n    \"\"\"This function creates an artificial dataset of features that represents the missing colocalisation to the neighbourhood genes.\n\n    Args:\n        feature_name (str): The name of the feature to extend\n        local_features (DataFrame): The dataframe of features to extend\n        variant_index (VariantIndex): Variant index containing all variant/gene relationships\n        target_index (TargetIndex): Target index to fetch the gene information\n        study_locus (StudyLocus): Study locus to traverse between colocalisation and variant index\n\n    Returns:\n        DataFrame: Dataframe of features that include genes in the neighbourhood not present in the colocalisation results. For these genes, the feature value is set to 0.\n    \"\"\"\n    coding_variant_gene_lut = (\n        variant_index.df.select(\n            \"variantId\", f.explode(\"transcriptConsequences\").alias(\"tc\")\n        )\n        .select(f.col(\"tc.targetId\").alias(\"geneId\"), \"variantId\")\n        .join(\n            target_index.df.select(f.col(\"id\").alias(\"geneId\"), \"biotype\"),\n            \"geneId\",\n            \"left\",\n        )\n        .filter(f.col(\"biotype\") == \"protein_coding\")\n        .drop(\"biotype\")\n        .distinct()\n    )\n    local_features_w_variant = local_features.join(\n        study_locus.df.select(\"studyLocusId\", \"variantId\"), \"studyLocusId\"\n    )\n    return (\n        # Get the genes that are not present in the colocalisation results\n        coding_variant_gene_lut.join(\n            local_features_w_variant, [\"variantId\", \"geneId\"], \"left_anti\"\n        )\n        # We now link the missing variant/gene to the study locus from the original dataframe\n        .join(\n            local_features_w_variant.select(\"studyLocusId\", \"variantId\").distinct(),\n            \"variantId\",\n        )\n        .drop(\"variantId\")\n        # Fill the information for missing genes with 0\n        .withColumn(feature_name, f.lit(0.0))\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/colocalisation/#gentropy.dataset.l2g_features.colocalisation.common_neighbourhood_colocalisation_feature_logic","title":"<code>gentropy.dataset.l2g_features.colocalisation.common_neighbourhood_colocalisation_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, colocalisation_method: str, colocalisation_metric: str, feature_name: str, qtl_types: list[str] | str, *, colocalisation: Colocalisation, study_index: StudyIndex, target_index: TargetIndex, study_locus: StudyLocus, variant_index: VariantIndex) -&gt; DataFrame</code>","text":"<p>Wrapper to call the logic that creates a type of colocalisation features.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>colocalisation_method</code> <code>str</code> <p>The colocalisation method to filter the data by</p> required <code>colocalisation_metric</code> <code>str</code> <p>The colocalisation metric to use</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature to create</p> required <code>qtl_types</code> <code>list[str] | str</code> <p>The types of QTL to filter the data by</p> required <code>colocalisation</code> <code>Colocalisation</code> <p>Dataset with the colocalisation results</p> required <code>study_index</code> <code>StudyIndex</code> <p>Study index to fetch study type and gene</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index to add gene type</p> required <code>study_locus</code> <code>StudyLocus</code> <p>Study locus to traverse between colocalisation and study index</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Variant index to annotate all overlapping genes</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature annotation in long format with the columns: studyLocusId, geneId, featureName, featureValue</p> Source code in <code>src/gentropy/dataset/l2g_features/colocalisation.py</code> <pre><code>def common_neighbourhood_colocalisation_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    colocalisation_method: str,\n    colocalisation_metric: str,\n    feature_name: str,\n    qtl_types: list[str] | str,\n    *,\n    colocalisation: Colocalisation,\n    study_index: StudyIndex,\n    target_index: TargetIndex,\n    study_locus: StudyLocus,\n    variant_index: VariantIndex,\n) -&gt; DataFrame:\n    \"\"\"Wrapper to call the logic that creates a type of colocalisation features.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        colocalisation_method (str): The colocalisation method to filter the data by\n        colocalisation_metric (str): The colocalisation metric to use\n        feature_name (str): The name of the feature to create\n        qtl_types (list[str] | str): The types of QTL to filter the data by\n        colocalisation (Colocalisation): Dataset with the colocalisation results\n        study_index (StudyIndex): Study index to fetch study type and gene\n        target_index (TargetIndex): Target index to add gene type\n        study_locus (StudyLocus): Study locus to traverse between colocalisation and study index\n        variant_index (VariantIndex): Variant index to annotate all overlapping genes\n\n    Returns:\n        DataFrame: Feature annotation in long format with the columns: studyLocusId, geneId, featureName, featureValue\n    \"\"\"\n    # First maximum colocalisation score for each studylocus, gene\n    local_feature_name = feature_name.replace(\"Neighbourhood\", \"\")\n    local_max = common_colocalisation_feature_logic(\n        study_loci_to_annotate,\n        colocalisation_method,\n        colocalisation_metric,\n        local_feature_name,\n        qtl_types,\n        colocalisation=colocalisation,\n        study_index=study_index,\n        study_locus=study_locus,\n    )\n    extended_local_max = local_max.unionByName(\n        extend_missing_colocalisation_to_neighbourhood_genes(\n            local_feature_name,\n            local_max,\n            variant_index,\n            target_index,\n            study_locus,\n        )\n    )\n    return (\n        extended_local_max.join(\n            # Compute average score in the vicinity (feature will be the same for any gene associated with a studyLocus)\n            # (non protein coding genes in the vicinity are excluded see #3552)\n            target_index.df.filter(f.col(\"biotype\") == \"protein_coding\").select(\n                f.col(\"id\").alias(\"geneId\")\n            ),\n            \"geneId\",\n            \"inner\",\n        )\n        .withColumn(\n            \"regional_max\",\n            f.max(local_feature_name).over(Window.partitionBy(\"studyLocusId\")),\n        )\n        .withColumn(\n            feature_name,\n            f.when(\n                (f.col(\"regional_max\").isNotNull()) &amp; (f.col(\"regional_max\") != 0.0),\n                f.col(local_feature_name)\n                / f.coalesce(f.col(\"regional_max\"), f.lit(0.0)),\n            ).otherwise(f.lit(0.0)),\n        )\n        .drop(\"regional_max\", local_feature_name)\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/","title":"From distance","text":""},{"location":"python_api/datasets/l2g_features/distance/#list-of-features","title":"List of features","text":""},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelTssFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceSentinelTssFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance of the sentinel variant to gene TSS. This is not weighted by the causal probability.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceSentinelTssFeature(L2GFeature):\n    \"\"\"Distance of the sentinel variant to gene TSS. This is not weighted by the causal probability.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"distanceSentinelTss\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceSentinelTssFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceSentinelTssFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceSentinelTssFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromTss\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelTssFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceSentinelTssFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceSentinelTssFeature</code> <code>DistanceSentinelTssFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceSentinelTssFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceSentinelTssFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceSentinelTssFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromTss\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelTssNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceSentinelTssNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance between the sentinel variant and a gene TSS as a relation of the distnace with all the genes in the vicinity of a studyLocus. This is not weighted by the causal probability.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceSentinelTssNeighbourhoodFeature(L2GFeature):\n    \"\"\"Distance between the sentinel variant and a gene TSS as a relation of the distnace with all the genes in the vicinity of a studyLocus. This is not weighted by the causal probability.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"distanceSentinelTssNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceSentinelTssNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceSentinelTssNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceSentinelTssNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromTss\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelTssNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceSentinelTssNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceSentinelTssNeighbourhoodFeature</code> <code>DistanceSentinelTssNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceSentinelTssNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceSentinelTssNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceSentinelTssNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromTss\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceTssMeanFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceTssMeanFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Average distance of all tagging variants to gene TSS.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceTssMeanFeature(L2GFeature):\n    \"\"\"Average distance of all tagging variants to gene TSS.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"distanceTssMean\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceTssMeanFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceTssMeanFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceTssMeanFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromTss\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ).withColumn(\n                    cls.feature_name,\n                    f.when(f.col(cls.feature_name) &lt; 0, f.lit(0.0)).otherwise(\n                        f.col(cls.feature_name)\n                    ),\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceTssMeanFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceTssMeanFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceTssMeanFeature</code> <code>DistanceTssMeanFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceTssMeanFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceTssMeanFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceTssMeanFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromTss\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ).withColumn(\n                cls.feature_name,\n                f.when(f.col(cls.feature_name) &lt; 0, f.lit(0.0)).otherwise(\n                    f.col(cls.feature_name)\n                ),\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceTssMeanNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceTssMeanNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Minimum mean distance to TSS for all genes in the vicinity of a studyLocus.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceTssMeanNeighbourhoodFeature(L2GFeature):\n    \"\"\"Minimum mean distance to TSS for all genes in the vicinity of a studyLocus.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"distanceTssMeanNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceTssMeanNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceTssMeanNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceTssMeanNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromTss\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceTssMeanNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceTssMeanNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceTssMeanNeighbourhoodFeature</code> <code>DistanceTssMeanNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceTssMeanNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceTssMeanNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceTssMeanNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromTss\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance between the sentinel variant and the footprint of a gene.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceSentinelFootprintFeature(L2GFeature):\n    \"\"\"Distance between the sentinel variant and the footprint of a gene.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"distanceSentinelFootprint\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceSentinelFootprintFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceSentinelFootprintFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceSentinelFootprintFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromFootprint\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceSentinelFootprintFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceSentinelFootprintFeature</code> <code>DistanceSentinelFootprintFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceSentinelFootprintFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceSentinelFootprintFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceSentinelFootprintFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromFootprint\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance between the sentinel variant and a gene footprint as a relation of the distnace with all the genes in the vicinity of a studyLocus. This is not weighted by the causal probability.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceSentinelFootprintNeighbourhoodFeature(L2GFeature):\n    \"\"\"Distance between the sentinel variant and a gene footprint as a relation of the distnace with all the genes in the vicinity of a studyLocus. This is not weighted by the causal probability.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"distanceSentinelFootprintNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceSentinelFootprintNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceSentinelFootprintNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceSentinelFootprintNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromFootprint\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceSentinelFootprintNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceSentinelFootprintNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceSentinelFootprintNeighbourhoodFeature</code> <code>DistanceSentinelFootprintNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceSentinelFootprintNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceSentinelFootprintNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceSentinelFootprintNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromFootprint\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceFootprintMeanFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceFootprintMeanFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Average distance of all tagging variants to the footprint of a gene.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceFootprintMeanFeature(L2GFeature):\n    \"\"\"Average distance of all tagging variants to the footprint of a gene.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"distanceFootprintMean\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceFootprintMeanFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceFootprintMeanFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceFootprintMeanFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromFootprint\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ).withColumn(\n                    cls.feature_name,\n                    f.when(f.col(cls.feature_name) &lt; 0, f.lit(0.0)).otherwise(\n                        f.col(cls.feature_name)\n                    ),\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceFootprintMeanFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceFootprintMeanFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceFootprintMeanFeature</code> <code>DistanceFootprintMeanFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceFootprintMeanFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceFootprintMeanFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceFootprintMeanFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromFootprint\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ).withColumn(\n                cls.feature_name,\n                f.when(f.col(cls.feature_name) &lt; 0, f.lit(0.0)).otherwise(\n                    f.col(cls.feature_name)\n                ),\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceFootprintMeanNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.distance.DistanceFootprintMeanNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Minimum mean distance to footprint for all genes in the vicinity of a studyLocus.</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>class DistanceFootprintMeanNeighbourhoodFeature(L2GFeature):\n    \"\"\"Minimum mean distance to footprint for all genes in the vicinity of a studyLocus.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"distanceFootprintMeanNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[DistanceFootprintMeanNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; DistanceFootprintMeanNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            DistanceFootprintMeanNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        distance_type = \"distanceFromFootprint\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_distance_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    distance_type=distance_type,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.DistanceFootprintMeanNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; DistanceFootprintMeanNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>DistanceFootprintMeanNeighbourhoodFeature</code> <code>DistanceFootprintMeanNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[DistanceFootprintMeanNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; DistanceFootprintMeanNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        DistanceFootprintMeanNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    distance_type = \"distanceFromFootprint\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_distance_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                distance_type=distance_type,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#common-logic","title":"Common logic","text":""},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.common_distance_feature_logic","title":"<code>gentropy.dataset.l2g_features.distance.common_distance_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, variant_index: VariantIndex, feature_name: str, distance_type: str, genomic_window: int = 500000) -&gt; DataFrame</code>","text":"<p>Calculate the distance feature that correlates a variant in a credible set with a gene.</p> <p>The distance is weighted by the posterior probability of the variant to factor in its contribution to the trait when we look at the average distance score for all variants in the credible set.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>The dataset containing distance to gene information</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <code>distance_type</code> <code>str</code> <p>The type of distance to gene</p> required <code>genomic_window</code> <code>int</code> <p>The maximum window size to consider</p> <code>500000</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>def common_distance_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    variant_index: VariantIndex,\n    feature_name: str,\n    distance_type: str,\n    genomic_window: int = 500_000,\n) -&gt; DataFrame:\n    \"\"\"Calculate the distance feature that correlates a variant in a credible set with a gene.\n\n    The distance is weighted by the posterior probability of the variant to factor in its contribution to the trait when we look at the average distance score for all variants in the credible set.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        variant_index (VariantIndex): The dataset containing distance to gene information\n        feature_name (str): The name of the feature\n        distance_type (str): The type of distance to gene\n        genomic_window (int): The maximum window size to consider\n\n    Returns:\n        DataFrame: Feature dataset\n    \"\"\"\n    distances_dataset = variant_index.get_distance_to_gene(distance_type=distance_type)\n    if \"Mean\" in feature_name:\n        # Weighting by the SNP contribution is only applied when we are averaging all distances\n        df = study_loci_to_annotate.df.withColumn(\n            \"variantInLocus\", f.explode_outer(\"locus\")\n        ).select(\n            \"studyLocusId\",\n            f.col(\"variantInLocus.variantId\").alias(\"variantId\"),\n            f.col(\"variantInLocus.posteriorProbability\").alias(\"posteriorProbability\"),\n        )\n        distance_score_expr = (\n            f.lit(genomic_window) - f.abs(distance_type) + f.lit(1)\n        ) * f.col(\"posteriorProbability\")\n        agg_expr = f.sum(f.col(\"distance_score\"))\n    elif \"Sentinel\" in feature_name:\n        df = study_loci_to_annotate.df.select(\"studyLocusId\", \"variantId\")\n        # For minimum distances we calculate the unweighted distance between the sentinel (lead) and the gene.\n        distance_score_expr = f.lit(genomic_window) - f.abs(distance_type) + f.lit(1)\n        agg_expr = f.first(f.col(\"distance_score\"))\n    return (\n        df.join(\n            distances_dataset.withColumnRenamed(\"targetId\", \"geneId\"),\n            on=\"variantId\",\n            how=\"inner\",\n        )\n        .withColumn(\n            \"distance_score\",\n            distance_score_expr,\n        )\n        .groupBy(\"studyLocusId\", \"geneId\")\n        .agg(agg_expr.alias(\"distance_score_agg\"))\n        .withColumn(\n            feature_name,\n            f.log10(f.col(\"distance_score_agg\")) / f.log10(f.lit(genomic_window + 1)),\n        )\n        .drop(\"distance_score_agg\")\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/distance/#gentropy.dataset.l2g_features.distance.common_neighbourhood_distance_feature_logic","title":"<code>gentropy.dataset.l2g_features.distance.common_neighbourhood_distance_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, variant_index: VariantIndex, feature_name: str, distance_type: str, target_index: TargetIndex, genomic_window: int = 500000) -&gt; DataFrame</code>","text":"<p>Calculate the distance feature that correlates any variant in a credible set with any protein coding gene nearby the locus. The distance is weighted by the posterior probability of the variant to factor in its contribution to the trait.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>The dataset containing distance to gene information</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <code>distance_type</code> <code>str</code> <p>The type of distance to gene</p> required <code>target_index</code> <code>TargetIndex</code> <p>The dataset containing gene information</p> required <code>genomic_window</code> <code>int</code> <p>The maximum window size to consider</p> <code>500000</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/distance.py</code> <pre><code>def common_neighbourhood_distance_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    variant_index: VariantIndex,\n    feature_name: str,\n    distance_type: str,\n    target_index: TargetIndex,\n    genomic_window: int = 500_000,\n) -&gt; DataFrame:\n    \"\"\"Calculate the distance feature that correlates any variant in a credible set with any protein coding gene nearby the locus. The distance is weighted by the posterior probability of the variant to factor in its contribution to the trait.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        variant_index (VariantIndex): The dataset containing distance to gene information\n        feature_name (str): The name of the feature\n        distance_type (str): The type of distance to gene\n        target_index (TargetIndex): The dataset containing gene information\n        genomic_window (int): The maximum window size to consider\n\n    Returns:\n        DataFrame: Feature dataset\n    \"\"\"\n    local_feature_name = feature_name.replace(\"Neighbourhood\", \"\")\n    # First compute mean distances to a gene\n    local_metric = common_distance_feature_logic(\n        study_loci_to_annotate,\n        feature_name=local_feature_name,\n        distance_type=distance_type,\n        variant_index=variant_index,\n        genomic_window=genomic_window,\n    )\n    return (\n        # Then compute mean distance in the vicinity (feature will be the same for any gene associated with a studyLocus)\n        local_metric.join(\n            target_index.df.filter(f.col(\"biotype\") == \"protein_coding\").select(\n                f.col(\"id\").alias(\"geneId\")\n            ),\n            \"geneId\",\n            \"inner\",\n        )\n        .withColumn(\n            \"regional_max\",\n            f.max(local_feature_name).over(Window.partitionBy(\"studyLocusId\")),\n        )\n        .withColumn(\n            feature_name,\n            f.when(\n                (f.col(\"regional_max\").isNotNull()) &amp; (f.col(\"regional_max\") != 0.0),\n                f.col(local_feature_name)\n                / f.coalesce(f.col(\"regional_max\"), f.lit(0.0)),\n            ).otherwise(f.lit(0.0)),\n        )\n        .withColumn(\n            feature_name,\n            f.when(f.col(feature_name) &lt; 0, f.lit(0.0))\n            .when(f.col(feature_name) &gt; 1, f.lit(1.0))\n            .otherwise(f.col(feature_name)),\n        )\n        .drop(\"regional_max\", local_feature_name)\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/","title":"Other features","text":""},{"location":"python_api/datasets/l2g_features/other/#list-of-features","title":"List of features","text":""},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.GeneCountFeature","title":"<code>gentropy.dataset.l2g_features.other.GeneCountFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Counts the number of genes within a specified window size from the study locus.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>class GeneCountFeature(L2GFeature):\n    \"\"\"Counts the number of genes within a specified window size from the study locus.\"\"\"\n\n    feature_dependency_type = TargetIndex\n    feature_name = \"geneCount500kb\"\n\n    @classmethod\n    def compute(\n        cls: type[GeneCountFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; GeneCountFeature:\n        \"\"\"Computes the gene count feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary containing dependencies, with target index and window size\n\n        Returns:\n            GeneCountFeature: Feature dataset\n        \"\"\"\n        genomic_window = 500000\n        gene_count_df = common_genecount_feature_logic(\n            study_loci_to_annotate=study_loci_to_annotate,\n            feature_name=cls.feature_name,\n            genomic_window=genomic_window,\n            **feature_dependency,\n        )\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                gene_count_df,\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.GeneCountFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; GeneCountFeature</code>  <code>classmethod</code>","text":"<p>Computes the gene count feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary containing dependencies, with target index and window size</p> required <p>Returns:</p> Name Type Description <code>GeneCountFeature</code> <code>GeneCountFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[GeneCountFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; GeneCountFeature:\n    \"\"\"Computes the gene count feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary containing dependencies, with target index and window size\n\n    Returns:\n        GeneCountFeature: Feature dataset\n    \"\"\"\n    genomic_window = 500000\n    gene_count_df = common_genecount_feature_logic(\n        study_loci_to_annotate=study_loci_to_annotate,\n        feature_name=cls.feature_name,\n        genomic_window=genomic_window,\n        **feature_dependency,\n    )\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            gene_count_df,\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.ProteinGeneCountFeature","title":"<code>gentropy.dataset.l2g_features.other.ProteinGeneCountFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Counts the number of protein coding genes within a specified window size from the study locus.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>class ProteinGeneCountFeature(L2GFeature):\n    \"\"\"Counts the number of protein coding genes within a specified window size from the study locus.\"\"\"\n\n    feature_dependency_type = TargetIndex\n    feature_name = \"proteinGeneCount500kb\"\n\n    @classmethod\n    def compute(\n        cls: type[ProteinGeneCountFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; ProteinGeneCountFeature:\n        \"\"\"Computes the gene count feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary containing dependencies, with target index and window size\n\n        Returns:\n            ProteinGeneCountFeature: Feature dataset\n        \"\"\"\n        genomic_window = 500000\n        gene_count_df = common_genecount_feature_logic(\n            study_loci_to_annotate=study_loci_to_annotate,\n            feature_name=cls.feature_name,\n            genomic_window=genomic_window,\n            protein_coding_only=True,\n            **feature_dependency,\n        )\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                gene_count_df,\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.ProteinGeneCountFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; ProteinGeneCountFeature</code>  <code>classmethod</code>","text":"<p>Computes the gene count feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary containing dependencies, with target index and window size</p> required <p>Returns:</p> Name Type Description <code>ProteinGeneCountFeature</code> <code>ProteinGeneCountFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[ProteinGeneCountFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; ProteinGeneCountFeature:\n    \"\"\"Computes the gene count feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary containing dependencies, with target index and window size\n\n    Returns:\n        ProteinGeneCountFeature: Feature dataset\n    \"\"\"\n    genomic_window = 500000\n    gene_count_df = common_genecount_feature_logic(\n        study_loci_to_annotate=study_loci_to_annotate,\n        feature_name=cls.feature_name,\n        genomic_window=genomic_window,\n        protein_coding_only=True,\n        **feature_dependency,\n    )\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            gene_count_df,\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.ProteinCodingFeature","title":"<code>gentropy.dataset.l2g_features.other.ProteinCodingFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Indicates whether a gene is protein-coding within a specified window size from the study locus.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>class ProteinCodingFeature(L2GFeature):\n    \"\"\"Indicates whether a gene is protein-coding within a specified window size from the study locus.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"isProteinCoding\"\n\n    @classmethod\n    def compute(\n        cls: type[ProteinCodingFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; ProteinCodingFeature:\n        \"\"\"Computes the protein coding feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dictionary containing dependencies, including variant index\n\n        Returns:\n            ProteinCodingFeature: Feature dataset with 1 if the gene is protein-coding, 0 otherwise\n        \"\"\"\n        genomic_window = 500_000\n        protein_coding_df = is_protein_coding_feature_logic(\n            study_loci_to_annotate=study_loci_to_annotate,\n            feature_name=cls.feature_name,\n            genomic_window=genomic_window,\n            **feature_dependency,\n        )\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                protein_coding_df,\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.ProteinCodingFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; ProteinCodingFeature</code>  <code>classmethod</code>","text":"<p>Computes the protein coding feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dictionary containing dependencies, including variant index</p> required <p>Returns:</p> Name Type Description <code>ProteinCodingFeature</code> <code>ProteinCodingFeature</code> <p>Feature dataset with 1 if the gene is protein-coding, 0 otherwise</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[ProteinCodingFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; ProteinCodingFeature:\n    \"\"\"Computes the protein coding feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dictionary containing dependencies, including variant index\n\n    Returns:\n        ProteinCodingFeature: Feature dataset with 1 if the gene is protein-coding, 0 otherwise\n    \"\"\"\n    genomic_window = 500_000\n    protein_coding_df = is_protein_coding_feature_logic(\n        study_loci_to_annotate=study_loci_to_annotate,\n        feature_name=cls.feature_name,\n        genomic_window=genomic_window,\n        **feature_dependency,\n    )\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            protein_coding_df,\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.CredibleSetConfidenceFeature","title":"<code>gentropy.dataset.l2g_features.other.CredibleSetConfidenceFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Distance of the sentinel variant to gene TSS. This is not weighted by the causal probability.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>class CredibleSetConfidenceFeature(L2GFeature):\n    \"\"\"Distance of the sentinel variant to gene TSS. This is not weighted by the causal probability.\"\"\"\n\n    feature_dependency_type = [StudyLocus, VariantIndex]\n    feature_name = \"credibleSetConfidence\"\n\n    @classmethod\n    def compute(\n        cls: type[CredibleSetConfidenceFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; CredibleSetConfidenceFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n        Returns:\n            CredibleSetConfidenceFeature: Feature dataset\n        \"\"\"\n        full_credible_set = feature_dependency[\"study_locus\"].df.select(\n            \"studyLocusId\",\n            \"studyId\",\n            f.explode(\"locus.variantId\").alias(\"variantId\"),\n            cls.score_credible_set_confidence(f.col(\"confidence\")).alias(\n                cls.feature_name\n            ),\n        )\n\n        return cls(\n            _df=convert_from_wide_to_long(\n                (\n                    study_loci_to_annotate.df.drop(\"studyLocusId\")\n                    # Annotate genes\n                    .join(\n                        feature_dependency[\"variant_index\"].df.select(\n                            \"variantId\",\n                            f.explode(\"transcriptConsequences.targetId\").alias(\n                                \"geneId\"\n                            ),\n                        ),\n                        on=\"variantId\",\n                        how=\"inner\",\n                    )\n                    # Annotate credible set confidence\n                    .join(full_credible_set, [\"variantId\", \"studyId\"])\n                    .select(\"studyLocusId\", \"geneId\", cls.feature_name)\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n\n    @classmethod\n    def score_credible_set_confidence(\n        cls: type[CredibleSetConfidenceFeature],\n        confidence_column: Column,\n    ) -&gt; Column:\n        \"\"\"Expression that assigns a score to the credible set confidence.\n\n        Args:\n            confidence_column (Column): Confidence column in the StudyLocus object\n\n        Returns:\n            Column: A confidence score between 0 and 1\n        \"\"\"\n        return (\n            f.when(\n                f.col(\"confidence\")\n                == CredibleSetConfidenceClasses.FINEMAPPED_IN_SAMPLE_LD.value,\n                f.lit(1.0),\n            )\n            .when(\n                f.col(\"confidence\")\n                == CredibleSetConfidenceClasses.FINEMAPPED_OUT_OF_SAMPLE_LD.value,\n                f.lit(0.75),\n            )\n            .when(\n                f.col(\"confidence\")\n                == CredibleSetConfidenceClasses.PICSED_SUMMARY_STATS.value,\n                f.lit(0.5),\n            )\n            .when(\n                f.col(\"confidence\")\n                == CredibleSetConfidenceClasses.PICSED_TOP_HIT.value,\n                f.lit(0.25),\n            )\n            .when(\n                f.col(\"confidence\") == CredibleSetConfidenceClasses.UNKNOWN.value,\n                f.lit(0.0),\n            )\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.CredibleSetConfidenceFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; CredibleSetConfidenceFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the distance information</p> required <p>Returns:</p> Name Type Description <code>CredibleSetConfidenceFeature</code> <code>CredibleSetConfidenceFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[CredibleSetConfidenceFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; CredibleSetConfidenceFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the distance information\n\n    Returns:\n        CredibleSetConfidenceFeature: Feature dataset\n    \"\"\"\n    full_credible_set = feature_dependency[\"study_locus\"].df.select(\n        \"studyLocusId\",\n        \"studyId\",\n        f.explode(\"locus.variantId\").alias(\"variantId\"),\n        cls.score_credible_set_confidence(f.col(\"confidence\")).alias(\n            cls.feature_name\n        ),\n    )\n\n    return cls(\n        _df=convert_from_wide_to_long(\n            (\n                study_loci_to_annotate.df.drop(\"studyLocusId\")\n                # Annotate genes\n                .join(\n                    feature_dependency[\"variant_index\"].df.select(\n                        \"variantId\",\n                        f.explode(\"transcriptConsequences.targetId\").alias(\n                            \"geneId\"\n                        ),\n                    ),\n                    on=\"variantId\",\n                    how=\"inner\",\n                )\n                # Annotate credible set confidence\n                .join(full_credible_set, [\"variantId\", \"studyId\"])\n                .select(\"studyLocusId\", \"geneId\", cls.feature_name)\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.CredibleSetConfidenceFeature.score_credible_set_confidence","title":"<code>score_credible_set_confidence(confidence_column: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Expression that assigns a score to the credible set confidence.</p> <p>Parameters:</p> Name Type Description Default <code>confidence_column</code> <code>Column</code> <p>Confidence column in the StudyLocus object</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A confidence score between 0 and 1</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>@classmethod\ndef score_credible_set_confidence(\n    cls: type[CredibleSetConfidenceFeature],\n    confidence_column: Column,\n) -&gt; Column:\n    \"\"\"Expression that assigns a score to the credible set confidence.\n\n    Args:\n        confidence_column (Column): Confidence column in the StudyLocus object\n\n    Returns:\n        Column: A confidence score between 0 and 1\n    \"\"\"\n    return (\n        f.when(\n            f.col(\"confidence\")\n            == CredibleSetConfidenceClasses.FINEMAPPED_IN_SAMPLE_LD.value,\n            f.lit(1.0),\n        )\n        .when(\n            f.col(\"confidence\")\n            == CredibleSetConfidenceClasses.FINEMAPPED_OUT_OF_SAMPLE_LD.value,\n            f.lit(0.75),\n        )\n        .when(\n            f.col(\"confidence\")\n            == CredibleSetConfidenceClasses.PICSED_SUMMARY_STATS.value,\n            f.lit(0.5),\n        )\n        .when(\n            f.col(\"confidence\")\n            == CredibleSetConfidenceClasses.PICSED_TOP_HIT.value,\n            f.lit(0.25),\n        )\n        .when(\n            f.col(\"confidence\") == CredibleSetConfidenceClasses.UNKNOWN.value,\n            f.lit(0.0),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#common-logic","title":"Common logic","text":""},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.common_genecount_feature_logic","title":"<code>gentropy.dataset.l2g_features.other.common_genecount_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, target_index: TargetIndex, feature_name: str, genomic_window: int, protein_coding_only: bool = False) -&gt; DataFrame</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>target_index</code> <code>TargetIndex</code> <p>Dataset containing information related to all genes in release.</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <code>genomic_window</code> <code>int</code> <p>The maximum window size to consider</p> required <code>protein_coding_only</code> <code>bool</code> <p>Whether to only consider protein coding genes in calculation.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>def common_genecount_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    target_index: TargetIndex,\n    feature_name: str,\n    genomic_window: int,\n    protein_coding_only: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci\n            that will be used for annotation\n        target_index (TargetIndex): Dataset containing information related to all genes in release.\n        feature_name (str): The name of the feature\n        genomic_window (int): The maximum window size to consider\n        protein_coding_only (bool): Whether to only consider protein coding genes in calculation.\n\n    Returns:\n            DataFrame: Feature dataset\n    \"\"\"\n    study_loci_window = (\n        study_loci_to_annotate.df.withColumn(\n            \"window_start\", f.col(\"position\") - (genomic_window / 2)\n        )\n        .withColumn(\"window_end\", f.col(\"position\") + (genomic_window / 2))\n        .withColumnRenamed(\"chromosome\", \"SL_chromosome\")\n    )\n    target_index_filter = target_index.df\n\n    if protein_coding_only:\n        target_index_filter = target_index_filter.filter(\n            f.col(\"biotype\") == \"protein_coding\"\n        )\n\n    distinct_gene_counts = (\n        study_loci_window.join(\n            target_index_filter.alias(\"genes\"),\n            on=(\n                (f.col(\"SL_chromosome\") == f.col(\"genes.genomicLocation.chromosome\"))\n                &amp; (f.col(\"genes.tss\") &gt;= f.col(\"window_start\"))\n                &amp; (f.col(\"genes.tss\") &lt;= f.col(\"window_end\"))\n            ),\n            how=\"inner\",\n        )\n        .groupBy(\"studyLocusId\")\n        .agg(f.approx_count_distinct(f.col(\"id\").alias(\"geneId\")).alias(feature_name))\n    )\n\n    return (\n        study_loci_window.join(\n            target_index_filter.alias(\"genes\"),\n            on=(\n                (f.col(\"SL_chromosome\") == f.col(\"genes.genomicLocation.chromosome\"))\n                &amp; (f.col(\"genes.tss\") &gt;= f.col(\"window_start\"))\n                &amp; (f.col(\"genes.tss\") &lt;= f.col(\"window_end\"))\n            ),\n            how=\"inner\",\n        )\n        .join(distinct_gene_counts, on=\"studyLocusId\", how=\"inner\")\n        .select(\"studyLocusId\", f.col(\"id\").alias(\"geneId\"), feature_name)\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/other/#gentropy.dataset.l2g_features.other.is_protein_coding_feature_logic","title":"<code>gentropy.dataset.l2g_features.other.is_protein_coding_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, variant_index: VariantIndex, feature_name: str, genomic_window: int = 500000) -&gt; DataFrame</code>","text":"<p>Computes the feature to indicate if a gene is protein-coding or not.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Dataset containing information related to all overlapping genes within a genomic window.</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <code>genomic_window</code> <code>int</code> <p>The window size around the locus to consider. Defaults to its maximum value: 500kb up and downstream the locus</p> <code>500000</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset, with 1 if the gene is protein-coding, 0 if not.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>when provided <code>genomic_window</code> is more or equal to 500kb.</p> Source code in <code>src/gentropy/dataset/l2g_features/other.py</code> <pre><code>def is_protein_coding_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    variant_index: VariantIndex,\n    feature_name: str,\n    genomic_window: int = 500_000,\n) -&gt; DataFrame:\n    \"\"\"Computes the feature to indicate if a gene is protein-coding or not.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci\n            that will be used for annotation\n        variant_index (VariantIndex): Dataset containing information related to all overlapping genes within a genomic window.\n        feature_name (str): The name of the feature\n        genomic_window (int): The window size around the locus to consider. Defaults to its maximum value: 500kb up and downstream the locus\n\n    Returns:\n        DataFrame: Feature dataset, with 1 if the gene is protein-coding, 0 if not.\n\n    Raises:\n        AssertionError: when provided `genomic_window` is more or equal to 500kb.\n    \"\"\"\n    assert genomic_window &lt;= 500_000, \"Genomic window must be less than 500kb.\"\n    genes_in_window = (\n        variant_index.df.withColumn(\n            \"transcriptConsequence\", f.explode(\"transcriptConsequences\")\n        )\n        .select(\n            \"variantId\",\n            f.col(\"transcriptConsequence.targetId\").alias(\"geneId\"),\n            f.col(\"transcriptConsequence.biotype\").alias(\"biotype\"),\n            f.col(\"transcriptConsequence.distanceFromFootprint\").alias(\n                \"distanceFromFootprint\"\n            ),\n        )\n        .filter(f.col(\"distanceFromFootprint\") &lt;= genomic_window)\n    )\n    if isinstance(study_loci_to_annotate, StudyLocus):\n        variants_df = study_loci_to_annotate.df.select(\n            f.explode_outer(\"locus.variantId\").alias(\"variantId\"),\n            \"studyLocusId\",\n        ).filter(f.col(\"variantId\").isNotNull())\n    elif isinstance(study_loci_to_annotate, L2GGoldStandard):\n        variants_df = study_loci_to_annotate.df.select(\"studyLocusId\", \"variantId\")\n    return (\n        # Annotate all genes in the window of a locus\n        variants_df.join(\n            genes_in_window,\n            on=\"variantId\",\n        )\n        # Apply flag across all variants in the locus\n        .withColumn(\n            feature_name,\n            f.when(f.col(\"biotype\") == \"protein_coding\", f.lit(1.0)).otherwise(\n                f.lit(0.0)\n            ),\n        )\n        .select(\"studyLocusId\", \"geneId\", feature_name)\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/","title":"From VEP","text":""},{"location":"python_api/datasets/l2g_features/vep/#list-of-features","title":"List of features","text":""},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMeanFeature","title":"<code>gentropy.dataset.l2g_features.vep.VepMeanFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Average functional consequence score among all variants in a credible set for a studyLocus/gene.</p> <p>The mean severity score is weighted by the posterior probability of each variant.</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>class VepMeanFeature(L2GFeature):\n    \"\"\"Average functional consequence score among all variants in a credible set for a studyLocus/gene.\n\n    The mean severity score is weighted by the posterior probability of each variant.\n    \"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"vepMean\"\n\n    @classmethod\n    def compute(\n        cls: type[VepMeanFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; VepMeanFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n        Returns:\n            VepMeanFeature: Feature dataset\n        \"\"\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_vep_feature_logic(\n                    study_loci_to_annotate=study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMeanFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; VepMeanFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the functional consequence information</p> required <p>Returns:</p> Name Type Description <code>VepMeanFeature</code> <code>VepMeanFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[VepMeanFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; VepMeanFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n    Returns:\n        VepMeanFeature: Feature dataset\n    \"\"\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_vep_feature_logic(\n                study_loci_to_annotate=study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMeanNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.vep.VepMeanNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Mean functional consequence score among all variants in a credible set for a studyLocus/gene relative to the mean VEP score across all protein coding genes in the vicinity.</p> <p>The mean severity score is weighted by the posterior probability of each variant.</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>class VepMeanNeighbourhoodFeature(L2GFeature):\n    \"\"\"Mean functional consequence score among all variants in a credible set for a studyLocus/gene relative to the mean VEP score across all protein coding genes in the vicinity.\n\n    The mean severity score is weighted by the posterior probability of each variant.\n    \"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"vepMeanNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[VepMeanNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; VepMeanNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n        Returns:\n            VepMeanNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_vep_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMeanNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; VepMeanNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the functional consequence information</p> required <p>Returns:</p> Name Type Description <code>VepMeanNeighbourhoodFeature</code> <code>VepMeanNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[VepMeanNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; VepMeanNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n    Returns:\n        VepMeanNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_vep_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMaximumFeature","title":"<code>gentropy.dataset.l2g_features.vep.VepMaximumFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Maximum functional consequence score among all variants in a credible set for a studyLocus/gene.</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>class VepMaximumFeature(L2GFeature):\n    \"\"\"Maximum functional consequence score among all variants in a credible set for a studyLocus/gene.\"\"\"\n\n    feature_dependency_type = VariantIndex\n    feature_name = \"vepMaximum\"\n\n    @classmethod\n    def compute(\n        cls: type[VepMaximumFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; VepMaximumFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n        Returns:\n            VepMaximumFeature: Feature dataset\n        \"\"\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_vep_feature_logic(\n                    study_loci_to_annotate=study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMaximumFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; VepMaximumFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the functional consequence information</p> required <p>Returns:</p> Name Type Description <code>VepMaximumFeature</code> <code>VepMaximumFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[VepMaximumFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; VepMaximumFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n    Returns:\n        VepMaximumFeature: Feature dataset\n    \"\"\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_vep_feature_logic(\n                study_loci_to_annotate=study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMaximumNeighbourhoodFeature","title":"<code>gentropy.dataset.l2g_features.vep.VepMaximumNeighbourhoodFeature</code>  <code>dataclass</code>","text":"<p>               Bases: <code>L2GFeature</code></p> <p>Maximum functional consequence score among all variants in a credible set for a studyLocus/gene relative to the mean VEP score across all protein coding genes in the vicinity.</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>class VepMaximumNeighbourhoodFeature(L2GFeature):\n    \"\"\"Maximum functional consequence score among all variants in a credible set for a studyLocus/gene relative to the mean VEP score across all protein coding genes in the vicinity.\"\"\"\n\n    feature_dependency_type = [VariantIndex, TargetIndex]\n    feature_name = \"vepMaximumNeighbourhood\"\n\n    @classmethod\n    def compute(\n        cls: type[VepMaximumNeighbourhoodFeature],\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        feature_dependency: dict[str, Any],\n    ) -&gt; VepMaximumNeighbourhoodFeature:\n        \"\"\"Computes the feature.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n        Returns:\n            VepMaximumNeighbourhoodFeature: Feature dataset\n        \"\"\"\n        return cls(\n            _df=convert_from_wide_to_long(\n                common_neighbourhood_vep_feature_logic(\n                    study_loci_to_annotate,\n                    feature_name=cls.feature_name,\n                    **feature_dependency,\n                ),\n                id_vars=(\"studyLocusId\", \"geneId\"),\n                var_name=\"featureName\",\n                value_name=\"featureValue\",\n            ),\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.VepMaximumNeighbourhoodFeature.compute","title":"<code>compute(study_loci_to_annotate: StudyLocus | L2GGoldStandard, feature_dependency: dict[str, Any]) -&gt; VepMaximumNeighbourhoodFeature</code>  <code>classmethod</code>","text":"<p>Computes the feature.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>feature_dependency</code> <code>dict[str, Any]</code> <p>Dataset that contains the functional consequence information</p> required <p>Returns:</p> Name Type Description <code>VepMaximumNeighbourhoodFeature</code> <code>VepMaximumNeighbourhoodFeature</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>@classmethod\ndef compute(\n    cls: type[VepMaximumNeighbourhoodFeature],\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    feature_dependency: dict[str, Any],\n) -&gt; VepMaximumNeighbourhoodFeature:\n    \"\"\"Computes the feature.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        feature_dependency (dict[str, Any]): Dataset that contains the functional consequence information\n\n    Returns:\n        VepMaximumNeighbourhoodFeature: Feature dataset\n    \"\"\"\n    return cls(\n        _df=convert_from_wide_to_long(\n            common_neighbourhood_vep_feature_logic(\n                study_loci_to_annotate,\n                feature_name=cls.feature_name,\n                **feature_dependency,\n            ),\n            id_vars=(\"studyLocusId\", \"geneId\"),\n            var_name=\"featureName\",\n            value_name=\"featureValue\",\n        ),\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#common-logic","title":"Common logic","text":""},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.common_vep_feature_logic","title":"<code>gentropy.dataset.l2g_features.vep.common_vep_feature_logic(study_loci_to_annotate: L2GGoldStandard | StudyLocus, *, variant_index: VariantIndex, feature_name: str) -&gt; DataFrame</code>","text":"<p>Extracts variant severity score computed from VEP.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>L2GGoldStandard | StudyLocus</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>The dataset containing functional consequence information</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>def common_vep_feature_logic(\n    study_loci_to_annotate: L2GGoldStandard | StudyLocus,\n    *,\n    variant_index: VariantIndex,\n    feature_name: str,\n) -&gt; DataFrame:\n    \"\"\"Extracts variant severity score computed from VEP.\n\n    Args:\n        study_loci_to_annotate (L2GGoldStandard | StudyLocus): The dataset containing study loci that will be used for annotation\n        variant_index (VariantIndex): The dataset containing functional consequence information\n        feature_name (str): The name of the feature\n\n    Returns:\n        DataFrame: Feature dataset\n    \"\"\"\n    # Variant/Target/Severity dataframe\n    consequences_dataset = variant_index.df.withColumn(\n        \"transcriptConsequence\", f.explode(\"transcriptConsequences\")\n    ).select(\n        \"variantId\",\n        f.col(\"transcriptConsequence.targetId\").alias(\"geneId\"),\n        f.col(\"transcriptConsequence.consequenceScore\").alias(\"severityScore\"),\n    )\n    if isinstance(study_loci_to_annotate, StudyLocus):\n        variants_df = (\n            study_loci_to_annotate.df.withColumn(\n                \"variantInLocus\", f.explode_outer(\"locus\")\n            )\n            .select(\n                \"studyLocusId\",\n                f.col(\"variantInLocus.variantId\").alias(\"variantId\"),\n                f.col(\"variantInLocus.posteriorProbability\").alias(\n                    \"posteriorProbability\"\n                ),\n            )\n            .join(consequences_dataset, \"variantId\")\n        )\n    elif isinstance(study_loci_to_annotate, L2GGoldStandard):\n        variants_df = study_loci_to_annotate.df.select(\n            \"studyLocusId\", \"variantId\", f.lit(1.0).alias(\"posteriorProbability\")\n        ).join(consequences_dataset, \"variantId\")\n\n    if \"Maximum\" in feature_name:\n        agg_expr = f.max(\"severityScore\")\n    elif \"Mean\" in feature_name:\n        variants_df = variants_df.withColumn(\n            \"weightedScore\", f.col(\"severityScore\") * f.col(\"posteriorProbability\")\n        )\n        agg_expr = f.mean(\"weightedScore\")\n    return variants_df.groupBy(\"studyLocusId\", \"geneId\").agg(\n        agg_expr.alias(feature_name)\n    )\n</code></pre>"},{"location":"python_api/datasets/l2g_features/vep/#gentropy.dataset.l2g_features.vep.common_neighbourhood_vep_feature_logic","title":"<code>gentropy.dataset.l2g_features.vep.common_neighbourhood_vep_feature_logic(study_loci_to_annotate: StudyLocus | L2GGoldStandard, *, variant_index: VariantIndex, target_index: TargetIndex, feature_name: str) -&gt; DataFrame</code>","text":"<p>Extracts variant severity score computed from VEP for any gene, based on what is the max score for protein coding genes that are nearby the locus.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>The dataset containing functional consequence information</p> required <code>target_index</code> <code>TargetIndex</code> <p>The dataset containing the gene biotype</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Feature dataset</p> Source code in <code>src/gentropy/dataset/l2g_features/vep.py</code> <pre><code>def common_neighbourhood_vep_feature_logic(\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    *,\n    variant_index: VariantIndex,\n    target_index: TargetIndex,\n    feature_name: str,\n) -&gt; DataFrame:\n    \"\"\"Extracts variant severity score computed from VEP for any gene, based on what is the max score for protein coding genes that are nearby the locus.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        variant_index (VariantIndex): The dataset containing functional consequence information\n        target_index (TargetIndex): The dataset containing the gene biotype\n        feature_name (str): The name of the feature\n\n    Returns:\n        DataFrame: Feature dataset\n    \"\"\"\n    local_feature_name = feature_name.replace(\"Neighbourhood\", \"\")\n    local_metric = common_vep_feature_logic(\n        study_loci_to_annotate,\n        feature_name=local_feature_name,\n        variant_index=variant_index,\n    )\n    return (\n        local_metric\n        # Compute average score in the vicinity (feature will be the same for any gene associated with a studyLocus)\n        # (non protein coding genes in the vicinity are excluded see #3552)\n        .join(\n            target_index.df.filter(f.col(\"biotype\") == \"protein_coding\").select(\n                f.col(\"id\").alias(\"geneId\")\n            ),\n            \"geneId\",\n            \"inner\",\n        )\n        .withColumn(\n            \"regional_max\",\n            f.max(local_feature_name).over(Window.partitionBy(\"studyLocusId\")),\n        )\n        .withColumn(\n            feature_name,\n            f.when(\n                (f.col(\"regional_max\").isNotNull()) &amp; (f.col(\"regional_max\") != 0.0),\n                f.col(local_feature_name)\n                / f.coalesce(f.col(\"regional_max\"), f.lit(0.0)),\n            ).otherwise(f.lit(0.0)),\n        )\n        .drop(\"regional_max\", local_feature_name)\n    )\n</code></pre>"},{"location":"python_api/datasources/_datasources/","title":"Data Sources","text":"<p>This section contains information about the data source harmonisation tools available in Open Targets Gentropy.</p>"},{"location":"python_api/datasources/_datasources/#gwas-study-sources","title":"GWAS study sources","text":"<ol> <li>GWAS Catalog (with or without full summary statistics)</li> <li>FinnGen</li> <li>FinnGen UK Biobank and Million Veteran Program (MVP) meta-analysis</li> </ol>"},{"location":"python_api/datasources/_datasources/#molecular-qtls","title":"Molecular QTLs","text":"<ol> <li>GTEx (eQTL catalogue)</li> <li>UKB PPP (EUR)</li> </ol>"},{"location":"python_api/datasources/_datasources/#interaction-interval-based-experiments","title":"Interaction / Interval-based Experiments","text":"<ol> <li>Intervals-based datasets, informing about the relationships between genetic elements and their functional implications.</li> </ol>"},{"location":"python_api/datasources/_datasources/#variant-annotationvalidation","title":"Variant annotation/validation","text":"<ol> <li>GnomAD v4.1</li> <li>GWAS catalog's harmonisation pipeline</li> <li>Ensembl's Variant Effect Predictor</li> </ol>"},{"location":"python_api/datasources/_datasources/#linkage-disequilibrium","title":"Linkage disequilibrium","text":"<ol> <li>GnomAD v2.1.1 LD matrixes (7 ancestries)</li> </ol>"},{"location":"python_api/datasources/_datasources/#locus-to-gene-gold-standard","title":"Locus-to-gene gold standard","text":"<ol> <li>Open Targets training set</li> </ol>"},{"location":"python_api/datasources/_datasources/#gene-annotation","title":"Gene annotation","text":"<ol> <li>Open Targets Platform Target Dataset (derived from Ensembl)</li> </ol>"},{"location":"python_api/datasources/_datasources/#biological-samples","title":"Biological samples","text":"<ol> <li>Uberon</li> <li>Cell Ontology</li> </ol>"},{"location":"python_api/datasources/biosample_ontologies/_biosample_ontologies/","title":"Biosample Ontologies","text":"<p>The Cell Ontology is a structured controlled vocabulary for cell types. It is used to annotate cell types in single-cell RNA-seq data and other omics data.</p> <p>The Uberon ontology is a multi-species anatomy ontology that integrates cross-species ontologies into a single ontology.</p>"},{"location":"python_api/datasources/biosample_ontologies/_biosample_ontologies/#gentropy.datasource.biosample_ontologies.utils","title":"<code>gentropy.datasource.biosample_ontologies.utils</code>","text":"<p>Utility functions for Biosample ontology processing.</p>"},{"location":"python_api/datasources/biosample_ontologies/_biosample_ontologies/#gentropy.datasource.biosample_ontologies.utils.extract_ontology_from_json","title":"<code>extract_ontology_from_json(ontology_json: str, spark: SparkSession) -&gt; BiosampleIndex</code>","text":"<p>Extracts the ontology information from a JSON file. Currently only supports Uberon and Cell Ontology.</p> <p>Parameters:</p> Name Type Description Default <code>ontology_json</code> <code>str</code> <p>Path to the JSON file containing the ontology information.</p> required <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <p>Returns:</p> Name Type Description <code>BiosampleIndex</code> <code>BiosampleIndex</code> <p>Parsed and annotated biosample index table.</p> Source code in <code>src/gentropy/datasource/biosample_ontologies/utils.py</code> <pre><code>def extract_ontology_from_json(\n    ontology_json: str, spark: SparkSession\n) -&gt; BiosampleIndex:\n    \"\"\"Extracts the ontology information from a JSON file. Currently only supports Uberon and Cell Ontology.\n\n    Args:\n        ontology_json (str): Path to the JSON file containing the ontology information.\n        spark (SparkSession): Spark session.\n\n    Returns:\n        BiosampleIndex: Parsed and annotated biosample index table.\n    \"\"\"\n\n    def json_graph_traversal(\n        df: DataFrame, node_col: str, link_col: str, traversal_type: str\n    ) -&gt; DataFrame:\n        \"\"\"Traverse a graph represented in a DataFrame to find all ancestors or descendants.\n\n        Args:\n            df (DataFrame): DataFrame containing the graph data.\n            node_col (str): Column name for the node.\n            link_col (str): Column name for the link.\n            traversal_type (str): Type of traversal - \"ancestors\" or \"descendants\".\n\n        Returns:\n            DataFrame: DataFrame with the result column added.\n        \"\"\"\n        # Collect graph data as a map\n        graph_map = df.select(node_col, link_col).rdd.collectAsMap()\n        broadcasted_graph = spark.sparkContext.broadcast(graph_map)\n\n        def get_relationships(node: str) -&gt; list[str]:\n            \"\"\"Get all relationships for a given node.\n\n            Args:\n                node (str): Node ID.\n\n            Returns:\n                list[str]: List of relationships.\n            \"\"\"\n            relationships = set()\n            stack = [node]\n            while stack:\n                current = stack.pop()\n                if current in broadcasted_graph.value:\n                    current_links = broadcasted_graph.value[current]\n                    stack.extend(current_links)\n                    relationships.update(current_links)\n            return list(relationships)\n\n        # Choose column name based on traversal type\n        result_col = \"ancestors\" if traversal_type == \"ancestors\" else \"descendants\"\n\n        # Register the UDF based on traversal type\n        relationship_udf = f.udf(get_relationships, ArrayType(StringType()))\n\n        # Apply the UDF to create the result column\n        return df.withColumn(result_col, relationship_udf(f.col(node_col)))\n\n    # Load the JSON file\n    df = spark.read.json(ontology_json, multiLine=True)\n\n    # Exploding the 'graphs' array to make individual records easier to access\n    df_graphs = df.select(f.explode_outer(\"graphs\").alias(\"graph\"))\n\n    # Exploding the 'nodes' array within each graph\n    df_nodes = df_graphs.select(\n        f.col(\"graph.id\").alias(\"graph_id\"),\n        f.explode_outer(\"graph.nodes\").alias(\"node\"),\n    )\n\n    # Exploding the 'edges' array within each graph for relationship data\n    df_edges = df_graphs.select(\n        f.col(\"graph.id\").alias(\"graph_id\"),\n        f.explode_outer(\"graph.edges\").alias(\"edge\"),\n    ).select(\n        f.col(\"edge.sub\").alias(\"subject\"),\n        f.col(\"edge.pred\").alias(\"predicate\"),\n        f.col(\"edge.obj\").alias(\"object\"),\n    )\n\n    # Remove certain URL prefixes from IDs\n    urls_to_remove = [\"http://purl.obolibrary.org/obo/\", \"http://www.ebi.ac.uk/efo/\"]\n    # Create a regex pattern that matches any of the URLs\n    escaped_urls_pattern = \"|\".join([re.escape(url) for url in urls_to_remove])\n\n    df_edges = df_edges.withColumn(\n        \"subject\", f.regexp_replace(f.col(\"subject\"), escaped_urls_pattern, \"\")\n    )\n    df_edges = df_edges.withColumn(\n        \"object\", f.regexp_replace(f.col(\"object\"), escaped_urls_pattern, \"\")\n    )\n\n    # Extract the relevant information from the nodes\n    transformed_df = df_nodes.select(\n        f.regexp_replace(f.col(\"node.id\"), escaped_urls_pattern, \"\").alias(\n            \"biosampleId\"\n        ),\n        f.coalesce(f.col(\"node.lbl\"), f.col(\"node.id\")).alias(\"biosampleName\"),\n        f.col(\"node.meta.definition.val\").alias(\"description\"),\n        f.collect_set(f.col(\"node.meta.xrefs.val\"))\n        .over(Window.partitionBy(\"node.id\"))\n        .getItem(0)\n        .alias(\"xrefs\"),\n        f.collect_set(f.col(\"node.meta.synonyms.val\"))\n        .over(Window.partitionBy(\"node.id\"))\n        .getItem(0)\n        .alias(\"synonyms\"),\n    )\n\n    # Extract the relationships from the edges\n    # Prepare relationship-specific DataFrames\n    df_parents = (\n        df_edges.filter(f.col(\"predicate\") == \"is_a\")\n        .select(\"subject\", \"object\")\n        .withColumnRenamed(\"object\", \"parent\")\n    )\n    df_children = (\n        df_edges.filter(f.col(\"predicate\") == \"is_a\")\n        .select(\"object\", \"subject\")\n        .withColumnRenamed(\"subject\", \"child\")\n    )\n\n    # Aggregate relationships back to nodes\n    df_parents_grouped = df_parents.groupBy(\"subject\").agg(\n        f.array_distinct(f.collect_list(\"parent\")).alias(\"parents\")\n    )\n    df_children_grouped = df_children.groupBy(\"object\").agg(\n        f.array_distinct(f.collect_list(\"child\")).alias(\"children\")\n    )\n\n    # Get all ancestors\n    df_with_ancestors = json_graph_traversal(\n        df_parents_grouped, \"subject\", \"parents\", \"ancestors\"\n    )\n    # Get all descendants\n    df_with_descendants = json_graph_traversal(\n        df_children_grouped, \"object\", \"children\", \"descendants\"\n    )\n\n    # Join the ancestor and descendant DataFrames\n    df_with_relationships = (\n        df_with_ancestors.join(\n            df_with_descendants,\n            df_with_ancestors.subject == df_with_descendants.object,\n            \"full_outer\",\n        )\n        .withColumn(\n            \"biosampleId\",\n            f.coalesce(df_with_ancestors.subject, df_with_descendants.object),\n        )\n        .drop(\"subject\", \"object\")\n    )\n\n    # Join the original DataFrame with the relationship DataFrame\n    final_df = transformed_df.join(df_with_relationships, [\"biosampleId\"], \"left\")\n\n    return BiosampleIndex(_df=final_df, _schema=BiosampleIndex.get_schema())\n</code></pre>"},{"location":"python_api/datasources/ensembl/_ensembl/","title":"Ensembl annotations","text":"Ensembl <p>Ensembl provides a diverse set of genetic data Gentropy takes advantage of including gene set, and variant annotations.</p>"},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/","title":"Variant effector parser","text":""},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/#gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser","title":"<code>gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser</code>","text":"<p>Collection of methods to parse VEP output in json format.</p> Source code in <code>src/gentropy/datasource/ensembl/vep_parser.py</code> <pre><code>class VariantEffectPredictorParser:\n    \"\"\"Collection of methods to parse VEP output in json format.\"\"\"\n\n    # NOTE: Due to the fact that the comparison of the xrefs is done om the base of rsids\n    # if the field `colocalised_variants` have multiple rsids, this extracting xrefs will result in\n    # an array of xref structs, rather then the struct itself.\n\n    DBXREF_SCHEMA = VariantIndex.get_schema()[\"dbXrefs\"].dataType\n\n    # Schema description of the variant effect object:\n    VARIANT_EFFECT_SCHEMA = get_nested_struct_schema(\n        VariantIndex.get_schema()[\"variantEffect\"]\n    )\n\n    # Schema for the allele frequency column:\n    ALLELE_FREQUENCY_SCHEMA = VariantIndex.get_schema()[\"alleleFrequencies\"].dataType\n\n    # Consequence to sequence ontology map:\n    SEQUENCE_ONTOLOGY_MAP = VariantConsequence.map_sequence_ontology()\n\n    # Sequence ontology to score map:\n    LABEL_TO_SCORE_MAP = VariantConsequence.map_score()\n\n    @staticmethod\n    def get_schema() -&gt; t.StructType:\n        \"\"\"Return the schema of the VEP output.\n\n        Returns:\n            t.StructType: VEP output schema.\n\n        Examples:\n            &gt;&gt;&gt; type(VariantEffectPredictorParser.get_schema())\n            &lt;class 'pyspark.sql.types.StructType'&gt;\n        \"\"\"\n        return parse_spark_schema(\"vep_json_output.json\")\n\n    @classmethod\n    def extract_variant_index_from_vep(\n        cls: type[VariantEffectPredictorParser],\n        spark: SparkSession,\n        vep_output_path: str | list[str],\n        hash_threshold: int,\n        **kwargs: bool | float | int | str | None,\n    ) -&gt; VariantIndex:\n        \"\"\"Extract variant index from VEP output.\n\n        Args:\n            spark (SparkSession): Spark session.\n            vep_output_path (str | list[str]): Path to the VEP output.\n            hash_threshold (int): Threshold above which variant identifiers will be hashed.\n            **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.json.\n\n        Returns:\n            VariantIndex: Variant index dataset.\n\n        Raises:\n            ValueError: Failed reading file or if the dataset is empty.\n        \"\"\"\n        # To speed things up and simplify the json structure, read data following an expected schema:\n        vep_schema = cls.get_schema()\n\n        try:\n            vep_data = spark.read.json(vep_output_path, schema=vep_schema, **kwargs)\n        except ValueError as e:\n            raise ValueError(f\"Failed reading file: {vep_output_path}.\") from e\n\n        if vep_data.isEmpty():\n            raise ValueError(f\"The dataset is empty: {vep_output_path}\")\n\n        # Convert to VariantAnnotation dataset:\n        return VariantIndex(\n            _df=VariantEffectPredictorParser.process_vep_output(\n                vep_data, hash_threshold\n            ),\n            _schema=VariantIndex.get_schema(),\n            id_threshold=hash_threshold,\n        )\n\n    @staticmethod\n    def _extract_ensembl_xrefs(colocated_variants: Column) -&gt; Column:\n        \"\"\"Extract rs identifiers and build cross reference to Ensembl's variant page.\n\n        Args:\n            colocated_variants (Column): Colocated variants field from VEP output.\n\n        Returns:\n            Column: List of dbXrefs for rs identifiers.\n        \"\"\"\n        return VariantEffectPredictorParser._generate_dbxrefs(\n            VariantEffectPredictorParser._colocated_variants_to_rsids(\n                colocated_variants\n            ),\n            \"ensembl_variation\",\n        )\n\n    @enforce_schema(DBXREF_SCHEMA)\n    @staticmethod\n    def _generate_dbxrefs(ids: Column, source: str) -&gt; Column:\n        \"\"\"Convert a list of variant identifiers to dbXrefs given the source label.\n\n        Identifiers are cast to strings, then Null values are filtered out of the id list.\n\n        Args:\n            ids (Column): List of variant identifiers.\n            source (str): Source label for the dbXrefs.\n\n        Returns:\n            Column: List of dbXrefs.\n\n        Examples:\n            &gt;&gt;&gt; (\n            ...     spark.createDataFrame([('rs12',),(None,)], ['test_id'])\n            ...     .select(VariantEffectPredictorParser._generate_dbxrefs(f.array(f.col('test_id')), \"ensemblVariation\").alias('col'))\n            ...     .show(truncate=False)\n            ... )\n            +--------------------------+\n            |col                       |\n            +--------------------------+\n            |[{rs12, ensemblVariation}]|\n            |[]                        |\n            +--------------------------+\n            &lt;BLANKLINE&gt;\n            &gt;&gt;&gt; (\n            ...     spark.createDataFrame([('rs12',),(None,)], ['test_id'])\n            ...     .select(VariantEffectPredictorParser._generate_dbxrefs(f.array(f.col('test_id')), \"ensemblVariation\").alias('col'))\n            ...     .first().col[0].asDict()\n            ... )\n            {'id': 'rs12', 'source': 'ensemblVariation'}\n        \"\"\"\n        ids = f.filter(ids, lambda id: id.isNotNull())\n        xref_column = f.transform(\n            ids,\n            lambda id: f.struct(\n                id.cast(t.StringType()).alias(\"id\"), f.lit(source).alias(\"source\")\n            ),\n        )\n\n        return f.when(xref_column.isNull(), f.array()).otherwise(xref_column)\n\n    @staticmethod\n    def _colocated_variants_to_rsids(colocated_variants: Column) -&gt; Column:\n        \"\"\"Extract rs identifiers from the colocated variants VEP field.\n\n        Args:\n            colocated_variants (Column): Colocated variants field from VEP output.\n\n        Returns:\n            Column: List of rs identifiers.\n\n        Examples:\n            &gt;&gt;&gt; data = [('s1', 'rs1'),('s1', 'rs2'),('s1', 'rs3'),('s2', None),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['s','v'])\n            ...    .groupBy('s')\n            ...    .agg(f.collect_list(f.struct(f.col('v').alias('id'))).alias('cv'))\n            ...    .select(VariantEffectPredictorParser._colocated_variants_to_rsids(f.col('cv')).alias('rsIds'),)\n            ...    .show(truncate=False)\n            ... )\n            +---------------+\n            |rsIds          |\n            +---------------+\n            |[rs1, rs2, rs3]|\n            |[NULL]         |\n            +---------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.when(\n            colocated_variants.isNotNull(),\n            f.transform(\n                colocated_variants, lambda variant: variant.getItem(\"id\").alias(\"id\")\n            ),\n        ).alias(\"rsIds\")\n\n    @staticmethod\n    def _extract_omim_xrefs(colocated_variants: Column) -&gt; Column:\n        \"\"\"Build xdbRefs for OMIM identifiers.\n\n        OMIM identifiers are extracted from the colocated variants field, casted to strings and formatted as dbXrefs.\n\n        Args:\n            colocated_variants (Column): Colocated variants field from VEP output.\n\n        Returns:\n            Column: List of dbXrefs for OMIM identifiers.\n\n        Examples:\n            &gt;&gt;&gt; data = [('234234.32', 'rs1', 's1',),(None, 'rs1', 's1',),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['id', 'rsid', 's'])\n            ...    .groupBy('s')\n            ...    .agg(f.collect_list(f.struct(f.struct(f.array(f.col('id')).alias('OMIM')).alias('var_synonyms'),f.col('rsid').alias('id'),),).alias('cv'),).select(VariantEffectPredictorParser._extract_omim_xrefs(f.col('cv')).alias('dbXrefs'))\n            ...    .show(truncate=False)\n            ... )\n            +-------------------+\n            |dbXrefs            |\n            +-------------------+\n            |[{234234#32, omim}]|\n            +-------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        variants_w_omim_ref = f.filter(\n            colocated_variants,\n            lambda variant: variant.getItem(\"var_synonyms\").getItem(\"OMIM\").isNotNull(),\n        )\n\n        omim_var_ids = f.transform(\n            variants_w_omim_ref,\n            lambda var: f.transform(\n                var.getItem(\"var_synonyms\").getItem(\"OMIM\"),\n                lambda var: f.regexp_replace(var.cast(t.StringType()), r\"\\.\", r\"#\"),\n            ),\n        )\n\n        return VariantEffectPredictorParser._generate_dbxrefs(\n            f.flatten(omim_var_ids), \"omim\"\n        )\n\n    @staticmethod\n    def _extract_clinvar_xrefs(colocated_variants: Column) -&gt; Column:\n        \"\"\"Build xdbRefs for VCV ClinVar identifiers.\n\n        ClinVar identifiers are extracted from the colocated variants field.\n        VCV-identifiers are filtered out to generate cross-references.\n\n        Args:\n            colocated_variants (Column): Colocated variants field from VEP output.\n\n        Returns:\n            Column: List of dbXrefs for VCV ClinVar identifiers.\n\n        Examples:\n            &gt;&gt;&gt; data = [('VCV2323,RCV3423', 'rs1', 's1',),(None, 'rs1', 's1',),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['id', 'rsid', 's'])\n            ...    .groupBy('s')\n            ...    .agg(f.collect_list(f.struct(f.struct(f.split(f.col('id'),',').alias('ClinVar')).alias('var_synonyms'),f.col('rsid').alias('id'),),).alias('cv'),).select(VariantEffectPredictorParser._extract_clinvar_xrefs(f.col('cv')).alias('dbXrefs'))\n            ...    .show(truncate=False)\n            ... )\n            +--------------------+\n            |dbXrefs             |\n            +--------------------+\n            |[{VCV2323, clinvar}]|\n            +--------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        variants_w_clinvar_ref = f.filter(\n            colocated_variants,\n            lambda variant: variant.getItem(\"var_synonyms\")\n            .getItem(\"ClinVar\")\n            .isNotNull(),\n        )\n\n        clin_var_ids = f.transform(\n            variants_w_clinvar_ref,\n            lambda var: f.filter(\n                var.getItem(\"var_synonyms\").getItem(\"ClinVar\"),\n                lambda x: x.startswith(\"VCV\"),\n            ),\n        )\n\n        return VariantEffectPredictorParser._generate_dbxrefs(\n            f.flatten(clin_var_ids), \"clinvar\"\n        )\n\n    @staticmethod\n    def _get_most_severe_transcript(\n        transcript_column_name: str, score_field_name: str\n    ) -&gt; Column:\n        \"\"\"Return transcript with the highest in silico predicted score.\n\n        This method assumes the higher the score, the more severe the consequence of the variant is.\n        Hence, by selecting the transcript with the highest score, we are selecting the most severe consequence.\n\n        Args:\n            transcript_column_name (str): Name of the column containing the list of transcripts.\n            score_field_name (str): Name of the field containing the severity score.\n\n        Returns:\n            Column: Most severe transcript.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"v1\", 0.2, 'transcript1'),(\"v1\", None, 'transcript2'),(\"v1\", 0.6, 'transcript3'),(\"v1\", 0.4, 'transcript4'),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['v', 'score', 'transcriptId'])\n            ...    .groupBy('v')\n            ...    .agg(f.collect_list(f.struct(f.col('score'),f.col('transcriptId'))).alias('transcripts'))\n            ...    .select(VariantEffectPredictorParser._get_most_severe_transcript('transcripts', 'score').alias('most_severe_transcript'))\n            ...    .show(truncate=False)\n            ... )\n            +----------------------+\n            |most_severe_transcript|\n            +----------------------+\n            |{0.6, transcript3}    |\n            +----------------------+\n            &lt;BLANKLINE&gt;\n\n        Raises:\n            AssertionError: When `transcript_column_name` is not a string.\n        \"\"\"\n        assert isinstance(\n            transcript_column_name, str\n        ), \"transcript_column_name must be a string and not a column.\"\n        # Order transcripts by severity score:\n        ordered_transcripts = order_array_of_structs_by_field(\n            transcript_column_name, score_field_name\n        )\n\n        # Drop transcripts with no severity score and return the most severe one:\n        return f.filter(\n            ordered_transcripts,\n            lambda transcript: transcript.getItem(score_field_name).isNotNull(),\n        )[0]\n\n    @classmethod\n    @enforce_schema(VARIANT_EFFECT_SCHEMA)\n    def _get_vep_prediction(cls, most_severe_consequence: Column) -&gt; Column:\n        return f.struct(\n            f.lit(\"VEP\").alias(\"method\"),\n            most_severe_consequence.alias(\"assessment\"),\n            map_column_by_dictionary(\n                most_severe_consequence, cls.LABEL_TO_SCORE_MAP\n            ).alias(\"score\"),\n        )\n\n    @staticmethod\n    @enforce_schema(VARIANT_EFFECT_SCHEMA)\n    def _get_max_alpha_missense(transcripts: Column) -&gt; Column:\n        \"\"\"Return the most severe alpha missense prediction from all transcripts.\n\n        This function assumes one variant can fall onto only one gene with alpha-sense prediction available on the canonical transcript.\n\n        Args:\n            transcripts (Column): List of transcripts.\n\n        Returns:\n            Column: Most severe alpha missense prediction.\n\n        Examples:\n        &gt;&gt;&gt; data = [('v1', 0.4, 'assessment 1'), ('v1', None, None), ('v1', None, None),('v2', None, None),]\n        &gt;&gt;&gt; (\n        ...     spark.createDataFrame(data, ['v', 'a', 'b'])\n        ...    .groupBy('v')\n        ...    .agg(f.collect_list(f.struct(f.struct(\n        ...        f.col('a').alias('am_pathogenicity'),\n        ...        f.col('b').alias('am_class')).alias('alphamissense'),\n        ...        f.lit('gene1').alias('gene_id'))).alias('transcripts')\n        ...    )\n        ...    .select(VariantEffectPredictorParser._get_max_alpha_missense(f.col('transcripts')).alias('am'))\n        ...    .show(truncate=False)\n        ... )\n        +-----------------------------------------------------+\n        |am                                                   |\n        +-----------------------------------------------------+\n        |{AlphaMissense, assessment 1, 0.4, NULL, gene1, NULL}|\n        |{AlphaMissense, NULL, NULL, NULL, gene1, NULL}       |\n        +-----------------------------------------------------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        # Extracting transcript with alpha missense values:\n        transcript = f.filter(\n            transcripts,\n            lambda transcript: transcript.getItem(\"alphamissense\").isNotNull(),\n        )[0]\n\n        return f.when(\n            transcript.isNotNull(),\n            f.struct(\n                # Adding method:\n                f.lit(\"AlphaMissense\").alias(\"method\"),\n                # Extracting assessment:\n                transcript.alphamissense.am_class.alias(\"assessment\"),\n                # Extracting score:\n                transcript.alphamissense.am_pathogenicity.cast(t.FloatType()).alias(\n                    \"score\"\n                ),\n                # Adding assessment flag:\n                f.lit(None).cast(t.StringType()).alias(\"assessmentFlag\"),\n                # Extracting target id:\n                transcript.gene_id.alias(\"targetId\"),\n            ),\n        )\n\n    @classmethod\n    @enforce_schema(VARIANT_EFFECT_SCHEMA)\n    def _vep_variant_effect_extractor(\n        cls: type[VariantEffectPredictorParser],\n        transcript_column_name: str,\n        method_name: str,\n        score_column_name: str | None = None,\n        assessment_column_name: str | None = None,\n        assessment_flag_column_name: str | None = None,\n    ) -&gt; Column:\n        \"\"\"Extract variant effect from VEP output.\n\n        Args:\n            transcript_column_name (str): Name of the column containing the list of transcripts.\n            method_name (str): Name of the variant effect.\n            score_column_name (str | None): Name of the column containing the score.\n            assessment_column_name (str | None): Name of the column containing the assessment.\n            assessment_flag_column_name (str | None): Name of the column containing the assessment flag.\n\n        Returns:\n            Column: Variant effect.\n        \"\"\"\n        # Get transcript with the highest score:\n        most_severe_transcript: Column = (\n            # Getting the most severe transcript:\n            VariantEffectPredictorParser._get_most_severe_transcript(\n                transcript_column_name, score_column_name\n            )\n            if score_column_name is not None\n            # If we don't have score, just pick one of the transcript where assessment is available:\n            else f.filter(\n                f.col(transcript_column_name),\n                lambda transcript: transcript.getItem(\n                    assessment_column_name\n                ).isNotNull(),\n            )\n        )\n\n        # Get assessment:\n        assessment = (\n            f.lit(None).cast(t.StringType()).alias(\"assessment\")\n            if assessment_column_name is None\n            else most_severe_transcript.getField(assessment_column_name).alias(\n                \"assessment\"\n            )\n        )\n\n        # Get score:\n        score = (\n            f.lit(None).cast(t.FloatType()).alias(\"score\")\n            if score_column_name is None\n            else most_severe_transcript.getField(score_column_name)\n            .cast(t.FloatType())\n            .alias(\"score\")\n        )\n\n        # Get assessment flag:\n        assessment_flag = (\n            f.lit(None).cast(t.StringType()).alias(\"assessmentFlag\")\n            if assessment_flag_column_name is None\n            else most_severe_transcript.getField(assessment_flag_column_name)\n            .cast(t.StringType())\n            .alias(\"assessmentFlag\")\n        )\n\n        # Extract gene id:\n        gene_id = most_severe_transcript.getItem(\"gene_id\").alias(\"targetId\")\n\n        return f.when(\n            most_severe_transcript.isNotNull(),\n            f.struct(\n                f.lit(method_name).alias(\"method\"),\n                assessment,\n                score,\n                assessment_flag,\n                gene_id,\n            ),\n        )\n\n    @staticmethod\n    def _parser_amino_acid_change(amino_acids: Column, protein_end: Column) -&gt; Column:\n        \"\"\"Convert VEP amino acid change information to one letter code aa substitution code.\n\n        The logic assumes that the amino acid change is given in the format \"from/to\" and the protein end is given also.\n        If any of the information is missing, the amino acid change will be set to None.\n\n        Args:\n            amino_acids (Column): Amino acid change information.\n            protein_end (Column): Protein end information.\n\n        Returns:\n            Column: Amino acid change in one letter code.\n\n        Examples:\n            &gt;&gt;&gt; data = [('A/B', 1),('A/B', None),(None, 1),]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(data, ['amino_acids', 'protein_end'])\n            ...    .select(VariantEffectPredictorParser._parser_amino_acid_change(f.col('amino_acids'), f.col('protein_end')).alias('amino_acid_change'))\n            ...    .show()\n            ... )\n            +-----------------+\n            |amino_acid_change|\n            +-----------------+\n            |              A1B|\n            |             NULL|\n            |             NULL|\n            +-----------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.when(\n            amino_acids.isNotNull() &amp; protein_end.isNotNull(),\n            f.concat(\n                f.split(amino_acids, r\"\\/\")[0],\n                protein_end,\n                f.split(amino_acids, r\"\\/\")[1],\n            ),\n        ).otherwise(f.lit(None))\n\n    @staticmethod\n    def _collect_uniprot_accessions(trembl: Column, swissprot: Column) -&gt; Column:\n        \"\"\"Flatten arrays containing Uniprot accessions.\n\n        Args:\n            trembl (Column): List of TrEMBL protein accessions.\n            swissprot (Column): List of SwissProt protein accessions.\n\n        Returns:\n            Column: List of unique Uniprot accessions extracted from swissprot and trembl arrays, splitted from version numbers.\n\n        Examples:\n        &gt;&gt;&gt; data = [('v1', [\"sp_1\"], [\"tr_1\"]), ('v1', None, None), ('v1', None, [\"tr_2\"]),]\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame(data, ['v', 'sp', 'tr'])\n        ...    .select(VariantEffectPredictorParser._collect_uniprot_accessions(f.col('sp'), f.col('tr')).alias('proteinIds'))\n        ...    .show()\n        ... )\n        +------------+\n        |  proteinIds|\n        +------------+\n        |[sp_1, tr_1]|\n        |          []|\n        |      [tr_2]|\n        +------------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        # Dropping Null values and flattening the arrays:\n        return f.filter(\n            f.array_distinct(\n                f.transform(\n                    f.flatten(\n                        f.filter(\n                            f.array(trembl, swissprot),\n                            lambda x: x.isNotNull(),\n                        )\n                    ),\n                    lambda x: f.split(x, r\"\\.\")[0],\n                )\n            ),\n            lambda x: x.isNotNull(),\n        )\n\n    @staticmethod\n    def _parse_variant_location_id(vep_input_field: Column) -&gt; list[Column]:\n        r\"\"\"Parse variant identifier, chromosome, position, reference allele and alternate allele from VEP input field.\n\n        Args:\n            vep_input_field (Column): Column containing variant vcf string used as VEP input.\n\n        Returns:\n            list[Column]: List of columns containing chromosome, position, reference allele and alternate allele.\n        \"\"\"\n        variant_fields = f.split(vep_input_field, r\"\\t\")\n        return [\n            f.concat_ws(\n                \"_\",\n                f.array(\n                    variant_fields[0],\n                    variant_fields[1],\n                    variant_fields[3],\n                    variant_fields[4],\n                ),\n            ).alias(\"variantId\"),\n            variant_fields[0].cast(t.StringType()).alias(\"chromosome\"),\n            variant_fields[1].cast(t.IntegerType()).alias(\"position\"),\n            variant_fields[3].cast(t.StringType()).alias(\"referenceAllele\"),\n            variant_fields[4].cast(t.StringType()).alias(\"alternateAllele\"),\n        ]\n\n    @classmethod\n    def process_vep_output(\n        cls, vep_output: DataFrame, hash_threshold: int = 100\n    ) -&gt; DataFrame:\n        \"\"\"Process and format a VEP output in JSON format.\n\n        Args:\n            vep_output (DataFrame): raw VEP output, read as spark DataFrame.\n            hash_threshold (int): threshold above which variant identifiers will be hashed.\n\n        Returns:\n           DataFrame: processed data in the right shape.\n        \"\"\"\n        # Processing VEP output:\n        return (\n            vep_output\n            # Dropping non-canonical transcript consequences:  # TODO: parametrize this.\n            .withColumn(\n                \"transcript_consequences\",\n                f.filter(\n                    f.col(\"transcript_consequences\"),\n                    lambda consequence: consequence.getItem(\"canonical\") == 1,\n                ),\n            )\n            .select(\n                # Parse id and chr:pos:alt:ref:\n                *cls._parse_variant_location_id(f.col(\"input\")),\n                # Extracting corss_references from colocated variants:\n                cls._extract_ensembl_xrefs(f.col(\"colocated_variants\")).alias(\n                    \"ensembl_xrefs\"\n                ),\n                cls._extract_omim_xrefs(f.col(\"colocated_variants\")).alias(\n                    \"omim_xrefs\"\n                ),\n                cls._extract_clinvar_xrefs(f.col(\"colocated_variants\")).alias(\n                    \"clinvar_xrefs\"\n                ),\n                # Extracting variant effect assessments\n                f.when(\n                    # The following variant effect assessments are only available for variants with transcript consequences:\n                    f.col(\"transcript_consequences\").isNotNull(),\n                    f.filter(\n                        f.array(\n                            # Extract CADD scores:\n                            cls._vep_variant_effect_extractor(\n                                transcript_column_name=\"transcript_consequences\",\n                                method_name=\"CADD\",\n                                score_column_name=\"cadd_phred\",\n                            ),\n                            # Extract polyphen scores:\n                            cls._vep_variant_effect_extractor(\n                                transcript_column_name=\"transcript_consequences\",\n                                method_name=\"PolyPhen\",\n                                score_column_name=\"polyphen_score\",\n                                assessment_column_name=\"polyphen_prediction\",\n                            ),\n                            # Extract sift scores:\n                            cls._vep_variant_effect_extractor(\n                                transcript_column_name=\"transcript_consequences\",\n                                method_name=\"SIFT\",\n                                score_column_name=\"sift_score\",\n                                assessment_column_name=\"sift_prediction\",\n                            ),\n                            # Extract loftee scores:\n                            cls._vep_variant_effect_extractor(\n                                method_name=\"LOFTEE\",\n                                transcript_column_name=\"transcript_consequences\",\n                                score_column_name=\"lof\",\n                                assessment_column_name=\"lof\",\n                                assessment_flag_column_name=\"lof_filter\",\n                            ),\n                            # Extract GERP conservation score:\n                            cls._vep_variant_effect_extractor(\n                                method_name=\"GERP\",\n                                transcript_column_name=\"transcript_consequences\",\n                                score_column_name=\"conservation\",\n                            ),\n                            # Extract max alpha missense:\n                            cls._get_max_alpha_missense(\n                                f.col(\"transcript_consequences\")\n                            ),\n                            # Extract VEP prediction:\n                            cls._get_vep_prediction(f.col(\"most_severe_consequence\")),\n                        ),\n                        lambda predictor: predictor.isNotNull(),\n                    ),\n                )\n                .otherwise(\n                    f.array(\n                        # Extract VEP prediction:\n                        cls._get_vep_prediction(f.col(\"most_severe_consequence\")),\n                    )\n                )\n                .alias(\"variantEffect\"),\n                # Convert consequence to SO:\n                map_column_by_dictionary(\n                    f.col(\"most_severe_consequence\"), cls.SEQUENCE_ONTOLOGY_MAP\n                ).alias(\"mostSevereConsequenceId\"),\n                # Propagate most severe consequence:\n                \"most_severe_consequence\",\n                # Extract HGVS identifier:\n                f.when(\n                    f.size(\"transcript_consequences\") &gt; 0,\n                    f.col(\"transcript_consequences\").getItem(0).getItem(\"hgvsg\"),\n                )\n                .when(\n                    f.size(\"intergenic_consequences\") &gt; 0,\n                    f.col(\"intergenic_consequences\").getItem(0).getItem(\"hgvsg\"),\n                )\n                .otherwise(f.lit(None))\n                .alias(\"hgvsId\"),\n                # Collect transcript consequence:\n                f.when(\n                    f.col(\"transcript_consequences\").isNotNull(),\n                    f.transform(\n                        f.col(\"transcript_consequences\"),\n                        lambda transcript: f.struct(\n                            # Convert consequence terms to SO identifier:\n                            f.transform(\n                                transcript.consequence_terms,\n                                lambda y: map_column_by_dictionary(\n                                    y, cls.SEQUENCE_ONTOLOGY_MAP\n                                ),\n                            ).alias(\"variantFunctionalConsequenceIds\"),\n                            # Convert consequence terms to consequence score:\n                            f.array_max(\n                                f.transform(\n                                    transcript.consequence_terms,\n                                    lambda term: map_column_by_dictionary(\n                                        term, cls.LABEL_TO_SCORE_MAP\n                                    ),\n                                )\n                            )\n                            .cast(t.FloatType())\n                            .alias(\"consequenceScore\"),\n                            # Format amino acid change:\n                            cls._parser_amino_acid_change(\n                                transcript.amino_acids, transcript.protein_end\n                            ).alias(\"aminoAcidChange\"),\n                            # Extract and clean uniprot identifiers:\n                            cls._collect_uniprot_accessions(\n                                transcript.swissprot,\n                                transcript.trembl,\n                            ).alias(\"uniprotAccessions\"),\n                            # Add canonical flag:\n                            f.when(transcript.canonical == 1, f.lit(True))\n                            .otherwise(f.lit(False))\n                            .alias(\"isEnsemblCanonical\"),\n                            # Extract footprint distance:\n                            transcript.codons.alias(\"codons\"),\n                            f.when(transcript.distance.isNotNull(), transcript.distance)\n                            .otherwise(f.lit(0))\n                            .cast(t.LongType())\n                            .alias(\"distanceFromFootprint\"),\n                            # Extract distance from the transcription start site:\n                            transcript.tssdistance.cast(t.LongType()).alias(\n                                \"distanceFromTss\"\n                            ),\n                            # Extracting APPRIS isoform annotation for this transcript:\n                            transcript.appris.alias(\"appris\"),\n                            # Extracting MANE select transcript:\n                            transcript.mane_select.alias(\"maneSelect\"),\n                            transcript.gene_id.alias(\"targetId\"),\n                            transcript.impact.alias(\"impact\"),\n                            transcript.lof.cast(t.StringType()).alias(\n                                \"lofteePrediction\"\n                            ),\n                            transcript.lof.cast(t.FloatType()).alias(\"siftPrediction\"),\n                            transcript.lof.cast(t.FloatType()).alias(\n                                \"polyphenPrediction\"\n                            ),\n                            transcript.transcript_id.alias(\"transcriptId\"),\n                            transcript.biotype.alias(\"biotype\"),\n                            transcript.gene_symbol.alias(\"approvedSymbol\"),\n                        ),\n                    ),\n                ).alias(\"transcriptConsequences\"),\n                # Extracting rsids:\n                cls._colocated_variants_to_rsids(f.col(\"colocated_variants\")).alias(\n                    \"rsIds\"\n                ),\n                # Adding empty array for allele frequencies - now this piece of data is not coming form the VEP data:\n                f.array().cast(cls.ALLELE_FREQUENCY_SCHEMA).alias(\"alleleFrequencies\"),\n            )\n            # Dropping transcripts where the consequence score or the distance is null:\n            .withColumn(\n                \"transcriptConsequences\",\n                f.filter(\n                    f.col(\"transcriptConsequences\"),\n                    lambda x: x.getItem(\"consequenceScore\").isNotNull()\n                    &amp; x.getItem(\"distanceFromFootprint\").isNotNull(),\n                ),\n            )\n            # Sort transcript consequences by consequence score and distance from footprint and add index:\n            .withColumn(\n                \"transcriptConsequences\",\n                f.when(\n                    f.col(\"transcriptConsequences\").isNotNull(),\n                    f.transform(\n                        order_array_of_structs_by_two_fields(\n                            \"transcriptConsequences\",\n                            \"consequenceScore\",\n                            \"distanceFromFootprint\",\n                        ),\n                        lambda x, i: x.withField(\"transcriptIndex\", i + f.lit(1)),\n                    ),\n                ),\n            )\n            # Adding protvar xref for missense variants:  # TODO: making and extendable list of consequences\n            .withColumn(\n                \"protvar_xrefs\",\n                f.when(\n                    f.size(\n                        f.filter(\n                            f.col(\"transcriptConsequences\"),\n                            lambda x: f.array_contains(\n                                x.variantFunctionalConsequenceIds, \"SO_0001583\"\n                            ),\n                        )\n                    )\n                    &gt; 0,\n                    cls._generate_dbxrefs(f.array(f.col(\"variantId\")), \"protvar\"),\n                ),\n            )\n            .withColumn(\n                \"dbXrefs\",\n                f.flatten(\n                    f.filter(\n                        f.array(\n                            f.col(\"ensembl_xrefs\"),\n                            f.col(\"omim_xrefs\"),\n                            f.col(\"clinvar_xrefs\"),\n                            f.col(\"protvar_xrefs\"),\n                        ),\n                        lambda x: x.isNotNull(),\n                    )\n                ),\n            )\n            # If the variantId is too long, hash it:\n            .withColumn(\n                \"variantId\",\n                VariantIndex.hash_long_variant_ids(\n                    f.col(\"variantId\"),\n                    f.col(\"chromosome\"),\n                    f.col(\"position\"),\n                    hash_threshold,\n                ),\n            )\n            # Generating a temporary column with only protein coding transcripts:\n            .withColumn(\n                \"proteinCodingTranscripts\",\n                f.filter(\n                    f.col(\"transcriptConsequences\"),\n                    lambda x: x.getItem(\"biotype\") == \"protein_coding\",\n                ),\n            )\n            # Generate variant descrioption:\n            .withColumn(\n                \"variantDescription\",\n                cls._compose_variant_description(\n                    # Passing the most severe consequence:\n                    f.col(\"most_severe_consequence\"),\n                    # The first transcript:\n                    f.filter(\n                        f.col(\"transcriptConsequences\"),\n                        lambda vep: vep.transcriptIndex == 1,\n                    ).getItem(0),\n                    # The first protein coding transcript:\n                    order_array_of_structs_by_field(\n                        \"proteinCodingTranscripts\", \"transcriptIndex\"\n                    )[f.size(\"proteinCodingTranscripts\") - 1],\n                ),\n            )\n            # Normalising variant effect assessments:\n            .withColumn(\n                \"variantEffect\",\n                VariantEffectNormaliser.normalise_variant_effect(\n                    f.col(\"variantEffect\")\n                ),\n            )\n            # Dropping intermediate xref columns:\n            .drop(\n                *[\n                    \"ensembl_xrefs\",\n                    \"omim_xrefs\",\n                    \"clinvar_xrefs\",\n                    \"protvar_xrefs\",\n                    \"most_severe_consequence\",\n                    \"proteinCodingTranscripts\",\n                ]\n            )\n            # Drooping rows with null position:\n            .filter(f.col(\"position\").isNotNull())\n        )\n\n    @classmethod\n    def _compose_variant_description(\n        cls: type[VariantEffectPredictorParser],\n        most_severe_consequence: Column,\n        first_transcript: Column,\n        first_protein_coding: Column,\n    ) -&gt; Column:\n        \"\"\"Compose variant description based on the most severe consequence.\n\n        Args:\n            most_severe_consequence (Column): Most severe consequence\n            first_transcript (Column): First transcript\n            first_protein_coding (Column): First protein coding transcript\n\n        Returns:\n            Column: Variant description\n        \"\"\"\n        return (\n            # When there's no transcript whatsoever:\n            f.when(\n                first_transcript.isNull(),\n                f.lit(\"Intergenic variant no gene in window\"),\n            )\n            # When the biotype of the first gene is protein coding:\n            .when(\n                first_transcript.getItem(\"biotype\") == \"protein_coding\",\n                cls._process_protein_coding_transcript(\n                    first_transcript, most_severe_consequence\n                ),\n            )\n            # When the first gene is not protein coding, we also pass the first protein coding gene:\n            .otherwise(\n                cls._process_non_protein_coding_transcript(\n                    most_severe_consequence, first_transcript, first_protein_coding\n                )\n            )\n        )\n\n    @staticmethod\n    def _process_consequence_term(consequence_term: Column) -&gt; Column:\n        \"\"\"Cleaning up consequence term: capitalizing and replacing underscores.\n\n        Args:\n            consequence_term (Column): Consequence term.\n\n        Returns:\n            Column: Cleaned up consequence term.\n        \"\"\"\n        last = f.when(consequence_term.contains(\"variant\"), f.lit(\"\")).otherwise(\n            \" variant\"\n        )\n        return f.concat(f.regexp_replace(f.initcap(consequence_term), \"_\", \" \"), last)\n\n    @staticmethod\n    def _process_overlap(transcript: Column) -&gt; Column:\n        \"\"\"Process overlap with gene: if the variant overlaps with the gene, return the gene name or distance.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: string column with overlap description.\n        \"\"\"\n        gene_label = f.when(\n            transcript.getField(\"approvedSymbol\").isNotNull(),\n            transcript.getField(\"approvedSymbol\"),\n        ).otherwise(transcript.getField(\"targetId\"))\n\n        return f.when(\n            transcript.getField(\"distanceFromFootprint\") == 0,\n            # \"overlapping with CCDC8\"\n            f.concat(f.lit(\" overlapping with \"), gene_label),\n        ).otherwise(\n            # \" 123 basepair away from CCDC8\"\n            f.concat(\n                f.lit(\" \"),\n                f.format_number(transcript.getField(\"distanceFromFootprint\"), 0),\n                f.lit(\" basepair away from \"),\n                gene_label,\n            )\n        )\n\n    @staticmethod\n    def _process_aa_change(transcript: Column) -&gt; Column:\n        \"\"\"Extract amino acid change information from transcript when available.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: Amino acid change information.\n        \"\"\"\n        return f.when(\n            transcript.getField(\"aminoAcidChange\").isNotNull(),\n            f.concat(\n                f.lit(\", causing amino-acid change: \"),\n                transcript.getField(\"aminoAcidChange\"),\n                f.lit(\" with \"),\n                f.lower(transcript.getField(\"impact\")),\n                f.lit(\" impact.\"),\n            ),\n        ).otherwise(f.lit(\".\"))\n\n    @staticmethod\n    def _process_lof(transcript: Column) -&gt; Column:\n        \"\"\"Process loss of function annotation from LOFTEE prediction.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: Loss of function annotation.\n        \"\"\"\n        return f.when(\n            transcript.getField(\"lofteePrediction\").isNotNull()\n            &amp; (transcript.getField(\"lofteePrediction\") == \"HC\"),\n            f.lit(\" A high-confidence loss-of-function variant by loftee.\"),\n        ).otherwise(f.lit(\"\"))\n\n    @classmethod\n    def _process_protein_coding_transcript(\n        cls: type[VariantEffectPredictorParser],\n        transcript: Column,\n        most_severe_consequence: Column,\n    ) -&gt; Column:\n        \"\"\"Extract information from the first, protein coding transcript.\n\n        Args:\n            transcript (Column): Transcript.\n            most_severe_consequence (Column): Most severe consequence.\n\n        Returns:\n            Column: Variant description.\n        \"\"\"\n        # Process consequence term:\n        consequence_text = cls._process_consequence_term(most_severe_consequence)\n\n        # Does it overlap with the gene:\n        overlap = cls._process_overlap(transcript)\n\n        # Does it cause amino acid change:\n        amino_acid_change = cls._process_aa_change(transcript)\n\n        # Processing lof annotation:\n        lof_assessment = cls._process_lof(transcript)\n\n        # Concat all together:\n        return f.concat(consequence_text, overlap, amino_acid_change, lof_assessment)\n\n    @staticmethod\n    def _adding_biotype(transcript: Column) -&gt; Column:\n        \"\"\"Adding biotype information to the variant description.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: Biotype information.\n        \"\"\"\n        biotype = f.when(\n            transcript.getField(\"biotype\").contains(\"gene\"),\n            f.regexp_replace(transcript.getField(\"biotype\"), \"_\", \" \"),\n        ).otherwise(\n            f.concat(\n                f.regexp_replace(transcript.getField(\"biotype\"), \"_\", \" \"),\n                f.lit(\" gene.\"),\n            )\n        )\n\n        return f.concat(f.lit(\", a \"), biotype)\n\n    @staticmethod\n    def _parse_protein_coding_transcript(transcript: Column) -&gt; Column:\n        \"\"\"Parse the closest, not first protein coding transcript: extract gene symbol and distance.\n\n        Args:\n            transcript (Column): Transcript.\n\n        Returns:\n            Column: Protein coding transcript information.\n        \"\"\"\n        gene_label = f.when(\n            transcript.getField(\"approvedSymbol\").isNotNull(),\n            transcript.getField(\"approvedSymbol\"),\n        ).otherwise(transcript.getField(\"targetId\"))\n\n        return f.when(\n            transcript.isNotNull(),\n            f.concat(\n                f.lit(\" The closest protein-coding gene is \"),\n                gene_label,\n                f.lit(\" (\"),\n                f.format_number(transcript.getField(\"distanceFromFootprint\"), 0),\n                f.lit(\" basepair away).\"),\n            ),\n        ).otherwise(f.lit(\"\"))\n\n    @classmethod\n    def _process_non_protein_coding_transcript(\n        cls: type[VariantEffectPredictorParser],\n        most_severe_consequence: Column,\n        first_transcript: Column,\n        first_protein_coding: Column,\n    ) -&gt; Column:\n        \"\"\"Extract information from the first, non-protein coding transcript.\n\n        Args:\n            most_severe_consequence (Column): Most severe consequence.\n            first_transcript (Column): First transcript.\n            first_protein_coding (Column): First protein coding transcript.\n\n        Returns:\n            Column: Variant description.\n        \"\"\"\n        # Process consequence term:\n        consequence_text = cls._process_consequence_term(most_severe_consequence)\n\n        # Does it overlap with the gene:\n        overlap = cls._process_overlap(first_transcript)\n\n        # Adding biotype:\n        biotype = cls._adding_biotype(first_transcript)\n\n        # Adding protein coding gene:\n        protein_transcript = cls._parse_protein_coding_transcript(first_protein_coding)\n\n        # Concat all together:\n        return f.concat(consequence_text, overlap, biotype, protein_transcript)\n</code></pre>"},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/#gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser.extract_variant_index_from_vep","title":"<code>extract_variant_index_from_vep(spark: SparkSession, vep_output_path: str | list[str], hash_threshold: int, **kwargs: bool | float | int | str | None) -&gt; VariantIndex</code>  <code>classmethod</code>","text":"<p>Extract variant index from VEP output.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>vep_output_path</code> <code>str | list[str]</code> <p>Path to the VEP output.</p> required <code>hash_threshold</code> <code>int</code> <p>Threshold above which variant identifiers will be hashed.</p> required <code>**kwargs</code> <code>bool | float | int | str | None</code> <p>Additional arguments to pass to spark.read.json.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>Variant index dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Failed reading file or if the dataset is empty.</p> Source code in <code>src/gentropy/datasource/ensembl/vep_parser.py</code> <pre><code>@classmethod\ndef extract_variant_index_from_vep(\n    cls: type[VariantEffectPredictorParser],\n    spark: SparkSession,\n    vep_output_path: str | list[str],\n    hash_threshold: int,\n    **kwargs: bool | float | int | str | None,\n) -&gt; VariantIndex:\n    \"\"\"Extract variant index from VEP output.\n\n    Args:\n        spark (SparkSession): Spark session.\n        vep_output_path (str | list[str]): Path to the VEP output.\n        hash_threshold (int): Threshold above which variant identifiers will be hashed.\n        **kwargs (bool | float | int | str | None): Additional arguments to pass to spark.read.json.\n\n    Returns:\n        VariantIndex: Variant index dataset.\n\n    Raises:\n        ValueError: Failed reading file or if the dataset is empty.\n    \"\"\"\n    # To speed things up and simplify the json structure, read data following an expected schema:\n    vep_schema = cls.get_schema()\n\n    try:\n        vep_data = spark.read.json(vep_output_path, schema=vep_schema, **kwargs)\n    except ValueError as e:\n        raise ValueError(f\"Failed reading file: {vep_output_path}.\") from e\n\n    if vep_data.isEmpty():\n        raise ValueError(f\"The dataset is empty: {vep_output_path}\")\n\n    # Convert to VariantAnnotation dataset:\n    return VariantIndex(\n        _df=VariantEffectPredictorParser.process_vep_output(\n            vep_data, hash_threshold\n        ),\n        _schema=VariantIndex.get_schema(),\n        id_threshold=hash_threshold,\n    )\n</code></pre>"},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/#gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser.get_schema","title":"<code>get_schema() -&gt; t.StructType</code>  <code>staticmethod</code>","text":"<p>Return the schema of the VEP output.</p> <p>Returns:</p> Type Description <code>StructType</code> <p>t.StructType: VEP output schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; type(VariantEffectPredictorParser.get_schema())\n&lt;class 'pyspark.sql.types.StructType'&gt;\n</code></pre> Source code in <code>src/gentropy/datasource/ensembl/vep_parser.py</code> <pre><code>@staticmethod\ndef get_schema() -&gt; t.StructType:\n    \"\"\"Return the schema of the VEP output.\n\n    Returns:\n        t.StructType: VEP output schema.\n\n    Examples:\n        &gt;&gt;&gt; type(VariantEffectPredictorParser.get_schema())\n        &lt;class 'pyspark.sql.types.StructType'&gt;\n    \"\"\"\n    return parse_spark_schema(\"vep_json_output.json\")\n</code></pre>"},{"location":"python_api/datasources/ensembl/variant_effect_predictor_parser/#gentropy.datasource.ensembl.vep_parser.VariantEffectPredictorParser.process_vep_output","title":"<code>process_vep_output(vep_output: DataFrame, hash_threshold: int = 100) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Process and format a VEP output in JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>vep_output</code> <code>DataFrame</code> <p>raw VEP output, read as spark DataFrame.</p> required <code>hash_threshold</code> <code>int</code> <p>threshold above which variant identifiers will be hashed.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>processed data in the right shape.</p> Source code in <code>src/gentropy/datasource/ensembl/vep_parser.py</code> <pre><code>@classmethod\ndef process_vep_output(\n    cls, vep_output: DataFrame, hash_threshold: int = 100\n) -&gt; DataFrame:\n    \"\"\"Process and format a VEP output in JSON format.\n\n    Args:\n        vep_output (DataFrame): raw VEP output, read as spark DataFrame.\n        hash_threshold (int): threshold above which variant identifiers will be hashed.\n\n    Returns:\n       DataFrame: processed data in the right shape.\n    \"\"\"\n    # Processing VEP output:\n    return (\n        vep_output\n        # Dropping non-canonical transcript consequences:  # TODO: parametrize this.\n        .withColumn(\n            \"transcript_consequences\",\n            f.filter(\n                f.col(\"transcript_consequences\"),\n                lambda consequence: consequence.getItem(\"canonical\") == 1,\n            ),\n        )\n        .select(\n            # Parse id and chr:pos:alt:ref:\n            *cls._parse_variant_location_id(f.col(\"input\")),\n            # Extracting corss_references from colocated variants:\n            cls._extract_ensembl_xrefs(f.col(\"colocated_variants\")).alias(\n                \"ensembl_xrefs\"\n            ),\n            cls._extract_omim_xrefs(f.col(\"colocated_variants\")).alias(\n                \"omim_xrefs\"\n            ),\n            cls._extract_clinvar_xrefs(f.col(\"colocated_variants\")).alias(\n                \"clinvar_xrefs\"\n            ),\n            # Extracting variant effect assessments\n            f.when(\n                # The following variant effect assessments are only available for variants with transcript consequences:\n                f.col(\"transcript_consequences\").isNotNull(),\n                f.filter(\n                    f.array(\n                        # Extract CADD scores:\n                        cls._vep_variant_effect_extractor(\n                            transcript_column_name=\"transcript_consequences\",\n                            method_name=\"CADD\",\n                            score_column_name=\"cadd_phred\",\n                        ),\n                        # Extract polyphen scores:\n                        cls._vep_variant_effect_extractor(\n                            transcript_column_name=\"transcript_consequences\",\n                            method_name=\"PolyPhen\",\n                            score_column_name=\"polyphen_score\",\n                            assessment_column_name=\"polyphen_prediction\",\n                        ),\n                        # Extract sift scores:\n                        cls._vep_variant_effect_extractor(\n                            transcript_column_name=\"transcript_consequences\",\n                            method_name=\"SIFT\",\n                            score_column_name=\"sift_score\",\n                            assessment_column_name=\"sift_prediction\",\n                        ),\n                        # Extract loftee scores:\n                        cls._vep_variant_effect_extractor(\n                            method_name=\"LOFTEE\",\n                            transcript_column_name=\"transcript_consequences\",\n                            score_column_name=\"lof\",\n                            assessment_column_name=\"lof\",\n                            assessment_flag_column_name=\"lof_filter\",\n                        ),\n                        # Extract GERP conservation score:\n                        cls._vep_variant_effect_extractor(\n                            method_name=\"GERP\",\n                            transcript_column_name=\"transcript_consequences\",\n                            score_column_name=\"conservation\",\n                        ),\n                        # Extract max alpha missense:\n                        cls._get_max_alpha_missense(\n                            f.col(\"transcript_consequences\")\n                        ),\n                        # Extract VEP prediction:\n                        cls._get_vep_prediction(f.col(\"most_severe_consequence\")),\n                    ),\n                    lambda predictor: predictor.isNotNull(),\n                ),\n            )\n            .otherwise(\n                f.array(\n                    # Extract VEP prediction:\n                    cls._get_vep_prediction(f.col(\"most_severe_consequence\")),\n                )\n            )\n            .alias(\"variantEffect\"),\n            # Convert consequence to SO:\n            map_column_by_dictionary(\n                f.col(\"most_severe_consequence\"), cls.SEQUENCE_ONTOLOGY_MAP\n            ).alias(\"mostSevereConsequenceId\"),\n            # Propagate most severe consequence:\n            \"most_severe_consequence\",\n            # Extract HGVS identifier:\n            f.when(\n                f.size(\"transcript_consequences\") &gt; 0,\n                f.col(\"transcript_consequences\").getItem(0).getItem(\"hgvsg\"),\n            )\n            .when(\n                f.size(\"intergenic_consequences\") &gt; 0,\n                f.col(\"intergenic_consequences\").getItem(0).getItem(\"hgvsg\"),\n            )\n            .otherwise(f.lit(None))\n            .alias(\"hgvsId\"),\n            # Collect transcript consequence:\n            f.when(\n                f.col(\"transcript_consequences\").isNotNull(),\n                f.transform(\n                    f.col(\"transcript_consequences\"),\n                    lambda transcript: f.struct(\n                        # Convert consequence terms to SO identifier:\n                        f.transform(\n                            transcript.consequence_terms,\n                            lambda y: map_column_by_dictionary(\n                                y, cls.SEQUENCE_ONTOLOGY_MAP\n                            ),\n                        ).alias(\"variantFunctionalConsequenceIds\"),\n                        # Convert consequence terms to consequence score:\n                        f.array_max(\n                            f.transform(\n                                transcript.consequence_terms,\n                                lambda term: map_column_by_dictionary(\n                                    term, cls.LABEL_TO_SCORE_MAP\n                                ),\n                            )\n                        )\n                        .cast(t.FloatType())\n                        .alias(\"consequenceScore\"),\n                        # Format amino acid change:\n                        cls._parser_amino_acid_change(\n                            transcript.amino_acids, transcript.protein_end\n                        ).alias(\"aminoAcidChange\"),\n                        # Extract and clean uniprot identifiers:\n                        cls._collect_uniprot_accessions(\n                            transcript.swissprot,\n                            transcript.trembl,\n                        ).alias(\"uniprotAccessions\"),\n                        # Add canonical flag:\n                        f.when(transcript.canonical == 1, f.lit(True))\n                        .otherwise(f.lit(False))\n                        .alias(\"isEnsemblCanonical\"),\n                        # Extract footprint distance:\n                        transcript.codons.alias(\"codons\"),\n                        f.when(transcript.distance.isNotNull(), transcript.distance)\n                        .otherwise(f.lit(0))\n                        .cast(t.LongType())\n                        .alias(\"distanceFromFootprint\"),\n                        # Extract distance from the transcription start site:\n                        transcript.tssdistance.cast(t.LongType()).alias(\n                            \"distanceFromTss\"\n                        ),\n                        # Extracting APPRIS isoform annotation for this transcript:\n                        transcript.appris.alias(\"appris\"),\n                        # Extracting MANE select transcript:\n                        transcript.mane_select.alias(\"maneSelect\"),\n                        transcript.gene_id.alias(\"targetId\"),\n                        transcript.impact.alias(\"impact\"),\n                        transcript.lof.cast(t.StringType()).alias(\n                            \"lofteePrediction\"\n                        ),\n                        transcript.lof.cast(t.FloatType()).alias(\"siftPrediction\"),\n                        transcript.lof.cast(t.FloatType()).alias(\n                            \"polyphenPrediction\"\n                        ),\n                        transcript.transcript_id.alias(\"transcriptId\"),\n                        transcript.biotype.alias(\"biotype\"),\n                        transcript.gene_symbol.alias(\"approvedSymbol\"),\n                    ),\n                ),\n            ).alias(\"transcriptConsequences\"),\n            # Extracting rsids:\n            cls._colocated_variants_to_rsids(f.col(\"colocated_variants\")).alias(\n                \"rsIds\"\n            ),\n            # Adding empty array for allele frequencies - now this piece of data is not coming form the VEP data:\n            f.array().cast(cls.ALLELE_FREQUENCY_SCHEMA).alias(\"alleleFrequencies\"),\n        )\n        # Dropping transcripts where the consequence score or the distance is null:\n        .withColumn(\n            \"transcriptConsequences\",\n            f.filter(\n                f.col(\"transcriptConsequences\"),\n                lambda x: x.getItem(\"consequenceScore\").isNotNull()\n                &amp; x.getItem(\"distanceFromFootprint\").isNotNull(),\n            ),\n        )\n        # Sort transcript consequences by consequence score and distance from footprint and add index:\n        .withColumn(\n            \"transcriptConsequences\",\n            f.when(\n                f.col(\"transcriptConsequences\").isNotNull(),\n                f.transform(\n                    order_array_of_structs_by_two_fields(\n                        \"transcriptConsequences\",\n                        \"consequenceScore\",\n                        \"distanceFromFootprint\",\n                    ),\n                    lambda x, i: x.withField(\"transcriptIndex\", i + f.lit(1)),\n                ),\n            ),\n        )\n        # Adding protvar xref for missense variants:  # TODO: making and extendable list of consequences\n        .withColumn(\n            \"protvar_xrefs\",\n            f.when(\n                f.size(\n                    f.filter(\n                        f.col(\"transcriptConsequences\"),\n                        lambda x: f.array_contains(\n                            x.variantFunctionalConsequenceIds, \"SO_0001583\"\n                        ),\n                    )\n                )\n                &gt; 0,\n                cls._generate_dbxrefs(f.array(f.col(\"variantId\")), \"protvar\"),\n            ),\n        )\n        .withColumn(\n            \"dbXrefs\",\n            f.flatten(\n                f.filter(\n                    f.array(\n                        f.col(\"ensembl_xrefs\"),\n                        f.col(\"omim_xrefs\"),\n                        f.col(\"clinvar_xrefs\"),\n                        f.col(\"protvar_xrefs\"),\n                    ),\n                    lambda x: x.isNotNull(),\n                )\n            ),\n        )\n        # If the variantId is too long, hash it:\n        .withColumn(\n            \"variantId\",\n            VariantIndex.hash_long_variant_ids(\n                f.col(\"variantId\"),\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                hash_threshold,\n            ),\n        )\n        # Generating a temporary column with only protein coding transcripts:\n        .withColumn(\n            \"proteinCodingTranscripts\",\n            f.filter(\n                f.col(\"transcriptConsequences\"),\n                lambda x: x.getItem(\"biotype\") == \"protein_coding\",\n            ),\n        )\n        # Generate variant descrioption:\n        .withColumn(\n            \"variantDescription\",\n            cls._compose_variant_description(\n                # Passing the most severe consequence:\n                f.col(\"most_severe_consequence\"),\n                # The first transcript:\n                f.filter(\n                    f.col(\"transcriptConsequences\"),\n                    lambda vep: vep.transcriptIndex == 1,\n                ).getItem(0),\n                # The first protein coding transcript:\n                order_array_of_structs_by_field(\n                    \"proteinCodingTranscripts\", \"transcriptIndex\"\n                )[f.size(\"proteinCodingTranscripts\") - 1],\n            ),\n        )\n        # Normalising variant effect assessments:\n        .withColumn(\n            \"variantEffect\",\n            VariantEffectNormaliser.normalise_variant_effect(\n                f.col(\"variantEffect\")\n            ),\n        )\n        # Dropping intermediate xref columns:\n        .drop(\n            *[\n                \"ensembl_xrefs\",\n                \"omim_xrefs\",\n                \"clinvar_xrefs\",\n                \"protvar_xrefs\",\n                \"most_severe_consequence\",\n                \"proteinCodingTranscripts\",\n            ]\n        )\n        # Drooping rows with null position:\n        .filter(f.col(\"position\").isNotNull())\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/_eqtl_catalogue/","title":"eQTL Catalogue","text":"<p>The eQTL Catalogue aims to provide unified gene, protein expression and splicing Quantitative Trait Loci (QTLs) from all available human public studies.</p> <p>It serves as the ultimate resource of molecular QTLs that we use for colocalization and target prioritization.</p>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/","title":"Fine mapping results","text":""},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping","title":"<code>gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping</code>  <code>dataclass</code>","text":"<p>SuSIE finemapping dataset for eQTL Catalogue.</p> <p>Credible sets from SuSIE are extracted and transformed into StudyLocus objects: - A study ID is defined as a triad between: the publication, the tissue, and the measured trait (e.g. Braineac2_substantia_nigra_ENSG00000248275) - Each row in the <code>credible_sets.tsv.gz</code> files is represented by molecular_trait_id/variant/rsid trios relevant for a given tissue. Each have their own finemapping statistics - log Bayes Factors are available for all variants in the <code>lbf_variable.txt</code> files</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@dataclass\nclass EqtlCatalogueFinemapping:\n    \"\"\"SuSIE finemapping dataset for eQTL Catalogue.\n\n    Credible sets from SuSIE are extracted and transformed into StudyLocus objects:\n    - A study ID is defined as a triad between: the publication, the tissue, and the measured trait (e.g. Braineac2_substantia_nigra_ENSG00000248275)\n    - Each row in the `credible_sets.tsv.gz` files is represented by molecular_trait_id/variant/rsid trios relevant for a given tissue. Each have their own finemapping statistics\n    - log Bayes Factors are available for all variants in the `lbf_variable.txt` files\n    \"\"\"\n\n    raw_credible_set_schema: StructType = StructType(\n        [\n            StructField(\"molecular_trait_id\", StringType(), True),\n            StructField(\"gene_id\", StringType(), True),\n            StructField(\"cs_id\", StringType(), True),\n            StructField(\"variant\", StringType(), True),\n            StructField(\"rsid\", StringType(), True),\n            StructField(\"cs_size\", IntegerType(), True),\n            StructField(\"pip\", DoubleType(), True),\n            StructField(\"pvalue\", DoubleType(), True),\n            StructField(\"beta\", DoubleType(), True),\n            StructField(\"se\", DoubleType(), True),\n            StructField(\"z\", DoubleType(), True),\n            StructField(\"cs_min_r2\", DoubleType(), True),\n            StructField(\"region\", StringType(), True),\n        ]\n    )\n    raw_lbf_schema: StructType = StructType(\n        [\n            StructField(\"molecular_trait_id\", StringType(), True),\n            StructField(\"region\", StringType(), True),\n            StructField(\"variant\", StringType(), True),\n            StructField(\"chromosome\", StringType(), True),\n            StructField(\"position\", IntegerType(), True),\n            StructField(\"lbf_variable1\", DoubleType(), True),\n            StructField(\"lbf_variable2\", DoubleType(), True),\n            StructField(\"lbf_variable3\", DoubleType(), True),\n            StructField(\"lbf_variable4\", DoubleType(), True),\n            StructField(\"lbf_variable5\", DoubleType(), True),\n            StructField(\"lbf_variable6\", DoubleType(), True),\n            StructField(\"lbf_variable7\", DoubleType(), True),\n            StructField(\"lbf_variable8\", DoubleType(), True),\n            StructField(\"lbf_variable9\", DoubleType(), True),\n            StructField(\"lbf_variable10\", DoubleType(), True),\n        ]\n    )\n\n    @classmethod\n    def _extract_credible_set_index(\n        cls: type[EqtlCatalogueFinemapping], cs_id: Column\n    ) -&gt; Column:\n        \"\"\"Extract the credible set index from the cs_id.\n\n        Args:\n            cs_id (Column): column with the credible set id as defined in the eQTL Catalogue.\n\n        Returns:\n            Column: The credible set index.\n\n        Examples:\n            &gt;&gt;&gt; spark.createDataFrame([(\"QTD000046_L1\",)], [\"cs_id\"]).select(EqtlCatalogueFinemapping._extract_credible_set_index(f.col(\"cs_id\"))).show()\n            +----------------+\n            |credibleSetIndex|\n            +----------------+\n            |               1|\n            +----------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.split(cs_id, \"_L\")[1].cast(IntegerType()).alias(\"credibleSetIndex\")\n\n    @classmethod\n    def _extract_dataset_id_from_file_path(\n        cls: type[EqtlCatalogueFinemapping], file_path: Column\n    ) -&gt; Column:\n        \"\"\"Extract the dataset_id from the file_path. The dataset_id follows the pattern QTD{6}.\n\n        Args:\n            file_path (Column): A column containing the file path.\n\n        Returns:\n            Column: The dataset_id.\n\n        Examples:\n            &gt;&gt;&gt; spark.createDataFrame([(\"QTD000046.credible_sets.tsv\",)], [\"filename\"]).select(EqtlCatalogueFinemapping._extract_dataset_id_from_file_path(f.col(\"filename\"))).show()\n            +----------+\n            |dataset_id|\n            +----------+\n            | QTD000046|\n            +----------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.regexp_extract(file_path, r\"QTD\\d{6}\", 0).alias(\"dataset_id\")\n\n    @classmethod\n    def parse_susie_results(\n        cls: type[EqtlCatalogueFinemapping],\n        credible_sets: DataFrame,\n        lbf: DataFrame,\n        studies_metadata: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"Parse the SuSIE results into a DataFrame containing the finemapping statistics and metadata about the studies.\n\n        Args:\n            credible_sets (DataFrame): DataFrame containing raw statistics of all variants in the credible sets.\n            lbf (DataFrame): DataFrame containing the raw log Bayes Factors for all variants.\n            studies_metadata (DataFrame): DataFrame containing the study metadata.\n\n        Returns:\n            DataFrame: Processed SuSIE results to contain metadata about the studies and the finemapping statistics.\n        \"\"\"\n        ss_ftp_path_template = \"https://ftp.ebi.ac.uk/pub/databases/spot/eQTL/sumstats\"\n        return (\n            lbf.join(\n                credible_sets.join(f.broadcast(studies_metadata), on=\"dataset_id\"),\n                on=[\"molecular_trait_id\", \"region\", \"variant\", \"dataset_id\"],\n                how=\"inner\",\n            )\n            .withColumn(\n                \"logBF\",\n                f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"lbf_variable1\"))\n                .when(f.col(\"credibleSetIndex\") == 2, f.col(\"lbf_variable2\"))\n                .when(f.col(\"credibleSetIndex\") == 3, f.col(\"lbf_variable3\"))\n                .when(f.col(\"credibleSetIndex\") == 4, f.col(\"lbf_variable4\"))\n                .when(f.col(\"credibleSetIndex\") == 5, f.col(\"lbf_variable5\"))\n                .when(f.col(\"credibleSetIndex\") == 6, f.col(\"lbf_variable6\"))\n                .when(f.col(\"credibleSetIndex\") == 7, f.col(\"lbf_variable7\"))\n                .when(f.col(\"credibleSetIndex\") == 8, f.col(\"lbf_variable8\"))\n                .when(f.col(\"credibleSetIndex\") == 9, f.col(\"lbf_variable9\"))\n                .when(f.col(\"credibleSetIndex\") == 10, f.col(\"lbf_variable10\")),\n            )\n            .select(\n                f.regexp_replace(f.col(\"variant\"), r\"chr\", \"\").alias(\"variantId\"),\n                f.col(\"region\"),\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                f.col(\"pip\").alias(\"posteriorProbability\"),\n                *split_pvalue_column(f.col(\"pvalue\")),\n                f.col(\"sample_size\").alias(\"nSamples\"),\n                f.col(\"beta\"),\n                f.col(\"se\").alias(\"standardError\"),\n                f.col(\"credibleSetIndex\"),\n                f.col(\"logBF\"),\n                f.lit(FinemappingMethod.SUSIE.value).alias(\"finemappingMethod\"),\n                # Study metadata\n                f.col(\"molecular_trait_id\").alias(\"traitFromSource\"),\n                f.col(\"gene_id\").alias(\"geneId\"),\n                f.col(\"dataset_id\"),\n                # Upon creation, the studyId cleaned from symbols:\n                clean_strings_from_symbols(\n                    f.concat_ws(\n                        \"_\",\n                        f.col(\"study_label\"),\n                        f.col(\"quant_method\"),\n                        f.col(\"sample_group\"),\n                        f.col(\"molecular_trait_id\"),\n                    )\n                ).alias(\"studyId\"),\n                f.col(\"tissue_id\").alias(\"biosampleFromSourceId\"),\n                EqtlCatalogueStudyIndex._identify_study_type().alias(\"studyType\"),\n                f.col(\"study_label\").alias(\"projectId\"),\n                f.concat_ws(\n                    \"/\",\n                    f.lit(ss_ftp_path_template),\n                    f.col(\"study_id\"),\n                    f.col(\"dataset_id\"),\n                ).alias(\"summarystatsLocation\"),\n                f.lit(True).alias(\"hasSumstats\"),\n                f.col(\"molecular_trait_id\"),\n                f.col(\"pmid\").alias(\"pubmedId\"),\n                f.col(\"condition_label\").alias(\"condition\"),\n            )\n        )\n\n    @classmethod\n    def from_susie_results(\n        cls: type[EqtlCatalogueFinemapping], processed_finemapping_df: DataFrame\n    ) -&gt; StudyLocus:\n        \"\"\"Create a StudyLocus object from the processed SuSIE results.\n\n        Args:\n            processed_finemapping_df (DataFrame): DataFrame containing the processed SuSIE results.\n\n        Returns:\n            StudyLocus: eQTL Catalogue credible sets.\n        \"\"\"\n        lead_w = Window.partitionBy(\n            \"dataset_id\", \"molecular_trait_id\", \"region\", \"credibleSetIndex\"\n        )\n        study_locus_cols = [\n            field.name\n            for field in StudyLocus.get_schema().fields\n            if field.name in processed_finemapping_df.columns\n        ] + [\"locus\"]\n        return StudyLocus(\n            _df=(\n                processed_finemapping_df.withColumn(\n                    \"isLead\",\n                    f.row_number().over(lead_w.orderBy(f.desc(\"posteriorProbability\")))\n                    == f.lit(1),\n                )\n                .withColumn(\n                    # Collecting all variants that constitute the credible set brings as many variants as the credible set size\n                    \"locus\",\n                    f.when(\n                        f.col(\"isLead\"),\n                        f.collect_list(\n                            f.struct(\n                                \"variantId\",\n                                \"posteriorProbability\",\n                                \"pValueMantissa\",\n                                \"pValueExponent\",\n                                \"logBF\",\n                                \"beta\",\n                                \"standardError\",\n                            )\n                        ).over(lead_w),\n                    ),\n                )\n                .filter(f.col(\"isLead\"))\n                .drop(\"isLead\")\n                .select(\n                    *study_locus_cols,\n                    StudyLocus.assign_study_locus_id(\n                        [\"studyId\", \"variantId\", \"finemappingMethod\"]\n                    ),\n                    StudyLocus.calculate_credible_set_log10bf(\n                        f.col(\"locus.logBF\")\n                    ).alias(\"credibleSetlog10BF\"),\n                )\n            ),\n            _schema=StudyLocus.get_schema(),\n        ).annotate_credible_sets()\n\n    @classmethod\n    def read_credible_set_from_source(\n        cls: type[EqtlCatalogueFinemapping],\n        session: Session,\n        credible_set_path: str | list[str],\n    ) -&gt; DataFrame:\n        \"\"\"Load raw credible sets from eQTL Catalogue.\n\n        Args:\n            session (Session): Spark session.\n            credible_set_path (str | list[str]): Path to raw table(s) containing finemapping results for any variant belonging to a credible set.\n\n        Returns:\n            DataFrame: Credible sets DataFrame.\n        \"\"\"\n        return (\n            session.spark.read.csv(\n                credible_set_path,\n                sep=\"\\t\",\n                header=True,\n                schema=cls.raw_credible_set_schema,\n            )\n            .withColumns(\n                {\n                    # Adding dataset id based on the input file name:\n                    \"dataset_id\": cls._extract_dataset_id_from_file_path(\n                        f.input_file_name()\n                    ),\n                    # Parsing credible set index from the cs_id:\n                    \"credibleSetIndex\": cls._extract_credible_set_index(f.col(\"cs_id\")),\n                }\n            )\n            # Remove duplicates caused by explosion of single variants to multiple rsid-s:\n            .drop(\"rsid\")\n            .distinct()\n        )\n\n    @classmethod\n    def read_lbf_from_source(\n        cls: type[EqtlCatalogueFinemapping],\n        session: Session,\n        lbf_path: str | list[str],\n    ) -&gt; DataFrame:\n        \"\"\"Load raw log Bayes Factors from eQTL Catalogue.\n\n        Args:\n            session (Session): Spark session.\n            lbf_path (str | list[str]): Path to raw table(s) containing Log Bayes Factors for each variant.\n\n        Returns:\n            DataFrame: Log Bayes Factors DataFrame.\n        \"\"\"\n        return (\n            session.spark.read.csv(\n                lbf_path,\n                sep=\"\\t\",\n                header=True,\n                schema=cls.raw_lbf_schema,\n            )\n            .withColumn(\n                \"dataset_id\",\n                cls._extract_dataset_id_from_file_path(f.input_file_name()),\n            )\n            .distinct()\n        )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping.from_susie_results","title":"<code>from_susie_results(processed_finemapping_df: DataFrame) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Create a StudyLocus object from the processed SuSIE results.</p> <p>Parameters:</p> Name Type Description Default <code>processed_finemapping_df</code> <code>DataFrame</code> <p>DataFrame containing the processed SuSIE results.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>eQTL Catalogue credible sets.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@classmethod\ndef from_susie_results(\n    cls: type[EqtlCatalogueFinemapping], processed_finemapping_df: DataFrame\n) -&gt; StudyLocus:\n    \"\"\"Create a StudyLocus object from the processed SuSIE results.\n\n    Args:\n        processed_finemapping_df (DataFrame): DataFrame containing the processed SuSIE results.\n\n    Returns:\n        StudyLocus: eQTL Catalogue credible sets.\n    \"\"\"\n    lead_w = Window.partitionBy(\n        \"dataset_id\", \"molecular_trait_id\", \"region\", \"credibleSetIndex\"\n    )\n    study_locus_cols = [\n        field.name\n        for field in StudyLocus.get_schema().fields\n        if field.name in processed_finemapping_df.columns\n    ] + [\"locus\"]\n    return StudyLocus(\n        _df=(\n            processed_finemapping_df.withColumn(\n                \"isLead\",\n                f.row_number().over(lead_w.orderBy(f.desc(\"posteriorProbability\")))\n                == f.lit(1),\n            )\n            .withColumn(\n                # Collecting all variants that constitute the credible set brings as many variants as the credible set size\n                \"locus\",\n                f.when(\n                    f.col(\"isLead\"),\n                    f.collect_list(\n                        f.struct(\n                            \"variantId\",\n                            \"posteriorProbability\",\n                            \"pValueMantissa\",\n                            \"pValueExponent\",\n                            \"logBF\",\n                            \"beta\",\n                            \"standardError\",\n                        )\n                    ).over(lead_w),\n                ),\n            )\n            .filter(f.col(\"isLead\"))\n            .drop(\"isLead\")\n            .select(\n                *study_locus_cols,\n                StudyLocus.assign_study_locus_id(\n                    [\"studyId\", \"variantId\", \"finemappingMethod\"]\n                ),\n                StudyLocus.calculate_credible_set_log10bf(\n                    f.col(\"locus.logBF\")\n                ).alias(\"credibleSetlog10BF\"),\n            )\n        ),\n        _schema=StudyLocus.get_schema(),\n    ).annotate_credible_sets()\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping.parse_susie_results","title":"<code>parse_susie_results(credible_sets: DataFrame, lbf: DataFrame, studies_metadata: DataFrame) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Parse the SuSIE results into a DataFrame containing the finemapping statistics and metadata about the studies.</p> <p>Parameters:</p> Name Type Description Default <code>credible_sets</code> <code>DataFrame</code> <p>DataFrame containing raw statistics of all variants in the credible sets.</p> required <code>lbf</code> <code>DataFrame</code> <p>DataFrame containing the raw log Bayes Factors for all variants.</p> required <code>studies_metadata</code> <code>DataFrame</code> <p>DataFrame containing the study metadata.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Processed SuSIE results to contain metadata about the studies and the finemapping statistics.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@classmethod\ndef parse_susie_results(\n    cls: type[EqtlCatalogueFinemapping],\n    credible_sets: DataFrame,\n    lbf: DataFrame,\n    studies_metadata: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"Parse the SuSIE results into a DataFrame containing the finemapping statistics and metadata about the studies.\n\n    Args:\n        credible_sets (DataFrame): DataFrame containing raw statistics of all variants in the credible sets.\n        lbf (DataFrame): DataFrame containing the raw log Bayes Factors for all variants.\n        studies_metadata (DataFrame): DataFrame containing the study metadata.\n\n    Returns:\n        DataFrame: Processed SuSIE results to contain metadata about the studies and the finemapping statistics.\n    \"\"\"\n    ss_ftp_path_template = \"https://ftp.ebi.ac.uk/pub/databases/spot/eQTL/sumstats\"\n    return (\n        lbf.join(\n            credible_sets.join(f.broadcast(studies_metadata), on=\"dataset_id\"),\n            on=[\"molecular_trait_id\", \"region\", \"variant\", \"dataset_id\"],\n            how=\"inner\",\n        )\n        .withColumn(\n            \"logBF\",\n            f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"lbf_variable1\"))\n            .when(f.col(\"credibleSetIndex\") == 2, f.col(\"lbf_variable2\"))\n            .when(f.col(\"credibleSetIndex\") == 3, f.col(\"lbf_variable3\"))\n            .when(f.col(\"credibleSetIndex\") == 4, f.col(\"lbf_variable4\"))\n            .when(f.col(\"credibleSetIndex\") == 5, f.col(\"lbf_variable5\"))\n            .when(f.col(\"credibleSetIndex\") == 6, f.col(\"lbf_variable6\"))\n            .when(f.col(\"credibleSetIndex\") == 7, f.col(\"lbf_variable7\"))\n            .when(f.col(\"credibleSetIndex\") == 8, f.col(\"lbf_variable8\"))\n            .when(f.col(\"credibleSetIndex\") == 9, f.col(\"lbf_variable9\"))\n            .when(f.col(\"credibleSetIndex\") == 10, f.col(\"lbf_variable10\")),\n        )\n        .select(\n            f.regexp_replace(f.col(\"variant\"), r\"chr\", \"\").alias(\"variantId\"),\n            f.col(\"region\"),\n            f.col(\"chromosome\"),\n            f.col(\"position\"),\n            f.col(\"pip\").alias(\"posteriorProbability\"),\n            *split_pvalue_column(f.col(\"pvalue\")),\n            f.col(\"sample_size\").alias(\"nSamples\"),\n            f.col(\"beta\"),\n            f.col(\"se\").alias(\"standardError\"),\n            f.col(\"credibleSetIndex\"),\n            f.col(\"logBF\"),\n            f.lit(FinemappingMethod.SUSIE.value).alias(\"finemappingMethod\"),\n            # Study metadata\n            f.col(\"molecular_trait_id\").alias(\"traitFromSource\"),\n            f.col(\"gene_id\").alias(\"geneId\"),\n            f.col(\"dataset_id\"),\n            # Upon creation, the studyId cleaned from symbols:\n            clean_strings_from_symbols(\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"study_label\"),\n                    f.col(\"quant_method\"),\n                    f.col(\"sample_group\"),\n                    f.col(\"molecular_trait_id\"),\n                )\n            ).alias(\"studyId\"),\n            f.col(\"tissue_id\").alias(\"biosampleFromSourceId\"),\n            EqtlCatalogueStudyIndex._identify_study_type().alias(\"studyType\"),\n            f.col(\"study_label\").alias(\"projectId\"),\n            f.concat_ws(\n                \"/\",\n                f.lit(ss_ftp_path_template),\n                f.col(\"study_id\"),\n                f.col(\"dataset_id\"),\n            ).alias(\"summarystatsLocation\"),\n            f.lit(True).alias(\"hasSumstats\"),\n            f.col(\"molecular_trait_id\"),\n            f.col(\"pmid\").alias(\"pubmedId\"),\n            f.col(\"condition_label\").alias(\"condition\"),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping.read_credible_set_from_source","title":"<code>read_credible_set_from_source(session: Session, credible_set_path: str | list[str]) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Load raw credible sets from eQTL Catalogue.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session.</p> required <code>credible_set_path</code> <code>str | list[str]</code> <p>Path to raw table(s) containing finemapping results for any variant belonging to a credible set.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Credible sets DataFrame.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@classmethod\ndef read_credible_set_from_source(\n    cls: type[EqtlCatalogueFinemapping],\n    session: Session,\n    credible_set_path: str | list[str],\n) -&gt; DataFrame:\n    \"\"\"Load raw credible sets from eQTL Catalogue.\n\n    Args:\n        session (Session): Spark session.\n        credible_set_path (str | list[str]): Path to raw table(s) containing finemapping results for any variant belonging to a credible set.\n\n    Returns:\n        DataFrame: Credible sets DataFrame.\n    \"\"\"\n    return (\n        session.spark.read.csv(\n            credible_set_path,\n            sep=\"\\t\",\n            header=True,\n            schema=cls.raw_credible_set_schema,\n        )\n        .withColumns(\n            {\n                # Adding dataset id based on the input file name:\n                \"dataset_id\": cls._extract_dataset_id_from_file_path(\n                    f.input_file_name()\n                ),\n                # Parsing credible set index from the cs_id:\n                \"credibleSetIndex\": cls._extract_credible_set_index(f.col(\"cs_id\")),\n            }\n        )\n        # Remove duplicates caused by explosion of single variants to multiple rsid-s:\n        .drop(\"rsid\")\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/finemapping/#gentropy.datasource.eqtl_catalogue.finemapping.EqtlCatalogueFinemapping.read_lbf_from_source","title":"<code>read_lbf_from_source(session: Session, lbf_path: str | list[str]) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Load raw log Bayes Factors from eQTL Catalogue.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session.</p> required <code>lbf_path</code> <code>str | list[str]</code> <p>Path to raw table(s) containing Log Bayes Factors for each variant.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Log Bayes Factors DataFrame.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/finemapping.py</code> <pre><code>@classmethod\ndef read_lbf_from_source(\n    cls: type[EqtlCatalogueFinemapping],\n    session: Session,\n    lbf_path: str | list[str],\n) -&gt; DataFrame:\n    \"\"\"Load raw log Bayes Factors from eQTL Catalogue.\n\n    Args:\n        session (Session): Spark session.\n        lbf_path (str | list[str]): Path to raw table(s) containing Log Bayes Factors for each variant.\n\n    Returns:\n        DataFrame: Log Bayes Factors DataFrame.\n    \"\"\"\n    return (\n        session.spark.read.csv(\n            lbf_path,\n            sep=\"\\t\",\n            header=True,\n            schema=cls.raw_lbf_schema,\n        )\n        .withColumn(\n            \"dataset_id\",\n            cls._extract_dataset_id_from_file_path(f.input_file_name()),\n        )\n        .distinct()\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/study_index/","title":"Study Index","text":""},{"location":"python_api/datasources/eqtl_catalogue/study_index/#gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex","title":"<code>gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex</code>","text":"<p>Study index dataset from eQTL Catalogue.</p> <p>We extract study level metadata from eQTL Catalogue's fine mapping results. All available studies can be found here.</p> <p>One study from the eQTL Catalogue clusters together all the molecular QTLs (mQTLs) that were found:</p> <pre><code>- in the same publication (e.g. Alasoo_2018)\n- in the same cell type or tissue (e.g. monocytes)\n- and for the same measured molecular trait (e.g. ENSG00000141510)\n</code></pre> Source code in <code>src/gentropy/datasource/eqtl_catalogue/study_index.py</code> <pre><code>class EqtlCatalogueStudyIndex:\n    \"\"\"Study index dataset from eQTL Catalogue.\n\n    We extract study level metadata from eQTL Catalogue's fine mapping results. All available studies can be found [here](https://www.ebi.ac.uk/eqtl/Studies/).\n\n    One study from the eQTL Catalogue clusters together all the molecular QTLs (mQTLs) that were found:\n\n        - in the same publication (e.g. Alasoo_2018)\n        - in the same cell type or tissue (e.g. monocytes)\n        - and for the same measured molecular trait (e.g. ENSG00000141510)\n\n    \"\"\"\n\n    raw_studies_metadata_schema: StructType = StructType(\n        [\n            StructField(\"study_id\", StringType(), True),\n            StructField(\"dataset_id\", StringType(), True),\n            StructField(\"study_label\", StringType(), True),\n            StructField(\"sample_group\", StringType(), True),\n            StructField(\"tissue_id\", StringType(), True),\n            StructField(\"tissue_label\", StringType(), True),\n            StructField(\"condition_label\", StringType(), True),\n            StructField(\"sample_size\", IntegerType(), True),\n            StructField(\"quant_method\", StringType(), True),\n            StructField(\"pmid\", StringType(), True),\n            StructField(\"study_type\", StringType(), True),\n        ]\n    )\n    raw_studies_metadata_path = \"https://raw.githubusercontent.com/eQTL-Catalogue/eQTL-Catalogue-resources/fe3c4b4ed911b3a184271a6aadcd8c8769a66aba/data_tables/dataset_metadata.tsv\"\n    method_to_qtl_type_mapping = {\n        \"ge\": \"eqtl\",\n        \"exon\": \"eqtl\",\n        \"tx\": \"eqtl\",\n        \"microarray\": \"eqtl\",\n        \"leafcutter\": \"sqtl\",\n        \"aptamer\": \"pqtl\",\n        \"txrev\": \"tuqtl\",\n    }\n\n    @classmethod\n    def _identify_study_type(\n        cls: type[EqtlCatalogueStudyIndex],\n    ) -&gt; Column:\n        \"\"\"Identify the qtl type based on the quantification method and eqtl catalogue study type.\n\n        Returns:\n            Column: The study type.\n\n        Examples:\n            &gt;&gt;&gt; df = spark.createDataFrame([(\"ge\", \"bulk\"), (\"leafcutter\", \"bulk\"), (\"tx\", \"single-cell\")], [\"quant_method\", \"study_type\"])\n            &gt;&gt;&gt; df.withColumn(\"studyType\", EqtlCatalogueStudyIndex._identify_study_type()).show()\n            +------------+-----------+---------+\n            |quant_method| study_type|studyType|\n            +------------+-----------+---------+\n            |          ge|       bulk|     eqtl|\n            |  leafcutter|       bulk|     sqtl|\n            |          tx|single-cell|   sceqtl|\n            +------------+-----------+---------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        qtl_type_mapping = f.create_map(\n            *[f.lit(x) for x in chain(*cls.method_to_qtl_type_mapping.items())]\n        )[f.col(\"quant_method\")]\n        return f.when(\n            f.col(\"study_type\") == \"single-cell\",\n            f.concat(f.lit(\"sc\"), qtl_type_mapping),\n        ).otherwise(qtl_type_mapping)\n\n    @classmethod\n    def get_studies_of_interest(\n        cls: type[EqtlCatalogueStudyIndex],\n        studies_metadata: DataFrame,\n    ) -&gt; list[str]:\n        \"\"\"Filter studies of interest from the raw studies metadata.\n\n        Args:\n            studies_metadata (DataFrame): raw studies metadata filtered with studies of interest.\n\n        Returns:\n            list[str]: QTD IDs defining the studies of interest for ingestion.\n        \"\"\"\n        return (\n            studies_metadata.select(\"dataset_id\")\n            .distinct()\n            .toPandas()[\"dataset_id\"]\n            .tolist()\n        )\n\n    @classmethod\n    def from_susie_results(\n        cls: type[EqtlCatalogueStudyIndex],\n        processed_finemapping_df: DataFrame,\n    ) -&gt; StudyIndex:\n        \"\"\"Ingest study level metadata from eQTL Catalogue.\n\n        Args:\n            processed_finemapping_df (DataFrame): processed fine mapping results with study metadata.\n\n        Returns:\n            StudyIndex: eQTL Catalogue study index dataset derived from the selected SuSIE results.\n        \"\"\"\n        study_index_cols = [\n            field.name\n            for field in StudyIndex.get_schema().fields\n            if field.name in processed_finemapping_df.columns\n        ]\n        return StudyIndex(\n            _df=processed_finemapping_df.select(study_index_cols).distinct(),\n            _schema=StudyIndex.get_schema(),\n        )\n\n    @classmethod\n    def read_studies_from_source(\n        cls: type[EqtlCatalogueStudyIndex],\n        session: Session,\n        mqtl_quantification_methods_blacklist: list[str],\n    ) -&gt; DataFrame:\n        \"\"\"Read raw studies metadata from eQTL Catalogue.\n\n        Args:\n            session (Session): Spark session.\n            mqtl_quantification_methods_blacklist (list[str]): Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv\n\n        Returns:\n            DataFrame: raw studies metadata.\n        \"\"\"\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        return session.spark.createDataFrame(\n            pd.read_csv(cls.raw_studies_metadata_path, sep=\"\\t\"),\n            schema=cls.raw_studies_metadata_schema,\n        ).filter(~(f.col(\"quant_method\").isin(mqtl_quantification_methods_blacklist)))\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/study_index/#gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex.from_susie_results","title":"<code>from_susie_results(processed_finemapping_df: DataFrame) -&gt; StudyIndex</code>  <code>classmethod</code>","text":"<p>Ingest study level metadata from eQTL Catalogue.</p> <p>Parameters:</p> Name Type Description Default <code>processed_finemapping_df</code> <code>DataFrame</code> <p>processed fine mapping results with study metadata.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>eQTL Catalogue study index dataset derived from the selected SuSIE results.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/study_index.py</code> <pre><code>@classmethod\ndef from_susie_results(\n    cls: type[EqtlCatalogueStudyIndex],\n    processed_finemapping_df: DataFrame,\n) -&gt; StudyIndex:\n    \"\"\"Ingest study level metadata from eQTL Catalogue.\n\n    Args:\n        processed_finemapping_df (DataFrame): processed fine mapping results with study metadata.\n\n    Returns:\n        StudyIndex: eQTL Catalogue study index dataset derived from the selected SuSIE results.\n    \"\"\"\n    study_index_cols = [\n        field.name\n        for field in StudyIndex.get_schema().fields\n        if field.name in processed_finemapping_df.columns\n    ]\n    return StudyIndex(\n        _df=processed_finemapping_df.select(study_index_cols).distinct(),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/study_index/#gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex.get_studies_of_interest","title":"<code>get_studies_of_interest(studies_metadata: DataFrame) -&gt; list[str]</code>  <code>classmethod</code>","text":"<p>Filter studies of interest from the raw studies metadata.</p> <p>Parameters:</p> Name Type Description Default <code>studies_metadata</code> <code>DataFrame</code> <p>raw studies metadata filtered with studies of interest.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: QTD IDs defining the studies of interest for ingestion.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/study_index.py</code> <pre><code>@classmethod\ndef get_studies_of_interest(\n    cls: type[EqtlCatalogueStudyIndex],\n    studies_metadata: DataFrame,\n) -&gt; list[str]:\n    \"\"\"Filter studies of interest from the raw studies metadata.\n\n    Args:\n        studies_metadata (DataFrame): raw studies metadata filtered with studies of interest.\n\n    Returns:\n        list[str]: QTD IDs defining the studies of interest for ingestion.\n    \"\"\"\n    return (\n        studies_metadata.select(\"dataset_id\")\n        .distinct()\n        .toPandas()[\"dataset_id\"]\n        .tolist()\n    )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/study_index/#gentropy.datasource.eqtl_catalogue.study_index.EqtlCatalogueStudyIndex.read_studies_from_source","title":"<code>read_studies_from_source(session: Session, mqtl_quantification_methods_blacklist: list[str]) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Read raw studies metadata from eQTL Catalogue.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session.</p> required <code>mqtl_quantification_methods_blacklist</code> <code>list[str]</code> <p>Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>raw studies metadata.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/study_index.py</code> <pre><code>@classmethod\ndef read_studies_from_source(\n    cls: type[EqtlCatalogueStudyIndex],\n    session: Session,\n    mqtl_quantification_methods_blacklist: list[str],\n) -&gt; DataFrame:\n    \"\"\"Read raw studies metadata from eQTL Catalogue.\n\n    Args:\n        session (Session): Spark session.\n        mqtl_quantification_methods_blacklist (list[str]): Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv\n\n    Returns:\n        DataFrame: raw studies metadata.\n    \"\"\"\n    pd.DataFrame.iteritems = pd.DataFrame.items\n    return session.spark.createDataFrame(\n        pd.read_csv(cls.raw_studies_metadata_path, sep=\"\\t\"),\n        schema=cls.raw_studies_metadata_schema,\n    ).filter(~(f.col(\"quant_method\").isin(mqtl_quantification_methods_blacklist)))\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/summary_stats/","title":"Summary Stats","text":""},{"location":"python_api/datasources/eqtl_catalogue/summary_stats/#gentropy.datasource.eqtl_catalogue.summary_stats.EqtlCatalogueSummaryStats","title":"<code>gentropy.datasource.eqtl_catalogue.summary_stats.EqtlCatalogueSummaryStats</code>  <code>dataclass</code>","text":"<p>Summary statistics dataset for eQTL Catalogue.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/summary_stats.py</code> <pre><code>@dataclass\nclass EqtlCatalogueSummaryStats:\n    \"\"\"Summary statistics dataset for eQTL Catalogue.\"\"\"\n\n    @staticmethod\n    def _full_study_id_regexp() -&gt; Column:\n        \"\"\"Constructs a full study ID from the URI.\n\n        Returns:\n            Column: expression to extract a full study ID from the URI.\n        \"\"\"\n        # Example of a URI which is used for parsing:\n        # \"gs://genetics_etl_python_playground/input/preprocess/eqtl_catalogue/imported/GTEx_V8/ge/Adipose_Subcutaneous.tsv.gz\".\n\n        # Regular expession to extract project ID from URI.  Example: \"GTEx_V8\".\n        _project_id = f.regexp_extract(\n            f.input_file_name(),\n            r\"imported/([^/]+)/.*\",\n            1,\n        )\n        # Regular expression to extract QTL group from URI.  Example: \"Adipose_Subcutaneous\".\n        _qtl_group = f.regexp_extract(f.input_file_name(), r\"([^/]+)\\.tsv\\.gz\", 1)\n        # Extracting gene ID from the column.  Example: \"ENSG00000225630\".\n        _gene_id = f.col(\"gene_id\")\n\n        # We can now construct the full study ID based on all fields.\n        # Example: \"GTEx_V8_Adipose_Subcutaneous_ENSG00000225630\".\n        return f.concat(_project_id, f.lit(\"_\"), _qtl_group, f.lit(\"_\"), _gene_id)\n\n    @classmethod\n    def from_source(\n        cls: type[EqtlCatalogueSummaryStats],\n        summary_stats_df: DataFrame,\n    ) -&gt; SummaryStatistics:\n        \"\"\"Ingests all summary stats for all eQTL Catalogue studies.\n\n        Args:\n            summary_stats_df (DataFrame): an ingested but unprocessed summary statistics dataframe from eQTL Catalogue.\n\n        Returns:\n            SummaryStatistics: a processed summary statistics dataframe for eQTL Catalogue.\n        \"\"\"\n        processed_summary_stats_df = (\n            summary_stats_df.select(\n                # Construct study ID from the appropriate columns.\n                cls._full_study_id_regexp().alias(\"studyId\"),\n                # Add variant information.\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"chromosome\"),\n                    f.col(\"position\"),\n                    f.col(\"ref\"),\n                    f.col(\"alt\"),\n                ).alias(\"variantId\"),\n                f.col(\"chromosome\"),\n                f.col(\"position\").cast(t.IntegerType()),\n                # Parse p-value into mantissa and exponent.\n                *split_pvalue_column(f.col(\"pvalue\")),\n                # Add beta, standard error, and allele frequency information.\n                f.col(\"beta\").cast(\"double\"),\n                f.col(\"se\").cast(\"double\").alias(\"standardError\"),\n                f.col(\"maf\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n            )\n            # Drop rows which don't have proper position or beta value.\n            .filter(\n                f.col(\"position\").cast(t.IntegerType()).isNotNull()\n                &amp; (f.col(\"beta\") != 0)\n            )\n        )\n\n        # Initialise a summary statistics object.\n        return SummaryStatistics(\n            _df=processed_summary_stats_df,\n            _schema=SummaryStatistics.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/eqtl_catalogue/summary_stats/#gentropy.datasource.eqtl_catalogue.summary_stats.EqtlCatalogueSummaryStats.from_source","title":"<code>from_source(summary_stats_df: DataFrame) -&gt; SummaryStatistics</code>  <code>classmethod</code>","text":"<p>Ingests all summary stats for all eQTL Catalogue studies.</p> <p>Parameters:</p> Name Type Description Default <code>summary_stats_df</code> <code>DataFrame</code> <p>an ingested but unprocessed summary statistics dataframe from eQTL Catalogue.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>a processed summary statistics dataframe for eQTL Catalogue.</p> Source code in <code>src/gentropy/datasource/eqtl_catalogue/summary_stats.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[EqtlCatalogueSummaryStats],\n    summary_stats_df: DataFrame,\n) -&gt; SummaryStatistics:\n    \"\"\"Ingests all summary stats for all eQTL Catalogue studies.\n\n    Args:\n        summary_stats_df (DataFrame): an ingested but unprocessed summary statistics dataframe from eQTL Catalogue.\n\n    Returns:\n        SummaryStatistics: a processed summary statistics dataframe for eQTL Catalogue.\n    \"\"\"\n    processed_summary_stats_df = (\n        summary_stats_df.select(\n            # Construct study ID from the appropriate columns.\n            cls._full_study_id_regexp().alias(\"studyId\"),\n            # Add variant information.\n            f.concat_ws(\n                \"_\",\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                f.col(\"ref\"),\n                f.col(\"alt\"),\n            ).alias(\"variantId\"),\n            f.col(\"chromosome\"),\n            f.col(\"position\").cast(t.IntegerType()),\n            # Parse p-value into mantissa and exponent.\n            *split_pvalue_column(f.col(\"pvalue\")),\n            # Add beta, standard error, and allele frequency information.\n            f.col(\"beta\").cast(\"double\"),\n            f.col(\"se\").cast(\"double\").alias(\"standardError\"),\n            f.col(\"maf\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n        )\n        # Drop rows which don't have proper position or beta value.\n        .filter(\n            f.col(\"position\").cast(t.IntegerType()).isNotNull()\n            &amp; (f.col(\"beta\") != 0)\n        )\n    )\n\n    # Initialise a summary statistics object.\n    return SummaryStatistics(\n        _df=processed_summary_stats_df,\n        _schema=SummaryStatistics.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen/_finngen/","title":"FinnGen","text":"<p>FinnGen is a research project in genomics and personalized medicine, representing a large public-private partnership. The project has collected and analyzed genome and health data from 500,000 Finnish biobank donors to understand the genetic basis of diseases. FinnGen is now expanding its focus to comprehend the progression and biological mechanisms of diseases. This initiative provides a world-class resource for further breakthroughs in disease prevention, diagnosis, and treatment, offering insights into our genetic makeup.</p> <p>For a comprehensive understanding of the dataset and methods, refer to Kurki et al., 2023.</p> <p>We ingested full GWAS summary statistics and SuSiE-based fine-mapping results.</p>"},{"location":"python_api/datasources/finngen/efo_mapping/","title":"FinnGen EFO Phenotype Mapping","text":""},{"location":"python_api/datasources/finngen/efo_mapping/#gentropy.datasource.finngen.efo_mapping.EFOMapping","title":"<code>gentropy.datasource.finngen.efo_mapping.EFOMapping</code>","text":"<p>EFO-finnGen phecode mapping datasource.</p> Source code in <code>src/gentropy/datasource/finngen/efo_mapping.py</code> <pre><code>class EFOMapping:\n    \"\"\"EFO-finnGen phecode mapping datasource.\"\"\"\n\n    required_columns = {\"STUDY\", \"PROPERTY_VALUE\", \"SEMANTIC_TAG\"}\n\n    def __init__(self, df: DataFrame) -&gt; None:\n        \"\"\"Initialize the EFO curation.\n\n        Args:\n            df (DataFrame): DataFrame containing the EFO curation data.\n        \"\"\"\n        self.df = df\n\n    @classmethod\n    def from_path(cls, session: Session, efo_curation_path: str) -&gt; EFOMapping:\n        \"\"\"Load the EFO curation from a specified path.\n\n        Note:\n            This method asserts that the EFO curation file is tab-delimited and contains header with following columns:\n            ```\n            |-- STUDY: string (nullable = true)           # required\n            |-- PROPERTY_VALUE: string (nullable = true)  # required\n            |-- SEMANTIC_TAG: string (nullable = true)    # required\n            ```\n\n        Note:\n            Example of the file can be found in https://raw.githubusercontent.com/opentargets/curation/refs/heads/master/mappings/disease/manual_string.tsv.\n\n        Args:\n            session (Session): Session object.\n            efo_curation_path (str): Path to the EFO curation file.\n\n        Returns:\n            EFOMapping: Loaded EFO curation object.\n\n        Raises:\n            AssertionError: If the EFO curation file does not contain the required columns.\n\n        \"\"\"\n        if efo_curation_path.startswith(\"http\"):\n            csv_data = urlopen(efo_curation_path).readlines()\n            csv_rows: list[str] = [row.decode(\"utf8\") for row in csv_data]\n            rdd = session.spark.sparkContext.parallelize(csv_rows)\n            # NOTE: type annotations for spark.read.csv miss the fact that the first param can be [RDD[str]]\n            efo_curation_mapping = session.spark.read.csv(rdd, header=True, sep=\"\\t\")\n        else:\n            efo_curation_mapping = session.spark.read.csv(\n                efo_curation_path,\n                sep=\"\\t\",\n                header=True,\n            )\n        assert cls.required_columns.issubset(\n            set(efo_curation_mapping.columns)\n        ), f\"EFO curation file must contain the following columns: {cls.required_columns}.\"\n        columns = [\n            f.col(col).cast(t.StringType()).alias(col) for col in cls.required_columns\n        ]\n        df = efo_curation_mapping.select(*columns)\n        return cls(df=df)\n\n    def annotate_study_index(\n        self,\n        study_index: StudyIndex,\n        finngen_release: str = \"R12\",\n    ) -&gt; StudyIndex:\n        \"\"\"Add EFO mapping to the Finngen study index table.\n\n        This function performs inner join on table of EFO mappings to the study index table by trait name.\n        All studies without EFO traits are dropped. The EFO mappings are then aggregated into lists per\n        studyId.\n\n        NOTE: preserve all studyId entries even if they don't have EFO mappings.\n        This is to avoid discrepancies between `study_index` and `credible_set` `studyId` column.\n        The rows with missing EFO mappings will be dropped in the study_index validation step.\n\n        Args:\n            study_index (StudyIndex): Study index table.\n            finngen_release (str): FinnGen release.\n\n        Returns:\n            StudyIndex: Study index table with added EFO mappings.\n        \"\"\"\n        efo_mappings = (\n            self.df.withColumn(\"STUDY\", f.upper(f.col(\"STUDY\")))\n            .filter(f.col(\"STUDY\").contains(\"FINNGEN\"))\n            .filter(f.upper(f.col(\"STUDY\")).contains(finngen_release))\n            .select(\n                f.regexp_replace(f.col(\"SEMANTIC_TAG\"), r\"^.*/\", \"\").alias(\n                    \"traitFromSourceMappedId\"\n                ),\n                f.col(\"PROPERTY_VALUE\").alias(\"traitFromSource\"),\n            )\n        )\n\n        si_df = study_index.df.join(\n            efo_mappings, on=\"traitFromSource\", how=\"left_outer\"\n        )\n        common_cols = [c for c in si_df.columns if c != \"traitFromSourceMappedId\"]\n        si_df = si_df.groupby(common_cols).agg(\n            f.collect_list(\"traitFromSourceMappedId\").alias(\"traitFromSourceMappedIds\")\n        )\n        return StudyIndex(_df=si_df)\n</code></pre>"},{"location":"python_api/datasources/finngen/efo_mapping/#gentropy.datasource.finngen.efo_mapping.EFOMapping.__init__","title":"<code>__init__(df: DataFrame) -&gt; None</code>","text":"<p>Initialize the EFO curation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the EFO curation data.</p> required Source code in <code>src/gentropy/datasource/finngen/efo_mapping.py</code> <pre><code>def __init__(self, df: DataFrame) -&gt; None:\n    \"\"\"Initialize the EFO curation.\n\n    Args:\n        df (DataFrame): DataFrame containing the EFO curation data.\n    \"\"\"\n    self.df = df\n</code></pre>"},{"location":"python_api/datasources/finngen/efo_mapping/#gentropy.datasource.finngen.efo_mapping.EFOMapping.annotate_study_index","title":"<code>annotate_study_index(study_index: StudyIndex, finngen_release: str = 'R12') -&gt; StudyIndex</code>","text":"<p>Add EFO mapping to the Finngen study index table.</p> <p>This function performs inner join on table of EFO mappings to the study index table by trait name. All studies without EFO traits are dropped. The EFO mappings are then aggregated into lists per studyId.</p> <p>NOTE: preserve all studyId entries even if they don't have EFO mappings. This is to avoid discrepancies between <code>study_index</code> and <code>credible_set</code> <code>studyId</code> column. The rows with missing EFO mappings will be dropped in the study_index validation step.</p> <p>Parameters:</p> Name Type Description Default <code>study_index</code> <code>StudyIndex</code> <p>Study index table.</p> required <code>finngen_release</code> <code>str</code> <p>FinnGen release.</p> <code>'R12'</code> <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Study index table with added EFO mappings.</p> Source code in <code>src/gentropy/datasource/finngen/efo_mapping.py</code> <pre><code>def annotate_study_index(\n    self,\n    study_index: StudyIndex,\n    finngen_release: str = \"R12\",\n) -&gt; StudyIndex:\n    \"\"\"Add EFO mapping to the Finngen study index table.\n\n    This function performs inner join on table of EFO mappings to the study index table by trait name.\n    All studies without EFO traits are dropped. The EFO mappings are then aggregated into lists per\n    studyId.\n\n    NOTE: preserve all studyId entries even if they don't have EFO mappings.\n    This is to avoid discrepancies between `study_index` and `credible_set` `studyId` column.\n    The rows with missing EFO mappings will be dropped in the study_index validation step.\n\n    Args:\n        study_index (StudyIndex): Study index table.\n        finngen_release (str): FinnGen release.\n\n    Returns:\n        StudyIndex: Study index table with added EFO mappings.\n    \"\"\"\n    efo_mappings = (\n        self.df.withColumn(\"STUDY\", f.upper(f.col(\"STUDY\")))\n        .filter(f.col(\"STUDY\").contains(\"FINNGEN\"))\n        .filter(f.upper(f.col(\"STUDY\")).contains(finngen_release))\n        .select(\n            f.regexp_replace(f.col(\"SEMANTIC_TAG\"), r\"^.*/\", \"\").alias(\n                \"traitFromSourceMappedId\"\n            ),\n            f.col(\"PROPERTY_VALUE\").alias(\"traitFromSource\"),\n        )\n    )\n\n    si_df = study_index.df.join(\n        efo_mappings, on=\"traitFromSource\", how=\"left_outer\"\n    )\n    common_cols = [c for c in si_df.columns if c != \"traitFromSourceMappedId\"]\n    si_df = si_df.groupby(common_cols).agg(\n        f.collect_list(\"traitFromSourceMappedId\").alias(\"traitFromSourceMappedIds\")\n    )\n    return StudyIndex(_df=si_df)\n</code></pre>"},{"location":"python_api/datasources/finngen/efo_mapping/#gentropy.datasource.finngen.efo_mapping.EFOMapping.from_path","title":"<code>from_path(session: Session, efo_curation_path: str) -&gt; EFOMapping</code>  <code>classmethod</code>","text":"<p>Load the EFO curation from a specified path.</p> Note <p>This method asserts that the EFO curation file is tab-delimited and contains header with following columns: <pre><code>|-- STUDY: string (nullable = true)           # required\n|-- PROPERTY_VALUE: string (nullable = true)  # required\n|-- SEMANTIC_TAG: string (nullable = true)    # required\n</code></pre></p> Note <p>Example of the file can be found in https://raw.githubusercontent.com/opentargets/curation/refs/heads/master/mappings/disease/manual_string.tsv.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>efo_curation_path</code> <code>str</code> <p>Path to the EFO curation file.</p> required <p>Returns:</p> Name Type Description <code>EFOMapping</code> <code>EFOMapping</code> <p>Loaded EFO curation object.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the EFO curation file does not contain the required columns.</p> Source code in <code>src/gentropy/datasource/finngen/efo_mapping.py</code> <pre><code>@classmethod\ndef from_path(cls, session: Session, efo_curation_path: str) -&gt; EFOMapping:\n    \"\"\"Load the EFO curation from a specified path.\n\n    Note:\n        This method asserts that the EFO curation file is tab-delimited and contains header with following columns:\n        ```\n        |-- STUDY: string (nullable = true)           # required\n        |-- PROPERTY_VALUE: string (nullable = true)  # required\n        |-- SEMANTIC_TAG: string (nullable = true)    # required\n        ```\n\n    Note:\n        Example of the file can be found in https://raw.githubusercontent.com/opentargets/curation/refs/heads/master/mappings/disease/manual_string.tsv.\n\n    Args:\n        session (Session): Session object.\n        efo_curation_path (str): Path to the EFO curation file.\n\n    Returns:\n        EFOMapping: Loaded EFO curation object.\n\n    Raises:\n        AssertionError: If the EFO curation file does not contain the required columns.\n\n    \"\"\"\n    if efo_curation_path.startswith(\"http\"):\n        csv_data = urlopen(efo_curation_path).readlines()\n        csv_rows: list[str] = [row.decode(\"utf8\") for row in csv_data]\n        rdd = session.spark.sparkContext.parallelize(csv_rows)\n        # NOTE: type annotations for spark.read.csv miss the fact that the first param can be [RDD[str]]\n        efo_curation_mapping = session.spark.read.csv(rdd, header=True, sep=\"\\t\")\n    else:\n        efo_curation_mapping = session.spark.read.csv(\n            efo_curation_path,\n            sep=\"\\t\",\n            header=True,\n        )\n    assert cls.required_columns.issubset(\n        set(efo_curation_mapping.columns)\n    ), f\"EFO curation file must contain the following columns: {cls.required_columns}.\"\n    columns = [\n        f.col(col).cast(t.StringType()).alias(col) for col in cls.required_columns\n    ]\n    df = efo_curation_mapping.select(*columns)\n    return cls(df=df)\n</code></pre>"},{"location":"python_api/datasources/finngen/finemapping/","title":"Finemapping","text":""},{"location":"python_api/datasources/finngen/finemapping/#gentropy.datasource.finngen.finemapping.FinnGenFinemapping","title":"<code>gentropy.datasource.finngen.finemapping.FinnGenFinemapping</code>  <code>dataclass</code>","text":"<p>SuSIE finemapping dataset for FinnGen.</p> <p>Credible sets from SuSIE are extracted and transformed into StudyLocus objects:</p> <ul> <li>Study ID in the special format (e.g. FINNGEN_R11*)</li> <li>Credible set specific finemapping statistics (e.g. LogBayesFactors, Alphas/Posterior)</li> <li>Additional credible set level BayesFactor filtering is applied (LBF &gt; 2)</li> <li>StudyLocusId is annotated for each credible set.</li> </ul> <p>Finemapping method is populated as a constant (\"SuSIE\").</p> Source code in <code>src/gentropy/datasource/finngen/finemapping.py</code> <pre><code>@dataclass\nclass FinnGenFinemapping:\n    \"\"\"SuSIE finemapping dataset for FinnGen.\n\n    Credible sets from SuSIE are extracted and transformed into StudyLocus objects:\n\n    - Study ID in the special format (e.g. FINNGEN_R11*)\n    - Credible set specific finemapping statistics (e.g. LogBayesFactors, Alphas/Posterior)\n    - Additional credible set level BayesFactor filtering is applied (LBF &gt; 2)\n    - StudyLocusId is annotated for each credible set.\n\n    Finemapping method is populated as a constant (\"SuSIE\").\n    \"\"\"\n\n    raw_schema: t.StructType = StructType(\n        [\n            StructField(\"trait\", StringType(), True),\n            StructField(\"region\", StringType(), True),\n            StructField(\"v\", StringType(), True),\n            StructField(\"rsid\", StringType(), True),\n            StructField(\"chromosome\", StringType(), True),\n            StructField(\"position\", StringType(), True),\n            StructField(\"allele1\", StringType(), True),\n            StructField(\"allele2\", StringType(), True),\n            StructField(\"maf\", StringType(), True),\n            StructField(\"beta\", StringType(), True),\n            StructField(\"se\", StringType(), True),\n            StructField(\"p\", StringType(), True),\n            StructField(\"mean\", StringType(), True),\n            StructField(\"sd\", StringType(), True),\n            StructField(\"prob\", StringType(), True),\n            StructField(\"cs\", StringType(), True),\n            StructField(\"cs_specific_prob\", DoubleType(), True),\n            StructField(\"low_purity\", StringType(), True),\n            StructField(\"lead_r2\", StringType(), True),\n            StructField(\"mean_99\", StringType(), True),\n            StructField(\"sd_99\", StringType(), True),\n            StructField(\"prob_99\", StringType(), True),\n            StructField(\"cs_99\", StringType(), True),\n            StructField(\"cs_specific_prob_99\", StringType(), True),\n            StructField(\"low_purity_99\", StringType(), True),\n            StructField(\"lead_r2_99\", StringType(), True),\n            StructField(\"alpha1\", DoubleType(), True),\n            StructField(\"alpha2\", DoubleType(), True),\n            StructField(\"alpha3\", DoubleType(), True),\n            StructField(\"alpha4\", DoubleType(), True),\n            StructField(\"alpha5\", DoubleType(), True),\n            StructField(\"alpha6\", DoubleType(), True),\n            StructField(\"alpha7\", DoubleType(), True),\n            StructField(\"alpha8\", DoubleType(), True),\n            StructField(\"alpha9\", DoubleType(), True),\n            StructField(\"alpha10\", DoubleType(), True),\n            StructField(\"mean1\", StringType(), True),\n            StructField(\"mean2\", StringType(), True),\n            StructField(\"mean3\", StringType(), True),\n            StructField(\"mean4\", StringType(), True),\n            StructField(\"mean5\", StringType(), True),\n            StructField(\"mean6\", StringType(), True),\n            StructField(\"mean7\", StringType(), True),\n            StructField(\"mean8\", StringType(), True),\n            StructField(\"mean9\", StringType(), True),\n            StructField(\"mean10\", StringType(), True),\n            StructField(\"sd1\", StringType(), True),\n            StructField(\"sd2\", StringType(), True),\n            StructField(\"sd3\", StringType(), True),\n            StructField(\"sd4\", StringType(), True),\n            StructField(\"sd5\", StringType(), True),\n            StructField(\"sd6\", StringType(), True),\n            StructField(\"sd7\", StringType(), True),\n            StructField(\"sd8\", StringType(), True),\n            StructField(\"sd9\", StringType(), True),\n            StructField(\"sd10\", StringType(), True),\n            StructField(\"lbf_variable1\", DoubleType(), True),\n            StructField(\"lbf_variable2\", DoubleType(), True),\n            StructField(\"lbf_variable3\", DoubleType(), True),\n            StructField(\"lbf_variable4\", DoubleType(), True),\n            StructField(\"lbf_variable5\", DoubleType(), True),\n            StructField(\"lbf_variable6\", DoubleType(), True),\n            StructField(\"lbf_variable7\", DoubleType(), True),\n            StructField(\"lbf_variable8\", DoubleType(), True),\n            StructField(\"lbf_variable9\", DoubleType(), True),\n            StructField(\"lbf_variable10\", DoubleType(), True),\n        ]\n    )\n\n    summary_schema: t.StructType = StructType(\n        [\n            StructField(\"trait\", StringType(), True),\n            StructField(\"region\", StringType(), True),\n            StructField(\"cs\", StringType(), True),\n            StructField(\"cs_log10bf\", DoubleType(), True),\n            StructField(\"cs_avg_r2\", DoubleType(), True),\n            StructField(\"cs_min_r2\", DoubleType(), True),\n        ]\n    )\n\n    raw_hail_shema: hl.tstruct = hl.tstruct(\n        trait=hl.tstr,\n        region=hl.tstr,\n        v=hl.tstr,\n        rsid=hl.tstr,\n        chromosome=hl.tstr,\n        position=hl.tstr,\n        allele1=hl.tstr,\n        allele2=hl.tstr,\n        maf=hl.tstr,\n        beta=hl.tstr,\n        se=hl.tstr,\n        p=hl.tstr,\n        mean=hl.tstr,\n        sd=hl.tstr,\n        prob=hl.tstr,\n        cs=hl.tstr,\n        cs_specific_prob=hl.tfloat64,\n        low_purity=hl.tstr,\n        lead_r2=hl.tstr,\n        mean_99=hl.tstr,\n        sd_99=hl.tstr,\n        prob_99=hl.tstr,\n        cs_99=hl.tstr,\n        cs_specific_prob_99=hl.tstr,\n        low_purity_99=hl.tstr,\n        lead_r2_99=hl.tstr,\n        alpha1=hl.tfloat64,\n        alpha2=hl.tfloat64,\n        alpha3=hl.tfloat64,\n        alpha4=hl.tfloat64,\n        alpha5=hl.tfloat64,\n        alpha6=hl.tfloat64,\n        alpha7=hl.tfloat64,\n        alpha8=hl.tfloat64,\n        alpha9=hl.tfloat64,\n        alpha10=hl.tfloat64,\n        mean1=hl.tstr,\n        mean2=hl.tstr,\n        mean3=hl.tstr,\n        mean4=hl.tstr,\n        mean5=hl.tstr,\n        mean6=hl.tstr,\n        mean7=hl.tstr,\n        mean8=hl.tstr,\n        mean9=hl.tstr,\n        mean10=hl.tstr,\n        sd1=hl.tstr,\n        sd2=hl.tstr,\n        sd3=hl.tstr,\n        sd4=hl.tstr,\n        sd5=hl.tstr,\n        sd6=hl.tstr,\n        sd7=hl.tstr,\n        sd8=hl.tstr,\n        sd9=hl.tstr,\n        sd10=hl.tstr,\n        lbf_variable1=hl.tfloat64,\n        lbf_variable2=hl.tfloat64,\n        lbf_variable3=hl.tfloat64,\n        lbf_variable4=hl.tfloat64,\n        lbf_variable5=hl.tfloat64,\n        lbf_variable6=hl.tfloat64,\n        lbf_variable7=hl.tfloat64,\n        lbf_variable8=hl.tfloat64,\n        lbf_variable9=hl.tfloat64,\n        lbf_variable10=hl.tfloat64,\n    )\n\n    summary_hail_schema: hl.tstruct = hl.tstruct(\n        trait=hl.tstr,\n        region=hl.tstr,\n        cs=hl.tstr,\n        cs_log10bf=hl.tfloat64,\n        cs_avg_r2=hl.tfloat64,\n        cs_min_r2=hl.tfloat64,\n    )\n\n    @staticmethod\n    def _infer_block_gzip_compression(paths: str | list[str]) -&gt; bool:\n        \"\"\"Naively infer compression type based on the file extension.\n\n        Args:\n            paths (str | list[str]): File path(s).\n\n        Returns:\n            bool: True if block gzipped, False otherwise.\n        \"\"\"\n        if isinstance(paths, str):\n            return paths.endswith(\".bgz\")\n        return all(path.endswith(\".bgz\") for path in paths)\n\n    @classmethod\n    def from_finngen_susie_finemapping(\n        cls: type[FinnGenFinemapping],\n        spark: SparkSession,\n        finngen_susie_finemapping_snp_files: (str | list[str]),\n        finngen_susie_finemapping_cs_summary_files: (str | list[str]),\n        finngen_release_prefix: str,\n        credset_lbf_threshold: float = 0.8685889638065036,\n    ) -&gt; StudyLocus:\n        \"\"\"Process the SuSIE finemapping output for FinnGen studies.\n\n        The finngen_susie_finemapping_snp_files are files that contain variant summaries with credible set information with following shema:\n            - trait: phenotype\n            - region: region for which the fine-mapping was run.\n            - v, rsid: variant ids\n            - chromosome\n            - position\n            - allele1\n            - allele2\n            - maf: minor allele frequency\n            - beta: original marginal beta\n            - se: original se\n            - p: original p\n            - mean: posterior mean beta after fine-mapping\n            - sd: posterior standard deviation after fine-mapping.\n            - prob: posterior inclusion probability\n            - cs: credible set index within region\n            - lead_r2: r2 value to a lead variant (the one with maximum PIP) in a credible set\n            - alphax: posterior inclusion probability for the x-th single effect (x := 1..L where L is the number of single effects (causal variants) specified; default: L = 10).\n            - lbfx: log-Bayes Factor for each variable and single effect (i.e credible set).\n            - meanx: posterior mean for each variable and single effect (i.e credible set).\n            - sdx: posterior sd of mean for each variable and single effect (i.e credible set).\n        As for r11 finngen release these files are ingested from `https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/` by\n            - *.snp.bgz\n            - *.snp.bgz.tbi\n        Each file contains index (.tbi) file that is required to read the block gzipped compressed snp file. These files needs to be\n        downloaded, transfromed from block gzipped to plain gzipped and then uploaded to the storage bucket, before they can be read by spark or read by hail directly as import table.\n\n        The finngen_susie_finemapping_cs_summary_files are files that Contains credible set summaries from SuSiE fine-mapping for all genome-wide significant regions with following schema:\n            - trait: phenotype\n            - region: region for which the fine-mapping was run.\n            - cs: running number for independent credible sets in a region, assigned to 95% PIP\n            - cs_log10bf: Log10 bayes factor of comparing the solution of this model (cs independent credible sets) to cs -1 credible sets\n            - cs_avg_r2: Average correlation R2 between variants in the credible set\n            - cs_min_r2: minimum r2 between variants in the credible set\n            - low_purity: boolean (TRUE,FALSE) indicator if the CS is low purity (low min r2)\n            - cs_size: how many snps does this credible set contain\n            - good_cs: boolean (TRUE,FALSE) indicator if this CS is considered reliable. IF this is FALSE then top variant reported for the CS will be chosen based on minimum p-value in the credible set, otherwise top variant is chosen by maximum PIP\n            - cs_id:\n            - v: top variant (chr:pos:ref:alt)\n            - p: top variant p-value\n            - beta: top variant beta\n            - sd: top variant standard deviation\n            - prob: overall PIP of the variant in the region\n            - cs_specific_prob: PIP of the variant in the current credible set (this and previous are typically almost identical)\n            - 0..n: configured annotation columns. Typical default most_severe,gene_most_severe giving consequence and gene of top variant\n        These files needs to be downloaded from the `https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/summary/` by *.cred.summary.tsv pattern,\n\n        Args:\n            spark (SparkSession): SparkSession object.\n            finngen_susie_finemapping_snp_files (str | list[str]): SuSIE finemapping output filename(s).\n            finngen_susie_finemapping_cs_summary_files (str | list[str]): filename of SuSIE finemapping credible set summaries.\n            finngen_release_prefix (str): Finngen project release prefix. Should look like FINNGEN_R*.\n            credset_lbf_threshold (float, optional): Filter out credible sets below, Default 0.8685889638065036 == np.log10(np.exp(2)), this is threshold from publication.\n\n        Returns:\n            StudyLocus: Processed SuSIE finemapping output in StudyLocus format.\n        \"\"\"\n        # NOTE: hail allows for importing block gzipped files, spark does not without external libraries.\n        # check https://github.com/projectglow/glow/blob/36bf6121fbc4ccc33a13b028deb87b63faeba7a9/core/src/main/scala/io/projectglow/vcf/VCFFileFormat.scala#L274\n        # how it could be implemented with spark.\n        bgzip_compressed_snps = cls._infer_block_gzip_compression(\n            finngen_susie_finemapping_snp_files\n        )\n\n        # NOTE: fallback to spark read if not block gzipped file in the input\n        if bgzip_compressed_snps:\n            snps_df = hl.import_table(\n                finngen_susie_finemapping_snp_files,\n                delimiter=\"\\t\",\n                types=cls.raw_hail_shema,\n            ).to_spark()\n        else:\n            snps_df = (\n                spark.read.schema(cls.raw_schema)\n                .option(\"delimiter\", \"\\t\")\n                .option(\"compression\", \"gzip\")\n                .csv(finngen_susie_finemapping_snp_files, header=True)\n            )\n\n        processed_finngen_finemapping_df = (\n            # Drop rows which don't have proper position.\n            snps_df.filter(f.col(\"position\").cast(t.IntegerType()).isNotNull())\n            # Drop non credible set SNPs:\n            .filter(f.col(\"cs\").cast(t.IntegerType()) &gt; 0)\n            .select(\n                # Add study idenfitier.\n                f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"trait\"))\n                .cast(t.StringType())\n                .alias(\"studyId\"),\n                f.col(\"region\"),\n                # Add variant information.\n                f.regexp_replace(f.col(\"v\"), \":\", \"_\").alias(\"variantId\"),\n                f.col(\"cs\").cast(\"integer\").alias(\"credibleSetIndex\"),\n                f.regexp_replace(f.col(\"chromosome\"), \"^chr\", \"\")\n                .cast(t.StringType())\n                .alias(\"chromosome\"),\n                f.col(\"position\").cast(t.IntegerType()),\n                f.col(\"allele1\").cast(t.StringType()).alias(\"ref\"),\n                f.col(\"allele2\").cast(t.StringType()).alias(\"alt\"),\n                # Parse p-value into mantissa and exponent.\n                *split_pvalue_column(f.col(\"p\")),\n                # Add standard error, and allele frequency information.\n                f.col(\"se\").cast(\"double\").alias(\"standardError\"),\n                f.col(\"maf\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n                f.lit(FinemappingMethod.SUSIE.value).alias(\"finemappingMethod\"),\n                *[\n                    f.col(f\"alpha{i}\").cast(t.DoubleType()).alias(f\"alpha_{i}\")\n                    for i in range(1, 11)\n                ],\n                *[\n                    f.col(f\"lbf_variable{i}\").cast(t.DoubleType()).alias(f\"lbf_{i}\")\n                    for i in range(1, 11)\n                ],\n                *[\n                    f.col(f\"mean{i}\").cast(t.DoubleType()).alias(f\"beta_{i}\")\n                    for i in range(1, 11)\n                ],\n            )\n            .withColumn(\n                \"posteriorProbability\",\n                f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"alpha_1\"))\n                .when(f.col(\"credibleSetIndex\") == 2, f.col(\"alpha_2\"))\n                .when(f.col(\"credibleSetIndex\") == 3, f.col(\"alpha_3\"))\n                .when(f.col(\"credibleSetIndex\") == 4, f.col(\"alpha_4\"))\n                .when(f.col(\"credibleSetIndex\") == 5, f.col(\"alpha_5\"))\n                .when(f.col(\"credibleSetIndex\") == 6, f.col(\"alpha_6\"))\n                .when(f.col(\"credibleSetIndex\") == 7, f.col(\"alpha_7\"))\n                .when(f.col(\"credibleSetIndex\") == 8, f.col(\"alpha_8\"))\n                .when(f.col(\"credibleSetIndex\") == 9, f.col(\"alpha_9\"))\n                .when(f.col(\"credibleSetIndex\") == 10, f.col(\"alpha_10\")),\n            )\n            .drop(\n                \"alpha_1\",\n                \"alpha_2\",\n                \"alpha_3\",\n                \"alpha_4\",\n                \"alpha_5\",\n                \"alpha_6\",\n                \"alpha_7\",\n                \"alpha_8\",\n                \"alpha_9\",\n                \"alpha_10\",\n            )\n            .withColumn(\n                \"logBF\",\n                f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"lbf_1\"))\n                .when(f.col(\"credibleSetIndex\") == 2, f.col(\"lbf_2\"))\n                .when(f.col(\"credibleSetIndex\") == 3, f.col(\"lbf_3\"))\n                .when(f.col(\"credibleSetIndex\") == 4, f.col(\"lbf_4\"))\n                .when(f.col(\"credibleSetIndex\") == 5, f.col(\"lbf_5\"))\n                .when(f.col(\"credibleSetIndex\") == 6, f.col(\"lbf_6\"))\n                .when(f.col(\"credibleSetIndex\") == 7, f.col(\"lbf_7\"))\n                .when(f.col(\"credibleSetIndex\") == 8, f.col(\"lbf_8\"))\n                .when(f.col(\"credibleSetIndex\") == 9, f.col(\"lbf_9\"))\n                .when(f.col(\"credibleSetIndex\") == 10, f.col(\"lbf_10\")),\n            )\n            .drop(\n                \"lbf_1\",\n                \"lbf_2\",\n                \"lbf_3\",\n                \"lbf_4\",\n                \"lbf_5\",\n                \"lbf_6\",\n                \"lbf_7\",\n                \"lbf_8\",\n                \"lbf_9\",\n                \"lbf_10\",\n            )\n            .withColumn(\n                \"beta\",\n                f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"beta_1\"))\n                .when(f.col(\"credibleSetIndex\") == 2, f.col(\"beta_2\"))\n                .when(f.col(\"credibleSetIndex\") == 3, f.col(\"beta_3\"))\n                .when(f.col(\"credibleSetIndex\") == 4, f.col(\"beta_4\"))\n                .when(f.col(\"credibleSetIndex\") == 5, f.col(\"beta_5\"))\n                .when(f.col(\"credibleSetIndex\") == 6, f.col(\"beta_6\"))\n                .when(f.col(\"credibleSetIndex\") == 7, f.col(\"beta_7\"))\n                .when(f.col(\"credibleSetIndex\") == 8, f.col(\"beta_8\"))\n                .when(f.col(\"credibleSetIndex\") == 9, f.col(\"beta_9\"))\n                .when(f.col(\"credibleSetIndex\") == 10, f.col(\"beta_10\")),\n            )\n            .drop(\n                \"beta_1\",\n                \"beta_2\",\n                \"beta_3\",\n                \"beta_4\",\n                \"beta_5\",\n                \"beta_6\",\n                \"beta_7\",\n                \"beta_8\",\n                \"beta_9\",\n                \"beta_10\",\n            )\n        )\n\n        bgzip_compressed_cs_summaries = cls._infer_block_gzip_compression(\n            finngen_susie_finemapping_cs_summary_files\n        )\n\n        # NOTE: fallback to spark read if not block gzipped file in the input\n        # in case we want to use the raw files from the\n        # https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/*.cred.gz\n        if bgzip_compressed_cs_summaries:\n            cs_summary_df = hl.import_table(\n                finngen_susie_finemapping_cs_summary_files,\n                delimiter=\"\\t\",\n                types=cls.summary_hail_schema,\n            ).to_spark()\n        else:\n            cs_summary_df = (\n                spark.read.schema(cls.summary_schema)\n                .option(\"delimiter\", \"\\t\")\n                .csv(finngen_susie_finemapping_cs_summary_files, header=True)\n            )\n\n        # drop credible sets where logbf &gt; 2. Except when there's only one credible set in region:\n        # 0.8685889638065036 corresponds to np.log10(np.exp(2)), to match the orginal threshold in publication.\n        finngen_finemapping_summaries_df = (\n            # Read credible set level lbf, it is output as a different file which is not ideal.\n            cs_summary_df.select(\n                f.col(\"region\"),\n                f.col(\"trait\"),\n                f.col(\"cs\").cast(\"integer\").alias(\"credibleSetIndex\"),\n                f.col(\"cs_log10bf\").cast(\"double\").alias(\"credibleSetlog10BF\"),\n                f.col(\"cs_avg_r2\").cast(\"double\").alias(\"purityMeanR2\"),\n                f.col(\"cs_min_r2\").cast(\"double\").alias(\"purityMinR2\"),\n            )\n            .filter(\n                (f.col(\"credibleSetlog10BF\") &gt; credset_lbf_threshold)\n                | (f.col(\"credibleSetIndex\") == 1)\n            )\n            .withColumn(\n                \"studyId\",\n                f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"trait\")),\n            )\n        )\n\n        processed_finngen_finemapping_df = processed_finngen_finemapping_df.join(\n            finngen_finemapping_summaries_df,\n            on=[\"studyId\", \"region\", \"credibleSetIndex\"],\n            how=\"inner\",\n        )\n\n        toploci_df = get_top_ranked_in_window(\n            processed_finngen_finemapping_df,\n            Window.partitionBy(\"studyId\", \"region\", \"credibleSetIndex\").orderBy(\n                f.desc(\"posteriorProbability\")\n            ),\n        ).select(\n            \"variantId\",\n            \"chromosome\",\n            \"position\",\n            \"studyId\",\n            \"beta\",\n            \"pValueMantissa\",\n            \"pValueExponent\",\n            \"effectAlleleFrequencyFromSource\",\n            \"standardError\",\n            \"region\",\n            \"credibleSetIndex\",\n            \"finemappingMethod\",\n            \"credibleSetlog10BF\",\n            \"purityMeanR2\",\n            \"purityMinR2\",\n        )\n\n        processed_finngen_finemapping_df = (\n            processed_finngen_finemapping_df.groupBy(\n                \"studyId\", \"region\", \"credibleSetIndex\"\n            )\n            .agg(\n                f.collect_list(\n                    f.struct(\n                        f.col(\"variantId\").cast(\"string\").alias(\"variantId\"),\n                        f.col(\"posteriorProbability\")\n                        .cast(\"double\")\n                        .alias(\"posteriorProbability\"),\n                        f.col(\"logBF\").cast(\"double\").alias(\"logBF\"),\n                        f.col(\"pValueMantissa\").cast(\"float\").alias(\"pValueMantissa\"),\n                        f.col(\"pValueExponent\").cast(\"integer\").alias(\"pValueExponent\"),\n                        f.col(\"beta\").cast(\"double\").alias(\"beta\"),\n                        f.col(\"standardError\").cast(\"double\").alias(\"standardError\"),\n                    )\n                ).alias(\"locus\"),\n            )\n            .select(\n                \"studyId\",\n                \"region\",\n                \"credibleSetIndex\",\n                \"locus\",\n            )\n            .join(\n                toploci_df,\n                on=[\"studyId\", \"region\", \"credibleSetIndex\"],\n                how=\"inner\",\n            )\n            .withColumns(\n                {\n                    \"locusStart\": f.split(f.split(\"region\", \":\")[1], \"-\")[0].cast(\n                        \"int\"\n                    ),\n                    \"locusEnd\": f.split(f.split(\"region\", \":\")[1], \"-\")[1].cast(\"int\"),\n                }\n            )\n        ).withColumn(\n            \"studyLocusId\",\n            StudyLocus.assign_study_locus_id(\n                [\"studyId\", \"variantId\", \"finemappingMethod\"]\n            ),\n        )\n\n        return StudyLocus(\n            _df=processed_finngen_finemapping_df,\n            _schema=StudyLocus.get_schema(),\n        ).annotate_credible_sets()\n</code></pre>"},{"location":"python_api/datasources/finngen/finemapping/#gentropy.datasource.finngen.finemapping.FinnGenFinemapping.from_finngen_susie_finemapping","title":"<code>from_finngen_susie_finemapping(spark: SparkSession, finngen_susie_finemapping_snp_files: str | list[str], finngen_susie_finemapping_cs_summary_files: str | list[str], finngen_release_prefix: str, credset_lbf_threshold: float = 0.8685889638065036) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Process the SuSIE finemapping output for FinnGen studies.</p> The finngen_susie_finemapping_snp_files are files that contain variant summaries with credible set information with following shema <ul> <li>trait: phenotype</li> <li>region: region for which the fine-mapping was run.</li> <li>v, rsid: variant ids</li> <li>chromosome</li> <li>position</li> <li>allele1</li> <li>allele2</li> <li>maf: minor allele frequency</li> <li>beta: original marginal beta</li> <li>se: original se</li> <li>p: original p</li> <li>mean: posterior mean beta after fine-mapping</li> <li>sd: posterior standard deviation after fine-mapping.</li> <li>prob: posterior inclusion probability</li> <li>cs: credible set index within region</li> <li>lead_r2: r2 value to a lead variant (the one with maximum PIP) in a credible set</li> <li>alphax: posterior inclusion probability for the x-th single effect (x := 1..L where L is the number of single effects (causal variants) specified; default: L = 10).</li> <li>lbfx: log-Bayes Factor for each variable and single effect (i.e credible set).</li> <li>meanx: posterior mean for each variable and single effect (i.e credible set).</li> <li>sdx: posterior sd of mean for each variable and single effect (i.e credible set).</li> </ul> <p>As for r11 finngen release these files are ingested from <code>https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/</code> by     - .snp.bgz     - .snp.bgz.tbi Each file contains index (.tbi) file that is required to read the block gzipped compressed snp file. These files needs to be downloaded, transfromed from block gzipped to plain gzipped and then uploaded to the storage bucket, before they can be read by spark or read by hail directly as import table.</p> The finngen_susie_finemapping_cs_summary_files are files that Contains credible set summaries from SuSiE fine-mapping for all genome-wide significant regions with following schema <ul> <li>trait: phenotype</li> <li>region: region for which the fine-mapping was run.</li> <li>cs: running number for independent credible sets in a region, assigned to 95% PIP</li> <li>cs_log10bf: Log10 bayes factor of comparing the solution of this model (cs independent credible sets) to cs -1 credible sets</li> <li>cs_avg_r2: Average correlation R2 between variants in the credible set</li> <li>cs_min_r2: minimum r2 between variants in the credible set</li> <li>low_purity: boolean (TRUE,FALSE) indicator if the CS is low purity (low min r2)</li> <li>cs_size: how many snps does this credible set contain</li> <li>good_cs: boolean (TRUE,FALSE) indicator if this CS is considered reliable. IF this is FALSE then top variant reported for the CS will be chosen based on minimum p-value in the credible set, otherwise top variant is chosen by maximum PIP</li> <li>cs_id:</li> <li>v: top variant (chr:pos:ref:alt)</li> <li>p: top variant p-value</li> <li>beta: top variant beta</li> <li>sd: top variant standard deviation</li> <li>prob: overall PIP of the variant in the region</li> <li>cs_specific_prob: PIP of the variant in the current credible set (this and previous are typically almost identical)</li> <li>0..n: configured annotation columns. Typical default most_severe,gene_most_severe giving consequence and gene of top variant</li> </ul> <p>These files needs to be downloaded from the <code>https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/summary/</code> by *.cred.summary.tsv pattern,</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>SparkSession object.</p> required <code>finngen_susie_finemapping_snp_files</code> <code>str | list[str]</code> <p>SuSIE finemapping output filename(s).</p> required <code>finngen_susie_finemapping_cs_summary_files</code> <code>str | list[str]</code> <p>filename of SuSIE finemapping credible set summaries.</p> required <code>finngen_release_prefix</code> <code>str</code> <p>Finngen project release prefix. Should look like FINNGEN_R*.</p> required <code>credset_lbf_threshold</code> <code>float</code> <p>Filter out credible sets below, Default 0.8685889638065036 == np.log10(np.exp(2)), this is threshold from publication.</p> <code>0.8685889638065036</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Processed SuSIE finemapping output in StudyLocus format.</p> Source code in <code>src/gentropy/datasource/finngen/finemapping.py</code> <pre><code>@classmethod\ndef from_finngen_susie_finemapping(\n    cls: type[FinnGenFinemapping],\n    spark: SparkSession,\n    finngen_susie_finemapping_snp_files: (str | list[str]),\n    finngen_susie_finemapping_cs_summary_files: (str | list[str]),\n    finngen_release_prefix: str,\n    credset_lbf_threshold: float = 0.8685889638065036,\n) -&gt; StudyLocus:\n    \"\"\"Process the SuSIE finemapping output for FinnGen studies.\n\n    The finngen_susie_finemapping_snp_files are files that contain variant summaries with credible set information with following shema:\n        - trait: phenotype\n        - region: region for which the fine-mapping was run.\n        - v, rsid: variant ids\n        - chromosome\n        - position\n        - allele1\n        - allele2\n        - maf: minor allele frequency\n        - beta: original marginal beta\n        - se: original se\n        - p: original p\n        - mean: posterior mean beta after fine-mapping\n        - sd: posterior standard deviation after fine-mapping.\n        - prob: posterior inclusion probability\n        - cs: credible set index within region\n        - lead_r2: r2 value to a lead variant (the one with maximum PIP) in a credible set\n        - alphax: posterior inclusion probability for the x-th single effect (x := 1..L where L is the number of single effects (causal variants) specified; default: L = 10).\n        - lbfx: log-Bayes Factor for each variable and single effect (i.e credible set).\n        - meanx: posterior mean for each variable and single effect (i.e credible set).\n        - sdx: posterior sd of mean for each variable and single effect (i.e credible set).\n    As for r11 finngen release these files are ingested from `https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/` by\n        - *.snp.bgz\n        - *.snp.bgz.tbi\n    Each file contains index (.tbi) file that is required to read the block gzipped compressed snp file. These files needs to be\n    downloaded, transfromed from block gzipped to plain gzipped and then uploaded to the storage bucket, before they can be read by spark or read by hail directly as import table.\n\n    The finngen_susie_finemapping_cs_summary_files are files that Contains credible set summaries from SuSiE fine-mapping for all genome-wide significant regions with following schema:\n        - trait: phenotype\n        - region: region for which the fine-mapping was run.\n        - cs: running number for independent credible sets in a region, assigned to 95% PIP\n        - cs_log10bf: Log10 bayes factor of comparing the solution of this model (cs independent credible sets) to cs -1 credible sets\n        - cs_avg_r2: Average correlation R2 between variants in the credible set\n        - cs_min_r2: minimum r2 between variants in the credible set\n        - low_purity: boolean (TRUE,FALSE) indicator if the CS is low purity (low min r2)\n        - cs_size: how many snps does this credible set contain\n        - good_cs: boolean (TRUE,FALSE) indicator if this CS is considered reliable. IF this is FALSE then top variant reported for the CS will be chosen based on minimum p-value in the credible set, otherwise top variant is chosen by maximum PIP\n        - cs_id:\n        - v: top variant (chr:pos:ref:alt)\n        - p: top variant p-value\n        - beta: top variant beta\n        - sd: top variant standard deviation\n        - prob: overall PIP of the variant in the region\n        - cs_specific_prob: PIP of the variant in the current credible set (this and previous are typically almost identical)\n        - 0..n: configured annotation columns. Typical default most_severe,gene_most_severe giving consequence and gene of top variant\n    These files needs to be downloaded from the `https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/summary/` by *.cred.summary.tsv pattern,\n\n    Args:\n        spark (SparkSession): SparkSession object.\n        finngen_susie_finemapping_snp_files (str | list[str]): SuSIE finemapping output filename(s).\n        finngen_susie_finemapping_cs_summary_files (str | list[str]): filename of SuSIE finemapping credible set summaries.\n        finngen_release_prefix (str): Finngen project release prefix. Should look like FINNGEN_R*.\n        credset_lbf_threshold (float, optional): Filter out credible sets below, Default 0.8685889638065036 == np.log10(np.exp(2)), this is threshold from publication.\n\n    Returns:\n        StudyLocus: Processed SuSIE finemapping output in StudyLocus format.\n    \"\"\"\n    # NOTE: hail allows for importing block gzipped files, spark does not without external libraries.\n    # check https://github.com/projectglow/glow/blob/36bf6121fbc4ccc33a13b028deb87b63faeba7a9/core/src/main/scala/io/projectglow/vcf/VCFFileFormat.scala#L274\n    # how it could be implemented with spark.\n    bgzip_compressed_snps = cls._infer_block_gzip_compression(\n        finngen_susie_finemapping_snp_files\n    )\n\n    # NOTE: fallback to spark read if not block gzipped file in the input\n    if bgzip_compressed_snps:\n        snps_df = hl.import_table(\n            finngen_susie_finemapping_snp_files,\n            delimiter=\"\\t\",\n            types=cls.raw_hail_shema,\n        ).to_spark()\n    else:\n        snps_df = (\n            spark.read.schema(cls.raw_schema)\n            .option(\"delimiter\", \"\\t\")\n            .option(\"compression\", \"gzip\")\n            .csv(finngen_susie_finemapping_snp_files, header=True)\n        )\n\n    processed_finngen_finemapping_df = (\n        # Drop rows which don't have proper position.\n        snps_df.filter(f.col(\"position\").cast(t.IntegerType()).isNotNull())\n        # Drop non credible set SNPs:\n        .filter(f.col(\"cs\").cast(t.IntegerType()) &gt; 0)\n        .select(\n            # Add study idenfitier.\n            f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"trait\"))\n            .cast(t.StringType())\n            .alias(\"studyId\"),\n            f.col(\"region\"),\n            # Add variant information.\n            f.regexp_replace(f.col(\"v\"), \":\", \"_\").alias(\"variantId\"),\n            f.col(\"cs\").cast(\"integer\").alias(\"credibleSetIndex\"),\n            f.regexp_replace(f.col(\"chromosome\"), \"^chr\", \"\")\n            .cast(t.StringType())\n            .alias(\"chromosome\"),\n            f.col(\"position\").cast(t.IntegerType()),\n            f.col(\"allele1\").cast(t.StringType()).alias(\"ref\"),\n            f.col(\"allele2\").cast(t.StringType()).alias(\"alt\"),\n            # Parse p-value into mantissa and exponent.\n            *split_pvalue_column(f.col(\"p\")),\n            # Add standard error, and allele frequency information.\n            f.col(\"se\").cast(\"double\").alias(\"standardError\"),\n            f.col(\"maf\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n            f.lit(FinemappingMethod.SUSIE.value).alias(\"finemappingMethod\"),\n            *[\n                f.col(f\"alpha{i}\").cast(t.DoubleType()).alias(f\"alpha_{i}\")\n                for i in range(1, 11)\n            ],\n            *[\n                f.col(f\"lbf_variable{i}\").cast(t.DoubleType()).alias(f\"lbf_{i}\")\n                for i in range(1, 11)\n            ],\n            *[\n                f.col(f\"mean{i}\").cast(t.DoubleType()).alias(f\"beta_{i}\")\n                for i in range(1, 11)\n            ],\n        )\n        .withColumn(\n            \"posteriorProbability\",\n            f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"alpha_1\"))\n            .when(f.col(\"credibleSetIndex\") == 2, f.col(\"alpha_2\"))\n            .when(f.col(\"credibleSetIndex\") == 3, f.col(\"alpha_3\"))\n            .when(f.col(\"credibleSetIndex\") == 4, f.col(\"alpha_4\"))\n            .when(f.col(\"credibleSetIndex\") == 5, f.col(\"alpha_5\"))\n            .when(f.col(\"credibleSetIndex\") == 6, f.col(\"alpha_6\"))\n            .when(f.col(\"credibleSetIndex\") == 7, f.col(\"alpha_7\"))\n            .when(f.col(\"credibleSetIndex\") == 8, f.col(\"alpha_8\"))\n            .when(f.col(\"credibleSetIndex\") == 9, f.col(\"alpha_9\"))\n            .when(f.col(\"credibleSetIndex\") == 10, f.col(\"alpha_10\")),\n        )\n        .drop(\n            \"alpha_1\",\n            \"alpha_2\",\n            \"alpha_3\",\n            \"alpha_4\",\n            \"alpha_5\",\n            \"alpha_6\",\n            \"alpha_7\",\n            \"alpha_8\",\n            \"alpha_9\",\n            \"alpha_10\",\n        )\n        .withColumn(\n            \"logBF\",\n            f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"lbf_1\"))\n            .when(f.col(\"credibleSetIndex\") == 2, f.col(\"lbf_2\"))\n            .when(f.col(\"credibleSetIndex\") == 3, f.col(\"lbf_3\"))\n            .when(f.col(\"credibleSetIndex\") == 4, f.col(\"lbf_4\"))\n            .when(f.col(\"credibleSetIndex\") == 5, f.col(\"lbf_5\"))\n            .when(f.col(\"credibleSetIndex\") == 6, f.col(\"lbf_6\"))\n            .when(f.col(\"credibleSetIndex\") == 7, f.col(\"lbf_7\"))\n            .when(f.col(\"credibleSetIndex\") == 8, f.col(\"lbf_8\"))\n            .when(f.col(\"credibleSetIndex\") == 9, f.col(\"lbf_9\"))\n            .when(f.col(\"credibleSetIndex\") == 10, f.col(\"lbf_10\")),\n        )\n        .drop(\n            \"lbf_1\",\n            \"lbf_2\",\n            \"lbf_3\",\n            \"lbf_4\",\n            \"lbf_5\",\n            \"lbf_6\",\n            \"lbf_7\",\n            \"lbf_8\",\n            \"lbf_9\",\n            \"lbf_10\",\n        )\n        .withColumn(\n            \"beta\",\n            f.when(f.col(\"credibleSetIndex\") == 1, f.col(\"beta_1\"))\n            .when(f.col(\"credibleSetIndex\") == 2, f.col(\"beta_2\"))\n            .when(f.col(\"credibleSetIndex\") == 3, f.col(\"beta_3\"))\n            .when(f.col(\"credibleSetIndex\") == 4, f.col(\"beta_4\"))\n            .when(f.col(\"credibleSetIndex\") == 5, f.col(\"beta_5\"))\n            .when(f.col(\"credibleSetIndex\") == 6, f.col(\"beta_6\"))\n            .when(f.col(\"credibleSetIndex\") == 7, f.col(\"beta_7\"))\n            .when(f.col(\"credibleSetIndex\") == 8, f.col(\"beta_8\"))\n            .when(f.col(\"credibleSetIndex\") == 9, f.col(\"beta_9\"))\n            .when(f.col(\"credibleSetIndex\") == 10, f.col(\"beta_10\")),\n        )\n        .drop(\n            \"beta_1\",\n            \"beta_2\",\n            \"beta_3\",\n            \"beta_4\",\n            \"beta_5\",\n            \"beta_6\",\n            \"beta_7\",\n            \"beta_8\",\n            \"beta_9\",\n            \"beta_10\",\n        )\n    )\n\n    bgzip_compressed_cs_summaries = cls._infer_block_gzip_compression(\n        finngen_susie_finemapping_cs_summary_files\n    )\n\n    # NOTE: fallback to spark read if not block gzipped file in the input\n    # in case we want to use the raw files from the\n    # https://console.cloud.google.com/storage/browser/finngen-public-data-r11/finemap/full/susie/*.cred.gz\n    if bgzip_compressed_cs_summaries:\n        cs_summary_df = hl.import_table(\n            finngen_susie_finemapping_cs_summary_files,\n            delimiter=\"\\t\",\n            types=cls.summary_hail_schema,\n        ).to_spark()\n    else:\n        cs_summary_df = (\n            spark.read.schema(cls.summary_schema)\n            .option(\"delimiter\", \"\\t\")\n            .csv(finngen_susie_finemapping_cs_summary_files, header=True)\n        )\n\n    # drop credible sets where logbf &gt; 2. Except when there's only one credible set in region:\n    # 0.8685889638065036 corresponds to np.log10(np.exp(2)), to match the orginal threshold in publication.\n    finngen_finemapping_summaries_df = (\n        # Read credible set level lbf, it is output as a different file which is not ideal.\n        cs_summary_df.select(\n            f.col(\"region\"),\n            f.col(\"trait\"),\n            f.col(\"cs\").cast(\"integer\").alias(\"credibleSetIndex\"),\n            f.col(\"cs_log10bf\").cast(\"double\").alias(\"credibleSetlog10BF\"),\n            f.col(\"cs_avg_r2\").cast(\"double\").alias(\"purityMeanR2\"),\n            f.col(\"cs_min_r2\").cast(\"double\").alias(\"purityMinR2\"),\n        )\n        .filter(\n            (f.col(\"credibleSetlog10BF\") &gt; credset_lbf_threshold)\n            | (f.col(\"credibleSetIndex\") == 1)\n        )\n        .withColumn(\n            \"studyId\",\n            f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"trait\")),\n        )\n    )\n\n    processed_finngen_finemapping_df = processed_finngen_finemapping_df.join(\n        finngen_finemapping_summaries_df,\n        on=[\"studyId\", \"region\", \"credibleSetIndex\"],\n        how=\"inner\",\n    )\n\n    toploci_df = get_top_ranked_in_window(\n        processed_finngen_finemapping_df,\n        Window.partitionBy(\"studyId\", \"region\", \"credibleSetIndex\").orderBy(\n            f.desc(\"posteriorProbability\")\n        ),\n    ).select(\n        \"variantId\",\n        \"chromosome\",\n        \"position\",\n        \"studyId\",\n        \"beta\",\n        \"pValueMantissa\",\n        \"pValueExponent\",\n        \"effectAlleleFrequencyFromSource\",\n        \"standardError\",\n        \"region\",\n        \"credibleSetIndex\",\n        \"finemappingMethod\",\n        \"credibleSetlog10BF\",\n        \"purityMeanR2\",\n        \"purityMinR2\",\n    )\n\n    processed_finngen_finemapping_df = (\n        processed_finngen_finemapping_df.groupBy(\n            \"studyId\", \"region\", \"credibleSetIndex\"\n        )\n        .agg(\n            f.collect_list(\n                f.struct(\n                    f.col(\"variantId\").cast(\"string\").alias(\"variantId\"),\n                    f.col(\"posteriorProbability\")\n                    .cast(\"double\")\n                    .alias(\"posteriorProbability\"),\n                    f.col(\"logBF\").cast(\"double\").alias(\"logBF\"),\n                    f.col(\"pValueMantissa\").cast(\"float\").alias(\"pValueMantissa\"),\n                    f.col(\"pValueExponent\").cast(\"integer\").alias(\"pValueExponent\"),\n                    f.col(\"beta\").cast(\"double\").alias(\"beta\"),\n                    f.col(\"standardError\").cast(\"double\").alias(\"standardError\"),\n                )\n            ).alias(\"locus\"),\n        )\n        .select(\n            \"studyId\",\n            \"region\",\n            \"credibleSetIndex\",\n            \"locus\",\n        )\n        .join(\n            toploci_df,\n            on=[\"studyId\", \"region\", \"credibleSetIndex\"],\n            how=\"inner\",\n        )\n        .withColumns(\n            {\n                \"locusStart\": f.split(f.split(\"region\", \":\")[1], \"-\")[0].cast(\n                    \"int\"\n                ),\n                \"locusEnd\": f.split(f.split(\"region\", \":\")[1], \"-\")[1].cast(\"int\"),\n            }\n        )\n    ).withColumn(\n        \"studyLocusId\",\n        StudyLocus.assign_study_locus_id(\n            [\"studyId\", \"variantId\", \"finemappingMethod\"]\n        ),\n    )\n\n    return StudyLocus(\n        _df=processed_finngen_finemapping_df,\n        _schema=StudyLocus.get_schema(),\n    ).annotate_credible_sets()\n</code></pre>"},{"location":"python_api/datasources/finngen/study_index/","title":"Study Index","text":""},{"location":"python_api/datasources/finngen/study_index/#gentropy.datasource.finngen.study_index.FinnGenStudyIndex","title":"<code>gentropy.datasource.finngen.study_index.FinnGenStudyIndex</code>","text":"<p>Study index dataset from FinnGen.</p> <p>The following information is aggregated/extracted:</p> <ul> <li>Study ID in the special format (e.g. FINNGEN_R11_*)</li> <li>Trait name (for example, Amoebiasis)</li> <li>Number of cases and controls</li> <li>Link to the summary statistics location</li> <li>EFO mapping from curated EFO mapping file</li> </ul> <p>Some fields are also populated as constants, such as study type and the initial sample size.</p> Source code in <code>src/gentropy/datasource/finngen/study_index.py</code> <pre><code>class FinnGenStudyIndex:\n    \"\"\"Study index dataset from FinnGen.\n\n    The following information is aggregated/extracted:\n\n    - Study ID in the special format (e.g. FINNGEN_R11_*)\n    - Trait name (for example, Amoebiasis)\n    - Number of cases and controls\n    - Link to the summary statistics location\n    - EFO mapping from curated EFO mapping file\n\n    Some fields are also populated as constants, such as study type and the initial sample size.\n    \"\"\"\n\n    CONSTANTS = {\n        \"studyType\": \"gwas\",\n        \"hasSumstats\": True,\n        \"initialSampleSize\": \"500,348 (282,064 females and 218,284 males)\",\n        \"pubmedId\": \"36653562\",\n    }\n\n    @staticmethod\n    def validate_release_prefix(release_prefix: str) -&gt; FinngenPrefixMatch:\n        \"\"\"Validate release prefix passed to finngen StudyIndex.\n\n        Args:\n            release_prefix (str): Finngen release prefix, should be a string like FINNGEN_R*.\n\n        Returns:\n            FinngenPrefixMatch: Object containing valid prefix and release strings.\n\n        Raises:\n            ValueError: when incorrect release prefix is provided.\n\n        This method ensures that the trailing underscore is removed from prefix.\n        \"\"\"\n        pattern = re.compile(r\"FINNGEN_(?P&lt;release&gt;R\\d+){1}_?\")\n        pattern_match = pattern.match(release_prefix)\n        if not pattern_match:\n            raise ValueError(\n                f\"Invalid FinnGen release prefix: {release_prefix}, use the format FINNGEN_R*\"\n            )\n        release = pattern_match.group(\"release\").upper()\n        if release_prefix.endswith(\"_\"):\n            release_prefix = release_prefix[:-1]\n        return FinngenPrefixMatch(prefix=release_prefix, release=release)\n\n    @classmethod\n    def from_source(\n        cls: type[FinnGenStudyIndex],\n        spark: SparkSession,\n        finngen_phenotype_table_url: str,\n        finngen_release_prefix: str,\n        finngen_summary_stats_url_prefix: str,\n        finngen_summary_stats_url_suffix: str,\n        sample_size: int,\n    ) -&gt; StudyIndex:\n        \"\"\"This function ingests study level metadata from FinnGen.\n\n        Args:\n            spark (SparkSession): Spark session object.\n            finngen_phenotype_table_url (str): URL to the FinnGen phenotype table.\n            finngen_release_prefix (str): FinnGen release prefix.\n            finngen_summary_stats_url_prefix (str): FinnGen summary stats URL prefix.\n            finngen_summary_stats_url_suffix (str): FinnGen summary stats URL suffix.\n            sample_size (int): Number of individuals participated in sample collection.\n\n        Returns:\n            StudyIndex: Parsed and annotated FinnGen study table.\n        \"\"\"\n        json_data = urlopen(finngen_phenotype_table_url).read().decode(\"utf-8\")\n        rdd = spark.sparkContext.parallelize([json_data])\n        raw_df = spark.read.json(rdd)\n\n        return StudyIndex(\n            _df=raw_df.select(\n                f.concat(\n                    f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"phenocode\"))\n                ).alias(\"studyId\"),\n                f.col(\"phenostring\").alias(\"traitFromSource\"),\n                f.col(\"num_cases\").cast(\"integer\").alias(\"nCases\"),\n                f.col(\"num_controls\").cast(\"integer\").alias(\"nControls\"),\n                (f.col(\"num_cases\") + f.col(\"num_controls\"))\n                .cast(\"integer\")\n                .alias(\"nSamples\"),\n                f.array(\n                    f.struct(\n                        f.lit(sample_size).cast(\"integer\").alias(\"sampleSize\"),\n                        f.lit(\"Finnish\").alias(\"ancestry\"),\n                    )\n                ).alias(\"discoverySamples\"),\n                # Cohort label is consistent with GWAS Catalog curation.\n                f.array(f.lit(\"FinnGen\")).alias(\"cohorts\"),\n                f.concat(\n                    f.lit(finngen_summary_stats_url_prefix),\n                    f.col(\"phenocode\"),\n                    f.lit(finngen_summary_stats_url_suffix),\n                ).alias(\"summarystatsLocation\"),\n                f.lit(finngen_release_prefix).alias(\"projectId\"),\n                *[f.lit(value).alias(key) for key, value in cls.CONSTANTS.items()],\n            ).withColumn(\n                \"ldPopulationStructure\",\n                StudyIndex.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n            ),\n            _schema=StudyIndex.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/finngen/study_index/#gentropy.datasource.finngen.study_index.FinnGenStudyIndex.from_source","title":"<code>from_source(spark: SparkSession, finngen_phenotype_table_url: str, finngen_release_prefix: str, finngen_summary_stats_url_prefix: str, finngen_summary_stats_url_suffix: str, sample_size: int) -&gt; StudyIndex</code>  <code>classmethod</code>","text":"<p>This function ingests study level metadata from FinnGen.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>finngen_phenotype_table_url</code> <code>str</code> <p>URL to the FinnGen phenotype table.</p> required <code>finngen_release_prefix</code> <code>str</code> <p>FinnGen release prefix.</p> required <code>finngen_summary_stats_url_prefix</code> <code>str</code> <p>FinnGen summary stats URL prefix.</p> required <code>finngen_summary_stats_url_suffix</code> <code>str</code> <p>FinnGen summary stats URL suffix.</p> required <code>sample_size</code> <code>int</code> <p>Number of individuals participated in sample collection.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Parsed and annotated FinnGen study table.</p> Source code in <code>src/gentropy/datasource/finngen/study_index.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[FinnGenStudyIndex],\n    spark: SparkSession,\n    finngen_phenotype_table_url: str,\n    finngen_release_prefix: str,\n    finngen_summary_stats_url_prefix: str,\n    finngen_summary_stats_url_suffix: str,\n    sample_size: int,\n) -&gt; StudyIndex:\n    \"\"\"This function ingests study level metadata from FinnGen.\n\n    Args:\n        spark (SparkSession): Spark session object.\n        finngen_phenotype_table_url (str): URL to the FinnGen phenotype table.\n        finngen_release_prefix (str): FinnGen release prefix.\n        finngen_summary_stats_url_prefix (str): FinnGen summary stats URL prefix.\n        finngen_summary_stats_url_suffix (str): FinnGen summary stats URL suffix.\n        sample_size (int): Number of individuals participated in sample collection.\n\n    Returns:\n        StudyIndex: Parsed and annotated FinnGen study table.\n    \"\"\"\n    json_data = urlopen(finngen_phenotype_table_url).read().decode(\"utf-8\")\n    rdd = spark.sparkContext.parallelize([json_data])\n    raw_df = spark.read.json(rdd)\n\n    return StudyIndex(\n        _df=raw_df.select(\n            f.concat(\n                f.concat_ws(\"_\", f.lit(finngen_release_prefix), f.col(\"phenocode\"))\n            ).alias(\"studyId\"),\n            f.col(\"phenostring\").alias(\"traitFromSource\"),\n            f.col(\"num_cases\").cast(\"integer\").alias(\"nCases\"),\n            f.col(\"num_controls\").cast(\"integer\").alias(\"nControls\"),\n            (f.col(\"num_cases\") + f.col(\"num_controls\"))\n            .cast(\"integer\")\n            .alias(\"nSamples\"),\n            f.array(\n                f.struct(\n                    f.lit(sample_size).cast(\"integer\").alias(\"sampleSize\"),\n                    f.lit(\"Finnish\").alias(\"ancestry\"),\n                )\n            ).alias(\"discoverySamples\"),\n            # Cohort label is consistent with GWAS Catalog curation.\n            f.array(f.lit(\"FinnGen\")).alias(\"cohorts\"),\n            f.concat(\n                f.lit(finngen_summary_stats_url_prefix),\n                f.col(\"phenocode\"),\n                f.lit(finngen_summary_stats_url_suffix),\n            ).alias(\"summarystatsLocation\"),\n            f.lit(finngen_release_prefix).alias(\"projectId\"),\n            *[f.lit(value).alias(key) for key, value in cls.CONSTANTS.items()],\n        ).withColumn(\n            \"ldPopulationStructure\",\n            StudyIndex.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n        ),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen/study_index/#gentropy.datasource.finngen.study_index.FinnGenStudyIndex.validate_release_prefix","title":"<code>validate_release_prefix(release_prefix: str) -&gt; FinngenPrefixMatch</code>  <code>staticmethod</code>","text":"<p>Validate release prefix passed to finngen StudyIndex.</p> <p>Parameters:</p> Name Type Description Default <code>release_prefix</code> <code>str</code> <p>Finngen release prefix, should be a string like FINNGEN_R*.</p> required <p>Returns:</p> Name Type Description <code>FinngenPrefixMatch</code> <code>FinngenPrefixMatch</code> <p>Object containing valid prefix and release strings.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>when incorrect release prefix is provided.</p> <p>This method ensures that the trailing underscore is removed from prefix.</p> Source code in <code>src/gentropy/datasource/finngen/study_index.py</code> <pre><code>@staticmethod\ndef validate_release_prefix(release_prefix: str) -&gt; FinngenPrefixMatch:\n    \"\"\"Validate release prefix passed to finngen StudyIndex.\n\n    Args:\n        release_prefix (str): Finngen release prefix, should be a string like FINNGEN_R*.\n\n    Returns:\n        FinngenPrefixMatch: Object containing valid prefix and release strings.\n\n    Raises:\n        ValueError: when incorrect release prefix is provided.\n\n    This method ensures that the trailing underscore is removed from prefix.\n    \"\"\"\n    pattern = re.compile(r\"FINNGEN_(?P&lt;release&gt;R\\d+){1}_?\")\n    pattern_match = pattern.match(release_prefix)\n    if not pattern_match:\n        raise ValueError(\n            f\"Invalid FinnGen release prefix: {release_prefix}, use the format FINNGEN_R*\"\n        )\n    release = pattern_match.group(\"release\").upper()\n    if release_prefix.endswith(\"_\"):\n        release_prefix = release_prefix[:-1]\n    return FinngenPrefixMatch(prefix=release_prefix, release=release)\n</code></pre>"},{"location":"python_api/datasources/finngen/summary_stats/","title":"Summary Statistics","text":""},{"location":"python_api/datasources/finngen/summary_stats/#gentropy.datasource.finngen.summary_stats.FinnGenSummaryStats","title":"<code>gentropy.datasource.finngen.summary_stats.FinnGenSummaryStats</code>  <code>dataclass</code>","text":"<p>Summary statistics dataset for FinnGen.</p> Source code in <code>src/gentropy/datasource/finngen/summary_stats.py</code> <pre><code>@dataclass\nclass FinnGenSummaryStats:\n    \"\"\"Summary statistics dataset for FinnGen.\"\"\"\n\n    raw_schema: t.StructType = StructType(\n        [\n            StructField(\"#chrom\", StringType(), True),\n            StructField(\"pos\", StringType(), True),\n            StructField(\"ref\", StringType(), True),\n            StructField(\"alt\", StringType(), True),\n            StructField(\"rsids\", StringType(), True),\n            StructField(\"nearest_genes\", StringType(), True),\n            StructField(\"pval\", StringType(), True),\n            StructField(\"mlogp\", StringType(), True),\n            StructField(\"beta\", StringType(), True),\n            StructField(\"sebeta\", StringType(), True),\n            StructField(\"af_alt\", StringType(), True),\n            StructField(\"af_alt_cases\", StringType(), True),\n            StructField(\"af_alt_controls\", StringType(), True),\n        ]\n    )\n\n    @classmethod\n    def from_source(\n        cls: type[FinnGenSummaryStats],\n        spark: SparkSession,\n        raw_file: str,\n    ) -&gt; SummaryStatistics:\n        \"\"\"Ingests all summary statst for all FinnGen studies.\n\n        Args:\n            spark (SparkSession): Spark session object.\n            raw_file (str): Path to raw summary statistics .gz files.\n\n        Returns:\n            SummaryStatistics: Processed summary statistics dataset\n        \"\"\"\n        processed_summary_stats_df = (\n            spark.read.schema(cls.raw_schema)\n            .option(\"delimiter\", \"\\t\")\n            .csv(raw_file, header=True)\n            # Drop rows which don't have proper position.\n            .filter(f.col(\"pos\").cast(t.IntegerType()).isNotNull())\n            .select(\n                # From the full path, extracts just the filename, and converts to upper case to get the study ID.\n                f.upper(\n                    f.regexp_extract(\n                        f.input_file_name(), r\"([^/]+)(\\.tsv\\.gz|\\.gz|\\.tsv)\", 1\n                    )\n                ).alias(\"studyId\"),\n                # Add variant information.\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"#chrom\"),\n                    f.col(\"pos\"),\n                    f.col(\"ref\"),\n                    f.col(\"alt\"),\n                ).alias(\"variantId\"),\n                f.col(\"#chrom\").alias(\"chromosome\"),\n                f.col(\"pos\").cast(t.IntegerType()).alias(\"position\"),\n                # Parse p-value into mantissa and exponent.\n                *split_pvalue_column(f.col(\"pval\")),\n                # Add beta, standard error, and allele frequency information.\n                f.col(\"beta\").cast(\"double\"),\n                f.col(\"sebeta\").cast(\"double\").alias(\"standardError\"),\n                f.col(\"af_alt\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n            )\n            # Calculating the confidence intervals.\n            .filter(\n                f.col(\"pos\").cast(t.IntegerType()).isNotNull() &amp; (f.col(\"beta\") != 0)\n            )\n            # Average ~20Mb partitions with 30 partitions per study\n            .repartitionByRange(30, \"chromosome\", \"position\")\n            .sortWithinPartitions(\"chromosome\", \"position\")\n        )\n\n        # Initializing summary statistics object:\n        return SummaryStatistics(\n            _df=processed_summary_stats_df,\n            _schema=SummaryStatistics.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/finngen/summary_stats/#gentropy.datasource.finngen.summary_stats.FinnGenSummaryStats.from_source","title":"<code>from_source(spark: SparkSession, raw_file: str) -&gt; SummaryStatistics</code>  <code>classmethod</code>","text":"<p>Ingests all summary statst for all FinnGen studies.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>raw_file</code> <code>str</code> <p>Path to raw summary statistics .gz files.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>Processed summary statistics dataset</p> Source code in <code>src/gentropy/datasource/finngen/summary_stats.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[FinnGenSummaryStats],\n    spark: SparkSession,\n    raw_file: str,\n) -&gt; SummaryStatistics:\n    \"\"\"Ingests all summary statst for all FinnGen studies.\n\n    Args:\n        spark (SparkSession): Spark session object.\n        raw_file (str): Path to raw summary statistics .gz files.\n\n    Returns:\n        SummaryStatistics: Processed summary statistics dataset\n    \"\"\"\n    processed_summary_stats_df = (\n        spark.read.schema(cls.raw_schema)\n        .option(\"delimiter\", \"\\t\")\n        .csv(raw_file, header=True)\n        # Drop rows which don't have proper position.\n        .filter(f.col(\"pos\").cast(t.IntegerType()).isNotNull())\n        .select(\n            # From the full path, extracts just the filename, and converts to upper case to get the study ID.\n            f.upper(\n                f.regexp_extract(\n                    f.input_file_name(), r\"([^/]+)(\\.tsv\\.gz|\\.gz|\\.tsv)\", 1\n                )\n            ).alias(\"studyId\"),\n            # Add variant information.\n            f.concat_ws(\n                \"_\",\n                f.col(\"#chrom\"),\n                f.col(\"pos\"),\n                f.col(\"ref\"),\n                f.col(\"alt\"),\n            ).alias(\"variantId\"),\n            f.col(\"#chrom\").alias(\"chromosome\"),\n            f.col(\"pos\").cast(t.IntegerType()).alias(\"position\"),\n            # Parse p-value into mantissa and exponent.\n            *split_pvalue_column(f.col(\"pval\")),\n            # Add beta, standard error, and allele frequency information.\n            f.col(\"beta\").cast(\"double\"),\n            f.col(\"sebeta\").cast(\"double\").alias(\"standardError\"),\n            f.col(\"af_alt\").cast(\"float\").alias(\"effectAlleleFrequencyFromSource\"),\n        )\n        # Calculating the confidence intervals.\n        .filter(\n            f.col(\"pos\").cast(t.IntegerType()).isNotNull() &amp; (f.col(\"beta\") != 0)\n        )\n        # Average ~20Mb partitions with 30 partitions per study\n        .repartitionByRange(30, \"chromosome\", \"position\")\n        .sortWithinPartitions(\"chromosome\", \"position\")\n    )\n\n    # Initializing summary statistics object:\n    return SummaryStatistics(\n        _df=processed_summary_stats_df,\n        _schema=SummaryStatistics.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/_finngen_meta/","title":"FinnGen Meta Analysis","text":"<p>FinnGen Meta Analysis is a meta analysis that combines genetic data from three major biobanks: FinnGen, UK Biobank (UKBB), and the Million Veteran Program (MVP).</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/","title":"FinnGen Meta Analysis Manifest","text":""},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest","title":"<code>gentropy.datasource.finngen_meta.FinnGenMetaManifest</code>","text":"<p>FinnGen meta-analysis manifest.</p> Source code in <code>src/gentropy/datasource/finngen_meta/__init__.py</code> <pre><code>class FinnGenMetaManifest:\n    \"\"\"FinnGen meta-analysis manifest.\"\"\"\n\n    ukbb_ancestry_cols = {\n        \"ukbb_n_cases\",\n        \"ukbb_n_controls\",\n    }\n\n    finngen_ancestry_cols = {\n        \"fg_n_cases\",\n        \"fg_n_controls\",\n    }\n\n    required_columns = {\n        \"fg_phenotype\",  # Original Finngen studyId (e.g. \"I9_HEARTFAIL\")\n        \"name\",  # Finngen phenotype name - used for mapping to EFO\n        # Ancestry columns for finngen and UKBB (should be in both meta analyses)\n        *finngen_ancestry_cols,\n        *ukbb_ancestry_cols,\n    }\n\n    mvp_ancestry_columns = {\n        \"MVP_AFR_n_cases\",\n        \"MVP_AFR_n_controls\",\n        \"MVP_EUR_n_cases\",\n        \"MVP_EUR_n_controls\",\n        \"MVP_AMR_n_cases\",\n        \"MVP_AMR_n_controls\",\n    }\n    sumstat_location_column = \"path_bucket\"\n\n    def __init__(self, df: DataFrame, meta: MetaAnalysisDataSource) -&gt; None:\n        \"\"\"Initialize the FinnGen meta-analysis manifest.\n\n        Args:\n            df (DataFrame): DataFrame containing the manifest data.\n            meta (MetaAnalysisDataSource): Meta-analysis data source enum.\n        \"\"\"\n        self.meta = meta\n        self._df = df\n\n    @property\n    def df(self) -&gt; DataFrame:\n        \"\"\"Get the manifest DataFrame.\n\n        The resulting DataFrame has the following schema:\n        ```\n        |-- studyPhenotype: string (nullable = true)\n        |-- traitFromSource: string (nullable = true)\n        |-- discoverySamples: array (nullable = true)\n            |-- element: struct (containsNull = true)\n                |-- sampleSize: integer (nullable = true)\n                |-- ancestry: string (nullable = true)\n        |-- nSamples: integer (nullable = true)\n        |-- nCases: integer (nullable = true)\n        |-- nSamplesPerCohort: array (nullable = true)\n            |-- element: struct (containsNull = true)\n                |-- cohort: string (nullable = true)\n                |-- nSamples: integer (nullable = true)\n        |-- nCasesPerCohort: array (nullable = true)\n            |-- element: struct (containsNull = true)\n                |-- cohort: string (nullable = true)\n                |-- nCases: integer (nullable = true)\n        |-- nControls: integer (nullable = true)\n        |-- hasSumstats: boolean (nullable = true)\n        |-- summarystatsLocation: string (nullable = true)  # may be null if not provided in the manifest\n        ```\n        \"\"\"\n        return self._df.select(\n            self.study_id.alias(\"studyId\"),\n            self.project_id.alias(\"projectId\"),\n            self.trait_from_source.alias(\"traitFromSource\"),\n            self.discovery_samples.alias(\"discoverySamples\"),\n            self.n_samples.alias(\"nSamples\"),\n            self.n_samples_per_cohort.alias(\"nSamplesPerCohort\"),\n            self.n_cases.alias(\"nCases\"),\n            self.n_cases_per_cohort.alias(\"nCasesPerCohort\"),\n            self.n_controls.alias(\"nControls\"),\n            self.summary_statistics_location.alias(\"summarystatsLocation\"),\n            self.has_summary_statistics.alias(\"hasSumstats\"),\n        )\n\n    @classmethod\n    def from_path(cls, session: Session, manifest_path: str) -&gt; FinnGenMetaManifest:\n        \"\"\"Load the FinnGen meta-analysis manifest from a specified path.\n\n        Note:\n            This method asserts that the manifest file is tab-delimited and contains header with following columns:\n            ```\n            |-- fg_phenotype: string (nullable = true)        # required\n            |-- name: string (nullable = true)                # required\n            |-- fg_n_cases: integer (nullable = true)         # required\n            |-- fg_n_controls: integer (nullable = true)      # required\n            |-- ukbb_n_cases: integer (nullable = true)       # required\n            |-- ukbb_n_controls: integer (nullable = true)    # required\n            |-- MVP_AFR_n_cases: integer (nullable = true)    # optional\n            |-- MVP_AFR_n_controls: integer (nullable = true) # optional\n            |-- MVP_EUR_n_cases: integer (nullable = true)    # optional\n            |-- MVP_EUR_n_controls: integer (nullable = true) # optional\n            |-- MVP_AMR_n_cases: integer (nullable = true)    # optional\n            |-- MVP_AMR_n_controls: integer (nullable = true) # optional\n            |-- path_bucket: string (nullable = true)         # optional\n            ```\n        Args:\n            session (Session): Session object.\n            manifest_path (str): Path to the manifest file.\n\n        Returns:\n            FinngenMetaManifest: Loaded manifest object.\n\n        Raises:\n            AssertionError: If the manifest file does not contain the required columns.\n            AssertionError: If the manifest file does not contain the required columns.\n        \"\"\"\n        df = (\n            session.spark.read.option(\"header\", True)\n            .option(\"sep\", \"\\t\")\n            .csv(manifest_path)\n        )\n        assert cls.required_columns.issubset(set(df.columns)), (\n            f\"Manifest file must contain the following columns: {cls.required_columns}. \"\n        )\n\n        # By default we assume we are dealing with the FinnGen UKBB meta-analysis\n        meta = MetaAnalysisDataSource.FINNGEN_UKBB\n        columns = [*cls.required_columns]\n\n        # If we have the MVP ancestry columns, then we are dealing with the FinnGen UKBB MVP meta-analysis\n        if cls.mvp_ancestry_columns.issubset(set(df.columns)):\n            meta = MetaAnalysisDataSource.FINNGEN_UKBB_MVP\n            columns += [*cls.mvp_ancestry_columns]\n\n        column_map = [\n            f.col(col).cast(t.IntegerType()).alias(col)\n            if \"n_cases\" in col or \"n_controls\" in col\n            else f.col(col).cast(t.StringType()).alias(col)\n            for col in columns\n        ]\n\n        # Handle the summary statistics location.\n        if cls.sumstat_location_column in df.columns:\n            column_map.append(\n                f.col(cls.sumstat_location_column)\n                .cast(t.StringType())\n                .alias(cls.sumstat_location_column)\n            )\n        else:\n            session.logger.warning(\n                f\"Manifest file does not contain the '{cls.sumstat_location_column}' column. Can not determine summary statistics location.\"\n            )\n            column_map.append(f.lit(None).alias(cls.sumstat_location_column))\n\n        df = df.select(*column_map)  # Final contract\n        return cls(df=df, meta=meta)\n\n    @property\n    def discovery_samples(self) -&gt; Column:\n        \"\"\"Get the discovery samples.\n\n        This method dispatches to the appropriate private method based on the meta-analysis data source.\n\n        Returns:\n            Column: Spark Column representing the discovery samples.\n        \"\"\"\n        if self.meta == MetaAnalysisDataSource.FINNGEN_UKBB:\n            return self._discovery_samples_finngen_ukbb()\n        elif self.meta == MetaAnalysisDataSource.FINNGEN_UKBB_MVP:\n            return self._discovery_samples_finngen_ukbb_mvp()\n        else:\n            raise ValueError(f\"Unsupported meta-analysis data source: {self.meta}\")\n\n    @staticmethod\n    def _add(*cols: Column) -&gt; Column:\n        \"\"\"Get the total number of samples from multiple columns.\n\n        Args:\n            *cols (Column): Columns to sum.\n\n        Returns:\n            Column: Column representing the total number of samples.\n\n\n        Examples:\n            &gt;&gt;&gt; df = spark.createDataFrame([(1, 2, 3), (1, 2, None)], [\"a\", \"b\", \"c\"])\n            &gt;&gt;&gt; df.select(FinnGenMetaManifest._add(f.col(\"a\"), f.col(\"b\"), f.col(\"c\")).alias(\"total\")).show()\n            +-----+\n            |total|\n            +-----+\n            |    6|\n            |    3|\n            +-----+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # Coalesce to 0 to handle nulls\n        ccols = [f.coalesce(col, f.lit(0)) for col in cols]\n        return reduce(operator.add, ccols).cast(t.IntegerType())\n\n    @property\n    def ancestry_columns(self) -&gt; set[str]:\n        \"\"\"Find all ancestry number columns in the manifest.\n\n        These columns are used to calculate the total number of samples, cases, and controls.\n\n        Returns:\n            set[str]: Set of ancestry number columns.\n\n        Raises:\n            ValueError: If the meta-analysis data source is unsupported.\n\n        Examples:\n            &gt;&gt;&gt; dummy_df = spark.createDataFrame([(1,2,3), (4,5,6)])\n            &gt;&gt;&gt; manifest = FinnGenMetaManifest(df=dummy_df, meta=MetaAnalysisDataSource.FINNGEN_UKBB)\n            &gt;&gt;&gt; sorted(manifest.ancestry_columns)\n            ['fg_n_cases', 'fg_n_controls', 'ukbb_n_cases', 'ukbb_n_controls']\n            &gt;&gt;&gt; manifest = FinnGenMetaManifest(df=dummy_df, meta=MetaAnalysisDataSource.FINNGEN_UKBB_MVP)\n            &gt;&gt;&gt; sorted(manifest.ancestry_columns)\n            ['MVP_AFR_n_cases', 'MVP_AFR_n_controls', 'MVP_AMR_n_cases', 'MVP_AMR_n_controls', 'MVP_EUR_n_cases', 'MVP_EUR_n_controls', 'fg_n_cases', 'fg_n_controls', 'ukbb_n_cases', 'ukbb_n_controls']\n        \"\"\"\n        if self.meta == MetaAnalysisDataSource.FINNGEN_UKBB:\n            return self.finngen_ancestry_cols | self.ukbb_ancestry_cols\n        elif self.meta == MetaAnalysisDataSource.FINNGEN_UKBB_MVP:\n            return (\n                self.finngen_ancestry_cols\n                | self.ukbb_ancestry_cols\n                | self.mvp_ancestry_columns\n            )\n        else:\n            raise ValueError(f\"Unsupported meta-analysis data source: {self.meta}\")\n\n    @property\n    def n_samples(self) -&gt; Column:\n        \"\"\"Get the total number of samples.\"\"\"\n        return self._add(*[f.col(c) for c in self.ancestry_columns]).alias(\"nSamples\")\n\n    @property\n    def n_cases(self) -&gt; Column:\n        \"\"\"Get the total number of cases.\"\"\"\n        ancestry_cols = [f.col(c) for c in self.ancestry_columns if \"n_cases\" in c]\n        return self._add(*ancestry_cols).alias(\"nCases\")\n\n    @property\n    def n_controls(self) -&gt; Column:\n        \"\"\"Get the total number of cases.\"\"\"\n        ancestry_cols = [f.col(c) for c in self.ancestry_columns if \"n_controls\" in c]\n        return self._add(*ancestry_cols).alias(\"nControls\")\n\n    def _discovery_samples_finngen_ukbb(self) -&gt; Column:\n        \"\"\"Get the discovery samples for FinnGen UKBB meta-analysis.\n\n        This meta analysis includes only two cohorts:\n        - Finnish (from FinnGen)\n        - Non-Finnish European (from Pan-UKBB European subset)\n\n        All ancestries with sample size &gt; 0 are included.\n\n        Returns:\n            Column: Spark Column representing the ancestry cocktail.\n        \"\"\"\n        return f.filter(\n            f.array(\n                f.struct(\n                    (\n                        f.coalesce(f.col(\"fg_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"fg_n_controls\"), f.lit(0))\n                    )\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                    f.lit(\"fin\").alias(\"ancestry\"),\n                ),\n                f.struct(\n                    (\n                        f.coalesce(f.col(\"ukbb_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"ukbb_n_controls\"), f.lit(0))\n                    )\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                    f.lit(\"nfe\").alias(\"ancestry\"),\n                ),\n            ),\n            lambda x: x.sampleSize &gt; 0.0,\n        ).alias(\"discoverySamples\")\n\n    def _discovery_samples_finngen_ukbb_mvp(self) -&gt; Column:\n        \"\"\"Get the discovery samples for FinnGen UKBB MVP meta-analysis.\n\n        This meta analysis includes n of four cohorts:\n        - Finnish (from FinnGen)\n        - European (from Pan-UKBB European subset and MVP European subset)\n        - African (from MVP African subset)\n        - American (from MVP American subset)\n\n        All ancestries with sample size &gt; 0 are included.\n\n        Returns:\n            Column: Spark Column representing the ancestry cocktail.\n        \"\"\"\n        return f.filter(\n            f.array(\n                f.struct(\n                    (\n                        f.coalesce(f.col(\"fg_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"fg_n_controls\"), f.lit(0))\n                    )\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                    f.lit(\"Finnish\").alias(\"ancestry\"),\n                ),\n                f.struct(\n                    (\n                        f.coalesce(f.col(\"ukbb_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"ukbb_n_controls\"), f.lit(0))\n                        + f.coalesce(f.col(\"MVP_EUR_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"MVP_EUR_n_controls\"), f.lit(0))\n                    )\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                    f.lit(\"European\").alias(\"ancestry\"),\n                ),\n                f.struct(\n                    (\n                        f.coalesce(f.col(\"MVP_AFR_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"MVP_AFR_n_controls\"), f.lit(0))\n                    )\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                    f.lit(\"African\").alias(\"ancestry\"),\n                ),\n                f.struct(\n                    (\n                        f.coalesce(f.col(\"MVP_AMR_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"MVP_AMR_n_controls\"), f.lit(0))\n                    )\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                    f.lit(\"Admixed American\").alias(\"ancestry\"),\n                ),\n            ),\n            lambda x: x.sampleSize &gt; 0.0,\n        ).alias(\"discoverySamples\")\n\n    @property\n    def summary_statistics_location(self) -&gt; Column:\n        \"\"\"Get the summary statistics location column.\n\n        Returns:\n            Column: Spark Column representing the summary statistics location.\n        \"\"\"\n        if self.sumstat_location_column in self._df.columns:\n            return (\n                f.col(self.sumstat_location_column)\n                .cast(t.StringType())\n                .alias(\"summarystatsLocation\")\n            )\n        else:\n            return f.lit(None).cast(t.StringType()).alias(\"summarystatsLocation\")\n\n    @property\n    def has_summary_statistics(self) -&gt; Column:\n        \"\"\"Get the has summary statistics column.\n\n        Returns:\n            Column: Spark Column representing whether the study has summary statistics.\n        \"\"\"\n        return f.lit(True).alias(\"hasSumstats\")\n\n    @property\n    def study_id(self) -&gt; Column:\n        \"\"\"Get the study ID column.\n\n        Returns:\n            Column: Spark Column representing the study ID.\n        \"\"\"\n        return f.concat_ws(\n            \"_\",\n            f.lit(self.meta.value),\n            f.col(\"fg_phenotype\"),\n        ).alias(\"studyId\")\n\n    @property\n    def project_id(self) -&gt; Column:\n        \"\"\"Get the project ID column.\n\n        Returns:\n            Column: Spark Column representing the project ID.\n        \"\"\"\n        return f.lit(self.meta.value).alias(\"projectId\")\n\n    @property\n    def trait_from_source(self) -&gt; Column:\n        \"\"\"Get the trait from source column.\n\n        Returns:\n            Column: Spark Column representing the trait from source.\n        \"\"\"\n        return f.col(\"name\").alias(\"traitFromSource\")\n\n    @property\n    def n_cases_per_cohort(self) -&gt; Column:\n        \"\"\"Get the number of cases per cohort column.\n\n        Returns:\n            Column: Spark Column representing the number of cases per cohort.\n        \"\"\"\n        n_cases = [\n            f.struct(\n                f.lit(\"FinnGen\").alias(\"cohort\"),\n                f.coalesce(f.col(\"fg_n_cases\"), f.lit(0)).alias(\"nCases\"),\n            ),\n            f.struct(\n                f.lit(\"UKBB\").alias(\"cohort\"),\n                f.coalesce(f.col(\"ukbb_n_cases\"), f.lit(0)).alias(\"nCases\"),\n            ),\n        ]\n        if self.meta == MetaAnalysisDataSource.FINNGEN_UKBB_MVP:\n            n_cases += [\n                f.struct(\n                    f.lit(\"MVP_EUR\").alias(\"cohort\"),\n                    f.coalesce(f.col(\"MVP_EUR_n_cases\"), f.lit(0)).alias(\"nCases\"),\n                ),\n                f.struct(\n                    f.lit(\"MVP_AFR\").alias(\"cohort\"),\n                    f.coalesce(f.col(\"MVP_AFR_n_cases\"), f.lit(0)).alias(\"nCases\"),\n                ),\n                f.struct(\n                    f.lit(\"MVP_AMR\").alias(\"cohort\"),\n                    f.coalesce(f.col(\"MVP_AMR_n_cases\"), f.lit(0)).alias(\"nCases\"),\n                ),\n            ]\n\n        return f.array(*n_cases).alias(\"nCasesPerCohort\")\n\n    @property\n    def n_samples_per_cohort(self) -&gt; Column:\n        \"\"\"Get the number of samples per cohort column.\n\n        Returns:\n            Column: Spark Column representing the number of samples per cohort.\n        \"\"\"\n        n_samples = [\n            f.struct(\n                f.lit(\"FinnGen\").alias(\"cohort\"),\n                (\n                    f.coalesce(f.col(\"fg_n_cases\"), f.lit(0))\n                    + f.coalesce(f.col(\"fg_n_controls\"), f.lit(0))\n                ).alias(\"nSamples\"),\n            ),\n            f.struct(\n                f.lit(\"UKBB\").alias(\"cohort\"),\n                (\n                    f.coalesce(f.col(\"ukbb_n_cases\"), f.lit(0))\n                    + f.coalesce(f.col(\"ukbb_n_controls\"), f.lit(0))\n                ).alias(\"nSamples\"),\n            ),\n        ]\n        if self.meta == MetaAnalysisDataSource.FINNGEN_UKBB_MVP:\n            n_samples += [\n                f.struct(\n                    f.lit(\"MVP_EUR\").alias(\"cohort\"),\n                    (\n                        f.coalesce(f.col(\"MVP_EUR_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"MVP_EUR_n_controls\"), f.lit(0))\n                    ).alias(\"nSamples\"),\n                ),\n                f.struct(\n                    f.lit(\"MVP_AFR\").alias(\"cohort\"),\n                    (\n                        f.coalesce(f.col(\"MVP_AFR_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"MVP_AFR_n_controls\"), f.lit(0))\n                    ).alias(\"nSamples\"),\n                ),\n                f.struct(\n                    f.lit(\"MVP_AMR\").alias(\"cohort\"),\n                    (\n                        f.coalesce(f.col(\"MVP_AMR_n_cases\"), f.lit(0))\n                        + f.coalesce(f.col(\"MVP_AMR_n_controls\"), f.lit(0))\n                    ).alias(\"nSamples\"),\n                ),\n            ]\n\n        return f.array(*n_samples).alias(\"nSamplesPerCohort\")\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.ancestry_columns","title":"<code>ancestry_columns: set[str]</code>  <code>property</code>","text":"<p>Find all ancestry number columns in the manifest.</p> <p>These columns are used to calculate the total number of samples, cases, and controls.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>set[str]: Set of ancestry number columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the meta-analysis data source is unsupported.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dummy_df = spark.createDataFrame([(1,2,3), (4,5,6)])\n&gt;&gt;&gt; manifest = FinnGenMetaManifest(df=dummy_df, meta=MetaAnalysisDataSource.FINNGEN_UKBB)\n&gt;&gt;&gt; sorted(manifest.ancestry_columns)\n['fg_n_cases', 'fg_n_controls', 'ukbb_n_cases', 'ukbb_n_controls']\n&gt;&gt;&gt; manifest = FinnGenMetaManifest(df=dummy_df, meta=MetaAnalysisDataSource.FINNGEN_UKBB_MVP)\n&gt;&gt;&gt; sorted(manifest.ancestry_columns)\n['MVP_AFR_n_cases', 'MVP_AFR_n_controls', 'MVP_AMR_n_cases', 'MVP_AMR_n_controls', 'MVP_EUR_n_cases', 'MVP_EUR_n_controls', 'fg_n_cases', 'fg_n_controls', 'ukbb_n_cases', 'ukbb_n_controls']\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.df","title":"<code>df: DataFrame</code>  <code>property</code>","text":"<p>Get the manifest DataFrame.</p> <p>The resulting DataFrame has the following schema: <pre><code>|-- studyPhenotype: string (nullable = true)\n|-- traitFromSource: string (nullable = true)\n|-- discoverySamples: array (nullable = true)\n    |-- element: struct (containsNull = true)\n        |-- sampleSize: integer (nullable = true)\n        |-- ancestry: string (nullable = true)\n|-- nSamples: integer (nullable = true)\n|-- nCases: integer (nullable = true)\n|-- nSamplesPerCohort: array (nullable = true)\n    |-- element: struct (containsNull = true)\n        |-- cohort: string (nullable = true)\n        |-- nSamples: integer (nullable = true)\n|-- nCasesPerCohort: array (nullable = true)\n    |-- element: struct (containsNull = true)\n        |-- cohort: string (nullable = true)\n        |-- nCases: integer (nullable = true)\n|-- nControls: integer (nullable = true)\n|-- hasSumstats: boolean (nullable = true)\n|-- summarystatsLocation: string (nullable = true)  # may be null if not provided in the manifest\n</code></pre></p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.discovery_samples","title":"<code>discovery_samples: Column</code>  <code>property</code>","text":"<p>Get the discovery samples.</p> <p>This method dispatches to the appropriate private method based on the meta-analysis data source.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Spark Column representing the discovery samples.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.has_summary_statistics","title":"<code>has_summary_statistics: Column</code>  <code>property</code>","text":"<p>Get the has summary statistics column.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Spark Column representing whether the study has summary statistics.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.n_cases","title":"<code>n_cases: Column</code>  <code>property</code>","text":"<p>Get the total number of cases.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.n_cases_per_cohort","title":"<code>n_cases_per_cohort: Column</code>  <code>property</code>","text":"<p>Get the number of cases per cohort column.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Spark Column representing the number of cases per cohort.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.n_controls","title":"<code>n_controls: Column</code>  <code>property</code>","text":"<p>Get the total number of cases.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.n_samples","title":"<code>n_samples: Column</code>  <code>property</code>","text":"<p>Get the total number of samples.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.n_samples_per_cohort","title":"<code>n_samples_per_cohort: Column</code>  <code>property</code>","text":"<p>Get the number of samples per cohort column.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Spark Column representing the number of samples per cohort.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.project_id","title":"<code>project_id: Column</code>  <code>property</code>","text":"<p>Get the project ID column.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Spark Column representing the project ID.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.study_id","title":"<code>study_id: Column</code>  <code>property</code>","text":"<p>Get the study ID column.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Spark Column representing the study ID.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.summary_statistics_location","title":"<code>summary_statistics_location: Column</code>  <code>property</code>","text":"<p>Get the summary statistics location column.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Spark Column representing the summary statistics location.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.trait_from_source","title":"<code>trait_from_source: Column</code>  <code>property</code>","text":"<p>Get the trait from source column.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Spark Column representing the trait from source.</p>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.__init__","title":"<code>__init__(df: DataFrame, meta: MetaAnalysisDataSource) -&gt; None</code>","text":"<p>Initialize the FinnGen meta-analysis manifest.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the manifest data.</p> required <code>meta</code> <code>MetaAnalysisDataSource</code> <p>Meta-analysis data source enum.</p> required Source code in <code>src/gentropy/datasource/finngen_meta/__init__.py</code> <pre><code>def __init__(self, df: DataFrame, meta: MetaAnalysisDataSource) -&gt; None:\n    \"\"\"Initialize the FinnGen meta-analysis manifest.\n\n    Args:\n        df (DataFrame): DataFrame containing the manifest data.\n        meta (MetaAnalysisDataSource): Meta-analysis data source enum.\n    \"\"\"\n    self.meta = meta\n    self._df = df\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.FinnGenMetaManifest.from_path","title":"<code>from_path(session: Session, manifest_path: str) -&gt; FinnGenMetaManifest</code>  <code>classmethod</code>","text":"<p>Load the FinnGen meta-analysis manifest from a specified path.</p> Note <p>This method asserts that the manifest file is tab-delimited and contains header with following columns: <pre><code>|-- fg_phenotype: string (nullable = true)        # required\n|-- name: string (nullable = true)                # required\n|-- fg_n_cases: integer (nullable = true)         # required\n|-- fg_n_controls: integer (nullable = true)      # required\n|-- ukbb_n_cases: integer (nullable = true)       # required\n|-- ukbb_n_controls: integer (nullable = true)    # required\n|-- MVP_AFR_n_cases: integer (nullable = true)    # optional\n|-- MVP_AFR_n_controls: integer (nullable = true) # optional\n|-- MVP_EUR_n_cases: integer (nullable = true)    # optional\n|-- MVP_EUR_n_controls: integer (nullable = true) # optional\n|-- MVP_AMR_n_cases: integer (nullable = true)    # optional\n|-- MVP_AMR_n_controls: integer (nullable = true) # optional\n|-- path_bucket: string (nullable = true)         # optional\n</code></pre></p> <p>Args:     session (Session): Session object.     manifest_path (str): Path to the manifest file.</p> <p>Returns:</p> Name Type Description <code>FinngenMetaManifest</code> <code>FinnGenMetaManifest</code> <p>Loaded manifest object.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the manifest file does not contain the required columns.</p> <code>AssertionError</code> <p>If the manifest file does not contain the required columns.</p> Source code in <code>src/gentropy/datasource/finngen_meta/__init__.py</code> <pre><code>@classmethod\ndef from_path(cls, session: Session, manifest_path: str) -&gt; FinnGenMetaManifest:\n    \"\"\"Load the FinnGen meta-analysis manifest from a specified path.\n\n    Note:\n        This method asserts that the manifest file is tab-delimited and contains header with following columns:\n        ```\n        |-- fg_phenotype: string (nullable = true)        # required\n        |-- name: string (nullable = true)                # required\n        |-- fg_n_cases: integer (nullable = true)         # required\n        |-- fg_n_controls: integer (nullable = true)      # required\n        |-- ukbb_n_cases: integer (nullable = true)       # required\n        |-- ukbb_n_controls: integer (nullable = true)    # required\n        |-- MVP_AFR_n_cases: integer (nullable = true)    # optional\n        |-- MVP_AFR_n_controls: integer (nullable = true) # optional\n        |-- MVP_EUR_n_cases: integer (nullable = true)    # optional\n        |-- MVP_EUR_n_controls: integer (nullable = true) # optional\n        |-- MVP_AMR_n_cases: integer (nullable = true)    # optional\n        |-- MVP_AMR_n_controls: integer (nullable = true) # optional\n        |-- path_bucket: string (nullable = true)         # optional\n        ```\n    Args:\n        session (Session): Session object.\n        manifest_path (str): Path to the manifest file.\n\n    Returns:\n        FinngenMetaManifest: Loaded manifest object.\n\n    Raises:\n        AssertionError: If the manifest file does not contain the required columns.\n        AssertionError: If the manifest file does not contain the required columns.\n    \"\"\"\n    df = (\n        session.spark.read.option(\"header\", True)\n        .option(\"sep\", \"\\t\")\n        .csv(manifest_path)\n    )\n    assert cls.required_columns.issubset(set(df.columns)), (\n        f\"Manifest file must contain the following columns: {cls.required_columns}. \"\n    )\n\n    # By default we assume we are dealing with the FinnGen UKBB meta-analysis\n    meta = MetaAnalysisDataSource.FINNGEN_UKBB\n    columns = [*cls.required_columns]\n\n    # If we have the MVP ancestry columns, then we are dealing with the FinnGen UKBB MVP meta-analysis\n    if cls.mvp_ancestry_columns.issubset(set(df.columns)):\n        meta = MetaAnalysisDataSource.FINNGEN_UKBB_MVP\n        columns += [*cls.mvp_ancestry_columns]\n\n    column_map = [\n        f.col(col).cast(t.IntegerType()).alias(col)\n        if \"n_cases\" in col or \"n_controls\" in col\n        else f.col(col).cast(t.StringType()).alias(col)\n        for col in columns\n    ]\n\n    # Handle the summary statistics location.\n    if cls.sumstat_location_column in df.columns:\n        column_map.append(\n            f.col(cls.sumstat_location_column)\n            .cast(t.StringType())\n            .alias(cls.sumstat_location_column)\n        )\n    else:\n        session.logger.warning(\n            f\"Manifest file does not contain the '{cls.sumstat_location_column}' column. Can not determine summary statistics location.\"\n        )\n        column_map.append(f.lit(None).alias(cls.sumstat_location_column))\n\n    df = df.select(*column_map)  # Final contract\n    return cls(df=df, meta=meta)\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/finngen_manifest/#gentropy.datasource.finngen_meta.MetaAnalysisDataSource","title":"<code>gentropy.datasource.finngen_meta.MetaAnalysisDataSource</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for meta-analysis data sources.</p> Source code in <code>src/gentropy/datasource/finngen_meta/__init__.py</code> <pre><code>class MetaAnalysisDataSource(str, Enum):\n    \"\"\"Enum for meta-analysis data sources.\"\"\"\n\n    FINNGEN_UKBB_MVP = \"FINNGEN_R12_UKB_MVP_META\"\n    FINNGEN_UKBB = \"FINNGEN_R12_UKB_META\"\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/study_index/","title":"FinnGen Meta Analysis Study Index","text":""},{"location":"python_api/datasources/finngen_meta/study_index/#gentropy.datasource.finngen_meta.study_index.FinnGenMetaStudyIndex","title":"<code>gentropy.datasource.finngen_meta.study_index.FinnGenMetaStudyIndex</code>","text":"<p>FinnGen meta-analysis study index.</p> Source code in <code>src/gentropy/datasource/finngen_meta/study_index.py</code> <pre><code>class FinnGenMetaStudyIndex:\n    \"\"\"FinnGen meta-analysis study index.\"\"\"\n\n    @classmethod\n    def get_constants(cls) -&gt; dict[str, dict[str, Column]]:\n        \"\"\"Get constants for FinnGen meta-analysis study index.\n\n        Returns:\n            dict[str, dict[str, Column]]: Constants for each meta-analysis data source.\n        \"\"\"\n        return {\n            MetaAnalysisDataSource.FINNGEN_UKBB.value: {\n                \"initialSampleSize\": f.lit(\n                    \"920,880 (FinnGenR12: nNFE=500,349; pan-UKBB-EUR: nEUR=420,531)\"\n                ),  # based on https://metaresults-ukbb.finngen.fi/about\n                \"cohorts\": f.array(f.lit(\"FinnGen\"), f.lit(\"pan-UKBB-EUR\")),\n                \"publicationDate\": f.lit(\"2024-11-01\"),\n            },\n            MetaAnalysisDataSource.FINNGEN_UKBB_MVP.value: {\n                \"initialSampleSize\": f.lit(\n                    \"1,550,147 (MVP: nEUR=449,042, nAFR=121,177, nAMR=59,048; FinnGenR12: nNFE=500,349; pan-UKBB-EUR: nEUR=420,531)\"\n                ),  # based on https://mvp-ukbb.finngen.fi/about\n                \"publicationDate\": f.lit(\"2024-11-01\"),\n                \"cohorts\": f.array(\n                    f.lit(\"MVP\"), f.lit(\"FinnGen\"), f.lit(\"pan-UKBB-EUR\")\n                ),\n            },\n        }\n\n    @classmethod\n    def from_finngen_manifest(\n        cls: type[FinnGenMetaStudyIndex],\n        manifest: FinnGenMetaManifest,\n        efo_mapping: EFOMapping,\n    ) -&gt; StudyIndex:\n        \"\"\"Create the FinnGen meta-analysis study index from the manifest.\n\n        Args:\n            manifest (FinnGenMetaManifest): FinnGen meta-analysis manifest.\n            efo_mapping (EFOMapping): EFO mapping data source.\n\n        Returns:\n            StudyIndex: FinnGen meta-analysis study index.\n        \"\"\"\n        # Read the mapping\n        df = manifest.df.select(\n            f.col(\"studyId\"),\n            f.col(\"projectId\"),\n            f.lit(\"gwas\").alias(\"studyType\"),\n            f.col(\"traitFromSource\"),\n            f.col(\"hasSumstats\"),\n            f.col(\"summarystatsLocation\"),\n            f.col(\"discoverySamples\"),\n            f.col(\"nSamples\"),\n            f.col(\"nCases\"),\n            f.col(\"nControls\"),\n            # Add constant columns\n            *[\n                value.alias(key)\n                for key, value in cls.get_constants()[manifest.meta.value].items()\n            ],\n            # Compute the ld structure `ldPopulationStructure` from discovery samples.\n            StudyIndex.aggregate_and_map_ancestries(f.col(\"discoverySamples\")).alias(\n                \"ldPopulationStructure\"\n            ),\n        )\n\n        # Create study index.\n        study_index = StudyIndex(_df=df)\n\n        # Add EFO mappings - `traitFromSourceMappedIds`.\n        study_index = efo_mapping.annotate_study_index(\n            study_index, finngen_release=\"R12\"\n        )\n\n        # Coalesce to a single file.\n        return StudyIndex(\n            _df=study_index.df.coalesce(1),\n            _schema=StudyIndex.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/study_index/#gentropy.datasource.finngen_meta.study_index.FinnGenMetaStudyIndex.from_finngen_manifest","title":"<code>from_finngen_manifest(manifest: FinnGenMetaManifest, efo_mapping: EFOMapping) -&gt; StudyIndex</code>  <code>classmethod</code>","text":"<p>Create the FinnGen meta-analysis study index from the manifest.</p> <p>Parameters:</p> Name Type Description Default <code>manifest</code> <code>FinnGenMetaManifest</code> <p>FinnGen meta-analysis manifest.</p> required <code>efo_mapping</code> <code>EFOMapping</code> <p>EFO mapping data source.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>FinnGen meta-analysis study index.</p> Source code in <code>src/gentropy/datasource/finngen_meta/study_index.py</code> <pre><code>@classmethod\ndef from_finngen_manifest(\n    cls: type[FinnGenMetaStudyIndex],\n    manifest: FinnGenMetaManifest,\n    efo_mapping: EFOMapping,\n) -&gt; StudyIndex:\n    \"\"\"Create the FinnGen meta-analysis study index from the manifest.\n\n    Args:\n        manifest (FinnGenMetaManifest): FinnGen meta-analysis manifest.\n        efo_mapping (EFOMapping): EFO mapping data source.\n\n    Returns:\n        StudyIndex: FinnGen meta-analysis study index.\n    \"\"\"\n    # Read the mapping\n    df = manifest.df.select(\n        f.col(\"studyId\"),\n        f.col(\"projectId\"),\n        f.lit(\"gwas\").alias(\"studyType\"),\n        f.col(\"traitFromSource\"),\n        f.col(\"hasSumstats\"),\n        f.col(\"summarystatsLocation\"),\n        f.col(\"discoverySamples\"),\n        f.col(\"nSamples\"),\n        f.col(\"nCases\"),\n        f.col(\"nControls\"),\n        # Add constant columns\n        *[\n            value.alias(key)\n            for key, value in cls.get_constants()[manifest.meta.value].items()\n        ],\n        # Compute the ld structure `ldPopulationStructure` from discovery samples.\n        StudyIndex.aggregate_and_map_ancestries(f.col(\"discoverySamples\")).alias(\n            \"ldPopulationStructure\"\n        ),\n    )\n\n    # Create study index.\n    study_index = StudyIndex(_df=df)\n\n    # Add EFO mappings - `traitFromSourceMappedIds`.\n    study_index = efo_mapping.annotate_study_index(\n        study_index, finngen_release=\"R12\"\n    )\n\n    # Coalesce to a single file.\n    return StudyIndex(\n        _df=study_index.df.coalesce(1),\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/study_index/#gentropy.datasource.finngen_meta.study_index.FinnGenMetaStudyIndex.get_constants","title":"<code>get_constants() -&gt; dict[str, dict[str, Column]]</code>  <code>classmethod</code>","text":"<p>Get constants for FinnGen meta-analysis study index.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, Column]]</code> <p>dict[str, dict[str, Column]]: Constants for each meta-analysis data source.</p> Source code in <code>src/gentropy/datasource/finngen_meta/study_index.py</code> <pre><code>@classmethod\ndef get_constants(cls) -&gt; dict[str, dict[str, Column]]:\n    \"\"\"Get constants for FinnGen meta-analysis study index.\n\n    Returns:\n        dict[str, dict[str, Column]]: Constants for each meta-analysis data source.\n    \"\"\"\n    return {\n        MetaAnalysisDataSource.FINNGEN_UKBB.value: {\n            \"initialSampleSize\": f.lit(\n                \"920,880 (FinnGenR12: nNFE=500,349; pan-UKBB-EUR: nEUR=420,531)\"\n            ),  # based on https://metaresults-ukbb.finngen.fi/about\n            \"cohorts\": f.array(f.lit(\"FinnGen\"), f.lit(\"pan-UKBB-EUR\")),\n            \"publicationDate\": f.lit(\"2024-11-01\"),\n        },\n        MetaAnalysisDataSource.FINNGEN_UKBB_MVP.value: {\n            \"initialSampleSize\": f.lit(\n                \"1,550,147 (MVP: nEUR=449,042, nAFR=121,177, nAMR=59,048; FinnGenR12: nNFE=500,349; pan-UKBB-EUR: nEUR=420,531)\"\n            ),  # based on https://mvp-ukbb.finngen.fi/about\n            \"publicationDate\": f.lit(\"2024-11-01\"),\n            \"cohorts\": f.array(\n                f.lit(\"MVP\"), f.lit(\"FinnGen\"), f.lit(\"pan-UKBB-EUR\")\n            ),\n        },\n    }\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/","title":"FinnGen Meta Analysis Summary Statistics","text":""},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics","title":"<code>gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics</code>","text":"<p>FinnGen meta summary statistics ingestion and harmonisation.</p> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>class FinnGenUkbMvpMetaSummaryStatistics:\n    \"\"\"FinnGen meta summary statistics ingestion and harmonisation.\"\"\"\n\n    N_THREAD_MAX = 32\n    N_THREAD_OPTIMAL = 10\n\n    @staticmethod\n    def extract_study_phenotype_from_path(file_path: Column) -&gt; Column:\n        \"\"\"Extract the study phenotype from FinnGen file path.\n\n        Note:\n            Assumes the file name format is some_path/to/&lt;studyPhenotype&gt;_meta_out.tsv.gz\n\n\n        Args:\n            file_path (Column): Column containing the file path as a string.\n\n        Returns:\n            Column: Extracted study phenotype as a string.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"/path/to/AB1_meta_out.tsv.gz\",), (\"/another/path/CD2_meta_out.tsv.gz\",)]\n            &gt;&gt;&gt; schema = \"filePath STRING\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df = df.withColumn(\"studyPhenotype\", FinnGenUkbMvpMetaSummaryStatistics.extract_study_phenotype_from_path(f.col(\"filePath\")))\n            &gt;&gt;&gt; df.show(truncate=False)\n            +---------------------------------+--------------+\n            |filePath                         |studyPhenotype|\n            +---------------------------------+--------------+\n            |/path/to/AB1_meta_out.tsv.gz     |AB1           |\n            |/another/path/CD2_meta_out.tsv.gz|CD2           |\n            +---------------------------------+--------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return f.regexp_replace(\n            f.element_at(f.split(file_path, \"/\"), -1), \"_meta_out.tsv.gz\", \"\"\n        )\n\n    # raw_schema represents the order of columns in the original summary statistics files\n    raw_schema = t.StructType(\n        [\n            t.StructField(\"#CHR\", t.StringType(), True),\n            t.StructField(\"POS\", t.LongType(), True),\n            t.StructField(\"REF\", t.StringType(), True),\n            t.StructField(\"ALT\", t.StringType(), True),\n            t.StructField(\"SNP\", t.StringType(), True),\n            # FinnGen\n            t.StructField(\"fg_beta\", t.DoubleType(), True),\n            t.StructField(\"fg_sebeta\", t.DoubleType(), True),\n            t.StructField(\"fg_pval\", t.DoubleType(), True),\n            t.StructField(\"fg_af_alt\", t.DoubleType(), True),\n            t.StructField(\"fg_af_alt_cases\", t.DoubleType(), True),\n            t.StructField(\"fg_af_alt_controls\", t.DoubleType(), True),\n            # MVP_EUR\n            t.StructField(\"MVP_EUR_beta\", t.DoubleType(), True),\n            t.StructField(\"MVP_EUR_sebeta\", t.DoubleType(), True),\n            t.StructField(\"MVP_EUR_pval\", t.DoubleType(), True),\n            t.StructField(\"MVP_EUR_af_alt\", t.DoubleType(), True),\n            t.StructField(\"MVP_EUR_r2\", t.DoubleType(), True),\n            # MVP_AFR\n            t.StructField(\"MVP_AFR_beta\", t.DoubleType(), True),\n            t.StructField(\"MVP_AFR_sebeta\", t.DoubleType(), True),\n            t.StructField(\"MVP_AFR_pval\", t.DoubleType(), True),\n            t.StructField(\"MVP_AFR_af_alt\", t.DoubleType(), True),\n            t.StructField(\"MVP_AFR_r2\", t.DoubleType(), True),\n            # MVP_HIS\n            t.StructField(\"MVP_HIS_beta\", t.DoubleType(), True),\n            t.StructField(\"MVP_HIS_sebeta\", t.DoubleType(), True),\n            t.StructField(\"MVP_HIS_pval\", t.DoubleType(), True),\n            t.StructField(\"MVP_HIS_af_alt\", t.DoubleType(), True),\n            t.StructField(\"MVP_HIS_r2\", t.DoubleType(), True),\n            # UKBB\n            t.StructField(\"ukbb_beta\", t.DoubleType(), True),\n            t.StructField(\"ukbb_sebeta\", t.DoubleType(), True),\n            t.StructField(\"ukbb_pval\", t.DoubleType(), True),\n            t.StructField(\"ukbb_af_alt\", t.DoubleType(), True),\n            # Meta\n            t.StructField(\"all_meta_N\", t.IntegerType(), True),\n            t.StructField(\"all_inv_var_meta_beta\", t.DoubleType(), True),\n            t.StructField(\"all_inv_var_meta_sebeta\", t.DoubleType(), True),\n            t.StructField(\"all_inv_var_meta_p\", t.DoubleType(), True),\n            t.StructField(\"all_inv_var_meta_mlogp\", t.DoubleType(), True),\n            t.StructField(\"all_inv_var_het_p\", t.DoubleType(), True),\n            # Leave-one-out: FinnGen\n            t.StructField(\"leave_fg_N\", t.IntegerType(), True),\n            t.StructField(\"leave_fg_inv_var_meta_beta\", t.DoubleType(), True),\n            t.StructField(\"leave_fg_inv_var_meta_sebeta\", t.DoubleType(), True),\n            t.StructField(\"leave_fg_inv_var_meta_p\", t.DoubleType(), True),\n            t.StructField(\"leave_fg_inv_var_meta_mlogp\", t.DoubleType(), True),\n            t.StructField(\"leave_fg_inv_var_meta_het_p\", t.DoubleType(), True),\n            # Leave-one-out: MVP_EUR\n            t.StructField(\"leave_MVP_EUR_N\", t.IntegerType(), True),\n            t.StructField(\"leave_MVP_EUR_inv_var_meta_beta\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_EUR_inv_var_meta_sebeta\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_EUR_inv_var_meta_p\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_EUR_inv_var_meta_mlogp\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_EUR_inv_var_meta_het_p\", t.DoubleType(), True),\n            # Leave-one-out: MVP_AFR\n            t.StructField(\"leave_MVP_AFR_N\", t.IntegerType(), True),\n            t.StructField(\"leave_MVP_AFR_inv_var_meta_beta\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_AFR_inv_var_meta_sebeta\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_AFR_inv_var_meta_p\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_AFR_inv_var_meta_mlogp\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_AFR_inv_var_meta_het_p\", t.DoubleType(), True),\n            # Leave-one-out: MVP_HIS\n            t.StructField(\"leave_MVP_HIS_N\", t.IntegerType(), True),\n            t.StructField(\"leave_MVP_HIS_inv_var_meta_beta\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_HIS_inv_var_meta_sebeta\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_HIS_inv_var_meta_p\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_HIS_inv_var_meta_mlogp\", t.DoubleType(), True),\n            t.StructField(\"leave_MVP_HIS_inv_var_meta_het_p\", t.DoubleType(), True),\n            # Leave-one-out: UKBB\n            t.StructField(\"leave_ukbb_N\", t.IntegerType(), True),\n            t.StructField(\"leave_ukbb_inv_var_meta_beta\", t.DoubleType(), True),\n            t.StructField(\"leave_ukbb_inv_var_meta_sebeta\", t.DoubleType(), True),\n            t.StructField(\"leave_ukbb_inv_var_meta_p\", t.DoubleType(), True),\n            t.StructField(\"leave_ukbb_inv_var_meta_mlogp\", t.DoubleType(), True),\n            t.StructField(\"leave_ukbb_inv_var_meta_het_p\", t.DoubleType(), True),\n            # RSID\n            t.StructField(\"rsid\", t.StringType(), True),\n        ]\n    )\n\n    @classmethod\n    def bgzip_to_parquet(\n        cls,\n        session: Session,\n        summary_statistics_list: list[str],\n        datasource: MetaAnalysisDataSource,\n        raw_summary_statistics_output_path: str,\n        n_threads: int = 10,\n    ) -&gt; None:\n        \"\"\"Convert gzipped summary statistics to Parquet format.\n\n        This is a pre-step that needs to be performed once to convert the block gzipped to parquet format. This step\n        should be run before the actual harmonisation step is performed and the output Parquet files can be used as input\n        for the `from_source` method.\n\n        Args:\n            session (Session): Session object.\n            summary_statistics_list (list[str]): List of paths where summary statistics files are located.\n            datasource (MetaAnalysisDataSource): Data source, can be FinnGenMetaDataSource.FINNGEN_UKBB_MVP.\n            raw_summary_statistics_output_path (str): Output path for the Parquet files.\n            n_threads (int): Maximum number of threads to use for ThreadPoolExecutor (default is 10).\n\n        The output requires a single path that will be populated with Parquet files partitioned by `studyId` extracted\n        from the input file names.\n\n        !!! note \"Block gzipped input files\"\n\n            Since the individual summary statistics files are **block gzipped** we use the enhanced bgzip codec for efficient reading.\n\n        !!! note \"Reading multiple files with divergent schemas\"\n\n            Since the schema for individual summary statistics **is not strictly the same we have to enforce the schema**.\n\n            **_enforcing schema_**\n            using the `enforceSchema` option in `spark.read.csv` **does not map columns that exist in the file provided schema**,\n            but rather aligns columns positionally, which breaks the column order per individual file.\n\n            **_inferring schema_**\n            Attempting to use the `inferSchema` option in `spark.read.csv` while reading multiple files in bulk drops columns, due\n            to the random sampling of files to infer the schema. (Files chosen to infer the schema may not contain entire superset of column space.)\n\n            **_manual schema enforcement_**\n            The only way to keep the columns in order and use full column superset is to loop over the files with `inferSchema` and manually\n            add missing columns with null values casted to expected type. The looping can be parallelized using a **thread pool** (ThreadPoolExecutor)\n            with `n_threads` as the maximum load of jobs to spark cluster.\n\n        ??? warning \"Performance considerations\"\n            This function requires a _Session_ with `use_enhanced_bgzip_codec` to be set to True. This function is strongly encouraged to be used in\n            a distributed environment.\n\n        Raises:\n            KeyError: If `use_enhanced_bgzip_codec` is set to False in the Session configuration.\n        \"\"\"\n        if len(summary_statistics_list) == 0:\n            session.logger.warning(\"No summary statistics paths found to process.\")\n            return\n        if not session.use_enhanced_bgzip_codec:\n            session.logger.error(\n                \"The use_enhanced_bgzip_codec is set to False. This will lead to inefficient reading of block gzipped files.\"\n            )\n            raise KeyError(\n                \"Please set `session.spark.use_enhanced_bgzip_codec` to True in the Session configuration.\"\n            )\n\n        # Handle n_threads limits and warnings\n        if not isinstance(n_threads, int) or n_threads &lt; 1:\n            session.logger.warning(\n                f\"Invalid n_threads value: {n_threads}. Falling back to 10 threads.\"\n            )\n            n_threads = FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_OPTIMAL\n        if n_threads &lt; FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_OPTIMAL:\n            session.logger.warning(\n                f\"Using low n_threads value: {n_threads}. This may lead to sub-optimal performance.\"\n            )\n        if n_threads &gt; FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_MAX:\n            session.logger.warning(\n                f\"Using high n_threads value: {n_threads}, this may lead to overloading spark driver. Limiting to 32.\"\n            )\n            n_threads = FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_MAX\n\n        def process_one(\n            input_path: str, session: Session, output_path: str\n        ) -&gt; DataFrame:\n            \"\"\"Function to process one finngen-ukbb-mvp summary statistics file to schema superset.\n\n            Args:\n                input_path (str): Input path to the gzipped summary statistics file.\n                session (Session): Session object.\n                output_path (str): Output path for the Parquet files.\n\n            Returns:\n                DataFrame: Processed dataframe.\n            \"\"\"\n            df = session.spark.read.csv(\n                input_path,\n                header=True,\n                inferSchema=True,\n                sep=\"\\t\",\n                enforceSchema=False,\n            )\n            # Apply schema enforcement by selecting columns in the expected order with proper types\n            # NOTE: Here we only add missing columns with null values casted to expected type\n            for c in cls.raw_schema.names:\n                if c not in df.columns:\n                    df = df.withColumn(c, f.lit(None).cast(cls.raw_schema[c].dataType))\n\n            # Replace all NA with nulls and cast to the expected type\n            # NOTE: Here we apply the transformation on full schema set (including added missing columns)\n            # NOTE: If we do not transform the `NA` to nulls, we will still have divergent schemas, as `NA` forces the column to StringType\n            for field in cls.raw_schema.fields:\n                df = df.withColumn(\n                    field.name,\n                    f.when(f.col(field.name) == \"NA\", f.lit(None))\n                    .otherwise(f.col(field.name))\n                    .cast(field.dataType),\n                )\n            # Add studyId based on the input path\n            df = (\n                df.withColumn(\n                    \"studyId\",\n                    f.concat_ws(\n                        \"_\",\n                        f.lit(datasource.value),\n                        f.lit(\n                            cls.extract_study_phenotype_from_path(f.input_file_name())\n                        ),\n                    ),\n                    # Optimal partition size is ~ 100MB, assuming the total size of the dataset is 2Tb\n                    # we can have up to 60 partitions per study (330 studies)\n                )\n                .orderBy(\"studyId\", \"#CHR\", \"POS\")\n                .repartition(60, \"#CHR\", \"POS\")\n            )\n            # Write out the processed dataframe to Parquet\n            # NOTE: Write is done per studyId partition from the thread pool to\n            # make sure we do not need to collect all data after the thread execution.\n            df.write.mode(\"append\").partitionBy(\"studyId\").parquet(output_path)\n            return df\n\n        session.logger.info(\n            f\"Converting gzipped summary statistics from {summary_statistics_list} to Parquet at {raw_summary_statistics_output_path}.\"\n        )\n        with ThreadPoolExecutor(max_workers=n_threads) as pool:\n            pool.map(\n                lambda path: process_one(\n                    path,\n                    session=session,\n                    output_path=raw_summary_statistics_output_path,\n                ),\n                summary_statistics_list,\n            )\n\n    @classmethod\n    def from_source(\n        cls,\n        raw_summary_statistics: DataFrame,\n        finngen_manifest: FinnGenMetaManifest,\n        variant_annotations: VariantDirection,\n        perform_meta_analysis_filter: bool = True,\n        imputation_score_threshold: float = 0.8,\n        perform_imputation_score_filter: bool = True,\n        min_allele_count_threshold: int = 20,\n        perform_min_allele_count_filter: bool = True,\n        min_allele_frequency_threshold: float = 1e-4,\n        perform_min_allele_frequency_filter: bool = False,\n        filter_out_ambiguous_variants: bool = False,\n    ) -&gt; SummaryStatistics:\n        \"\"\"Build the summary statistics dataset from raw summary statistics.\n\n        See original issue to find out more details on the harmonisation logic https://github.com/opentargets/issues/issues/3474\n\n        ??? note \"The logic behind the harmonisation\"\n            1. Build a slice of FinnGen Manifest to bring the information about nCases and nSamples per cohort (broadcast join).\n            2. Build a variant direction (gnomAD) dataset partitioned by `chromosome` and `variantId` for joining using Sort-Merge strategy.\n            3. Select required columns from raw summary statistics\n            4. Remove all non-meta analyzed variants (nBiobanks &lt; 2) - by default\n            5. Remove all variants with low imputation score - by default\n            6. Join with Finngen Manifest and Variant Direction datasets\n            7. Flip `beta` and `min allele frequency` based on the `direction` from Variant Direction dataset\n            8. Use originalVariantId from Variant Direction dataset if available, otherwise fall back to variantId (for variants missing from Variant Direction)\n            9. Calculate combined effect allele frequency from cohorts\n            10. Remove all variants with low Min Allele Count - by default\n            11. Remove all variants with low Min Allele Frequency - optional\n            12. Remove strand ambiguous variants - optional\n\n        ??? tip \"Variant Directionality\"\n            **Variant Direction**\n            By default we:\n\n            1. keep all strand ambiguous variants as is\n            2. keep all variants found in gnomAD aligned to gnomAD reference ( if alleles are flipped we flip the beta and allele frequency)\n            3. keep all variants not found in gnomAD as is (cannot determine strand or alignment) - we assume these are correct.\n\n        ??? note \"Important considerations\"\n            * The input summary statistics are expected to be already parquet formatted and partitioned by `studyId`.\n            * Both `perform_allele_count_filter` and `perform_allele_frequency_filter` are redundant, if both\n            are set to True, only `perform_allele_count_filter` will be applied.\n            * Threshold value for Min Allele Count &gt;= 20 means that with a MAF of 1e-4 we would expect to see\n            at least 1_000 samples with minor allele in a cohort. This is quiet stringent for very rare variants and\n            smaller cohorts, like MVP_AFR (nSamples ~120k) and MVP_HIS (~60k).\n            * MVP_HIS cohort has been mapped to admixed American population - see https://www.science.org/doi/10.1126/science.adj1182 for more details.\n\n        Args:\n            raw_summary_statistics (DataFrame): Raw summary statistics dataframe.\n            finngen_manifest (FinnGenMetaManifest): FinnGen meta analysis manifest.\n            variant_annotations (VariantDirection): Variant direction dataset.\n            perform_meta_analysis_filter (bool): Whether to remove variants found in just 1 biobank. Default is True.\n            imputation_score_threshold (float): Imputation score threshold for MVP cohorts. Default is 0.8.\n            perform_imputation_score_filter (bool): Whether to perform the imputation score filter. Default is True.\n            min_allele_count_threshold (int): Minimum allele count threshold. Default is 20.\n            perform_min_allele_count_filter (bool): Whether to perform the minimum allele count filter. Default is False.\n            min_allele_frequency_threshold (float): Minimum allele frequency threshold. Default is 1e-4.\n            perform_min_allele_frequency_filter (bool): Whether to perform the minimum allele frequency filter. Default is True.\n            filter_out_ambiguous_variants (bool): Whether to filter out strand ambiguous variants. Default is False.\n\n        Returns:\n            SummaryStatistics: Processed summary statistics dataset.\n        \"\"\"\n        if perform_min_allele_count_filter:\n            assert (\n                min_allele_count_threshold &gt; 0\n            ), \"Allele count threshold should be positive.\"\n        if perform_min_allele_frequency_filter:\n            assert (\n                0.0 &lt;= min_allele_frequency_threshold &lt;= 0.5\n            ), \"MAF needs to be between 0 and 0.5.\"\n\n        if perform_min_allele_count_filter and perform_min_allele_frequency_filter:\n            # NOTE - MAC filter would be more stringent at low allele frequencies, so no\n            # need to have both filters active at the same time\n            perform_min_allele_frequency_filter = False\n\n        si_slice = f.broadcast(\n            finngen_manifest.df.select(\n                f.col(\"studyId\"),\n                f.col(\"nCasesPerCohort\"),\n                f.col(\"nSamples\"),\n                f.col(\"nSamplesPerCohort\"),\n            ).persist()\n        )\n        vd_slice = (\n            variant_annotations.df.select(\n                f.col(\"chromosome\"),\n                f.col(\"originalVariantId\"),\n                f.col(\"variantId\"),\n                f.col(\"direction\"),\n                f.col(\"isStrandAmbiguous\"),\n            )\n            # NOTE: repartition(\"chromosome\") produces very uneven partitions,\n            # Spark attempts then to fall back to `dynamic partitioning` algorithm\n            # which fails after N failures.\n            .repartitionByRange(4_000, \"chromosome\", \"variantId\")\n            .persist()\n        )\n\n        sumstats = (\n            # Pre-select columns that are needed downstream\n            # NOTE: full set of columns is not required.\n            raw_summary_statistics.select(\n                f.col(\"#CHR\"),\n                f.col(\"POS\"),\n                f.col(\"REF\"),\n                f.col(\"ALT\"),\n                f.col(\"fg_af_alt\"),\n                f.col(\"MVP_EUR_r2\"),\n                f.col(\"MVP_EUR_af_alt\"),\n                f.col(\"MVP_AFR_r2\"),\n                f.col(\"MVP_AFR_af_alt\"),\n                f.col(\"MVP_HIS_r2\"),\n                f.col(\"MVP_HIS_af_alt\"),\n                f.col(\"ukbb_af_alt\"),\n                f.col(\"all_inv_var_meta_mlogp\"),\n                f.col(\"all_inv_var_meta_beta\"),\n                f.col(\"all_inv_var_meta_sebeta\"),\n                f.col(\"studyId\"),\n            )\n            # NOTE: make sure the chromosome is coded to 1:22, X, Y\n            .withColumn(\n                \"chromosome\",\n                normalize_chromosome(f.col(\"#CHR\").cast(t.StringType())),\n            )\n            .drop(\"#CHR\")\n            .withColumn(\"position\", f.col(\"POS\").cast(t.IntegerType()))\n            .withColumnRenamed(\"REF\", \"referenceAllele\")\n            .withColumnRenamed(\"ALT\", \"alternateAllele\")\n            .withColumn(\n                \"neglogpval\", f.col(\"all_inv_var_meta_mlogp\").cast(t.DoubleType())\n            )\n            .withColumn(\"beta\", f.col(\"all_inv_var_meta_beta\").cast(t.DoubleType()))\n            .withColumn(\n                \"standardError\", f.col(\"all_inv_var_meta_sebeta\").cast(t.DoubleType())\n            )\n            .withColumn(\n                \"variantId\",\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"chromosome\"),\n                    f.col(\"position\"),\n                    f.col(\"referenceAllele\"),\n                    f.col(\"alternateAllele\"),\n                ).alias(\"variantId\"),\n            )\n            .drop(\"referenceAllele\", \"alternateAllele\")\n            # Initial filters based on statistics presence\n            .filter(f.col(\"neglogpval\").isNotNull())\n            .filter(f.col(\"beta\").isNotNull())\n            .filter(f.col(\"standardError\").isNotNull())\n        )\n\n        # Filter out variants that are not meta analyzed (nBiobanks &lt; 1)\n        if perform_meta_analysis_filter:\n            sumstats = (\n                sumstats.withColumn(\"cohorts\", cls.cohorts())\n                .withColumn(\n                    \"isMetaAnalyzedVariant\",\n                    cls.is_meta_analyzed_variant(f.col(\"cohorts\")),\n                )\n                .filter(f.col(\"isMetaAnalyzedVariant\"))\n                .drop(\"isMetaAnalyzedVariant\", \"cohorts\")\n            )\n\n        # Filter out variants with low INFO score\n        if perform_imputation_score_filter:\n            assert (\n                imputation_score_threshold &gt;= 0.0\n            ), \"Imputation score threshold should be positive.\"\n            sumstats = (\n                sumstats.withColumn(\n                    \"hasLowImputationScore\",\n                    cls.has_low_imputation_score(imputation_score_threshold),\n                )\n                .filter(~f.col(\"hasLowImputationScore\"))\n                .drop(\"hasLowImputationScore\", \"imputationScore\")\n            )\n\n        sumstats = (\n            # Annotate with StudyIndex nCases, nSamples to obtain\n            # the cases Minor Allele Count and Samples for combined AF calculation\n            sumstats.join(si_slice, on=\"studyId\", how=\"left\")\n            # Join with variant direction dataset\n            # Keep variants if not found in gnomAD (left join)\n            .repartitionByRange(4_000, \"chromosome\", \"variantId\")\n            .join(vd_slice, on=[\"chromosome\", \"variantId\"], how=\"left\")\n            # Use originalVariantId (already flipped) or fall back to variantId if not found in gnomAD\n            .withColumn(\n                \"variantId\", f.coalesce(f.col(\"originalVariantId\"), f.col(\"variantId\"))\n            )\n            # Compute allele frequency per cohort and align with direction\n            # NOTE: `direction` column represents if variant aligned to gnomAD variant or it's flipped\n            # version, the values are `1` - direct, `-1` - flipped\n            .withColumn(\n                \"cohortAlleleFrequency\", cls.allele_frequencies(f.col(\"direction\"))\n            )\n            # Make sure the beta is alined with the direction, if direction is null, keep beta as is\n            .withColumn(\n                \"beta\", f.col(\"beta\") * f.coalesce(f.col(\"direction\"), f.lit(1))\n            )\n            # Calculate the combined effect allele frequency from cohorts\n            .withColumn(\n                \"effectAlleleFrequencyFromSource\",\n                cls.combined_allele_frequency(\n                    f.col(\"cohortAlleleFrequency\"), f.col(\"nSamplesPerCohort\")\n                ),\n            )\n        )\n        # Remove strand ambiguous variants, if not found in gnomAD, we keep the variant\n        # Not run by default\n        if filter_out_ambiguous_variants:\n            sumstats = sumstats.filter(\n                ~f.coalesce(f.col(\"isStrandAmbiguous\"), f.lit(False))\n            )\n        if perform_min_allele_count_filter or perform_min_allele_frequency_filter:\n            # Calculate the MAF per cohort\n            sumstats = sumstats.withColumn(\n                \"cohortMinAlleleFrequency\",\n                cls.min_allele_frequency(f.col(\"cohortAlleleFrequency\")),\n            )\n        if perform_min_allele_count_filter:\n            sumstats = (\n                sumstats\n                # Make sure to only keep cohorts that have nSamples &gt; 0\n                .withColumn(\n                    \"nSamplesPerCohort\",\n                    f.filter(\n                        f.col(\"nSamplesPerCohort\"),\n                        lambda x: x.getField(\"nSamples\").isNotNull()\n                        &amp; (x.getField(\"nSamples\") &gt; 0),\n                    ),\n                )\n                .withColumn(\n                    \"cohortMinAlleleCount\",\n                    cls.min_allele_count(\n                        f.col(\"cohortMinAlleleFrequency\"),\n                        f.col(\"nSamplesPerCohort\"),\n                    ),\n                )\n                .withColumn(\n                    \"hasLowMinAlleleCount\",\n                    cls.has_low_min_allele_count(\n                        f.col(\"cohortMinAlleleCount\"),\n                        min_allele_count_threshold,\n                    ),\n                )\n                .filter(~f.col(\"hasLowMinAlleleCount\"))\n                .drop(\n                    \"hasLowMinAlleleCount\",\n                    \"cohortMinAlleleCount\",\n                    \"nCasesPerCohort\",\n                )\n            )\n        # Not run by default.\n        if perform_min_allele_frequency_filter:\n            sumstats = (\n                sumstats.withColumn(\n                    \"hasLowMinAlleleFrequency\",\n                    cls.has_low_min_allele_frequency(\n                        f.col(\"cohortMinAlleleFrequency\"),\n                        min_allele_frequency_threshold,\n                    ),\n                )\n                .filter(~f.col(\"hasLowMinAlleleFrequency\"))\n                .drop(\"hasLowMinAlleleFrequency\")\n            )\n        # Convert to final summary statistics schema\n        sumstats = sumstats.select(\n            f.col(\"studyId\"),\n            f.col(\"variantId\"),\n            f.col(\"chromosome\"),\n            f.col(\"position\"),\n            f.col(\"beta\"),\n            f.col(\"nSamples\").alias(\"sampleSize\"),\n            *pvalue_from_neglogpval(f.col(\"neglogpval\")),\n            f.col(\"effectAlleleFrequencyFromSource\"),\n            f.col(\"standardError\"),\n        ).select(\n            f.col(\"studyId\"),\n            f.col(\"variantId\"),\n            f.col(\"chromosome\"),\n            f.col(\"position\"),\n            f.col(\"beta\"),\n            f.col(\"sampleSize\"),\n            f.col(\"pValueMantissa\"),\n            f.col(\"pValueExponent\"),\n            f.col(\"effectAlleleFrequencyFromSource\"),\n            f.col(\"standardError\"),\n        )\n\n        return SummaryStatistics(sumstats).sanity_filter()\n\n    @classmethod\n    def has_low_min_allele_frequency(\n        cls, maf: Column, threshold: float = 1e-4\n    ) -&gt; Column:\n        \"\"\"Find if variant has a low minor allele frequency in any cohort.\n\n        Args:\n            maf (Column): Column containing array of structs with `cohort` and `minAlleleFrequency` fields.\n            threshold (float): Threshold below which the minor allele frequency is considered low. Default is 1e-4.\n\n        Returns:\n            Column: Boolean column indicating if any cohort has a low minor allele frequency.\n\n        Examples:\n            &gt;&gt;&gt; maf = {\"v1\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.0001}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.0002}],\n            ...         \"v2\": [{\"cohort\": \"A\", \"minAlleleFrequency\": None}, {\"cohort\": \"D\", \"minAlleleFrequency\": 0.15}],\n            ...         \"v3\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.00001}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.2}],}\n            &gt;&gt;&gt; data = [(\"v1\", maf[\"v1\"]),\n            ...         (\"v2\", maf[\"v2\"]),\n            ...         (\"v3\", maf[\"v3\"]),]\n            &gt;&gt;&gt; schema = \"variantId STRING, cohortMinAlleleFrequency ARRAY&lt;STRUCT&lt;cohort: STRING, minAlleleFrequency: DOUBLE&gt;&gt;\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.show(truncate=False)\n            +---------+--------------------------+\n            |variantId|cohortMinAlleleFrequency  |\n            +---------+--------------------------+\n            |v1       |[{A, 1.0E-4}, {B, 2.0E-4}]|\n            |v2       |[{A, NULL}, {D, 0.15}]    |\n            |v3       |[{A, 1.0E-5}, {B, 0.2}]   |\n            +---------+--------------------------+\n            &lt;BLANKLINE&gt;\n\n            &gt;&gt;&gt; df = df.withColumn(\"hasMinAlleleFrequency\", FinnGenUkbMvpMetaSummaryStatistics.has_low_min_allele_frequency(f.col(\"cohortMinAlleleFrequency\")))\n            &gt;&gt;&gt; df.select(\"variantId\", \"hasMinAlleleFrequency\").show(truncate=False)\n            +---------+---------------------+\n            |variantId|hasMinAlleleFrequency|\n            +---------+---------------------+\n            |v1       |false                |\n            |v2       |false                |\n            |v3       |true                 |\n            +---------+---------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        non_empty_maf = f.filter(\n            maf, lambda x: x.getField(\"minAlleleFrequency\").isNotNull()\n        )\n\n        n_cohorts_with_maf_below_threshold = f.size(\n            f.filter(\n                non_empty_maf,\n                lambda x: x.getField(\"minAlleleFrequency\")\n                &lt; f.lit(threshold).cast(t.DecimalType(11, 10)),\n            )\n        )\n        return n_cohorts_with_maf_below_threshold &gt; 0\n\n    @classmethod\n    def normalize_af(cls, af: Column, flip: Column) -&gt; Column:\n        \"\"\"Normalize allele frequency based on variant direction.\n\n        Args:\n            af (Column): Allele frequency column.\n            flip (Column): Direction column indicating if the allele needs to be flipped.\n\n        Returns:\n            Column: Normalized allele frequency.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"v1\", 0.1, 1),\n            ...       (\"v2\", 0.2, -1),\n            ...       (\"v3\", None, -1),\n            ...       (\"V4\", 0.1, None),]\n            &gt;&gt;&gt; schema = \"variantId STRING, af DOUBLE, flip INT\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.show(truncate=False)\n            +---------+----+----+\n            |variantId|af  |flip|\n            +---------+----+----+\n            |v1       |0.1 |1   |\n            |v2       |0.2 |-1  |\n            |v3       |NULL|-1  |\n            |V4       |0.1 |NULL|\n            +---------+----+----+\n            &lt;BLANKLINE&gt;\n\n            &gt;&gt;&gt; df = df.withColumn(\"normalizedAf\", FinnGenUkbMvpMetaSummaryStatistics.normalize_af(f.col(\"af\"), f.col(\"flip\")))\n            &gt;&gt;&gt; df.select(\"variantId\", \"normalizedAf\").show(truncate=False)\n            +---------+------------+\n            |variantId|normalizedAf|\n            +---------+------------+\n            |v1       |0.1         |\n            |v2       |0.8         |\n            |v3       |NULL        |\n            |V4       |0.1         |\n            +---------+------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.when((flip == -1) &amp; af.isNotNull(), f.lit(1.0) - af).otherwise(af)\n\n    @classmethod\n    def allele_frequencies(cls, flip: Column, scale: int = 10) -&gt; Column:\n        \"\"\"Extract the allele frequencies per cohort.\n\n        Note:\n            if the `flip` column is -1, then the allele frequency is flipped (1 - af).\n\n        Args:\n            flip (Column): Direction column indicating if the allele needs to be flipped. (-1 for flip, 1 for no flip, null for no information)\n            scale (int): Scale for the decimal type conversion. Default is 10.\n\n        Returns:\n            Column: Column containing array of structs with `cohort` and `alleleFrequency` fields.\n\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"v1\", 0.1, 0.2, None, 0.3, 0.4, -1),\n            ...        (\"v2\", 0.000000001, 0.999999999, None, None, None,1),\n            ...        (\"v3\", 0.1, 0.1, None, None, None, None),]\n            &gt;&gt;&gt; schema = \"variantId STRING, MVP_EUR_af_alt DOUBLE, MVP_AFR_af_alt DOUBLE, MVP_HIS_af_alt DOUBLE, fg_af_alt DOUBLE, ukbb_af_alt DOUBLE, flip INT\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.show(truncate=False)\n            +---------+--------------+--------------+--------------+---------+-----------+----+\n            |variantId|MVP_EUR_af_alt|MVP_AFR_af_alt|MVP_HIS_af_alt|fg_af_alt|ukbb_af_alt|flip|\n            +---------+--------------+--------------+--------------+---------+-----------+----+\n            |v1       |0.1           |0.2           |NULL          |0.3      |0.4        |-1  |\n            |v2       |1.0E-9        |0.999999999   |NULL          |NULL     |NULL       |1   |\n            |v3       |0.1           |0.1           |NULL          |NULL     |NULL       |NULL|\n            +---------+--------------+--------------+--------------+---------+-----------+----+\n            &lt;BLANKLINE&gt;\n\n            &gt;&gt;&gt; df = df.withColumn(\"alleleFrequencies\", FinnGenUkbMvpMetaSummaryStatistics.allele_frequencies(f.col(\"flip\")))\n            &gt;&gt;&gt; df.select(\"alleleFrequencies\").show(truncate=False)\n            +-------------------------------------------------------------------------------------------------+\n            |alleleFrequencies                                                                                |\n            +-------------------------------------------------------------------------------------------------+\n            |[{MVP_EUR, 0.9000000000}, {MVP_AFR, 0.8000000000}, {FinnGen, 0.7000000000}, {UKBB, 0.6000000000}]|\n            |[{MVP_EUR, 0.0000000010}, {MVP_AFR, 0.9999999990}]                                               |\n            |[{MVP_EUR, 0.1000000000}, {MVP_AFR, 0.1000000000}]                                               |\n            +-------------------------------------------------------------------------------------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        precision = scale + 1  # to ensure we can represent values like 1.0000\n        return f.filter(\n            f.array(\n                f.struct(\n                    f.lit(\"MVP_EUR\").alias(\"cohort\"),\n                    cls.normalize_af(f.col(\"MVP_EUR_af_alt\"), flip)\n                    .cast(t.DecimalType(precision, scale))\n                    .alias(\"alleleFrequency\"),\n                ),\n                f.struct(\n                    f.lit(\"MVP_AFR\").alias(\"cohort\"),\n                    cls.normalize_af(f.col(\"MVP_AFR_af_alt\"), flip)\n                    .cast(t.DecimalType(precision, scale))\n                    .alias(\"alleleFrequency\"),\n                ),\n                f.struct(\n                    f.lit(\"MVP_AMR\").alias(\"cohort\"),\n                    # Note: HIS in sumstats is AMR in study index\n                    cls.normalize_af(f.col(\"MVP_HIS_af_alt\"), flip)\n                    .cast(t.DecimalType(precision, scale))\n                    .alias(\"alleleFrequency\"),\n                ),\n                f.struct(\n                    f.lit(\"FinnGen\").alias(\"cohort\"),\n                    cls.normalize_af(f.col(\"fg_af_alt\"), flip)\n                    .cast(t.DecimalType(precision, scale))\n                    .alias(\"alleleFrequency\"),\n                ),\n                f.struct(\n                    f.lit(\"UKBB\").alias(\"cohort\"),\n                    cls.normalize_af(f.col(\"ukbb_af_alt\"), flip)\n                    .cast(t.DecimalType(precision, scale))\n                    .alias(\"alleleFrequency\"),\n                ),\n            ),\n            lambda x: x.getField(\"alleleFrequency\").isNotNull(),\n        ).alias(\"alleleFrequencies\")\n\n    @classmethod\n    def combined_allele_frequency(\n        cls, allele_freq: Column, n_samples_per_cohort: Column\n    ) -&gt; Column:\n        \"\"\"Combined Allele Frequency across all cohorts.\n\n        Args:\n            allele_freq (Column): Column containing array of structs with `cohort` and `alleleFrequency` fields.\n            n_samples_per_cohort (Column): Column containing array of structs with `cohort` and `nSamples` fields.\n\n        Returns:\n            Column: Combined allele frequency across all cohorts.\n\n        Note:\n            The combination is made by weighting the allele frequencies by the number of samples in each cohort.\n\n        Examples:\n            &gt;&gt;&gt; data = [\n            ...    (\"v1\", [{\"cohort\": \"A\", \"alleleFrequency\": 0.6}, {\"cohort\": \"B\", \"alleleFrequency\": 0.2}, {\"cohort\": \"C\", \"alleleFrequency\": 0.3}],\n            ...           [{\"cohort\": \"A\", \"nSamples\": 100}, {\"cohort\": \"B\", \"nSamples\": 200}, {\"cohort\": \"D\", \"nSamples\": 20}]),\n            ...    (\"v2\", [{\"cohort\": \"A\", \"alleleFrequency\": None},], [{\"cohort\": \"A\", \"nSamples\": 50}]),\n            ...    (\"v3\", [{\"cohort\": \"A\", \"alleleFrequency\": 0.05},], [{\"cohort\": \"A\", \"nSamples\": None}]),]\n            &gt;&gt;&gt; schema = \"variantId STRING, alleleFrequencies ARRAY&lt;STRUCT&lt;cohort: STRING, alleleFrequency: DOUBLE&gt;&gt;, nSamplesPerCohort ARRAY&lt;STRUCT&lt;cohort: STRING, nSamples: INT&gt;&gt;\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.show(truncate=False)\n            +---------+------------------------------+-----------------------------+\n            |variantId|alleleFrequencies             |nSamplesPerCohort            |\n            +---------+------------------------------+-----------------------------+\n            |v1       |[{A, 0.6}, {B, 0.2}, {C, 0.3}]|[{A, 100}, {B, 200}, {D, 20}]|\n            |v2       |[{A, NULL}]                   |[{A, 50}]                    |\n            |v3       |[{A, 0.05}]                   |[{A, NULL}]                  |\n            +---------+------------------------------+-----------------------------+\n            &lt;BLANKLINE&gt;\n\n            &gt;&gt;&gt; df = df.withColumn(\"combinedAlleleFrequency\", FinnGenUkbMvpMetaSummaryStatistics.combined_allele_frequency(f.col(\"alleleFrequencies\"), f.col(\"nSamplesPerCohort\")))\n            &gt;&gt;&gt; df.select(\"variantId\", f.round(\"combinedAlleleFrequency\", 2).alias(\"caf\")).show(truncate=False)\n            +---------+----+\n            |variantId|caf |\n            +---------+----+\n            |v1       |0.33|\n            |v2       |NULL|\n            |v3       |NULL|\n            +---------+----+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        af_filtered = f.filter(\n            allele_freq, lambda x: x.getField(\"alleleFrequency\").isNotNull()\n        )\n        samples_filtered = f.filter(\n            n_samples_per_cohort, lambda x: x.getField(\"nSamples\").isNotNull()\n        )\n\n        intersect = f.array_intersect(\n            f.transform(af_filtered, lambda x: x.getField(\"cohort\")),\n            f.transform(samples_filtered, lambda x: x.getField(\"cohort\")),\n        )\n        af_map = f.map_from_entries(\n            f.filter(\n                af_filtered, lambda x: f.array_contains(intersect, x.getField(\"cohort\"))\n            )\n        )\n        n_samples_map = f.map_from_entries(\n            f.filter(\n                samples_filtered,\n                lambda x: f.array_contains(intersect, x.getField(\"cohort\")),\n            )\n        )\n        # Compute numerator: sum(AF * n)\n        af_times_n = f.aggregate(\n            f.map_entries(af_map),\n            f.lit(0.0),\n            lambda acc, kv: acc\n            + kv[\"value\"]\n            * f.coalesce(f.element_at(n_samples_map, kv[\"key\"]), f.lit(0.0)),\n        )\n\n        # Compute denominator: sum(n)\n        n_total = f.aggregate(\n            f.map_entries(n_samples_map),\n            f.lit(0),\n            lambda acc, kv: acc + f.coalesce(kv[\"value\"], f.lit(0)),\n        )\n\n        return (\n            f.when((n_total == 0) | (af_times_n == 0), f.lit(None))\n            .otherwise(af_times_n / n_total)\n            .cast(t.FloatType())\n        )\n\n    @classmethod\n    def min_allele_frequency(cls, allele_freq: Column) -&gt; Column:\n        \"\"\"Minor Allele Frequency (MAF) per cohort.\n\n        Note:\n            The resulting value is of DecimalType(11, 10) to ensure precision for low frequency variants.\n\n        Args:\n            allele_freq (Column): Column containing array of structs with `cohort` and `alleleFrequency` fields.\n\n        Returns:\n            Column: Column containing array of structs with `cohort` and `minAlleleFrequency` fields.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"v1\", [{\"cohort\": \"A\", \"alleleFrequency\": 0.1}, {\"cohort\": \"B\", \"alleleFrequency\": 0.7}]),]\n            &gt;&gt;&gt; schema = \"variantId STRING, alleleFrequencies ARRAY&lt;STRUCT&lt;cohort: STRING, alleleFrequency: DOUBLE&gt;&gt;\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.show(truncate=False)\n            +---------+--------------------+\n            |variantId|alleleFrequencies   |\n            +---------+--------------------+\n            |v1       |[{A, 0.1}, {B, 0.7}]|\n            +---------+--------------------+\n            &lt;BLANKLINE&gt;\n\n            &gt;&gt;&gt; df = df.withColumn(\"cohortMinAlleleFrequency\", FinnGenUkbMvpMetaSummaryStatistics.min_allele_frequency(f.col(\"alleleFrequencies\")))\n            &gt;&gt;&gt; df.show(truncate=False)\n            +---------+--------------------+--------------------------------------+\n            |variantId|alleleFrequencies   |cohortMinAlleleFrequency              |\n            +---------+--------------------+--------------------------------------+\n            |v1       |[{A, 0.1}, {B, 0.7}]|[{A, 0.1000000000}, {B, 0.3000000000}]|\n            +---------+--------------------+--------------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.transform(\n            allele_freq,\n            lambda x: f.struct(\n                x.getField(\"cohort\").alias(\"cohort\"),\n                maf(x.getField(\"alleleFrequency\")).alias(\"minAlleleFrequency\"),\n            ),\n        )\n\n    @classmethod\n    def min_allele_count(\n        cls, cohort_min_allele_frequency: Column, n_samples_per_cohort: Column\n    ) -&gt; Column:\n        \"\"\"Minor Allele Count (MAC) per cohort.\n\n        Note:\n            If a cohort either do not have the maf or nCases, it will be dropped from the resulting MAC array.\n\n        Args:\n            cohort_min_allele_frequency (Column): Column containing array of structs with `cohort` and `minAlleleFrequency` fields.\n            n_samples_per_cohort (Column): Column containing array of structs with `cohort` and `nSamples` fields.\n\n        Returns:\n            Column: Column containing array of structs with `cohort` and `minAlleleCount` fields.\n\n        Examples:\n            &gt;&gt;&gt; maf = {\"v1\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.1}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.2}],\n            ...         \"v2\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.05}, {\"cohort\": \"D\", \"minAlleleFrequency\": 0.15}],\n            ...         \"v3\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.01}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.02}],}\n            &gt;&gt;&gt; n_samples = {\"v1\": [{\"cohort\": \"A\", \"nSamples\": 100}, {\"cohort\": \"B\", \"nSamples\": 200}],\n            ...            \"v2\": [{\"cohort\": \"A\", \"nSamples\": 150}, {\"cohort\": \"C\", \"nSamples\": 250}],\n            ...            \"v3\": [{\"cohort\": \"C\", \"nSamples\": 50}, {\"cohort\": \"D\", \"nSamples\": 80}],}\n            &gt;&gt;&gt; data = [(\"v1\", maf[\"v1\"], n_samples[\"v1\"]),\n            ...         (\"v2\", maf[\"v2\"], n_samples[\"v2\"]),\n            ...         (\"v3\", maf[\"v3\"], n_samples[\"v3\"]),]\n            &gt;&gt;&gt; schema = \"variantId STRING, cohortMinAlleleFrequency ARRAY&lt;STRUCT&lt;cohort: STRING, minAlleleFrequency: DOUBLE&gt;&gt;, nSamplesPerCohort ARRAY&lt;STRUCT&lt;cohort: STRING, nSamples: INT&gt;&gt;\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.show(truncate=False)\n            +---------+------------------------+--------------------+\n            |variantId|cohortMinAlleleFrequency|nSamplesPerCohort   |\n            +---------+------------------------+--------------------+\n            |v1       |[{A, 0.1}, {B, 0.2}]    |[{A, 100}, {B, 200}]|\n            |v2       |[{A, 0.05}, {D, 0.15}]  |[{A, 150}, {C, 250}]|\n            |v3       |[{A, 0.01}, {B, 0.02}]  |[{C, 50}, {D, 80}]  |\n            +---------+------------------------+--------------------+\n            &lt;BLANKLINE&gt;\n            &gt;&gt;&gt; df = df.withColumn(\"cohortMinAlleleCount\", FinnGenUkbMvpMetaSummaryStatistics.min_allele_count(f.col(\"cohortMinAlleleFrequency\"), f.col(\"nSamplesPerCohort\")))\n            &gt;&gt;&gt; df.select(\"variantId\", \"cohortMinAlleleCount\").show(truncate=False)\n            +---------+--------------------+\n            |variantId|cohortMinAlleleCount|\n            +---------+--------------------+\n            |v1       |[{A, 20}, {B, 80}]  |\n            |v2       |[{A, 15}]           |\n            |v3       |[]                  |\n            +---------+--------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return f.transform(\n            f.filter(\n                cohort_min_allele_frequency,\n                lambda left: f.exists(\n                    n_samples_per_cohort,\n                    lambda right: right.getField(\"cohort\") == left.getField(\"cohort\"),\n                ),\n            ),\n            lambda left: f.struct(\n                left.getField(\"cohort\").alias(\"cohort\"),\n                mac(\n                    left.getField(\"minAlleleFrequency\"),\n                    f.filter(\n                        n_samples_per_cohort,\n                        lambda right: right.getField(\"cohort\")\n                        == left.getField(\"cohort\"),\n                    )\n                    .getItem(0)\n                    .getField(\"nSamples\"),\n                ).alias(\"minAlleleCount\"),\n            ),\n        )\n\n    @classmethod\n    def has_low_min_allele_count(\n        cls, min_allele_count: Column, min_allele_count_threshold: int = 20\n    ) -&gt; Column:\n        \"\"\"Find if variant has a low minor allele count in any of the cohorts.\n\n        Note:\n            If any cohort has a minor allele count below the threshold, the variant is considered to have a low minor allele count.\n\n\n        Args:\n            min_allele_count (Column): Column containing array of structs with `cohort` and `minAlleleCount` fields.\n            min_allele_count_threshold (int): Threshold below which the minor allele count is considered low.\n\n        Returns:\n            Column: Boolean column indicating if any cohort has a low minor allele count.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"v1\", [{\"cohort\": \"A\", \"minAlleleCount\": 30}, {\"cohort\": \"B\", \"minAlleleCount\": 25}]),\n            ...         (\"v2\", [{\"cohort\": \"A\", \"minAlleleCount\": 15}, {\"cohort\": \"B\", \"minAlleleCount\": 25}]),\n            ...         (\"v3\", [{\"cohort\": \"A\", \"minAlleleCount\": 30}, {\"cohort\": \"B\", \"minAlleleCount\": 10}]),\n            ...         (\"v4\", [{\"cohort\": \"A\", \"minAlleleCount\": 5},],)]\n            &gt;&gt;&gt; schema = \"variantId STRING, cohortMinAlleleCount ARRAY&lt;STRUCT&lt;cohort: STRING, minAlleleCount: INT&gt;&gt;\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df = df.withColumn(\"hasLowMinAlleleCount\", FinnGenUkbMvpMetaSummaryStatistics.has_low_min_allele_count(f.col(\"cohortMinAlleleCount\"), 20))\n            &gt;&gt;&gt; df.select(\"variantId\", \"hasLowMinAlleleCount\").show()\n            +---------+--------------------+\n            |variantId|hasLowMinAlleleCount|\n            +---------+--------------------+\n            |       v1|               false|\n            |       v2|                true|\n            |       v3|                true|\n            |       v4|                true|\n            +---------+--------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return (\n            f.size(\n                f.filter(\n                    min_allele_count,\n                    lambda x: x.getField(\"minAlleleCount\")\n                    &lt; f.lit(min_allele_count_threshold),\n                )\n            )\n            &gt; 0\n        ).alias(\"hasLowMinAlleleCount\")\n\n    @classmethod\n    def has_low_imputation_score(cls, imputation_threshold: float) -&gt; Column:\n        \"\"\"Find if variant has a low r2 imputation score in any of the MVP cohorts.\n\n        Note:\n            A missing imputation score is considered as passing the threshold, since it means that the variant was not\n            present in that cohort.\n\n        Note:\n            If any r2 imputation score is below the threshold, the variant is considered to have a low imputation score.\n\n        Args:\n            imputation_threshold (float): Threshold below which the imputation score is considered low.\n\n        Returns:\n            Column: Boolean column indicating if any cohort has a low imputation score.\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"v1\", 0.9, 0.8, 1.0), (\"v2\",0.7, 0.9, 0.9), (\"v3\", None, None, 0.8), (\"v4\", None, None, 0.7)]\n            &gt;&gt;&gt; schema = \"variantId STRING, MVP_EUR_r2 DOUBLE, MVP_AFR_r2 DOUBLE, MVP_HIS_r2 DOUBLE\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df = df.withColumn(\"hasLowImputationScore\", FinnGenUkbMvpMetaSummaryStatistics.has_low_imputation_score(0.8))\n            &gt;&gt;&gt; df.select(\"variantId\", \"hasLowImputationScore\").show()\n            +---------+---------------------+\n            |variantId|hasLowImputationScore|\n            +---------+---------------------+\n            |       v1|                false|\n            |       v2|                 true|\n            |       v3|                false|\n            |       v4|                 true|\n            +---------+---------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return (\n            f.size(\n                f.filter(\n                    f.array(\n                        f.struct(\n                            f.col(\"MVP_EUR_r2\").alias(\"r2\"),\n                            f.lit(\"MVP_EUR\").alias(\"cohort\"),\n                            f.when(\n                                f.col(\"MVP_EUR_r2\").isNull()\n                                | (f.col(\"MVP_EUR_r2\") &gt;= imputation_threshold),\n                                f.lit(True),\n                            )\n                            .otherwise(f.lit(False))\n                            .alias(\"filter\"),\n                        ),\n                        f.struct(\n                            f.col(\"MVP_AFR_r2\").alias(\"r2\"),\n                            f.lit(\"MVP_AFR\").alias(\"cohort\"),\n                            f.when(\n                                f.col(\"MVP_AFR_r2\").isNull()\n                                | (f.col(\"MVP_AFR_r2\") &gt;= imputation_threshold),\n                                f.lit(True),\n                            )\n                            .otherwise(f.lit(False))\n                            .alias(\"filter\"),\n                        ),\n                        f.struct(\n                            f.col(\"MVP_HIS_r2\").alias(\"r2\"),\n                            f.lit(\"MVP_AMR\").alias(\"cohort\"),\n                            f.when(\n                                f.col(\"MVP_HIS_r2\").isNull()\n                                | (f.col(\"MVP_HIS_r2\") &gt;= imputation_threshold),\n                                f.lit(True),\n                            )\n                            .otherwise(f.lit(False))\n                            .alias(\"filter\"),\n                        ),\n                    ),\n                    lambda x: ~x.getField(\"filter\"),\n                )\n            )\n            &gt; 0\n        ).alias(\"hasLowImputationScore\")\n\n    @classmethod\n    def cohorts(cls) -&gt; Column:\n        \"\"\"Cohorts involved in the meta-analysis.\n\n        This method creates an array of structs containing biobank and cohort information\n        for variants that have allele frequency data available in each respective cohort.\n\n        Returns:\n            Column: Array of structs with fields 'biobank' and 'cohort'.\n\n        Examples:\n            # Test case 1: All cohorts have data\n            &gt;&gt;&gt; data1 = [(0.3, 0.2, 0.4, 0.1, 0.25)]\n            &gt;&gt;&gt; schema1 = \"MVP_EUR_af_alt DOUBLE, MVP_AFR_af_alt DOUBLE, MVP_HIS_af_alt DOUBLE, fg_af_alt DOUBLE, ukbb_af_alt DOUBLE\"\n            &gt;&gt;&gt; df1 = spark.createDataFrame(data1, schema1)\n            &gt;&gt;&gt; df1.withColumn(\"cohorts\", FinnGenUkbMvpMetaSummaryStatistics.cohorts()).select(\"cohorts\").show(truncate=False)\n            +----------------------------------------------------------------------------------+\n            |cohorts                                                                           |\n            +----------------------------------------------------------------------------------+\n            |[{MVP, MVP_EUR}, {MVP, MVP_AFR}, {MVP, MVP_AMR}, {FinnGen, FinnGen}, {UKBB, UKBB}]|\n            +----------------------------------------------------------------------------------+\n            &lt;BLANKLINE&gt;\n\n            # Test case 2: Only some cohorts have data\n            &gt;&gt;&gt; data2 = [(0.3, None, None, 0.1, None)]\n            &gt;&gt;&gt; df2 = spark.createDataFrame(data2, schema1)\n            &gt;&gt;&gt; df2.withColumn(\"cohorts\", FinnGenUkbMvpMetaSummaryStatistics.cohorts()).select(\"cohorts\").show(truncate=False)\n            +------------------------------------+\n            |cohorts                             |\n            +------------------------------------+\n            |[{MVP, MVP_EUR}, {FinnGen, FinnGen}]|\n            +------------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.transform(\n            f.filter(\n                f.array(\n                    f.struct(\n                        f.when(f.col(\"MVP_EUR_af_alt\").isNotNull(), f.lit(True))\n                        .otherwise(f.lit(False))\n                        .alias(\"inCohort\"),\n                        f.lit(\"MVP_EUR\").alias(\"cohort\"),\n                        f.lit(\"MVP\").alias(\"biobank\"),\n                    ),\n                    f.struct(\n                        f.when(f.col(\"MVP_AFR_af_alt\").isNotNull(), f.lit(True))\n                        .otherwise(f.lit(False))\n                        .alias(\"inCohort\"),\n                        f.lit(\"MVP_AFR\").alias(\"cohort\"),\n                        f.lit(\"MVP\").alias(\"biobank\"),\n                    ),\n                    f.struct(\n                        f.when(f.col(\"MVP_HIS_af_alt\").isNotNull(), f.lit(True))\n                        .otherwise(f.lit(False))\n                        .alias(\"inCohort\"),\n                        f.lit(\"MVP_AMR\").alias(\"cohort\"),\n                        f.lit(\"MVP\").alias(\"biobank\"),\n                    ),\n                    f.struct(\n                        f.when(f.col(\"fg_af_alt\").isNotNull(), f.lit(True))\n                        .otherwise(f.lit(False))\n                        .alias(\"inCohort\"),\n                        f.lit(\"FinnGen\").alias(\"cohort\"),\n                        f.lit(\"FinnGen\").alias(\"biobank\"),\n                    ),\n                    f.struct(\n                        f.when(f.col(\"ukbb_af_alt\").isNotNull(), f.lit(True))\n                        .otherwise(f.lit(False))\n                        .alias(\"inCohort\"),\n                        f.lit(\"UKBB\").alias(\"cohort\"),\n                        f.lit(\"UKBB\").alias(\"biobank\"),\n                    ),\n                ),\n                lambda x: x[\"inCohort\"],\n            ),\n            lambda x: f.struct(x[\"biobank\"], x[\"cohort\"]),\n        ).alias(\"cohorts\")\n\n    @classmethod\n    def is_meta_analyzed_variant(cls, cohorts: Column) -&gt; Column:\n        \"\"\"Check if the variant is meta-analyzed (present in more than one biobank).\n\n        Note:\n            if the same biobank has multiple cohorts, it still counts as one biobank.\n\n        Args:\n            cohorts (Column): Array of structs with fields 'biobank'.\n\n        Returns:\n            Column: Boolean column indicating if the variant is meta-analyzed.\n\n        Examples:\n            &gt;&gt;&gt; data = [([(\"FinnGen\", \"FinnGen\"), (\"MVP\", \"MVP_EUR\"), (\"MVP\", \"MVP_AMR\")],), ([(\"MVP\", \"MVP_AMR\"), (\"MVP\", \"MVP_EUR\")],),([(\"UKBB\", \"UKBB\")],)]\n            &gt;&gt;&gt; schema = \"cohorts ARRAY&lt;STRUCT&lt;biobank: STRING, cohort: STRING&gt;&gt;\"\n            &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n            &gt;&gt;&gt; df.show(truncate=False)\n            +----------------------------------------------------+\n            |cohorts                                             |\n            +----------------------------------------------------+\n            |[{FinnGen, FinnGen}, {MVP, MVP_EUR}, {MVP, MVP_AMR}]|\n            |[{MVP, MVP_AMR}, {MVP, MVP_EUR}]                    |\n            |[{UKBB, UKBB}]                                      |\n            +----------------------------------------------------+\n            &lt;BLANKLINE&gt;\n\n            &gt;&gt;&gt; df.withColumn(\"isMetaAnalyzedVariant\", FinnGenUkbMvpMetaSummaryStatistics.is_meta_analyzed_variant(f.col(\"cohorts\"))).show(truncate=False)\n            +----------------------------------------------------+---------------------+\n            |cohorts                                             |isMetaAnalyzedVariant|\n            +----------------------------------------------------+---------------------+\n            |[{FinnGen, FinnGen}, {MVP, MVP_EUR}, {MVP, MVP_AMR}]|true                 |\n            |[{MVP, MVP_AMR}, {MVP, MVP_EUR}]                    |false                |\n            |[{UKBB, UKBB}]                                      |false                |\n            +----------------------------------------------------+---------------------+\n            &lt;BLANKLINE&gt;\n\n\n        \"\"\"\n        n_biobanks = f.size(\n            f.array_distinct(f.transform(cohorts, lambda x: x.getField(\"biobank\"))),\n        )\n        return (n_biobanks &gt; 1).alias(\"isMetaAnalyzedVariant\")\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.allele_frequencies","title":"<code>allele_frequencies(flip: Column, scale: int = 10) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Extract the allele frequencies per cohort.</p> Note <p>if the <code>flip</code> column is -1, then the allele frequency is flipped (1 - af).</p> <p>Parameters:</p> Name Type Description Default <code>flip</code> <code>Column</code> <p>Direction column indicating if the allele needs to be flipped. (-1 for flip, 1 for no flip, null for no information)</p> required <code>scale</code> <code>int</code> <p>Scale for the decimal type conversion. Default is 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>alleleFrequency</code> fields.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"v1\", 0.1, 0.2, None, 0.3, 0.4, -1),\n...        (\"v2\", 0.000000001, 0.999999999, None, None, None,1),\n...        (\"v3\", 0.1, 0.1, None, None, None, None),]\n&gt;&gt;&gt; schema = \"variantId STRING, MVP_EUR_af_alt DOUBLE, MVP_AFR_af_alt DOUBLE, MVP_HIS_af_alt DOUBLE, fg_af_alt DOUBLE, ukbb_af_alt DOUBLE, flip INT\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+---------+--------------+--------------+--------------+---------+-----------+----+\n|variantId|MVP_EUR_af_alt|MVP_AFR_af_alt|MVP_HIS_af_alt|fg_af_alt|ukbb_af_alt|flip|\n+---------+--------------+--------------+--------------+---------+-----------+----+\n|v1       |0.1           |0.2           |NULL          |0.3      |0.4        |-1  |\n|v2       |1.0E-9        |0.999999999   |NULL          |NULL     |NULL       |1   |\n|v3       |0.1           |0.1           |NULL          |NULL     |NULL       |NULL|\n+---------+--------------+--------------+--------------+---------+-----------+----+\n</code></pre> <pre><code>&gt;&gt;&gt; df = df.withColumn(\"alleleFrequencies\", FinnGenUkbMvpMetaSummaryStatistics.allele_frequencies(f.col(\"flip\")))\n&gt;&gt;&gt; df.select(\"alleleFrequencies\").show(truncate=False)\n+-------------------------------------------------------------------------------------------------+\n|alleleFrequencies                                                                                |\n+-------------------------------------------------------------------------------------------------+\n|[{MVP_EUR, 0.9000000000}, {MVP_AFR, 0.8000000000}, {FinnGen, 0.7000000000}, {UKBB, 0.6000000000}]|\n|[{MVP_EUR, 0.0000000010}, {MVP_AFR, 0.9999999990}]                                               |\n|[{MVP_EUR, 0.1000000000}, {MVP_AFR, 0.1000000000}]                                               |\n+-------------------------------------------------------------------------------------------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef allele_frequencies(cls, flip: Column, scale: int = 10) -&gt; Column:\n    \"\"\"Extract the allele frequencies per cohort.\n\n    Note:\n        if the `flip` column is -1, then the allele frequency is flipped (1 - af).\n\n    Args:\n        flip (Column): Direction column indicating if the allele needs to be flipped. (-1 for flip, 1 for no flip, null for no information)\n        scale (int): Scale for the decimal type conversion. Default is 10.\n\n    Returns:\n        Column: Column containing array of structs with `cohort` and `alleleFrequency` fields.\n\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"v1\", 0.1, 0.2, None, 0.3, 0.4, -1),\n        ...        (\"v2\", 0.000000001, 0.999999999, None, None, None,1),\n        ...        (\"v3\", 0.1, 0.1, None, None, None, None),]\n        &gt;&gt;&gt; schema = \"variantId STRING, MVP_EUR_af_alt DOUBLE, MVP_AFR_af_alt DOUBLE, MVP_HIS_af_alt DOUBLE, fg_af_alt DOUBLE, ukbb_af_alt DOUBLE, flip INT\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------+--------------+--------------+--------------+---------+-----------+----+\n        |variantId|MVP_EUR_af_alt|MVP_AFR_af_alt|MVP_HIS_af_alt|fg_af_alt|ukbb_af_alt|flip|\n        +---------+--------------+--------------+--------------+---------+-----------+----+\n        |v1       |0.1           |0.2           |NULL          |0.3      |0.4        |-1  |\n        |v2       |1.0E-9        |0.999999999   |NULL          |NULL     |NULL       |1   |\n        |v3       |0.1           |0.1           |NULL          |NULL     |NULL       |NULL|\n        +---------+--------------+--------------+--------------+---------+-----------+----+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df = df.withColumn(\"alleleFrequencies\", FinnGenUkbMvpMetaSummaryStatistics.allele_frequencies(f.col(\"flip\")))\n        &gt;&gt;&gt; df.select(\"alleleFrequencies\").show(truncate=False)\n        +-------------------------------------------------------------------------------------------------+\n        |alleleFrequencies                                                                                |\n        +-------------------------------------------------------------------------------------------------+\n        |[{MVP_EUR, 0.9000000000}, {MVP_AFR, 0.8000000000}, {FinnGen, 0.7000000000}, {UKBB, 0.6000000000}]|\n        |[{MVP_EUR, 0.0000000010}, {MVP_AFR, 0.9999999990}]                                               |\n        |[{MVP_EUR, 0.1000000000}, {MVP_AFR, 0.1000000000}]                                               |\n        +-------------------------------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    precision = scale + 1  # to ensure we can represent values like 1.0000\n    return f.filter(\n        f.array(\n            f.struct(\n                f.lit(\"MVP_EUR\").alias(\"cohort\"),\n                cls.normalize_af(f.col(\"MVP_EUR_af_alt\"), flip)\n                .cast(t.DecimalType(precision, scale))\n                .alias(\"alleleFrequency\"),\n            ),\n            f.struct(\n                f.lit(\"MVP_AFR\").alias(\"cohort\"),\n                cls.normalize_af(f.col(\"MVP_AFR_af_alt\"), flip)\n                .cast(t.DecimalType(precision, scale))\n                .alias(\"alleleFrequency\"),\n            ),\n            f.struct(\n                f.lit(\"MVP_AMR\").alias(\"cohort\"),\n                # Note: HIS in sumstats is AMR in study index\n                cls.normalize_af(f.col(\"MVP_HIS_af_alt\"), flip)\n                .cast(t.DecimalType(precision, scale))\n                .alias(\"alleleFrequency\"),\n            ),\n            f.struct(\n                f.lit(\"FinnGen\").alias(\"cohort\"),\n                cls.normalize_af(f.col(\"fg_af_alt\"), flip)\n                .cast(t.DecimalType(precision, scale))\n                .alias(\"alleleFrequency\"),\n            ),\n            f.struct(\n                f.lit(\"UKBB\").alias(\"cohort\"),\n                cls.normalize_af(f.col(\"ukbb_af_alt\"), flip)\n                .cast(t.DecimalType(precision, scale))\n                .alias(\"alleleFrequency\"),\n            ),\n        ),\n        lambda x: x.getField(\"alleleFrequency\").isNotNull(),\n    ).alias(\"alleleFrequencies\")\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.bgzip_to_parquet","title":"<code>bgzip_to_parquet(session: Session, summary_statistics_list: list[str], datasource: MetaAnalysisDataSource, raw_summary_statistics_output_path: str, n_threads: int = 10) -&gt; None</code>  <code>classmethod</code>","text":"<p>Convert gzipped summary statistics to Parquet format.</p> <p>This is a pre-step that needs to be performed once to convert the block gzipped to parquet format. This step should be run before the actual harmonisation step is performed and the output Parquet files can be used as input for the <code>from_source</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>summary_statistics_list</code> <code>list[str]</code> <p>List of paths where summary statistics files are located.</p> required <code>datasource</code> <code>MetaAnalysisDataSource</code> <p>Data source, can be FinnGenMetaDataSource.FINNGEN_UKBB_MVP.</p> required <code>raw_summary_statistics_output_path</code> <code>str</code> <p>Output path for the Parquet files.</p> required <code>n_threads</code> <code>int</code> <p>Maximum number of threads to use for ThreadPoolExecutor (default is 10).</p> <code>10</code> <p>The output requires a single path that will be populated with Parquet files partitioned by <code>studyId</code> extracted from the input file names.</p> <p>Block gzipped input files</p> <p>Since the individual summary statistics files are block gzipped we use the enhanced bgzip codec for efficient reading.</p> <p>Reading multiple files with divergent schemas</p> <p>Since the schema for individual summary statistics is not strictly the same we have to enforce the schema.</p> <p>enforcing schema using the <code>enforceSchema</code> option in <code>spark.read.csv</code> does not map columns that exist in the file provided schema, but rather aligns columns positionally, which breaks the column order per individual file.</p> <p>inferring schema Attempting to use the <code>inferSchema</code> option in <code>spark.read.csv</code> while reading multiple files in bulk drops columns, due to the random sampling of files to infer the schema. (Files chosen to infer the schema may not contain entire superset of column space.)</p> <p>manual schema enforcement The only way to keep the columns in order and use full column superset is to loop over the files with <code>inferSchema</code> and manually add missing columns with null values casted to expected type. The looping can be parallelized using a thread pool (ThreadPoolExecutor) with <code>n_threads</code> as the maximum load of jobs to spark cluster.</p> Performance considerations <p>This function requires a Session with <code>use_enhanced_bgzip_codec</code> to be set to True. This function is strongly encouraged to be used in a distributed environment.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>use_enhanced_bgzip_codec</code> is set to False in the Session configuration.</p> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef bgzip_to_parquet(\n    cls,\n    session: Session,\n    summary_statistics_list: list[str],\n    datasource: MetaAnalysisDataSource,\n    raw_summary_statistics_output_path: str,\n    n_threads: int = 10,\n) -&gt; None:\n    \"\"\"Convert gzipped summary statistics to Parquet format.\n\n    This is a pre-step that needs to be performed once to convert the block gzipped to parquet format. This step\n    should be run before the actual harmonisation step is performed and the output Parquet files can be used as input\n    for the `from_source` method.\n\n    Args:\n        session (Session): Session object.\n        summary_statistics_list (list[str]): List of paths where summary statistics files are located.\n        datasource (MetaAnalysisDataSource): Data source, can be FinnGenMetaDataSource.FINNGEN_UKBB_MVP.\n        raw_summary_statistics_output_path (str): Output path for the Parquet files.\n        n_threads (int): Maximum number of threads to use for ThreadPoolExecutor (default is 10).\n\n    The output requires a single path that will be populated with Parquet files partitioned by `studyId` extracted\n    from the input file names.\n\n    !!! note \"Block gzipped input files\"\n\n        Since the individual summary statistics files are **block gzipped** we use the enhanced bgzip codec for efficient reading.\n\n    !!! note \"Reading multiple files with divergent schemas\"\n\n        Since the schema for individual summary statistics **is not strictly the same we have to enforce the schema**.\n\n        **_enforcing schema_**\n        using the `enforceSchema` option in `spark.read.csv` **does not map columns that exist in the file provided schema**,\n        but rather aligns columns positionally, which breaks the column order per individual file.\n\n        **_inferring schema_**\n        Attempting to use the `inferSchema` option in `spark.read.csv` while reading multiple files in bulk drops columns, due\n        to the random sampling of files to infer the schema. (Files chosen to infer the schema may not contain entire superset of column space.)\n\n        **_manual schema enforcement_**\n        The only way to keep the columns in order and use full column superset is to loop over the files with `inferSchema` and manually\n        add missing columns with null values casted to expected type. The looping can be parallelized using a **thread pool** (ThreadPoolExecutor)\n        with `n_threads` as the maximum load of jobs to spark cluster.\n\n    ??? warning \"Performance considerations\"\n        This function requires a _Session_ with `use_enhanced_bgzip_codec` to be set to True. This function is strongly encouraged to be used in\n        a distributed environment.\n\n    Raises:\n        KeyError: If `use_enhanced_bgzip_codec` is set to False in the Session configuration.\n    \"\"\"\n    if len(summary_statistics_list) == 0:\n        session.logger.warning(\"No summary statistics paths found to process.\")\n        return\n    if not session.use_enhanced_bgzip_codec:\n        session.logger.error(\n            \"The use_enhanced_bgzip_codec is set to False. This will lead to inefficient reading of block gzipped files.\"\n        )\n        raise KeyError(\n            \"Please set `session.spark.use_enhanced_bgzip_codec` to True in the Session configuration.\"\n        )\n\n    # Handle n_threads limits and warnings\n    if not isinstance(n_threads, int) or n_threads &lt; 1:\n        session.logger.warning(\n            f\"Invalid n_threads value: {n_threads}. Falling back to 10 threads.\"\n        )\n        n_threads = FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_OPTIMAL\n    if n_threads &lt; FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_OPTIMAL:\n        session.logger.warning(\n            f\"Using low n_threads value: {n_threads}. This may lead to sub-optimal performance.\"\n        )\n    if n_threads &gt; FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_MAX:\n        session.logger.warning(\n            f\"Using high n_threads value: {n_threads}, this may lead to overloading spark driver. Limiting to 32.\"\n        )\n        n_threads = FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_MAX\n\n    def process_one(\n        input_path: str, session: Session, output_path: str\n    ) -&gt; DataFrame:\n        \"\"\"Function to process one finngen-ukbb-mvp summary statistics file to schema superset.\n\n        Args:\n            input_path (str): Input path to the gzipped summary statistics file.\n            session (Session): Session object.\n            output_path (str): Output path for the Parquet files.\n\n        Returns:\n            DataFrame: Processed dataframe.\n        \"\"\"\n        df = session.spark.read.csv(\n            input_path,\n            header=True,\n            inferSchema=True,\n            sep=\"\\t\",\n            enforceSchema=False,\n        )\n        # Apply schema enforcement by selecting columns in the expected order with proper types\n        # NOTE: Here we only add missing columns with null values casted to expected type\n        for c in cls.raw_schema.names:\n            if c not in df.columns:\n                df = df.withColumn(c, f.lit(None).cast(cls.raw_schema[c].dataType))\n\n        # Replace all NA with nulls and cast to the expected type\n        # NOTE: Here we apply the transformation on full schema set (including added missing columns)\n        # NOTE: If we do not transform the `NA` to nulls, we will still have divergent schemas, as `NA` forces the column to StringType\n        for field in cls.raw_schema.fields:\n            df = df.withColumn(\n                field.name,\n                f.when(f.col(field.name) == \"NA\", f.lit(None))\n                .otherwise(f.col(field.name))\n                .cast(field.dataType),\n            )\n        # Add studyId based on the input path\n        df = (\n            df.withColumn(\n                \"studyId\",\n                f.concat_ws(\n                    \"_\",\n                    f.lit(datasource.value),\n                    f.lit(\n                        cls.extract_study_phenotype_from_path(f.input_file_name())\n                    ),\n                ),\n                # Optimal partition size is ~ 100MB, assuming the total size of the dataset is 2Tb\n                # we can have up to 60 partitions per study (330 studies)\n            )\n            .orderBy(\"studyId\", \"#CHR\", \"POS\")\n            .repartition(60, \"#CHR\", \"POS\")\n        )\n        # Write out the processed dataframe to Parquet\n        # NOTE: Write is done per studyId partition from the thread pool to\n        # make sure we do not need to collect all data after the thread execution.\n        df.write.mode(\"append\").partitionBy(\"studyId\").parquet(output_path)\n        return df\n\n    session.logger.info(\n        f\"Converting gzipped summary statistics from {summary_statistics_list} to Parquet at {raw_summary_statistics_output_path}.\"\n    )\n    with ThreadPoolExecutor(max_workers=n_threads) as pool:\n        pool.map(\n            lambda path: process_one(\n                path,\n                session=session,\n                output_path=raw_summary_statistics_output_path,\n            ),\n            summary_statistics_list,\n        )\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.cohorts","title":"<code>cohorts() -&gt; Column</code>  <code>classmethod</code>","text":"<p>Cohorts involved in the meta-analysis.</p> <p>This method creates an array of structs containing biobank and cohort information for variants that have allele frequency data available in each respective cohort.</p> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Array of structs with fields 'biobank' and 'cohort'.</p> <p>Examples:</p>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.cohorts--test-case-1-all-cohorts-have-data","title":"Test case 1: All cohorts have data","text":"<pre><code>&gt;&gt;&gt; data1 = [(0.3, 0.2, 0.4, 0.1, 0.25)]\n&gt;&gt;&gt; schema1 = \"MVP_EUR_af_alt DOUBLE, MVP_AFR_af_alt DOUBLE, MVP_HIS_af_alt DOUBLE, fg_af_alt DOUBLE, ukbb_af_alt DOUBLE\"\n&gt;&gt;&gt; df1 = spark.createDataFrame(data1, schema1)\n&gt;&gt;&gt; df1.withColumn(\"cohorts\", FinnGenUkbMvpMetaSummaryStatistics.cohorts()).select(\"cohorts\").show(truncate=False)\n+----------------------------------------------------------------------------------+\n|cohorts                                                                           |\n+----------------------------------------------------------------------------------+\n|[{MVP, MVP_EUR}, {MVP, MVP_AFR}, {MVP, MVP_AMR}, {FinnGen, FinnGen}, {UKBB, UKBB}]|\n+----------------------------------------------------------------------------------+\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.cohorts--test-case-2-only-some-cohorts-have-data","title":"Test case 2: Only some cohorts have data","text":"<pre><code>&gt;&gt;&gt; data2 = [(0.3, None, None, 0.1, None)]\n&gt;&gt;&gt; df2 = spark.createDataFrame(data2, schema1)\n&gt;&gt;&gt; df2.withColumn(\"cohorts\", FinnGenUkbMvpMetaSummaryStatistics.cohorts()).select(\"cohorts\").show(truncate=False)\n+------------------------------------+\n|cohorts                             |\n+------------------------------------+\n|[{MVP, MVP_EUR}, {FinnGen, FinnGen}]|\n+------------------------------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef cohorts(cls) -&gt; Column:\n    \"\"\"Cohorts involved in the meta-analysis.\n\n    This method creates an array of structs containing biobank and cohort information\n    for variants that have allele frequency data available in each respective cohort.\n\n    Returns:\n        Column: Array of structs with fields 'biobank' and 'cohort'.\n\n    Examples:\n        # Test case 1: All cohorts have data\n        &gt;&gt;&gt; data1 = [(0.3, 0.2, 0.4, 0.1, 0.25)]\n        &gt;&gt;&gt; schema1 = \"MVP_EUR_af_alt DOUBLE, MVP_AFR_af_alt DOUBLE, MVP_HIS_af_alt DOUBLE, fg_af_alt DOUBLE, ukbb_af_alt DOUBLE\"\n        &gt;&gt;&gt; df1 = spark.createDataFrame(data1, schema1)\n        &gt;&gt;&gt; df1.withColumn(\"cohorts\", FinnGenUkbMvpMetaSummaryStatistics.cohorts()).select(\"cohorts\").show(truncate=False)\n        +----------------------------------------------------------------------------------+\n        |cohorts                                                                           |\n        +----------------------------------------------------------------------------------+\n        |[{MVP, MVP_EUR}, {MVP, MVP_AFR}, {MVP, MVP_AMR}, {FinnGen, FinnGen}, {UKBB, UKBB}]|\n        +----------------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        # Test case 2: Only some cohorts have data\n        &gt;&gt;&gt; data2 = [(0.3, None, None, 0.1, None)]\n        &gt;&gt;&gt; df2 = spark.createDataFrame(data2, schema1)\n        &gt;&gt;&gt; df2.withColumn(\"cohorts\", FinnGenUkbMvpMetaSummaryStatistics.cohorts()).select(\"cohorts\").show(truncate=False)\n        +------------------------------------+\n        |cohorts                             |\n        +------------------------------------+\n        |[{MVP, MVP_EUR}, {FinnGen, FinnGen}]|\n        +------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.transform(\n        f.filter(\n            f.array(\n                f.struct(\n                    f.when(f.col(\"MVP_EUR_af_alt\").isNotNull(), f.lit(True))\n                    .otherwise(f.lit(False))\n                    .alias(\"inCohort\"),\n                    f.lit(\"MVP_EUR\").alias(\"cohort\"),\n                    f.lit(\"MVP\").alias(\"biobank\"),\n                ),\n                f.struct(\n                    f.when(f.col(\"MVP_AFR_af_alt\").isNotNull(), f.lit(True))\n                    .otherwise(f.lit(False))\n                    .alias(\"inCohort\"),\n                    f.lit(\"MVP_AFR\").alias(\"cohort\"),\n                    f.lit(\"MVP\").alias(\"biobank\"),\n                ),\n                f.struct(\n                    f.when(f.col(\"MVP_HIS_af_alt\").isNotNull(), f.lit(True))\n                    .otherwise(f.lit(False))\n                    .alias(\"inCohort\"),\n                    f.lit(\"MVP_AMR\").alias(\"cohort\"),\n                    f.lit(\"MVP\").alias(\"biobank\"),\n                ),\n                f.struct(\n                    f.when(f.col(\"fg_af_alt\").isNotNull(), f.lit(True))\n                    .otherwise(f.lit(False))\n                    .alias(\"inCohort\"),\n                    f.lit(\"FinnGen\").alias(\"cohort\"),\n                    f.lit(\"FinnGen\").alias(\"biobank\"),\n                ),\n                f.struct(\n                    f.when(f.col(\"ukbb_af_alt\").isNotNull(), f.lit(True))\n                    .otherwise(f.lit(False))\n                    .alias(\"inCohort\"),\n                    f.lit(\"UKBB\").alias(\"cohort\"),\n                    f.lit(\"UKBB\").alias(\"biobank\"),\n                ),\n            ),\n            lambda x: x[\"inCohort\"],\n        ),\n        lambda x: f.struct(x[\"biobank\"], x[\"cohort\"]),\n    ).alias(\"cohorts\")\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.combined_allele_frequency","title":"<code>combined_allele_frequency(allele_freq: Column, n_samples_per_cohort: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Combined Allele Frequency across all cohorts.</p> <p>Parameters:</p> Name Type Description Default <code>allele_freq</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>alleleFrequency</code> fields.</p> required <code>n_samples_per_cohort</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>nSamples</code> fields.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Combined allele frequency across all cohorts.</p> Note <p>The combination is made by weighting the allele frequencies by the number of samples in each cohort.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [\n...    (\"v1\", [{\"cohort\": \"A\", \"alleleFrequency\": 0.6}, {\"cohort\": \"B\", \"alleleFrequency\": 0.2}, {\"cohort\": \"C\", \"alleleFrequency\": 0.3}],\n...           [{\"cohort\": \"A\", \"nSamples\": 100}, {\"cohort\": \"B\", \"nSamples\": 200}, {\"cohort\": \"D\", \"nSamples\": 20}]),\n...    (\"v2\", [{\"cohort\": \"A\", \"alleleFrequency\": None},], [{\"cohort\": \"A\", \"nSamples\": 50}]),\n...    (\"v3\", [{\"cohort\": \"A\", \"alleleFrequency\": 0.05},], [{\"cohort\": \"A\", \"nSamples\": None}]),]\n&gt;&gt;&gt; schema = \"variantId STRING, alleleFrequencies ARRAY&lt;STRUCT&lt;cohort: STRING, alleleFrequency: DOUBLE&gt;&gt;, nSamplesPerCohort ARRAY&lt;STRUCT&lt;cohort: STRING, nSamples: INT&gt;&gt;\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+---------+------------------------------+-----------------------------+\n|variantId|alleleFrequencies             |nSamplesPerCohort            |\n+---------+------------------------------+-----------------------------+\n|v1       |[{A, 0.6}, {B, 0.2}, {C, 0.3}]|[{A, 100}, {B, 200}, {D, 20}]|\n|v2       |[{A, NULL}]                   |[{A, 50}]                    |\n|v3       |[{A, 0.05}]                   |[{A, NULL}]                  |\n+---------+------------------------------+-----------------------------+\n</code></pre> <pre><code>&gt;&gt;&gt; df = df.withColumn(\"combinedAlleleFrequency\", FinnGenUkbMvpMetaSummaryStatistics.combined_allele_frequency(f.col(\"alleleFrequencies\"), f.col(\"nSamplesPerCohort\")))\n&gt;&gt;&gt; df.select(\"variantId\", f.round(\"combinedAlleleFrequency\", 2).alias(\"caf\")).show(truncate=False)\n+---------+----+\n|variantId|caf |\n+---------+----+\n|v1       |0.33|\n|v2       |NULL|\n|v3       |NULL|\n+---------+----+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef combined_allele_frequency(\n    cls, allele_freq: Column, n_samples_per_cohort: Column\n) -&gt; Column:\n    \"\"\"Combined Allele Frequency across all cohorts.\n\n    Args:\n        allele_freq (Column): Column containing array of structs with `cohort` and `alleleFrequency` fields.\n        n_samples_per_cohort (Column): Column containing array of structs with `cohort` and `nSamples` fields.\n\n    Returns:\n        Column: Combined allele frequency across all cohorts.\n\n    Note:\n        The combination is made by weighting the allele frequencies by the number of samples in each cohort.\n\n    Examples:\n        &gt;&gt;&gt; data = [\n        ...    (\"v1\", [{\"cohort\": \"A\", \"alleleFrequency\": 0.6}, {\"cohort\": \"B\", \"alleleFrequency\": 0.2}, {\"cohort\": \"C\", \"alleleFrequency\": 0.3}],\n        ...           [{\"cohort\": \"A\", \"nSamples\": 100}, {\"cohort\": \"B\", \"nSamples\": 200}, {\"cohort\": \"D\", \"nSamples\": 20}]),\n        ...    (\"v2\", [{\"cohort\": \"A\", \"alleleFrequency\": None},], [{\"cohort\": \"A\", \"nSamples\": 50}]),\n        ...    (\"v3\", [{\"cohort\": \"A\", \"alleleFrequency\": 0.05},], [{\"cohort\": \"A\", \"nSamples\": None}]),]\n        &gt;&gt;&gt; schema = \"variantId STRING, alleleFrequencies ARRAY&lt;STRUCT&lt;cohort: STRING, alleleFrequency: DOUBLE&gt;&gt;, nSamplesPerCohort ARRAY&lt;STRUCT&lt;cohort: STRING, nSamples: INT&gt;&gt;\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------+------------------------------+-----------------------------+\n        |variantId|alleleFrequencies             |nSamplesPerCohort            |\n        +---------+------------------------------+-----------------------------+\n        |v1       |[{A, 0.6}, {B, 0.2}, {C, 0.3}]|[{A, 100}, {B, 200}, {D, 20}]|\n        |v2       |[{A, NULL}]                   |[{A, 50}]                    |\n        |v3       |[{A, 0.05}]                   |[{A, NULL}]                  |\n        +---------+------------------------------+-----------------------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df = df.withColumn(\"combinedAlleleFrequency\", FinnGenUkbMvpMetaSummaryStatistics.combined_allele_frequency(f.col(\"alleleFrequencies\"), f.col(\"nSamplesPerCohort\")))\n        &gt;&gt;&gt; df.select(\"variantId\", f.round(\"combinedAlleleFrequency\", 2).alias(\"caf\")).show(truncate=False)\n        +---------+----+\n        |variantId|caf |\n        +---------+----+\n        |v1       |0.33|\n        |v2       |NULL|\n        |v3       |NULL|\n        +---------+----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    af_filtered = f.filter(\n        allele_freq, lambda x: x.getField(\"alleleFrequency\").isNotNull()\n    )\n    samples_filtered = f.filter(\n        n_samples_per_cohort, lambda x: x.getField(\"nSamples\").isNotNull()\n    )\n\n    intersect = f.array_intersect(\n        f.transform(af_filtered, lambda x: x.getField(\"cohort\")),\n        f.transform(samples_filtered, lambda x: x.getField(\"cohort\")),\n    )\n    af_map = f.map_from_entries(\n        f.filter(\n            af_filtered, lambda x: f.array_contains(intersect, x.getField(\"cohort\"))\n        )\n    )\n    n_samples_map = f.map_from_entries(\n        f.filter(\n            samples_filtered,\n            lambda x: f.array_contains(intersect, x.getField(\"cohort\")),\n        )\n    )\n    # Compute numerator: sum(AF * n)\n    af_times_n = f.aggregate(\n        f.map_entries(af_map),\n        f.lit(0.0),\n        lambda acc, kv: acc\n        + kv[\"value\"]\n        * f.coalesce(f.element_at(n_samples_map, kv[\"key\"]), f.lit(0.0)),\n    )\n\n    # Compute denominator: sum(n)\n    n_total = f.aggregate(\n        f.map_entries(n_samples_map),\n        f.lit(0),\n        lambda acc, kv: acc + f.coalesce(kv[\"value\"], f.lit(0)),\n    )\n\n    return (\n        f.when((n_total == 0) | (af_times_n == 0), f.lit(None))\n        .otherwise(af_times_n / n_total)\n        .cast(t.FloatType())\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.extract_study_phenotype_from_path","title":"<code>extract_study_phenotype_from_path(file_path: Column) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Extract the study phenotype from FinnGen file path.</p> Note <p>Assumes the file name format is some_path/to/_meta_out.tsv.gz <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Column</code> <p>Column containing the file path as a string.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Extracted study phenotype as a string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"/path/to/AB1_meta_out.tsv.gz\",), (\"/another/path/CD2_meta_out.tsv.gz\",)]\n&gt;&gt;&gt; schema = \"filePath STRING\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df = df.withColumn(\"studyPhenotype\", FinnGenUkbMvpMetaSummaryStatistics.extract_study_phenotype_from_path(f.col(\"filePath\")))\n&gt;&gt;&gt; df.show(truncate=False)\n+---------------------------------+--------------+\n|filePath                         |studyPhenotype|\n+---------------------------------+--------------+\n|/path/to/AB1_meta_out.tsv.gz     |AB1           |\n|/another/path/CD2_meta_out.tsv.gz|CD2           |\n+---------------------------------+--------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@staticmethod\ndef extract_study_phenotype_from_path(file_path: Column) -&gt; Column:\n    \"\"\"Extract the study phenotype from FinnGen file path.\n\n    Note:\n        Assumes the file name format is some_path/to/&lt;studyPhenotype&gt;_meta_out.tsv.gz\n\n\n    Args:\n        file_path (Column): Column containing the file path as a string.\n\n    Returns:\n        Column: Extracted study phenotype as a string.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"/path/to/AB1_meta_out.tsv.gz\",), (\"/another/path/CD2_meta_out.tsv.gz\",)]\n        &gt;&gt;&gt; schema = \"filePath STRING\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df = df.withColumn(\"studyPhenotype\", FinnGenUkbMvpMetaSummaryStatistics.extract_study_phenotype_from_path(f.col(\"filePath\")))\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------------------------------+--------------+\n        |filePath                         |studyPhenotype|\n        +---------------------------------+--------------+\n        |/path/to/AB1_meta_out.tsv.gz     |AB1           |\n        |/another/path/CD2_meta_out.tsv.gz|CD2           |\n        +---------------------------------+--------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return f.regexp_replace(\n        f.element_at(f.split(file_path, \"/\"), -1), \"_meta_out.tsv.gz\", \"\"\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.from_source","title":"<code>from_source(raw_summary_statistics: DataFrame, finngen_manifest: FinnGenMetaManifest, variant_annotations: VariantDirection, perform_meta_analysis_filter: bool = True, imputation_score_threshold: float = 0.8, perform_imputation_score_filter: bool = True, min_allele_count_threshold: int = 20, perform_min_allele_count_filter: bool = True, min_allele_frequency_threshold: float = 0.0001, perform_min_allele_frequency_filter: bool = False, filter_out_ambiguous_variants: bool = False) -&gt; SummaryStatistics</code>  <code>classmethod</code>","text":"<p>Build the summary statistics dataset from raw summary statistics.</p> <p>See original issue to find out more details on the harmonisation logic https://github.com/opentargets/issues/issues/3474</p> The logic behind the harmonisation <ol> <li>Build a slice of FinnGen Manifest to bring the information about nCases and nSamples per cohort (broadcast join).</li> <li>Build a variant direction (gnomAD) dataset partitioned by <code>chromosome</code> and <code>variantId</code> for joining using Sort-Merge strategy.</li> <li>Select required columns from raw summary statistics</li> <li>Remove all non-meta analyzed variants (nBiobanks &lt; 2) - by default</li> <li>Remove all variants with low imputation score - by default</li> <li>Join with Finngen Manifest and Variant Direction datasets</li> <li>Flip <code>beta</code> and <code>min allele frequency</code> based on the <code>direction</code> from Variant Direction dataset</li> <li>Use originalVariantId from Variant Direction dataset if available, otherwise fall back to variantId (for variants missing from Variant Direction)</li> <li>Calculate combined effect allele frequency from cohorts</li> <li>Remove all variants with low Min Allele Count - by default</li> <li>Remove all variants with low Min Allele Frequency - optional</li> <li>Remove strand ambiguous variants - optional</li> </ol> Variant Directionality <p>Variant Direction By default we:</p> <ol> <li>keep all strand ambiguous variants as is</li> <li>keep all variants found in gnomAD aligned to gnomAD reference ( if alleles are flipped we flip the beta and allele frequency)</li> <li>keep all variants not found in gnomAD as is (cannot determine strand or alignment) - we assume these are correct.</li> </ol> Important considerations <ul> <li>The input summary statistics are expected to be already parquet formatted and partitioned by <code>studyId</code>.</li> <li>Both <code>perform_allele_count_filter</code> and <code>perform_allele_frequency_filter</code> are redundant, if both are set to True, only <code>perform_allele_count_filter</code> will be applied.</li> <li>Threshold value for Min Allele Count &gt;= 20 means that with a MAF of 1e-4 we would expect to see at least 1_000 samples with minor allele in a cohort. This is quiet stringent for very rare variants and smaller cohorts, like MVP_AFR (nSamples ~120k) and MVP_HIS (~60k).</li> <li>MVP_HIS cohort has been mapped to admixed American population - see https://www.science.org/doi/10.1126/science.adj1182 for more details.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>raw_summary_statistics</code> <code>DataFrame</code> <p>Raw summary statistics dataframe.</p> required <code>finngen_manifest</code> <code>FinnGenMetaManifest</code> <p>FinnGen meta analysis manifest.</p> required <code>variant_annotations</code> <code>VariantDirection</code> <p>Variant direction dataset.</p> required <code>perform_meta_analysis_filter</code> <code>bool</code> <p>Whether to remove variants found in just 1 biobank. Default is True.</p> <code>True</code> <code>imputation_score_threshold</code> <code>float</code> <p>Imputation score threshold for MVP cohorts. Default is 0.8.</p> <code>0.8</code> <code>perform_imputation_score_filter</code> <code>bool</code> <p>Whether to perform the imputation score filter. Default is True.</p> <code>True</code> <code>min_allele_count_threshold</code> <code>int</code> <p>Minimum allele count threshold. Default is 20.</p> <code>20</code> <code>perform_min_allele_count_filter</code> <code>bool</code> <p>Whether to perform the minimum allele count filter. Default is False.</p> <code>True</code> <code>min_allele_frequency_threshold</code> <code>float</code> <p>Minimum allele frequency threshold. Default is 1e-4.</p> <code>0.0001</code> <code>perform_min_allele_frequency_filter</code> <code>bool</code> <p>Whether to perform the minimum allele frequency filter. Default is True.</p> <code>False</code> <code>filter_out_ambiguous_variants</code> <code>bool</code> <p>Whether to filter out strand ambiguous variants. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>Processed summary statistics dataset.</p> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef from_source(\n    cls,\n    raw_summary_statistics: DataFrame,\n    finngen_manifest: FinnGenMetaManifest,\n    variant_annotations: VariantDirection,\n    perform_meta_analysis_filter: bool = True,\n    imputation_score_threshold: float = 0.8,\n    perform_imputation_score_filter: bool = True,\n    min_allele_count_threshold: int = 20,\n    perform_min_allele_count_filter: bool = True,\n    min_allele_frequency_threshold: float = 1e-4,\n    perform_min_allele_frequency_filter: bool = False,\n    filter_out_ambiguous_variants: bool = False,\n) -&gt; SummaryStatistics:\n    \"\"\"Build the summary statistics dataset from raw summary statistics.\n\n    See original issue to find out more details on the harmonisation logic https://github.com/opentargets/issues/issues/3474\n\n    ??? note \"The logic behind the harmonisation\"\n        1. Build a slice of FinnGen Manifest to bring the information about nCases and nSamples per cohort (broadcast join).\n        2. Build a variant direction (gnomAD) dataset partitioned by `chromosome` and `variantId` for joining using Sort-Merge strategy.\n        3. Select required columns from raw summary statistics\n        4. Remove all non-meta analyzed variants (nBiobanks &lt; 2) - by default\n        5. Remove all variants with low imputation score - by default\n        6. Join with Finngen Manifest and Variant Direction datasets\n        7. Flip `beta` and `min allele frequency` based on the `direction` from Variant Direction dataset\n        8. Use originalVariantId from Variant Direction dataset if available, otherwise fall back to variantId (for variants missing from Variant Direction)\n        9. Calculate combined effect allele frequency from cohorts\n        10. Remove all variants with low Min Allele Count - by default\n        11. Remove all variants with low Min Allele Frequency - optional\n        12. Remove strand ambiguous variants - optional\n\n    ??? tip \"Variant Directionality\"\n        **Variant Direction**\n        By default we:\n\n        1. keep all strand ambiguous variants as is\n        2. keep all variants found in gnomAD aligned to gnomAD reference ( if alleles are flipped we flip the beta and allele frequency)\n        3. keep all variants not found in gnomAD as is (cannot determine strand or alignment) - we assume these are correct.\n\n    ??? note \"Important considerations\"\n        * The input summary statistics are expected to be already parquet formatted and partitioned by `studyId`.\n        * Both `perform_allele_count_filter` and `perform_allele_frequency_filter` are redundant, if both\n        are set to True, only `perform_allele_count_filter` will be applied.\n        * Threshold value for Min Allele Count &gt;= 20 means that with a MAF of 1e-4 we would expect to see\n        at least 1_000 samples with minor allele in a cohort. This is quiet stringent for very rare variants and\n        smaller cohorts, like MVP_AFR (nSamples ~120k) and MVP_HIS (~60k).\n        * MVP_HIS cohort has been mapped to admixed American population - see https://www.science.org/doi/10.1126/science.adj1182 for more details.\n\n    Args:\n        raw_summary_statistics (DataFrame): Raw summary statistics dataframe.\n        finngen_manifest (FinnGenMetaManifest): FinnGen meta analysis manifest.\n        variant_annotations (VariantDirection): Variant direction dataset.\n        perform_meta_analysis_filter (bool): Whether to remove variants found in just 1 biobank. Default is True.\n        imputation_score_threshold (float): Imputation score threshold for MVP cohorts. Default is 0.8.\n        perform_imputation_score_filter (bool): Whether to perform the imputation score filter. Default is True.\n        min_allele_count_threshold (int): Minimum allele count threshold. Default is 20.\n        perform_min_allele_count_filter (bool): Whether to perform the minimum allele count filter. Default is False.\n        min_allele_frequency_threshold (float): Minimum allele frequency threshold. Default is 1e-4.\n        perform_min_allele_frequency_filter (bool): Whether to perform the minimum allele frequency filter. Default is True.\n        filter_out_ambiguous_variants (bool): Whether to filter out strand ambiguous variants. Default is False.\n\n    Returns:\n        SummaryStatistics: Processed summary statistics dataset.\n    \"\"\"\n    if perform_min_allele_count_filter:\n        assert (\n            min_allele_count_threshold &gt; 0\n        ), \"Allele count threshold should be positive.\"\n    if perform_min_allele_frequency_filter:\n        assert (\n            0.0 &lt;= min_allele_frequency_threshold &lt;= 0.5\n        ), \"MAF needs to be between 0 and 0.5.\"\n\n    if perform_min_allele_count_filter and perform_min_allele_frequency_filter:\n        # NOTE - MAC filter would be more stringent at low allele frequencies, so no\n        # need to have both filters active at the same time\n        perform_min_allele_frequency_filter = False\n\n    si_slice = f.broadcast(\n        finngen_manifest.df.select(\n            f.col(\"studyId\"),\n            f.col(\"nCasesPerCohort\"),\n            f.col(\"nSamples\"),\n            f.col(\"nSamplesPerCohort\"),\n        ).persist()\n    )\n    vd_slice = (\n        variant_annotations.df.select(\n            f.col(\"chromosome\"),\n            f.col(\"originalVariantId\"),\n            f.col(\"variantId\"),\n            f.col(\"direction\"),\n            f.col(\"isStrandAmbiguous\"),\n        )\n        # NOTE: repartition(\"chromosome\") produces very uneven partitions,\n        # Spark attempts then to fall back to `dynamic partitioning` algorithm\n        # which fails after N failures.\n        .repartitionByRange(4_000, \"chromosome\", \"variantId\")\n        .persist()\n    )\n\n    sumstats = (\n        # Pre-select columns that are needed downstream\n        # NOTE: full set of columns is not required.\n        raw_summary_statistics.select(\n            f.col(\"#CHR\"),\n            f.col(\"POS\"),\n            f.col(\"REF\"),\n            f.col(\"ALT\"),\n            f.col(\"fg_af_alt\"),\n            f.col(\"MVP_EUR_r2\"),\n            f.col(\"MVP_EUR_af_alt\"),\n            f.col(\"MVP_AFR_r2\"),\n            f.col(\"MVP_AFR_af_alt\"),\n            f.col(\"MVP_HIS_r2\"),\n            f.col(\"MVP_HIS_af_alt\"),\n            f.col(\"ukbb_af_alt\"),\n            f.col(\"all_inv_var_meta_mlogp\"),\n            f.col(\"all_inv_var_meta_beta\"),\n            f.col(\"all_inv_var_meta_sebeta\"),\n            f.col(\"studyId\"),\n        )\n        # NOTE: make sure the chromosome is coded to 1:22, X, Y\n        .withColumn(\n            \"chromosome\",\n            normalize_chromosome(f.col(\"#CHR\").cast(t.StringType())),\n        )\n        .drop(\"#CHR\")\n        .withColumn(\"position\", f.col(\"POS\").cast(t.IntegerType()))\n        .withColumnRenamed(\"REF\", \"referenceAllele\")\n        .withColumnRenamed(\"ALT\", \"alternateAllele\")\n        .withColumn(\n            \"neglogpval\", f.col(\"all_inv_var_meta_mlogp\").cast(t.DoubleType())\n        )\n        .withColumn(\"beta\", f.col(\"all_inv_var_meta_beta\").cast(t.DoubleType()))\n        .withColumn(\n            \"standardError\", f.col(\"all_inv_var_meta_sebeta\").cast(t.DoubleType())\n        )\n        .withColumn(\n            \"variantId\",\n            f.concat_ws(\n                \"_\",\n                f.col(\"chromosome\"),\n                f.col(\"position\"),\n                f.col(\"referenceAllele\"),\n                f.col(\"alternateAllele\"),\n            ).alias(\"variantId\"),\n        )\n        .drop(\"referenceAllele\", \"alternateAllele\")\n        # Initial filters based on statistics presence\n        .filter(f.col(\"neglogpval\").isNotNull())\n        .filter(f.col(\"beta\").isNotNull())\n        .filter(f.col(\"standardError\").isNotNull())\n    )\n\n    # Filter out variants that are not meta analyzed (nBiobanks &lt; 1)\n    if perform_meta_analysis_filter:\n        sumstats = (\n            sumstats.withColumn(\"cohorts\", cls.cohorts())\n            .withColumn(\n                \"isMetaAnalyzedVariant\",\n                cls.is_meta_analyzed_variant(f.col(\"cohorts\")),\n            )\n            .filter(f.col(\"isMetaAnalyzedVariant\"))\n            .drop(\"isMetaAnalyzedVariant\", \"cohorts\")\n        )\n\n    # Filter out variants with low INFO score\n    if perform_imputation_score_filter:\n        assert (\n            imputation_score_threshold &gt;= 0.0\n        ), \"Imputation score threshold should be positive.\"\n        sumstats = (\n            sumstats.withColumn(\n                \"hasLowImputationScore\",\n                cls.has_low_imputation_score(imputation_score_threshold),\n            )\n            .filter(~f.col(\"hasLowImputationScore\"))\n            .drop(\"hasLowImputationScore\", \"imputationScore\")\n        )\n\n    sumstats = (\n        # Annotate with StudyIndex nCases, nSamples to obtain\n        # the cases Minor Allele Count and Samples for combined AF calculation\n        sumstats.join(si_slice, on=\"studyId\", how=\"left\")\n        # Join with variant direction dataset\n        # Keep variants if not found in gnomAD (left join)\n        .repartitionByRange(4_000, \"chromosome\", \"variantId\")\n        .join(vd_slice, on=[\"chromosome\", \"variantId\"], how=\"left\")\n        # Use originalVariantId (already flipped) or fall back to variantId if not found in gnomAD\n        .withColumn(\n            \"variantId\", f.coalesce(f.col(\"originalVariantId\"), f.col(\"variantId\"))\n        )\n        # Compute allele frequency per cohort and align with direction\n        # NOTE: `direction` column represents if variant aligned to gnomAD variant or it's flipped\n        # version, the values are `1` - direct, `-1` - flipped\n        .withColumn(\n            \"cohortAlleleFrequency\", cls.allele_frequencies(f.col(\"direction\"))\n        )\n        # Make sure the beta is alined with the direction, if direction is null, keep beta as is\n        .withColumn(\n            \"beta\", f.col(\"beta\") * f.coalesce(f.col(\"direction\"), f.lit(1))\n        )\n        # Calculate the combined effect allele frequency from cohorts\n        .withColumn(\n            \"effectAlleleFrequencyFromSource\",\n            cls.combined_allele_frequency(\n                f.col(\"cohortAlleleFrequency\"), f.col(\"nSamplesPerCohort\")\n            ),\n        )\n    )\n    # Remove strand ambiguous variants, if not found in gnomAD, we keep the variant\n    # Not run by default\n    if filter_out_ambiguous_variants:\n        sumstats = sumstats.filter(\n            ~f.coalesce(f.col(\"isStrandAmbiguous\"), f.lit(False))\n        )\n    if perform_min_allele_count_filter or perform_min_allele_frequency_filter:\n        # Calculate the MAF per cohort\n        sumstats = sumstats.withColumn(\n            \"cohortMinAlleleFrequency\",\n            cls.min_allele_frequency(f.col(\"cohortAlleleFrequency\")),\n        )\n    if perform_min_allele_count_filter:\n        sumstats = (\n            sumstats\n            # Make sure to only keep cohorts that have nSamples &gt; 0\n            .withColumn(\n                \"nSamplesPerCohort\",\n                f.filter(\n                    f.col(\"nSamplesPerCohort\"),\n                    lambda x: x.getField(\"nSamples\").isNotNull()\n                    &amp; (x.getField(\"nSamples\") &gt; 0),\n                ),\n            )\n            .withColumn(\n                \"cohortMinAlleleCount\",\n                cls.min_allele_count(\n                    f.col(\"cohortMinAlleleFrequency\"),\n                    f.col(\"nSamplesPerCohort\"),\n                ),\n            )\n            .withColumn(\n                \"hasLowMinAlleleCount\",\n                cls.has_low_min_allele_count(\n                    f.col(\"cohortMinAlleleCount\"),\n                    min_allele_count_threshold,\n                ),\n            )\n            .filter(~f.col(\"hasLowMinAlleleCount\"))\n            .drop(\n                \"hasLowMinAlleleCount\",\n                \"cohortMinAlleleCount\",\n                \"nCasesPerCohort\",\n            )\n        )\n    # Not run by default.\n    if perform_min_allele_frequency_filter:\n        sumstats = (\n            sumstats.withColumn(\n                \"hasLowMinAlleleFrequency\",\n                cls.has_low_min_allele_frequency(\n                    f.col(\"cohortMinAlleleFrequency\"),\n                    min_allele_frequency_threshold,\n                ),\n            )\n            .filter(~f.col(\"hasLowMinAlleleFrequency\"))\n            .drop(\"hasLowMinAlleleFrequency\")\n        )\n    # Convert to final summary statistics schema\n    sumstats = sumstats.select(\n        f.col(\"studyId\"),\n        f.col(\"variantId\"),\n        f.col(\"chromosome\"),\n        f.col(\"position\"),\n        f.col(\"beta\"),\n        f.col(\"nSamples\").alias(\"sampleSize\"),\n        *pvalue_from_neglogpval(f.col(\"neglogpval\")),\n        f.col(\"effectAlleleFrequencyFromSource\"),\n        f.col(\"standardError\"),\n    ).select(\n        f.col(\"studyId\"),\n        f.col(\"variantId\"),\n        f.col(\"chromosome\"),\n        f.col(\"position\"),\n        f.col(\"beta\"),\n        f.col(\"sampleSize\"),\n        f.col(\"pValueMantissa\"),\n        f.col(\"pValueExponent\"),\n        f.col(\"effectAlleleFrequencyFromSource\"),\n        f.col(\"standardError\"),\n    )\n\n    return SummaryStatistics(sumstats).sanity_filter()\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.has_low_imputation_score","title":"<code>has_low_imputation_score(imputation_threshold: float) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Find if variant has a low r2 imputation score in any of the MVP cohorts.</p> Note <p>A missing imputation score is considered as passing the threshold, since it means that the variant was not present in that cohort.</p> Note <p>If any r2 imputation score is below the threshold, the variant is considered to have a low imputation score.</p> <p>Parameters:</p> Name Type Description Default <code>imputation_threshold</code> <code>float</code> <p>Threshold below which the imputation score is considered low.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Boolean column indicating if any cohort has a low imputation score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"v1\", 0.9, 0.8, 1.0), (\"v2\",0.7, 0.9, 0.9), (\"v3\", None, None, 0.8), (\"v4\", None, None, 0.7)]\n&gt;&gt;&gt; schema = \"variantId STRING, MVP_EUR_r2 DOUBLE, MVP_AFR_r2 DOUBLE, MVP_HIS_r2 DOUBLE\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df = df.withColumn(\"hasLowImputationScore\", FinnGenUkbMvpMetaSummaryStatistics.has_low_imputation_score(0.8))\n&gt;&gt;&gt; df.select(\"variantId\", \"hasLowImputationScore\").show()\n+---------+---------------------+\n|variantId|hasLowImputationScore|\n+---------+---------------------+\n|       v1|                false|\n|       v2|                 true|\n|       v3|                false|\n|       v4|                 true|\n+---------+---------------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef has_low_imputation_score(cls, imputation_threshold: float) -&gt; Column:\n    \"\"\"Find if variant has a low r2 imputation score in any of the MVP cohorts.\n\n    Note:\n        A missing imputation score is considered as passing the threshold, since it means that the variant was not\n        present in that cohort.\n\n    Note:\n        If any r2 imputation score is below the threshold, the variant is considered to have a low imputation score.\n\n    Args:\n        imputation_threshold (float): Threshold below which the imputation score is considered low.\n\n    Returns:\n        Column: Boolean column indicating if any cohort has a low imputation score.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"v1\", 0.9, 0.8, 1.0), (\"v2\",0.7, 0.9, 0.9), (\"v3\", None, None, 0.8), (\"v4\", None, None, 0.7)]\n        &gt;&gt;&gt; schema = \"variantId STRING, MVP_EUR_r2 DOUBLE, MVP_AFR_r2 DOUBLE, MVP_HIS_r2 DOUBLE\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df = df.withColumn(\"hasLowImputationScore\", FinnGenUkbMvpMetaSummaryStatistics.has_low_imputation_score(0.8))\n        &gt;&gt;&gt; df.select(\"variantId\", \"hasLowImputationScore\").show()\n        +---------+---------------------+\n        |variantId|hasLowImputationScore|\n        +---------+---------------------+\n        |       v1|                false|\n        |       v2|                 true|\n        |       v3|                false|\n        |       v4|                 true|\n        +---------+---------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return (\n        f.size(\n            f.filter(\n                f.array(\n                    f.struct(\n                        f.col(\"MVP_EUR_r2\").alias(\"r2\"),\n                        f.lit(\"MVP_EUR\").alias(\"cohort\"),\n                        f.when(\n                            f.col(\"MVP_EUR_r2\").isNull()\n                            | (f.col(\"MVP_EUR_r2\") &gt;= imputation_threshold),\n                            f.lit(True),\n                        )\n                        .otherwise(f.lit(False))\n                        .alias(\"filter\"),\n                    ),\n                    f.struct(\n                        f.col(\"MVP_AFR_r2\").alias(\"r2\"),\n                        f.lit(\"MVP_AFR\").alias(\"cohort\"),\n                        f.when(\n                            f.col(\"MVP_AFR_r2\").isNull()\n                            | (f.col(\"MVP_AFR_r2\") &gt;= imputation_threshold),\n                            f.lit(True),\n                        )\n                        .otherwise(f.lit(False))\n                        .alias(\"filter\"),\n                    ),\n                    f.struct(\n                        f.col(\"MVP_HIS_r2\").alias(\"r2\"),\n                        f.lit(\"MVP_AMR\").alias(\"cohort\"),\n                        f.when(\n                            f.col(\"MVP_HIS_r2\").isNull()\n                            | (f.col(\"MVP_HIS_r2\") &gt;= imputation_threshold),\n                            f.lit(True),\n                        )\n                        .otherwise(f.lit(False))\n                        .alias(\"filter\"),\n                    ),\n                ),\n                lambda x: ~x.getField(\"filter\"),\n            )\n        )\n        &gt; 0\n    ).alias(\"hasLowImputationScore\")\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.has_low_min_allele_count","title":"<code>has_low_min_allele_count(min_allele_count: Column, min_allele_count_threshold: int = 20) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Find if variant has a low minor allele count in any of the cohorts.</p> Note <p>If any cohort has a minor allele count below the threshold, the variant is considered to have a low minor allele count.</p> <p>Parameters:</p> Name Type Description Default <code>min_allele_count</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>minAlleleCount</code> fields.</p> required <code>min_allele_count_threshold</code> <code>int</code> <p>Threshold below which the minor allele count is considered low.</p> <code>20</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Boolean column indicating if any cohort has a low minor allele count.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"v1\", [{\"cohort\": \"A\", \"minAlleleCount\": 30}, {\"cohort\": \"B\", \"minAlleleCount\": 25}]),\n...         (\"v2\", [{\"cohort\": \"A\", \"minAlleleCount\": 15}, {\"cohort\": \"B\", \"minAlleleCount\": 25}]),\n...         (\"v3\", [{\"cohort\": \"A\", \"minAlleleCount\": 30}, {\"cohort\": \"B\", \"minAlleleCount\": 10}]),\n...         (\"v4\", [{\"cohort\": \"A\", \"minAlleleCount\": 5},],)]\n&gt;&gt;&gt; schema = \"variantId STRING, cohortMinAlleleCount ARRAY&lt;STRUCT&lt;cohort: STRING, minAlleleCount: INT&gt;&gt;\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df = df.withColumn(\"hasLowMinAlleleCount\", FinnGenUkbMvpMetaSummaryStatistics.has_low_min_allele_count(f.col(\"cohortMinAlleleCount\"), 20))\n&gt;&gt;&gt; df.select(\"variantId\", \"hasLowMinAlleleCount\").show()\n+---------+--------------------+\n|variantId|hasLowMinAlleleCount|\n+---------+--------------------+\n|       v1|               false|\n|       v2|                true|\n|       v3|                true|\n|       v4|                true|\n+---------+--------------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef has_low_min_allele_count(\n    cls, min_allele_count: Column, min_allele_count_threshold: int = 20\n) -&gt; Column:\n    \"\"\"Find if variant has a low minor allele count in any of the cohorts.\n\n    Note:\n        If any cohort has a minor allele count below the threshold, the variant is considered to have a low minor allele count.\n\n\n    Args:\n        min_allele_count (Column): Column containing array of structs with `cohort` and `minAlleleCount` fields.\n        min_allele_count_threshold (int): Threshold below which the minor allele count is considered low.\n\n    Returns:\n        Column: Boolean column indicating if any cohort has a low minor allele count.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"v1\", [{\"cohort\": \"A\", \"minAlleleCount\": 30}, {\"cohort\": \"B\", \"minAlleleCount\": 25}]),\n        ...         (\"v2\", [{\"cohort\": \"A\", \"minAlleleCount\": 15}, {\"cohort\": \"B\", \"minAlleleCount\": 25}]),\n        ...         (\"v3\", [{\"cohort\": \"A\", \"minAlleleCount\": 30}, {\"cohort\": \"B\", \"minAlleleCount\": 10}]),\n        ...         (\"v4\", [{\"cohort\": \"A\", \"minAlleleCount\": 5},],)]\n        &gt;&gt;&gt; schema = \"variantId STRING, cohortMinAlleleCount ARRAY&lt;STRUCT&lt;cohort: STRING, minAlleleCount: INT&gt;&gt;\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df = df.withColumn(\"hasLowMinAlleleCount\", FinnGenUkbMvpMetaSummaryStatistics.has_low_min_allele_count(f.col(\"cohortMinAlleleCount\"), 20))\n        &gt;&gt;&gt; df.select(\"variantId\", \"hasLowMinAlleleCount\").show()\n        +---------+--------------------+\n        |variantId|hasLowMinAlleleCount|\n        +---------+--------------------+\n        |       v1|               false|\n        |       v2|                true|\n        |       v3|                true|\n        |       v4|                true|\n        +---------+--------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return (\n        f.size(\n            f.filter(\n                min_allele_count,\n                lambda x: x.getField(\"minAlleleCount\")\n                &lt; f.lit(min_allele_count_threshold),\n            )\n        )\n        &gt; 0\n    ).alias(\"hasLowMinAlleleCount\")\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.has_low_min_allele_frequency","title":"<code>has_low_min_allele_frequency(maf: Column, threshold: float = 0.0001) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Find if variant has a low minor allele frequency in any cohort.</p> <p>Parameters:</p> Name Type Description Default <code>maf</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>minAlleleFrequency</code> fields.</p> required <code>threshold</code> <code>float</code> <p>Threshold below which the minor allele frequency is considered low. Default is 1e-4.</p> <code>0.0001</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Boolean column indicating if any cohort has a low minor allele frequency.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maf = {\"v1\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.0001}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.0002}],\n...         \"v2\": [{\"cohort\": \"A\", \"minAlleleFrequency\": None}, {\"cohort\": \"D\", \"minAlleleFrequency\": 0.15}],\n...         \"v3\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.00001}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.2}],}\n&gt;&gt;&gt; data = [(\"v1\", maf[\"v1\"]),\n...         (\"v2\", maf[\"v2\"]),\n...         (\"v3\", maf[\"v3\"]),]\n&gt;&gt;&gt; schema = \"variantId STRING, cohortMinAlleleFrequency ARRAY&lt;STRUCT&lt;cohort: STRING, minAlleleFrequency: DOUBLE&gt;&gt;\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+---------+--------------------------+\n|variantId|cohortMinAlleleFrequency  |\n+---------+--------------------------+\n|v1       |[{A, 1.0E-4}, {B, 2.0E-4}]|\n|v2       |[{A, NULL}, {D, 0.15}]    |\n|v3       |[{A, 1.0E-5}, {B, 0.2}]   |\n+---------+--------------------------+\n</code></pre> <pre><code>&gt;&gt;&gt; df = df.withColumn(\"hasMinAlleleFrequency\", FinnGenUkbMvpMetaSummaryStatistics.has_low_min_allele_frequency(f.col(\"cohortMinAlleleFrequency\")))\n&gt;&gt;&gt; df.select(\"variantId\", \"hasMinAlleleFrequency\").show(truncate=False)\n+---------+---------------------+\n|variantId|hasMinAlleleFrequency|\n+---------+---------------------+\n|v1       |false                |\n|v2       |false                |\n|v3       |true                 |\n+---------+---------------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef has_low_min_allele_frequency(\n    cls, maf: Column, threshold: float = 1e-4\n) -&gt; Column:\n    \"\"\"Find if variant has a low minor allele frequency in any cohort.\n\n    Args:\n        maf (Column): Column containing array of structs with `cohort` and `minAlleleFrequency` fields.\n        threshold (float): Threshold below which the minor allele frequency is considered low. Default is 1e-4.\n\n    Returns:\n        Column: Boolean column indicating if any cohort has a low minor allele frequency.\n\n    Examples:\n        &gt;&gt;&gt; maf = {\"v1\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.0001}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.0002}],\n        ...         \"v2\": [{\"cohort\": \"A\", \"minAlleleFrequency\": None}, {\"cohort\": \"D\", \"minAlleleFrequency\": 0.15}],\n        ...         \"v3\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.00001}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.2}],}\n        &gt;&gt;&gt; data = [(\"v1\", maf[\"v1\"]),\n        ...         (\"v2\", maf[\"v2\"]),\n        ...         (\"v3\", maf[\"v3\"]),]\n        &gt;&gt;&gt; schema = \"variantId STRING, cohortMinAlleleFrequency ARRAY&lt;STRUCT&lt;cohort: STRING, minAlleleFrequency: DOUBLE&gt;&gt;\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------+--------------------------+\n        |variantId|cohortMinAlleleFrequency  |\n        +---------+--------------------------+\n        |v1       |[{A, 1.0E-4}, {B, 2.0E-4}]|\n        |v2       |[{A, NULL}, {D, 0.15}]    |\n        |v3       |[{A, 1.0E-5}, {B, 0.2}]   |\n        +---------+--------------------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df = df.withColumn(\"hasMinAlleleFrequency\", FinnGenUkbMvpMetaSummaryStatistics.has_low_min_allele_frequency(f.col(\"cohortMinAlleleFrequency\")))\n        &gt;&gt;&gt; df.select(\"variantId\", \"hasMinAlleleFrequency\").show(truncate=False)\n        +---------+---------------------+\n        |variantId|hasMinAlleleFrequency|\n        +---------+---------------------+\n        |v1       |false                |\n        |v2       |false                |\n        |v3       |true                 |\n        +---------+---------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    non_empty_maf = f.filter(\n        maf, lambda x: x.getField(\"minAlleleFrequency\").isNotNull()\n    )\n\n    n_cohorts_with_maf_below_threshold = f.size(\n        f.filter(\n            non_empty_maf,\n            lambda x: x.getField(\"minAlleleFrequency\")\n            &lt; f.lit(threshold).cast(t.DecimalType(11, 10)),\n        )\n    )\n    return n_cohorts_with_maf_below_threshold &gt; 0\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.is_meta_analyzed_variant","title":"<code>is_meta_analyzed_variant(cohorts: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Check if the variant is meta-analyzed (present in more than one biobank).</p> Note <p>if the same biobank has multiple cohorts, it still counts as one biobank.</p> <p>Parameters:</p> Name Type Description Default <code>cohorts</code> <code>Column</code> <p>Array of structs with fields 'biobank'.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Boolean column indicating if the variant is meta-analyzed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [([(\"FinnGen\", \"FinnGen\"), (\"MVP\", \"MVP_EUR\"), (\"MVP\", \"MVP_AMR\")],), ([(\"MVP\", \"MVP_AMR\"), (\"MVP\", \"MVP_EUR\")],),([(\"UKBB\", \"UKBB\")],)]\n&gt;&gt;&gt; schema = \"cohorts ARRAY&lt;STRUCT&lt;biobank: STRING, cohort: STRING&gt;&gt;\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+----------------------------------------------------+\n|cohorts                                             |\n+----------------------------------------------------+\n|[{FinnGen, FinnGen}, {MVP, MVP_EUR}, {MVP, MVP_AMR}]|\n|[{MVP, MVP_AMR}, {MVP, MVP_EUR}]                    |\n|[{UKBB, UKBB}]                                      |\n+----------------------------------------------------+\n</code></pre> <pre><code>&gt;&gt;&gt; df.withColumn(\"isMetaAnalyzedVariant\", FinnGenUkbMvpMetaSummaryStatistics.is_meta_analyzed_variant(f.col(\"cohorts\"))).show(truncate=False)\n+----------------------------------------------------+---------------------+\n|cohorts                                             |isMetaAnalyzedVariant|\n+----------------------------------------------------+---------------------+\n|[{FinnGen, FinnGen}, {MVP, MVP_EUR}, {MVP, MVP_AMR}]|true                 |\n|[{MVP, MVP_AMR}, {MVP, MVP_EUR}]                    |false                |\n|[{UKBB, UKBB}]                                      |false                |\n+----------------------------------------------------+---------------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef is_meta_analyzed_variant(cls, cohorts: Column) -&gt; Column:\n    \"\"\"Check if the variant is meta-analyzed (present in more than one biobank).\n\n    Note:\n        if the same biobank has multiple cohorts, it still counts as one biobank.\n\n    Args:\n        cohorts (Column): Array of structs with fields 'biobank'.\n\n    Returns:\n        Column: Boolean column indicating if the variant is meta-analyzed.\n\n    Examples:\n        &gt;&gt;&gt; data = [([(\"FinnGen\", \"FinnGen\"), (\"MVP\", \"MVP_EUR\"), (\"MVP\", \"MVP_AMR\")],), ([(\"MVP\", \"MVP_AMR\"), (\"MVP\", \"MVP_EUR\")],),([(\"UKBB\", \"UKBB\")],)]\n        &gt;&gt;&gt; schema = \"cohorts ARRAY&lt;STRUCT&lt;biobank: STRING, cohort: STRING&gt;&gt;\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +----------------------------------------------------+\n        |cohorts                                             |\n        +----------------------------------------------------+\n        |[{FinnGen, FinnGen}, {MVP, MVP_EUR}, {MVP, MVP_AMR}]|\n        |[{MVP, MVP_AMR}, {MVP, MVP_EUR}]                    |\n        |[{UKBB, UKBB}]                                      |\n        +----------------------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df.withColumn(\"isMetaAnalyzedVariant\", FinnGenUkbMvpMetaSummaryStatistics.is_meta_analyzed_variant(f.col(\"cohorts\"))).show(truncate=False)\n        +----------------------------------------------------+---------------------+\n        |cohorts                                             |isMetaAnalyzedVariant|\n        +----------------------------------------------------+---------------------+\n        |[{FinnGen, FinnGen}, {MVP, MVP_EUR}, {MVP, MVP_AMR}]|true                 |\n        |[{MVP, MVP_AMR}, {MVP, MVP_EUR}]                    |false                |\n        |[{UKBB, UKBB}]                                      |false                |\n        +----------------------------------------------------+---------------------+\n        &lt;BLANKLINE&gt;\n\n\n    \"\"\"\n    n_biobanks = f.size(\n        f.array_distinct(f.transform(cohorts, lambda x: x.getField(\"biobank\"))),\n    )\n    return (n_biobanks &gt; 1).alias(\"isMetaAnalyzedVariant\")\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.min_allele_count","title":"<code>min_allele_count(cohort_min_allele_frequency: Column, n_samples_per_cohort: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Minor Allele Count (MAC) per cohort.</p> Note <p>If a cohort either do not have the maf or nCases, it will be dropped from the resulting MAC array.</p> <p>Parameters:</p> Name Type Description Default <code>cohort_min_allele_frequency</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>minAlleleFrequency</code> fields.</p> required <code>n_samples_per_cohort</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>nSamples</code> fields.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>minAlleleCount</code> fields.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maf = {\"v1\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.1}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.2}],\n...         \"v2\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.05}, {\"cohort\": \"D\", \"minAlleleFrequency\": 0.15}],\n...         \"v3\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.01}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.02}],}\n&gt;&gt;&gt; n_samples = {\"v1\": [{\"cohort\": \"A\", \"nSamples\": 100}, {\"cohort\": \"B\", \"nSamples\": 200}],\n...            \"v2\": [{\"cohort\": \"A\", \"nSamples\": 150}, {\"cohort\": \"C\", \"nSamples\": 250}],\n...            \"v3\": [{\"cohort\": \"C\", \"nSamples\": 50}, {\"cohort\": \"D\", \"nSamples\": 80}],}\n&gt;&gt;&gt; data = [(\"v1\", maf[\"v1\"], n_samples[\"v1\"]),\n...         (\"v2\", maf[\"v2\"], n_samples[\"v2\"]),\n...         (\"v3\", maf[\"v3\"], n_samples[\"v3\"]),]\n&gt;&gt;&gt; schema = \"variantId STRING, cohortMinAlleleFrequency ARRAY&lt;STRUCT&lt;cohort: STRING, minAlleleFrequency: DOUBLE&gt;&gt;, nSamplesPerCohort ARRAY&lt;STRUCT&lt;cohort: STRING, nSamples: INT&gt;&gt;\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+---------+------------------------+--------------------+\n|variantId|cohortMinAlleleFrequency|nSamplesPerCohort   |\n+---------+------------------------+--------------------+\n|v1       |[{A, 0.1}, {B, 0.2}]    |[{A, 100}, {B, 200}]|\n|v2       |[{A, 0.05}, {D, 0.15}]  |[{A, 150}, {C, 250}]|\n|v3       |[{A, 0.01}, {B, 0.02}]  |[{C, 50}, {D, 80}]  |\n+---------+------------------------+--------------------+\n\n&gt;&gt;&gt; df = df.withColumn(\"cohortMinAlleleCount\", FinnGenUkbMvpMetaSummaryStatistics.min_allele_count(f.col(\"cohortMinAlleleFrequency\"), f.col(\"nSamplesPerCohort\")))\n&gt;&gt;&gt; df.select(\"variantId\", \"cohortMinAlleleCount\").show(truncate=False)\n+---------+--------------------+\n|variantId|cohortMinAlleleCount|\n+---------+--------------------+\n|v1       |[{A, 20}, {B, 80}]  |\n|v2       |[{A, 15}]           |\n|v3       |[]                  |\n+---------+--------------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef min_allele_count(\n    cls, cohort_min_allele_frequency: Column, n_samples_per_cohort: Column\n) -&gt; Column:\n    \"\"\"Minor Allele Count (MAC) per cohort.\n\n    Note:\n        If a cohort either do not have the maf or nCases, it will be dropped from the resulting MAC array.\n\n    Args:\n        cohort_min_allele_frequency (Column): Column containing array of structs with `cohort` and `minAlleleFrequency` fields.\n        n_samples_per_cohort (Column): Column containing array of structs with `cohort` and `nSamples` fields.\n\n    Returns:\n        Column: Column containing array of structs with `cohort` and `minAlleleCount` fields.\n\n    Examples:\n        &gt;&gt;&gt; maf = {\"v1\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.1}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.2}],\n        ...         \"v2\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.05}, {\"cohort\": \"D\", \"minAlleleFrequency\": 0.15}],\n        ...         \"v3\": [{\"cohort\": \"A\", \"minAlleleFrequency\": 0.01}, {\"cohort\": \"B\", \"minAlleleFrequency\": 0.02}],}\n        &gt;&gt;&gt; n_samples = {\"v1\": [{\"cohort\": \"A\", \"nSamples\": 100}, {\"cohort\": \"B\", \"nSamples\": 200}],\n        ...            \"v2\": [{\"cohort\": \"A\", \"nSamples\": 150}, {\"cohort\": \"C\", \"nSamples\": 250}],\n        ...            \"v3\": [{\"cohort\": \"C\", \"nSamples\": 50}, {\"cohort\": \"D\", \"nSamples\": 80}],}\n        &gt;&gt;&gt; data = [(\"v1\", maf[\"v1\"], n_samples[\"v1\"]),\n        ...         (\"v2\", maf[\"v2\"], n_samples[\"v2\"]),\n        ...         (\"v3\", maf[\"v3\"], n_samples[\"v3\"]),]\n        &gt;&gt;&gt; schema = \"variantId STRING, cohortMinAlleleFrequency ARRAY&lt;STRUCT&lt;cohort: STRING, minAlleleFrequency: DOUBLE&gt;&gt;, nSamplesPerCohort ARRAY&lt;STRUCT&lt;cohort: STRING, nSamples: INT&gt;&gt;\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------+------------------------+--------------------+\n        |variantId|cohortMinAlleleFrequency|nSamplesPerCohort   |\n        +---------+------------------------+--------------------+\n        |v1       |[{A, 0.1}, {B, 0.2}]    |[{A, 100}, {B, 200}]|\n        |v2       |[{A, 0.05}, {D, 0.15}]  |[{A, 150}, {C, 250}]|\n        |v3       |[{A, 0.01}, {B, 0.02}]  |[{C, 50}, {D, 80}]  |\n        +---------+------------------------+--------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df = df.withColumn(\"cohortMinAlleleCount\", FinnGenUkbMvpMetaSummaryStatistics.min_allele_count(f.col(\"cohortMinAlleleFrequency\"), f.col(\"nSamplesPerCohort\")))\n        &gt;&gt;&gt; df.select(\"variantId\", \"cohortMinAlleleCount\").show(truncate=False)\n        +---------+--------------------+\n        |variantId|cohortMinAlleleCount|\n        +---------+--------------------+\n        |v1       |[{A, 20}, {B, 80}]  |\n        |v2       |[{A, 15}]           |\n        |v3       |[]                  |\n        +---------+--------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return f.transform(\n        f.filter(\n            cohort_min_allele_frequency,\n            lambda left: f.exists(\n                n_samples_per_cohort,\n                lambda right: right.getField(\"cohort\") == left.getField(\"cohort\"),\n            ),\n        ),\n        lambda left: f.struct(\n            left.getField(\"cohort\").alias(\"cohort\"),\n            mac(\n                left.getField(\"minAlleleFrequency\"),\n                f.filter(\n                    n_samples_per_cohort,\n                    lambda right: right.getField(\"cohort\")\n                    == left.getField(\"cohort\"),\n                )\n                .getItem(0)\n                .getField(\"nSamples\"),\n            ).alias(\"minAlleleCount\"),\n        ),\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.min_allele_frequency","title":"<code>min_allele_frequency(allele_freq: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Minor Allele Frequency (MAF) per cohort.</p> Note <p>The resulting value is of DecimalType(11, 10) to ensure precision for low frequency variants.</p> <p>Parameters:</p> Name Type Description Default <code>allele_freq</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>alleleFrequency</code> fields.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Column containing array of structs with <code>cohort</code> and <code>minAlleleFrequency</code> fields.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"v1\", [{\"cohort\": \"A\", \"alleleFrequency\": 0.1}, {\"cohort\": \"B\", \"alleleFrequency\": 0.7}]),]\n&gt;&gt;&gt; schema = \"variantId STRING, alleleFrequencies ARRAY&lt;STRUCT&lt;cohort: STRING, alleleFrequency: DOUBLE&gt;&gt;\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+---------+--------------------+\n|variantId|alleleFrequencies   |\n+---------+--------------------+\n|v1       |[{A, 0.1}, {B, 0.7}]|\n+---------+--------------------+\n</code></pre> <pre><code>&gt;&gt;&gt; df = df.withColumn(\"cohortMinAlleleFrequency\", FinnGenUkbMvpMetaSummaryStatistics.min_allele_frequency(f.col(\"alleleFrequencies\")))\n&gt;&gt;&gt; df.show(truncate=False)\n+---------+--------------------+--------------------------------------+\n|variantId|alleleFrequencies   |cohortMinAlleleFrequency              |\n+---------+--------------------+--------------------------------------+\n|v1       |[{A, 0.1}, {B, 0.7}]|[{A, 0.1000000000}, {B, 0.3000000000}]|\n+---------+--------------------+--------------------------------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef min_allele_frequency(cls, allele_freq: Column) -&gt; Column:\n    \"\"\"Minor Allele Frequency (MAF) per cohort.\n\n    Note:\n        The resulting value is of DecimalType(11, 10) to ensure precision for low frequency variants.\n\n    Args:\n        allele_freq (Column): Column containing array of structs with `cohort` and `alleleFrequency` fields.\n\n    Returns:\n        Column: Column containing array of structs with `cohort` and `minAlleleFrequency` fields.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"v1\", [{\"cohort\": \"A\", \"alleleFrequency\": 0.1}, {\"cohort\": \"B\", \"alleleFrequency\": 0.7}]),]\n        &gt;&gt;&gt; schema = \"variantId STRING, alleleFrequencies ARRAY&lt;STRUCT&lt;cohort: STRING, alleleFrequency: DOUBLE&gt;&gt;\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------+--------------------+\n        |variantId|alleleFrequencies   |\n        +---------+--------------------+\n        |v1       |[{A, 0.1}, {B, 0.7}]|\n        +---------+--------------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df = df.withColumn(\"cohortMinAlleleFrequency\", FinnGenUkbMvpMetaSummaryStatistics.min_allele_frequency(f.col(\"alleleFrequencies\")))\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------+--------------------+--------------------------------------+\n        |variantId|alleleFrequencies   |cohortMinAlleleFrequency              |\n        +---------+--------------------+--------------------------------------+\n        |v1       |[{A, 0.1}, {B, 0.7}]|[{A, 0.1000000000}, {B, 0.3000000000}]|\n        +---------+--------------------+--------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.transform(\n        allele_freq,\n        lambda x: f.struct(\n            x.getField(\"cohort\").alias(\"cohort\"),\n            maf(x.getField(\"alleleFrequency\")).alias(\"minAlleleFrequency\"),\n        ),\n    )\n</code></pre>"},{"location":"python_api/datasources/finngen_meta/summary_stats/#gentropy.datasource.finngen_meta.summary_statistics.FinnGenUkbMvpMetaSummaryStatistics.normalize_af","title":"<code>normalize_af(af: Column, flip: Column) -&gt; Column</code>  <code>classmethod</code>","text":"<p>Normalize allele frequency based on variant direction.</p> <p>Parameters:</p> Name Type Description Default <code>af</code> <code>Column</code> <p>Allele frequency column.</p> required <code>flip</code> <code>Column</code> <p>Direction column indicating if the allele needs to be flipped.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>Normalized allele frequency.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"v1\", 0.1, 1),\n...       (\"v2\", 0.2, -1),\n...       (\"v3\", None, -1),\n...       (\"V4\", 0.1, None),]\n&gt;&gt;&gt; schema = \"variantId STRING, af DOUBLE, flip INT\"\n&gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n&gt;&gt;&gt; df.show(truncate=False)\n+---------+----+----+\n|variantId|af  |flip|\n+---------+----+----+\n|v1       |0.1 |1   |\n|v2       |0.2 |-1  |\n|v3       |NULL|-1  |\n|V4       |0.1 |NULL|\n+---------+----+----+\n</code></pre> <pre><code>&gt;&gt;&gt; df = df.withColumn(\"normalizedAf\", FinnGenUkbMvpMetaSummaryStatistics.normalize_af(f.col(\"af\"), f.col(\"flip\")))\n&gt;&gt;&gt; df.select(\"variantId\", \"normalizedAf\").show(truncate=False)\n+---------+------------+\n|variantId|normalizedAf|\n+---------+------------+\n|v1       |0.1         |\n|v2       |0.8         |\n|v3       |NULL        |\n|V4       |0.1         |\n+---------+------------+\n</code></pre> Source code in <code>src/gentropy/datasource/finngen_meta/summary_statistics.py</code> <pre><code>@classmethod\ndef normalize_af(cls, af: Column, flip: Column) -&gt; Column:\n    \"\"\"Normalize allele frequency based on variant direction.\n\n    Args:\n        af (Column): Allele frequency column.\n        flip (Column): Direction column indicating if the allele needs to be flipped.\n\n    Returns:\n        Column: Normalized allele frequency.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"v1\", 0.1, 1),\n        ...       (\"v2\", 0.2, -1),\n        ...       (\"v3\", None, -1),\n        ...       (\"V4\", 0.1, None),]\n        &gt;&gt;&gt; schema = \"variantId STRING, af DOUBLE, flip INT\"\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---------+----+----+\n        |variantId|af  |flip|\n        +---------+----+----+\n        |v1       |0.1 |1   |\n        |v2       |0.2 |-1  |\n        |v3       |NULL|-1  |\n        |V4       |0.1 |NULL|\n        +---------+----+----+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df = df.withColumn(\"normalizedAf\", FinnGenUkbMvpMetaSummaryStatistics.normalize_af(f.col(\"af\"), f.col(\"flip\")))\n        &gt;&gt;&gt; df.select(\"variantId\", \"normalizedAf\").show(truncate=False)\n        +---------+------------+\n        |variantId|normalizedAf|\n        +---------+------------+\n        |v1       |0.1         |\n        |v2       |0.8         |\n        |v3       |NULL        |\n        |V4       |0.1         |\n        +---------+------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.when((flip == -1) &amp; af.isNotNull(), f.lit(1.0) - af).otherwise(af)\n</code></pre>"},{"location":"python_api/datasources/gnomad/_gnomad/","title":"GnomAD","text":"<p>GnomAD (Genome Aggregation Database) is a comprehensive resource that provides aggregated genomic data from large-scale sequencing projects. It encompasses variants from diverse populations and is widely used for variant annotation and population genetics studies.</p> <p>We use GnomAD v4.0 as a source for variant annotation, offering detailed information about the prevalence and distribution of genetic variants across different populations. This version of GnomAD provides valuable insights into the genomic landscape, aiding in the interpretation of genetic variants and their potential functional implications.</p> <p>Additionally, GnomAD v2.1.1 is utilized as a source for linkage disequilibrium (LD) information.</p>"},{"location":"python_api/datasources/gnomad/gnomad_ld/","title":"LD Matrix","text":""},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix","title":"<code>gentropy.datasource.gnomad.ld.GnomADLDMatrix</code>","text":"<p>Toolset ot interact with GnomAD LD dataset (version: r2.1.1).</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>class GnomADLDMatrix:\n    \"\"\"Toolset ot interact with GnomAD LD dataset (version: r2.1.1).\"\"\"\n\n    def __init__(\n        self,\n        ld_matrix_template: str = LDIndexConfig().ld_matrix_template,\n        ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template,\n        grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path,\n        ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations,\n        liftover_ht_path: str = LDIndexConfig().liftover_ht_path,\n    ):\n        \"\"\"Initialize.\n\n        Datasets are accessed in Hail's native format, as provided by the [GnomAD consortium](https://gnomad.broadinstitute.org/downloads/#v2-linkage-disequilibrium).\n\n        Args:\n            ld_matrix_template (str): Template for the LD matrix path.\n            ld_index_raw_template (str): Template for the LD index path.\n            grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates.\n            ld_populations (list[LD_Population | str]): List of populations to use to build the LDIndex.\n            liftover_ht_path (str): Path to the liftover ht file.\n\n        Default values are set in LDIndexConfig.\n        \"\"\"\n        self.ld_matrix_template = ld_matrix_template\n        self.ld_index_raw_template = ld_index_raw_template\n        self.grch37_to_grch38_chain_path = grch37_to_grch38_chain_path\n        self.ld_populations = ld_populations\n        self.liftover_ht_path = liftover_ht_path\n\n    @staticmethod\n    def _aggregate_ld_index_across_populations(\n        unaggregated_ld_index: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"Aggregate LDIndex across populations.\n\n        Args:\n            unaggregated_ld_index (DataFrame): Unaggregate LDIndex index dataframe  each row is a variant pair in a population\n\n        Returns:\n            DataFrame: Aggregated LDIndex index dataframe  each row is a variant with the LD set across populations\n\n        Examples:\n            &gt;&gt;&gt; data = [(\"1.0\", \"var1\", \"X\", \"var1\", \"pop1\"), (\"1.0\", \"X\", \"var2\", \"var2\", \"pop1\"),\n            ...         (\"0.5\", \"var1\", \"X\", \"var2\", \"pop1\"), (\"0.5\", \"var1\", \"X\", \"var2\", \"pop2\"),\n            ...         (\"0.5\", \"var2\", \"X\", \"var1\", \"pop1\"), (\"0.5\", \"X\", \"var2\", \"var1\", \"pop2\")]\n            &gt;&gt;&gt; df = spark.createDataFrame(data, [\"r\", \"variantId\", \"chromosome\", \"tagvariantId\", \"population\"])\n            &gt;&gt;&gt; GnomADLDMatrix._aggregate_ld_index_across_populations(df).printSchema()\n            root\n             |-- variantId: string (nullable = true)\n             |-- chromosome: string (nullable = true)\n             |-- ldSet: array (nullable = false)\n             |    |-- element: struct (containsNull = false)\n             |    |    |-- tagVariantId: string (nullable = true)\n             |    |    |-- rValues: array (nullable = false)\n             |    |    |    |-- element: struct (containsNull = false)\n             |    |    |    |    |-- population: string (nullable = true)\n             |    |    |    |    |-- r: string (nullable = true)\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return (\n            unaggregated_ld_index\n            # First level of aggregation: get r/population for each variant/tagVariant pair\n            .withColumn(\"r_pop_struct\", f.struct(\"population\", \"r\"))\n            .groupBy(\"chromosome\", \"variantId\", \"tagVariantId\")\n            .agg(\n                f.collect_set(\"r_pop_struct\").alias(\"rValues\"),\n            )\n            # Second level of aggregation: get r/population for each variant\n            .withColumn(\"r_pop_tag_struct\", f.struct(\"tagVariantId\", \"rValues\"))\n            .groupBy(\"variantId\", \"chromosome\")\n            .agg(\n                f.collect_set(\"r_pop_tag_struct\").alias(\"ldSet\"),\n            )\n        )\n\n    @staticmethod\n    def _convert_ld_matrix_to_table(\n        block_matrix: BlockMatrix, min_r2: float\n    ) -&gt; DataFrame:\n        \"\"\"Convert LD matrix to table.\n\n        Args:\n            block_matrix (BlockMatrix): LD matrix\n            min_r2 (float): Minimum r2 value to keep in the table\n\n        Returns:\n            DataFrame: LD matrix as a Spark DataFrame\n        \"\"\"\n        table = block_matrix.entries(keyed=False)\n        return (\n            table.filter(hl.abs(table.entry) &gt;= min_r2**0.5)\n            .to_spark()\n            .withColumnRenamed(\"entry\", \"r\")\n        )\n\n    @staticmethod\n    def _create_ldindex_for_population(\n        population_id: str,\n        ld_matrix_path: str,\n        ld_index_raw_path: str,\n        grch37_to_grch38_chain_path: str,\n        min_r2: float,\n    ) -&gt; DataFrame:\n        \"\"\"Create LDIndex for a specific population.\n\n        Args:\n            population_id (str): Population ID\n            ld_matrix_path (str): Path to the LD matrix\n            ld_index_raw_path (str): Path to the LD index\n            grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates\n            min_r2 (float): Minimum r2 value to keep in the table\n\n        Returns:\n            DataFrame: LDIndex for a specific population\n        \"\"\"\n        # Prepare LD Block matrix\n        ld_matrix = GnomADLDMatrix._convert_ld_matrix_to_table(\n            BlockMatrix.read(ld_matrix_path), min_r2\n        )\n\n        # Prepare table with variant indices\n        ld_index = GnomADLDMatrix._process_variant_indices(\n            hl.read_table(ld_index_raw_path),\n            grch37_to_grch38_chain_path,\n        )\n\n        return GnomADLDMatrix._resolve_variant_indices(ld_index, ld_matrix).select(\n            \"*\",\n            f.lit(population_id).alias(\"population\"),\n        )\n\n    @staticmethod\n    def _process_variant_indices(\n        ld_index_raw: hl.Table, grch37_to_grch38_chain_path: str\n    ) -&gt; DataFrame:\n        \"\"\"Creates a look up table between variants and their coordinates in the LD Matrix.\n\n        !!! info \"Gnomad's LD Matrix and Index are based on GRCh37 coordinates. This function will lift over the coordinates to GRCh38 to build the lookup table.\"\n\n        Args:\n            ld_index_raw (hl.Table): LD index table from GnomAD\n            grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates\n\n        Returns:\n            DataFrame: Look up table between variants in build hg38 and their coordinates in the LD Matrix\n        \"\"\"\n        ld_index_38 = liftover_loci(ld_index_raw, grch37_to_grch38_chain_path, \"GRCh38\")\n\n        return (\n            ld_index_38.to_spark()\n            # Filter out variants where the liftover failed\n            .filter(f.col(\"`locus_GRCh38.position`\").isNotNull())\n            .select(\n                f.regexp_replace(\"`locus_GRCh38.contig`\", \"chr\", \"\").alias(\n                    \"chromosome\"\n                ),\n                f.col(\"`locus_GRCh38.position`\").alias(\"position\"),\n                f.concat_ws(\n                    \"_\",\n                    f.regexp_replace(\"`locus_GRCh38.contig`\", \"chr\", \"\"),\n                    f.col(\"`locus_GRCh38.position`\"),\n                    f.col(\"`alleles`\").getItem(0),\n                    f.col(\"`alleles`\").getItem(1),\n                ).alias(\"variantId\"),\n                f.col(\"idx\"),\n            )\n            # Filter out ambiguous liftover results: multiple indices for the same variant\n            .withColumn(\"count\", f.count(\"*\").over(Window.partitionBy([\"variantId\"])))\n            .filter(f.col(\"count\") == 1)\n            .drop(\"count\")\n        )\n\n    @staticmethod\n    def _resolve_variant_indices(\n        ld_index: DataFrame, ld_matrix: DataFrame\n    ) -&gt; DataFrame:\n        \"\"\"Resolve the `i` and `j` indices of the block matrix to variant IDs (build 38).\n\n        Args:\n            ld_index (DataFrame): Dataframe with resolved variant indices\n            ld_matrix (DataFrame): Dataframe with the filtered LD matrix\n\n        Returns:\n            DataFrame: Dataframe with variant IDs instead of `i` and `j` indices\n        \"\"\"\n        ld_index_i = ld_index.selectExpr(\n            \"idx as i\", \"variantId as variantIdI\", \"chromosome\"\n        )\n        ld_index_j = ld_index.selectExpr(\"idx as j\", \"variantId as variantIdJ\")\n        return (\n            ld_matrix.join(ld_index_i, on=\"i\", how=\"inner\")\n            .join(ld_index_j, on=\"j\", how=\"inner\")\n            .drop(\"i\", \"j\")\n        )\n\n    @staticmethod\n    def _transpose_ld_matrix(ld_matrix: DataFrame) -&gt; DataFrame:\n        \"\"\"Transpose LD matrix to a square matrix format.\n\n        Args:\n            ld_matrix (DataFrame): Triangular LD matrix converted to a Spark DataFrame\n\n        Returns:\n            DataFrame: Square LD matrix without diagonal duplicates\n\n        Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     [\n        ...         (1, 1, 1.0, \"1\", \"AFR\"),\n        ...         (1, 2, 0.5, \"1\", \"AFR\"),\n        ...         (2, 2, 1.0, \"1\", \"AFR\"),\n        ...     ],\n        ...     [\"variantIdI\", \"variantIdJ\", \"r\", \"chromosome\", \"population\"],\n        ... )\n        &gt;&gt;&gt; GnomADLDMatrix._transpose_ld_matrix(df).show()\n        +----------+----------+---+----------+----------+\n        |variantIdI|variantIdJ|  r|chromosome|population|\n        +----------+----------+---+----------+----------+\n        |         1|         2|0.5|         1|       AFR|\n        |         1|         1|1.0|         1|       AFR|\n        |         2|         1|0.5|         1|       AFR|\n        |         2|         2|1.0|         1|       AFR|\n        +----------+----------+---+----------+----------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        ld_matrix_transposed = ld_matrix.selectExpr(\n            \"variantIdI as variantIdJ\",\n            \"variantIdJ as variantIdI\",\n            \"r\",\n            \"chromosome\",\n            \"population\",\n        )\n        return ld_matrix.filter(f.col(\"variantIdI\") != f.col(\"variantIdJ\")).unionByName(\n            ld_matrix_transposed\n        )\n\n    def as_ld_index(\n        self: GnomADLDMatrix,\n        min_r2: float,\n    ) -&gt; LDIndex:\n        \"\"\"Create LDIndex dataset aggregating the LD information across a set of populations.\n\n        **The basic steps to generate the LDIndex are:**\n\n        1. Convert LD matrix to a Spark DataFrame.\n        2. Resolve the matrix indices to variant IDs by lifting over the coordinates to GRCh38.\n        3. Aggregate the LD information across populations.\n\n        Args:\n            min_r2 (float): Minimum r2 value to keep in the table\n\n        Returns:\n            LDIndex: LDIndex dataset\n        \"\"\"\n        ld_indices_unaggregated = []\n        for pop in self.ld_populations:\n            try:\n                ld_matrix_path = self.ld_matrix_template.format(POP=pop)\n                ld_index_raw_path = self.ld_index_raw_template.format(POP=pop)\n                pop_ld_index = self._create_ldindex_for_population(\n                    pop,\n                    ld_matrix_path,\n                    ld_index_raw_path.format(pop),\n                    self.grch37_to_grch38_chain_path,\n                    min_r2,\n                )\n                ld_indices_unaggregated.append(pop_ld_index)\n            except Exception as e:\n                print(f\"Failed to create LDIndex for population {pop}: {e}\")  # noqa: T201\n                sys.exit(1)\n\n        ld_index_unaggregated = (\n            GnomADLDMatrix._transpose_ld_matrix(\n                reduce(lambda df1, df2: df1.unionByName(df2), ld_indices_unaggregated)\n            )\n            .withColumnRenamed(\"variantIdI\", \"variantId\")\n            .withColumnRenamed(\"variantIdJ\", \"tagVariantId\")\n        )\n        return LDIndex(\n            _df=self._aggregate_ld_index_across_populations(ld_index_unaggregated),\n            _schema=LDIndex.get_schema(),\n        )\n\n    def get_ld_variants(\n        self: GnomADLDMatrix,\n        gnomad_ancestry: str,\n        chromosome: str,\n        start: int,\n        end: int,\n    ) -&gt; DataFrame | None:\n        \"\"\"Return melted LD table with resolved variant id based on ancestry and genomic location.\n\n        Args:\n            gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n            chromosome (str): chromosome label\n            start (int): window upper bound\n            end (int): window lower bound\n\n        Returns:\n            DataFrame | None: LD table with resolved variant id based on ancestry and genomic location\n        \"\"\"\n        # Extracting locus:\n        ld_index_df = (\n            self._process_variant_indices(\n                hl.read_table(self.ld_index_raw_template.format(POP=gnomad_ancestry)),\n                self.grch37_to_grch38_chain_path,\n            )\n            .filter(\n                (f.col(\"chromosome\") == chromosome)\n                &amp; (f.col(\"position\") &gt;= start)\n                &amp; (f.col(\"position\") &lt;= end)\n            )\n            .select(\"chromosome\", \"position\", \"variantId\", \"idx\")\n        )\n\n        if ld_index_df.limit(1).count() == 0:\n            # If the returned slice from the ld index is empty, return None\n            return None\n\n        # Compute start and end indices\n        start_index = get_value_from_row(\n            get_top_ranked_in_window(\n                ld_index_df, Window.partitionBy().orderBy(f.col(\"position\").asc())\n            ).collect()[0],\n            \"idx\",\n        )\n        end_index = get_value_from_row(\n            get_top_ranked_in_window(\n                ld_index_df, Window.partitionBy().orderBy(f.col(\"position\").desc())\n            ).collect()[0],\n            \"idx\",\n        )\n\n        return self._extract_square_matrix(\n            ld_index_df, gnomad_ancestry, start_index, end_index\n        )\n\n    def _extract_square_matrix(\n        self: GnomADLDMatrix,\n        ld_index_df: DataFrame,\n        gnomad_ancestry: str,\n        start_index: int,\n        end_index: int,\n    ) -&gt; DataFrame:\n        \"\"\"Return LD square matrix for a region where coordinates are normalised.\n\n        Args:\n            ld_index_df (DataFrame): Look up table between a variantId and its index in the LD matrix\n            gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n            start_index (int): start index of the slice\n            end_index (int): end index of the slice\n\n        Returns:\n            DataFrame: square LD matrix resolved to variants.\n        \"\"\"\n        return (\n            self.get_ld_matrix_slice(\n                gnomad_ancestry, start_index=start_index, end_index=end_index\n            )\n            .join(\n                ld_index_df.select(\n                    f.col(\"idx\").alias(\"idx_i\"),\n                    f.col(\"variantId\").alias(\"variantIdI\"),\n                ),\n                on=\"idx_i\",\n                how=\"inner\",\n            )\n            .join(\n                ld_index_df.select(\n                    f.col(\"idx\").alias(\"idx_j\"),\n                    f.col(\"variantId\").alias(\"variantIdJ\"),\n                ),\n                on=\"idx_j\",\n                how=\"inner\",\n            )\n            .select(\"variantIdI\", \"variantIdJ\", \"r\")\n        )\n\n    def get_ld_matrix_slice(\n        self: GnomADLDMatrix,\n        gnomad_ancestry: str,\n        start_index: int,\n        end_index: int,\n    ) -&gt; DataFrame:\n        \"\"\"Extract a slice of the LD matrix based on the provided ancestry and stop and end indices.\n\n        - The half matrix is completed into a full square.\n        - The returned indices are adjusted based on the start index.\n\n        Args:\n            gnomad_ancestry (str): LD population label eg. `nfe`\n            start_index (int): start index of the slice\n            end_index (int): end index of the slice\n\n        Returns:\n            DataFrame: square slice of the LD matrix melted as dataframe with idx_i, idx_j and r columns\n        \"\"\"\n        # Extracting block matrix slice:\n        half_matrix = BlockMatrix.read(\n            self.ld_matrix_template.format(POP=gnomad_ancestry)\n        ).filter(range(start_index, end_index + 1), range(start_index, end_index + 1))\n\n        # Return converted Dataframe:\n        return (\n            (half_matrix + half_matrix.T)\n            .entries()\n            .to_spark()\n            .select(\n                (f.col(\"i\") + start_index).alias(\"idx_i\"),\n                (f.col(\"j\") + start_index).alias(\"idx_j\"),\n                f.when(f.col(\"i\") == f.col(\"j\"), f.col(\"entry\") / 2)\n                .otherwise(f.col(\"entry\"))\n                .alias(\"r\"),\n            )\n        )\n\n    def get_locus_index(\n        self: GnomADLDMatrix,\n        study_locus_row: Row,\n        radius: int = 500_000,\n        major_population: str = \"nfe\",\n    ) -&gt; DataFrame:\n        \"\"\"Extract hail matrix index from StudyLocus rows.\n\n        Args:\n            study_locus_row (Row): Study-locus row\n            radius (int): Locus radius to extract from gnomad matrix\n            major_population (str): Major population to extract from gnomad matrix, default is \"nfe\"\n\n        Returns:\n            DataFrame: Returns the index of the gnomad matrix for the locus\n\n        \"\"\"\n        chromosome = str(\"chr\" + study_locus_row[\"chromosome\"])\n        start = study_locus_row[\"position\"] - radius\n        end = study_locus_row[\"position\"] + radius\n\n        liftover_ht = hl.read_table(self.liftover_ht_path)\n        liftover_ht = (\n            liftover_ht.filter(\n                (liftover_ht.locus.contig == chromosome)\n                &amp; (liftover_ht.locus.position &gt;= start)\n                &amp; (liftover_ht.locus.position &lt;= end)\n            )\n            .key_by()\n            .select(\"locus\", \"alleles\", \"original_locus\")\n            .key_by(\"original_locus\", \"alleles\")\n            .naive_coalesce(20)\n        )\n\n        hail_index = hl.read_table(\n            self.ld_index_raw_template.format(POP=major_population)\n        )\n\n        joined_index = (\n            liftover_ht.join(hail_index, how=\"inner\").order_by(\"idx\").to_spark()\n        )\n\n        return joined_index\n\n    def get_numpy_matrix(\n        self: GnomADLDMatrix,\n        locus_index: DataFrame,\n        gnomad_ancestry: str = \"nfe\",\n    ) -&gt; np.ndarray:\n        \"\"\"Extract the LD block matrix for a locus.\n\n        Args:\n            locus_index (DataFrame): hail matrix variant index table\n            gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n\n        Returns:\n            np.ndarray: LD block matrix for the locus\n        \"\"\"\n        idx = [row[\"idx\"] for row in locus_index.select(\"idx\").collect()]\n\n        half_matrix = (\n            BlockMatrix.read(\n                self.ld_matrix_template.format(POP=gnomad_ancestry)\n            )\n            .filter(idx, idx)\n            .to_numpy()\n        )\n\n        return (half_matrix + half_matrix.T) - np.diag(np.diag(half_matrix))\n\n    def get_locus_index_boundaries(\n        self: GnomADLDMatrix,\n        study_locus_row: Row,\n        major_population: str = \"nfe\",\n    ) -&gt; DataFrame:\n        \"\"\"Extract hail matrix index from StudyLocus rows.\n\n        Args:\n            study_locus_row (Row): Study-locus row\n            major_population (str): Major population to extract from gnomad matrix, default is \"nfe\"\n\n        Returns:\n            DataFrame: Returns the index of the gnomad matrix for the locus\n\n        \"\"\"\n        chromosome = str(\"chr\" + study_locus_row[\"chromosome\"])\n        start = int(study_locus_row[\"locusStart\"])\n        end = int(study_locus_row[\"locusEnd\"])\n\n        liftover_ht = hl.read_table(self.liftover_ht_path)\n        liftover_ht = self._filter_liftover_by_locus(\n            liftover_ht, chromosome, start, end\n        )\n\n        hail_index = hl.read_table(\n            self.ld_index_raw_template.format(POP=major_population)\n        )\n\n        joined_index = (\n            liftover_ht.join(hail_index, how=\"inner\").order_by(\"idx\").to_spark()\n        )\n\n        return joined_index\n\n    def _filter_liftover_by_locus(\n        self,\n        liftover_ht: hl.Table,\n        chromosome: str,\n        start: int,\n        end: int\n        ) -&gt; hl.Table:\n        liftover_ht = (\n            liftover_ht.filter(\n                (liftover_ht.locus.contig == chromosome)\n                &amp; (liftover_ht.locus.position &gt;= start)\n                &amp; (liftover_ht.locus.position &lt;= end)\n            )\n            .key_by()\n            .select(\"locus\", \"alleles\", \"original_locus\")\n            .key_by(\"original_locus\", \"alleles\")\n            .naive_coalesce(20)\n        )\n\n        return liftover_ht\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.__init__","title":"<code>__init__(ld_matrix_template: str = LDIndexConfig().ld_matrix_template, ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template, grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path, ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations, liftover_ht_path: str = LDIndexConfig().liftover_ht_path)</code>","text":"<p>Initialize.</p> <p>Datasets are accessed in Hail's native format, as provided by the GnomAD consortium.</p> <p>Parameters:</p> Name Type Description Default <code>ld_matrix_template</code> <code>str</code> <p>Template for the LD matrix path.</p> <code>ld_matrix_template</code> <code>ld_index_raw_template</code> <code>str</code> <p>Template for the LD index path.</p> <code>ld_index_raw_template</code> <code>grch37_to_grch38_chain_path</code> <code>str</code> <p>Path to the chain file used to lift over the coordinates.</p> <code>grch37_to_grch38_chain_path</code> <code>ld_populations</code> <code>list[LD_Population | str]</code> <p>List of populations to use to build the LDIndex.</p> <code>ld_populations</code> <code>liftover_ht_path</code> <code>str</code> <p>Path to the liftover ht file.</p> <code>liftover_ht_path</code> <p>Default values are set in LDIndexConfig.</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def __init__(\n    self,\n    ld_matrix_template: str = LDIndexConfig().ld_matrix_template,\n    ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template,\n    grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path,\n    ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations,\n    liftover_ht_path: str = LDIndexConfig().liftover_ht_path,\n):\n    \"\"\"Initialize.\n\n    Datasets are accessed in Hail's native format, as provided by the [GnomAD consortium](https://gnomad.broadinstitute.org/downloads/#v2-linkage-disequilibrium).\n\n    Args:\n        ld_matrix_template (str): Template for the LD matrix path.\n        ld_index_raw_template (str): Template for the LD index path.\n        grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates.\n        ld_populations (list[LD_Population | str]): List of populations to use to build the LDIndex.\n        liftover_ht_path (str): Path to the liftover ht file.\n\n    Default values are set in LDIndexConfig.\n    \"\"\"\n    self.ld_matrix_template = ld_matrix_template\n    self.ld_index_raw_template = ld_index_raw_template\n    self.grch37_to_grch38_chain_path = grch37_to_grch38_chain_path\n    self.ld_populations = ld_populations\n    self.liftover_ht_path = liftover_ht_path\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.as_ld_index","title":"<code>as_ld_index(min_r2: float) -&gt; LDIndex</code>","text":"<p>Create LDIndex dataset aggregating the LD information across a set of populations.</p> <p>The basic steps to generate the LDIndex are:</p> <ol> <li>Convert LD matrix to a Spark DataFrame.</li> <li>Resolve the matrix indices to variant IDs by lifting over the coordinates to GRCh38.</li> <li>Aggregate the LD information across populations.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>min_r2</code> <code>float</code> <p>Minimum r2 value to keep in the table</p> required <p>Returns:</p> Name Type Description <code>LDIndex</code> <code>LDIndex</code> <p>LDIndex dataset</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def as_ld_index(\n    self: GnomADLDMatrix,\n    min_r2: float,\n) -&gt; LDIndex:\n    \"\"\"Create LDIndex dataset aggregating the LD information across a set of populations.\n\n    **The basic steps to generate the LDIndex are:**\n\n    1. Convert LD matrix to a Spark DataFrame.\n    2. Resolve the matrix indices to variant IDs by lifting over the coordinates to GRCh38.\n    3. Aggregate the LD information across populations.\n\n    Args:\n        min_r2 (float): Minimum r2 value to keep in the table\n\n    Returns:\n        LDIndex: LDIndex dataset\n    \"\"\"\n    ld_indices_unaggregated = []\n    for pop in self.ld_populations:\n        try:\n            ld_matrix_path = self.ld_matrix_template.format(POP=pop)\n            ld_index_raw_path = self.ld_index_raw_template.format(POP=pop)\n            pop_ld_index = self._create_ldindex_for_population(\n                pop,\n                ld_matrix_path,\n                ld_index_raw_path.format(pop),\n                self.grch37_to_grch38_chain_path,\n                min_r2,\n            )\n            ld_indices_unaggregated.append(pop_ld_index)\n        except Exception as e:\n            print(f\"Failed to create LDIndex for population {pop}: {e}\")  # noqa: T201\n            sys.exit(1)\n\n    ld_index_unaggregated = (\n        GnomADLDMatrix._transpose_ld_matrix(\n            reduce(lambda df1, df2: df1.unionByName(df2), ld_indices_unaggregated)\n        )\n        .withColumnRenamed(\"variantIdI\", \"variantId\")\n        .withColumnRenamed(\"variantIdJ\", \"tagVariantId\")\n    )\n    return LDIndex(\n        _df=self._aggregate_ld_index_across_populations(ld_index_unaggregated),\n        _schema=LDIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_ld_matrix_slice","title":"<code>get_ld_matrix_slice(gnomad_ancestry: str, start_index: int, end_index: int) -&gt; DataFrame</code>","text":"<p>Extract a slice of the LD matrix based on the provided ancestry and stop and end indices.</p> <ul> <li>The half matrix is completed into a full square.</li> <li>The returned indices are adjusted based on the start index.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>gnomad_ancestry</code> <code>str</code> <p>LD population label eg. <code>nfe</code></p> required <code>start_index</code> <code>int</code> <p>start index of the slice</p> required <code>end_index</code> <code>int</code> <p>end index of the slice</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>square slice of the LD matrix melted as dataframe with idx_i, idx_j and r columns</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def get_ld_matrix_slice(\n    self: GnomADLDMatrix,\n    gnomad_ancestry: str,\n    start_index: int,\n    end_index: int,\n) -&gt; DataFrame:\n    \"\"\"Extract a slice of the LD matrix based on the provided ancestry and stop and end indices.\n\n    - The half matrix is completed into a full square.\n    - The returned indices are adjusted based on the start index.\n\n    Args:\n        gnomad_ancestry (str): LD population label eg. `nfe`\n        start_index (int): start index of the slice\n        end_index (int): end index of the slice\n\n    Returns:\n        DataFrame: square slice of the LD matrix melted as dataframe with idx_i, idx_j and r columns\n    \"\"\"\n    # Extracting block matrix slice:\n    half_matrix = BlockMatrix.read(\n        self.ld_matrix_template.format(POP=gnomad_ancestry)\n    ).filter(range(start_index, end_index + 1), range(start_index, end_index + 1))\n\n    # Return converted Dataframe:\n    return (\n        (half_matrix + half_matrix.T)\n        .entries()\n        .to_spark()\n        .select(\n            (f.col(\"i\") + start_index).alias(\"idx_i\"),\n            (f.col(\"j\") + start_index).alias(\"idx_j\"),\n            f.when(f.col(\"i\") == f.col(\"j\"), f.col(\"entry\") / 2)\n            .otherwise(f.col(\"entry\"))\n            .alias(\"r\"),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_ld_variants","title":"<code>get_ld_variants(gnomad_ancestry: str, chromosome: str, start: int, end: int) -&gt; DataFrame | None</code>","text":"<p>Return melted LD table with resolved variant id based on ancestry and genomic location.</p> <p>Parameters:</p> Name Type Description Default <code>gnomad_ancestry</code> <code>str</code> <p>GnomAD major ancestry label eg. <code>nfe</code></p> required <code>chromosome</code> <code>str</code> <p>chromosome label</p> required <code>start</code> <code>int</code> <p>window upper bound</p> required <code>end</code> <code>int</code> <p>window lower bound</p> required <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>DataFrame | None: LD table with resolved variant id based on ancestry and genomic location</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def get_ld_variants(\n    self: GnomADLDMatrix,\n    gnomad_ancestry: str,\n    chromosome: str,\n    start: int,\n    end: int,\n) -&gt; DataFrame | None:\n    \"\"\"Return melted LD table with resolved variant id based on ancestry and genomic location.\n\n    Args:\n        gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n        chromosome (str): chromosome label\n        start (int): window upper bound\n        end (int): window lower bound\n\n    Returns:\n        DataFrame | None: LD table with resolved variant id based on ancestry and genomic location\n    \"\"\"\n    # Extracting locus:\n    ld_index_df = (\n        self._process_variant_indices(\n            hl.read_table(self.ld_index_raw_template.format(POP=gnomad_ancestry)),\n            self.grch37_to_grch38_chain_path,\n        )\n        .filter(\n            (f.col(\"chromosome\") == chromosome)\n            &amp; (f.col(\"position\") &gt;= start)\n            &amp; (f.col(\"position\") &lt;= end)\n        )\n        .select(\"chromosome\", \"position\", \"variantId\", \"idx\")\n    )\n\n    if ld_index_df.limit(1).count() == 0:\n        # If the returned slice from the ld index is empty, return None\n        return None\n\n    # Compute start and end indices\n    start_index = get_value_from_row(\n        get_top_ranked_in_window(\n            ld_index_df, Window.partitionBy().orderBy(f.col(\"position\").asc())\n        ).collect()[0],\n        \"idx\",\n    )\n    end_index = get_value_from_row(\n        get_top_ranked_in_window(\n            ld_index_df, Window.partitionBy().orderBy(f.col(\"position\").desc())\n        ).collect()[0],\n        \"idx\",\n    )\n\n    return self._extract_square_matrix(\n        ld_index_df, gnomad_ancestry, start_index, end_index\n    )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_locus_index","title":"<code>get_locus_index(study_locus_row: Row, radius: int = 500000, major_population: str = 'nfe') -&gt; DataFrame</code>","text":"<p>Extract hail matrix index from StudyLocus rows.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus_row</code> <code>Row</code> <p>Study-locus row</p> required <code>radius</code> <code>int</code> <p>Locus radius to extract from gnomad matrix</p> <code>500000</code> <code>major_population</code> <code>str</code> <p>Major population to extract from gnomad matrix, default is \"nfe\"</p> <code>'nfe'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Returns the index of the gnomad matrix for the locus</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def get_locus_index(\n    self: GnomADLDMatrix,\n    study_locus_row: Row,\n    radius: int = 500_000,\n    major_population: str = \"nfe\",\n) -&gt; DataFrame:\n    \"\"\"Extract hail matrix index from StudyLocus rows.\n\n    Args:\n        study_locus_row (Row): Study-locus row\n        radius (int): Locus radius to extract from gnomad matrix\n        major_population (str): Major population to extract from gnomad matrix, default is \"nfe\"\n\n    Returns:\n        DataFrame: Returns the index of the gnomad matrix for the locus\n\n    \"\"\"\n    chromosome = str(\"chr\" + study_locus_row[\"chromosome\"])\n    start = study_locus_row[\"position\"] - radius\n    end = study_locus_row[\"position\"] + radius\n\n    liftover_ht = hl.read_table(self.liftover_ht_path)\n    liftover_ht = (\n        liftover_ht.filter(\n            (liftover_ht.locus.contig == chromosome)\n            &amp; (liftover_ht.locus.position &gt;= start)\n            &amp; (liftover_ht.locus.position &lt;= end)\n        )\n        .key_by()\n        .select(\"locus\", \"alleles\", \"original_locus\")\n        .key_by(\"original_locus\", \"alleles\")\n        .naive_coalesce(20)\n    )\n\n    hail_index = hl.read_table(\n        self.ld_index_raw_template.format(POP=major_population)\n    )\n\n    joined_index = (\n        liftover_ht.join(hail_index, how=\"inner\").order_by(\"idx\").to_spark()\n    )\n\n    return joined_index\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_locus_index_boundaries","title":"<code>get_locus_index_boundaries(study_locus_row: Row, major_population: str = 'nfe') -&gt; DataFrame</code>","text":"<p>Extract hail matrix index from StudyLocus rows.</p> <p>Parameters:</p> Name Type Description Default <code>study_locus_row</code> <code>Row</code> <p>Study-locus row</p> required <code>major_population</code> <code>str</code> <p>Major population to extract from gnomad matrix, default is \"nfe\"</p> <code>'nfe'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Returns the index of the gnomad matrix for the locus</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def get_locus_index_boundaries(\n    self: GnomADLDMatrix,\n    study_locus_row: Row,\n    major_population: str = \"nfe\",\n) -&gt; DataFrame:\n    \"\"\"Extract hail matrix index from StudyLocus rows.\n\n    Args:\n        study_locus_row (Row): Study-locus row\n        major_population (str): Major population to extract from gnomad matrix, default is \"nfe\"\n\n    Returns:\n        DataFrame: Returns the index of the gnomad matrix for the locus\n\n    \"\"\"\n    chromosome = str(\"chr\" + study_locus_row[\"chromosome\"])\n    start = int(study_locus_row[\"locusStart\"])\n    end = int(study_locus_row[\"locusEnd\"])\n\n    liftover_ht = hl.read_table(self.liftover_ht_path)\n    liftover_ht = self._filter_liftover_by_locus(\n        liftover_ht, chromosome, start, end\n    )\n\n    hail_index = hl.read_table(\n        self.ld_index_raw_template.format(POP=major_population)\n    )\n\n    joined_index = (\n        liftover_ht.join(hail_index, how=\"inner\").order_by(\"idx\").to_spark()\n    )\n\n    return joined_index\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_ld/#gentropy.datasource.gnomad.ld.GnomADLDMatrix.get_numpy_matrix","title":"<code>get_numpy_matrix(locus_index: DataFrame, gnomad_ancestry: str = 'nfe') -&gt; np.ndarray</code>","text":"<p>Extract the LD block matrix for a locus.</p> <p>Parameters:</p> Name Type Description Default <code>locus_index</code> <code>DataFrame</code> <p>hail matrix variant index table</p> required <code>gnomad_ancestry</code> <code>str</code> <p>GnomAD major ancestry label eg. <code>nfe</code></p> <code>'nfe'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: LD block matrix for the locus</p> Source code in <code>src/gentropy/datasource/gnomad/ld.py</code> <pre><code>def get_numpy_matrix(\n    self: GnomADLDMatrix,\n    locus_index: DataFrame,\n    gnomad_ancestry: str = \"nfe\",\n) -&gt; np.ndarray:\n    \"\"\"Extract the LD block matrix for a locus.\n\n    Args:\n        locus_index (DataFrame): hail matrix variant index table\n        gnomad_ancestry (str): GnomAD major ancestry label eg. `nfe`\n\n    Returns:\n        np.ndarray: LD block matrix for the locus\n    \"\"\"\n    idx = [row[\"idx\"] for row in locus_index.select(\"idx\").collect()]\n\n    half_matrix = (\n        BlockMatrix.read(\n            self.ld_matrix_template.format(POP=gnomad_ancestry)\n        )\n        .filter(idx, idx)\n        .to_numpy()\n    )\n\n    return (half_matrix + half_matrix.T) - np.diag(np.diag(half_matrix))\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_variants/","title":"Variants","text":""},{"location":"python_api/datasources/gnomad/gnomad_variants/#gentropy.datasource.gnomad.variants.GnomADVariantFrequencies","title":"<code>gentropy.datasource.gnomad.variants.GnomADVariantFrequencies</code>","text":"<p>Extract GnomAD variants including allele frequencies.</p> Source code in <code>src/gentropy/datasource/gnomad/variants.py</code> <pre><code>class GnomADVariantFrequencies:\n    \"\"\"Extract GnomAD variants including allele frequencies.\"\"\"\n\n    def __init__(\n        self: GnomADVariantFrequencies,\n        gnomad_joint_path: str = GnomadVariantConfig().gnomad_joint_path,\n        gnomad_variant_populations: list[\n            VariantPopulation | str\n        ] = GnomadVariantConfig().gnomad_variant_populations,\n        hash_threshold: int = VariantIndexConfig().hash_threshold,\n    ):\n        \"\"\"Initialize.\n\n        Args:\n            gnomad_joint_path (str): Path to gnomAD \"joint\" hail table.\n            gnomad_variant_populations (list[VariantPopulation | str]): List of populations to include.\n            hash_threshold (int): longer variant ids will be hashed.\n\n        All defaults are stored in GnomadVariantConfig.\n        \"\"\"\n        self.gnomad_joint_path = gnomad_joint_path\n        self.gnomad_variant_populations = gnomad_variant_populations\n        self.lenght_threshold = hash_threshold\n\n    def as_variant_index(self: GnomADVariantFrequencies) -&gt; VariantIndex:\n        \"\"\"Generate variant annotation dataset from gnomAD.\n\n        Some relevant modifications to the original dataset are:\n\n        1. The transcript consequences features provided by VEP are filtered to only refer to the Ensembl canonical transcript.\n        2. Genome coordinates are liftovered from GRCh38 to GRCh37 to keep as annotation.\n        3. Field names are converted to camel case to follow the convention.\n\n        Returns:\n            VariantIndex: GnomaAD variants dataset.\n        \"\"\"\n        # Load variants dataset\n        ht = hl.read_table(\n            self.gnomad_joint_path,\n            _load_refs=False,\n        )\n\n        # Drop non biallelic variants\n        ht = ht.filter(ht.alleles.length() == 2)\n\n        # Select relevant fields and nested records to create class\n        return VariantIndex(\n            _df=(\n                ht.select(\n                    # Extract mandatory fields:\n                    variantId=hl.str(\"_\").join(\n                        [\n                            ht.locus.contig.replace(\"chr\", \"\"),\n                            hl.str(ht.locus.position),\n                            ht.alleles[0],\n                            ht.alleles[1],\n                        ]\n                    ),\n                    chromosome=ht.locus.contig.replace(\"chr\", \"\"),\n                    position=ht.locus.position,\n                    referenceAllele=ht.alleles[0],\n                    alternateAllele=ht.alleles[1],\n                    # Extract allele frequencies from populations of interest:\n                    alleleFrequencies=hl.set(\n                        [f\"{pop}_adj\" for pop in self.gnomad_variant_populations]\n                    ).map(\n                        lambda p: hl.struct(\n                            populationName=p,\n                            alleleFrequency=ht.joint.freq[\n                                ht.joint_globals.freq_index_dict[p]\n                            ].AF,\n                        )\n                    ),\n                    # Extract cross references to GnomAD:\n                    dbXrefs=hl.array(\n                        [\n                            hl.struct(\n                                id=hl.str(\"-\").join(\n                                    [\n                                        ht.locus.contig.replace(\"chr\", \"\"),\n                                        hl.str(ht.locus.position),\n                                        ht.alleles[0],\n                                        ht.alleles[1],\n                                    ]\n                                ),\n                                source=hl.str(\"gnomad\"),\n                            )\n                        ]\n                    ),\n                )\n                .key_by(\"chromosome\", \"position\")\n                .drop(\"locus\", \"alleles\")\n                .select_globals()\n                .to_spark(flatten=False)\n                .withColumns(\n                    {\n                        # Generate a variantId that is hashed for long variant ids:\n                        \"variantId\": VariantIndex.hash_long_variant_ids(\n                            f.col(\"variantId\"),\n                            f.col(\"chromosome\"),\n                            f.col(\"position\"),\n                            self.lenght_threshold,\n                        ),\n                        # We are not capturing the most severe consequence from GnomAD, but this column needed for the schema:\n                        \"mostSevereConsequenceId\": f.lit(None).cast(t.StringType()),\n                    }\n                )\n            ),\n            _schema=VariantIndex.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_variants/#gentropy.datasource.gnomad.variants.GnomADVariantFrequencies.__init__","title":"<code>__init__(gnomad_joint_path: str = GnomadVariantConfig().gnomad_joint_path, gnomad_variant_populations: list[VariantPopulation | str] = GnomadVariantConfig().gnomad_variant_populations, hash_threshold: int = VariantIndexConfig().hash_threshold)</code>","text":"<p>Initialize.</p> <p>Parameters:</p> Name Type Description Default <code>gnomad_joint_path</code> <code>str</code> <p>Path to gnomAD \"joint\" hail table.</p> <code>gnomad_joint_path</code> <code>gnomad_variant_populations</code> <code>list[VariantPopulation | str]</code> <p>List of populations to include.</p> <code>gnomad_variant_populations</code> <code>hash_threshold</code> <code>int</code> <p>longer variant ids will be hashed.</p> <code>hash_threshold</code> <p>All defaults are stored in GnomadVariantConfig.</p> Source code in <code>src/gentropy/datasource/gnomad/variants.py</code> <pre><code>def __init__(\n    self: GnomADVariantFrequencies,\n    gnomad_joint_path: str = GnomadVariantConfig().gnomad_joint_path,\n    gnomad_variant_populations: list[\n        VariantPopulation | str\n    ] = GnomadVariantConfig().gnomad_variant_populations,\n    hash_threshold: int = VariantIndexConfig().hash_threshold,\n):\n    \"\"\"Initialize.\n\n    Args:\n        gnomad_joint_path (str): Path to gnomAD \"joint\" hail table.\n        gnomad_variant_populations (list[VariantPopulation | str]): List of populations to include.\n        hash_threshold (int): longer variant ids will be hashed.\n\n    All defaults are stored in GnomadVariantConfig.\n    \"\"\"\n    self.gnomad_joint_path = gnomad_joint_path\n    self.gnomad_variant_populations = gnomad_variant_populations\n    self.lenght_threshold = hash_threshold\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_variants/#gentropy.datasource.gnomad.variants.GnomADVariantFrequencies.as_variant_index","title":"<code>as_variant_index() -&gt; VariantIndex</code>","text":"<p>Generate variant annotation dataset from gnomAD.</p> <p>Some relevant modifications to the original dataset are:</p> <ol> <li>The transcript consequences features provided by VEP are filtered to only refer to the Ensembl canonical transcript.</li> <li>Genome coordinates are liftovered from GRCh38 to GRCh37 to keep as annotation.</li> <li>Field names are converted to camel case to follow the convention.</li> </ol> <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>GnomaAD variants dataset.</p> Source code in <code>src/gentropy/datasource/gnomad/variants.py</code> <pre><code>def as_variant_index(self: GnomADVariantFrequencies) -&gt; VariantIndex:\n    \"\"\"Generate variant annotation dataset from gnomAD.\n\n    Some relevant modifications to the original dataset are:\n\n    1. The transcript consequences features provided by VEP are filtered to only refer to the Ensembl canonical transcript.\n    2. Genome coordinates are liftovered from GRCh38 to GRCh37 to keep as annotation.\n    3. Field names are converted to camel case to follow the convention.\n\n    Returns:\n        VariantIndex: GnomaAD variants dataset.\n    \"\"\"\n    # Load variants dataset\n    ht = hl.read_table(\n        self.gnomad_joint_path,\n        _load_refs=False,\n    )\n\n    # Drop non biallelic variants\n    ht = ht.filter(ht.alleles.length() == 2)\n\n    # Select relevant fields and nested records to create class\n    return VariantIndex(\n        _df=(\n            ht.select(\n                # Extract mandatory fields:\n                variantId=hl.str(\"_\").join(\n                    [\n                        ht.locus.contig.replace(\"chr\", \"\"),\n                        hl.str(ht.locus.position),\n                        ht.alleles[0],\n                        ht.alleles[1],\n                    ]\n                ),\n                chromosome=ht.locus.contig.replace(\"chr\", \"\"),\n                position=ht.locus.position,\n                referenceAllele=ht.alleles[0],\n                alternateAllele=ht.alleles[1],\n                # Extract allele frequencies from populations of interest:\n                alleleFrequencies=hl.set(\n                    [f\"{pop}_adj\" for pop in self.gnomad_variant_populations]\n                ).map(\n                    lambda p: hl.struct(\n                        populationName=p,\n                        alleleFrequency=ht.joint.freq[\n                            ht.joint_globals.freq_index_dict[p]\n                        ].AF,\n                    )\n                ),\n                # Extract cross references to GnomAD:\n                dbXrefs=hl.array(\n                    [\n                        hl.struct(\n                            id=hl.str(\"-\").join(\n                                [\n                                    ht.locus.contig.replace(\"chr\", \"\"),\n                                    hl.str(ht.locus.position),\n                                    ht.alleles[0],\n                                    ht.alleles[1],\n                                ]\n                            ),\n                            source=hl.str(\"gnomad\"),\n                        )\n                    ]\n                ),\n            )\n            .key_by(\"chromosome\", \"position\")\n            .drop(\"locus\", \"alleles\")\n            .select_globals()\n            .to_spark(flatten=False)\n            .withColumns(\n                {\n                    # Generate a variantId that is hashed for long variant ids:\n                    \"variantId\": VariantIndex.hash_long_variant_ids(\n                        f.col(\"variantId\"),\n                        f.col(\"chromosome\"),\n                        f.col(\"position\"),\n                        self.lenght_threshold,\n                    ),\n                    # We are not capturing the most severe consequence from GnomAD, but this column needed for the schema:\n                    \"mostSevereConsequenceId\": f.lit(None).cast(t.StringType()),\n                }\n            )\n        ),\n        _schema=VariantIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_variants/#gentropy.datasource.gnomad.variants.GnomADVariantRsIds","title":"<code>gentropy.datasource.gnomad.variants.GnomADVariantRsIds</code>","text":"<p>Extract GnomAD variants including variant Rs identifiers.</p> Source code in <code>src/gentropy/datasource/gnomad/variants.py</code> <pre><code>class GnomADVariantRsIds:\n    \"\"\"Extract GnomAD variants including variant Rs identifiers.\"\"\"\n\n    def __init__(\n        self: GnomADVariantRsIds,\n        gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path,\n        hash_threshold: int = VariantIndexConfig().hash_threshold,\n    ):\n        \"\"\"Initialize.\n\n        Args:\n            gnomad_genomes_path (str): Path to gnomAD genomes hail table.\n            hash_threshold (int): longer variant ids will be hashed.\n\n        All defaults are stored in GnomadVariantConfig.\n        \"\"\"\n        self.gnomad_genomes_path = gnomad_genomes_path\n        self.lenght_threshold = hash_threshold\n\n    def as_variant_index(self: GnomADVariantRsIds) -&gt; VariantIndex:\n        \"\"\"Generate variant annotation dataset from gnomAD.\n\n        Some relevant modifications to the original dataset are:\n\n        1. The transcript consequences features provided by VEP are filtered to only refer to the Ensembl canonical transcript.\n        2. Genome coordinates are liftovered from GRCh38 to GRCh37 to keep as annotation.\n        3. Field names are converted to camel case to follow the convention.\n\n        Returns:\n            VariantIndex: GnomaAD variants dataset.\n        \"\"\"\n        # Load variants dataset\n        ht = hl.read_table(\n            self.gnomad_genomes_path,\n            _load_refs=False,\n        )\n\n        # Drop non biallelic variants\n        ht = ht.filter(ht.alleles.length() == 2)\n\n        # Select relevant fields and nested records to create class\n        return VariantIndex(\n            _df=(\n                ht.select(\n                    # Extract mandatory fields:\n                    variantId=hl.str(\"_\").join(\n                        [\n                            ht.locus.contig.replace(\"chr\", \"\"),\n                            hl.str(ht.locus.position),\n                            ht.alleles[0],\n                            ht.alleles[1],\n                        ]\n                    ),\n                    chromosome=ht.locus.contig.replace(\"chr\", \"\"),\n                    position=ht.locus.position,\n                    referenceAllele=ht.alleles[0],\n                    alternateAllele=ht.alleles[1],\n                    # Extract rsIds:\n                    rsIds=ht.rsid,\n                    # Extract cross references to GnomAD:\n                    dbXrefs=hl.array(\n                        [\n                            hl.struct(\n                                id=hl.str(\"-\").join(\n                                    [\n                                        ht.locus.contig.replace(\"chr\", \"\"),\n                                        hl.str(ht.locus.position),\n                                        ht.alleles[0],\n                                        ht.alleles[1],\n                                    ]\n                                ),\n                                source=hl.str(\"gnomad\"),\n                            )\n                        ]\n                    ),\n                )\n                .key_by(\"chromosome\", \"position\")\n                .drop(\"locus\", \"alleles\")\n                .select_globals()\n                .to_spark(flatten=False)\n                .withColumns(\n                    {\n                        # Generate a variantId that is hashed for long variant ids:\n                        \"variantId\": VariantIndex.hash_long_variant_ids(\n                            f.col(\"variantId\"),\n                            f.col(\"chromosome\"),\n                            f.col(\"position\"),\n                            self.lenght_threshold,\n                        ),\n                        # We are not capturing the most severe consequence from GnomAD, but this column needed for the schema:\n                        \"mostSevereConsequenceId\": f.lit(None).cast(t.StringType()),\n                    }\n                )\n            ),\n            _schema=VariantIndex.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_variants/#gentropy.datasource.gnomad.variants.GnomADVariantRsIds.__init__","title":"<code>__init__(gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path, hash_threshold: int = VariantIndexConfig().hash_threshold)</code>","text":"<p>Initialize.</p> <p>Parameters:</p> Name Type Description Default <code>gnomad_genomes_path</code> <code>str</code> <p>Path to gnomAD genomes hail table.</p> <code>gnomad_genomes_path</code> <code>hash_threshold</code> <code>int</code> <p>longer variant ids will be hashed.</p> <code>hash_threshold</code> <p>All defaults are stored in GnomadVariantConfig.</p> Source code in <code>src/gentropy/datasource/gnomad/variants.py</code> <pre><code>def __init__(\n    self: GnomADVariantRsIds,\n    gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path,\n    hash_threshold: int = VariantIndexConfig().hash_threshold,\n):\n    \"\"\"Initialize.\n\n    Args:\n        gnomad_genomes_path (str): Path to gnomAD genomes hail table.\n        hash_threshold (int): longer variant ids will be hashed.\n\n    All defaults are stored in GnomadVariantConfig.\n    \"\"\"\n    self.gnomad_genomes_path = gnomad_genomes_path\n    self.lenght_threshold = hash_threshold\n</code></pre>"},{"location":"python_api/datasources/gnomad/gnomad_variants/#gentropy.datasource.gnomad.variants.GnomADVariantRsIds.as_variant_index","title":"<code>as_variant_index() -&gt; VariantIndex</code>","text":"<p>Generate variant annotation dataset from gnomAD.</p> <p>Some relevant modifications to the original dataset are:</p> <ol> <li>The transcript consequences features provided by VEP are filtered to only refer to the Ensembl canonical transcript.</li> <li>Genome coordinates are liftovered from GRCh38 to GRCh37 to keep as annotation.</li> <li>Field names are converted to camel case to follow the convention.</li> </ol> <p>Returns:</p> Name Type Description <code>VariantIndex</code> <code>VariantIndex</code> <p>GnomaAD variants dataset.</p> Source code in <code>src/gentropy/datasource/gnomad/variants.py</code> <pre><code>def as_variant_index(self: GnomADVariantRsIds) -&gt; VariantIndex:\n    \"\"\"Generate variant annotation dataset from gnomAD.\n\n    Some relevant modifications to the original dataset are:\n\n    1. The transcript consequences features provided by VEP are filtered to only refer to the Ensembl canonical transcript.\n    2. Genome coordinates are liftovered from GRCh38 to GRCh37 to keep as annotation.\n    3. Field names are converted to camel case to follow the convention.\n\n    Returns:\n        VariantIndex: GnomaAD variants dataset.\n    \"\"\"\n    # Load variants dataset\n    ht = hl.read_table(\n        self.gnomad_genomes_path,\n        _load_refs=False,\n    )\n\n    # Drop non biallelic variants\n    ht = ht.filter(ht.alleles.length() == 2)\n\n    # Select relevant fields and nested records to create class\n    return VariantIndex(\n        _df=(\n            ht.select(\n                # Extract mandatory fields:\n                variantId=hl.str(\"_\").join(\n                    [\n                        ht.locus.contig.replace(\"chr\", \"\"),\n                        hl.str(ht.locus.position),\n                        ht.alleles[0],\n                        ht.alleles[1],\n                    ]\n                ),\n                chromosome=ht.locus.contig.replace(\"chr\", \"\"),\n                position=ht.locus.position,\n                referenceAllele=ht.alleles[0],\n                alternateAllele=ht.alleles[1],\n                # Extract rsIds:\n                rsIds=ht.rsid,\n                # Extract cross references to GnomAD:\n                dbXrefs=hl.array(\n                    [\n                        hl.struct(\n                            id=hl.str(\"-\").join(\n                                [\n                                    ht.locus.contig.replace(\"chr\", \"\"),\n                                    hl.str(ht.locus.position),\n                                    ht.alleles[0],\n                                    ht.alleles[1],\n                                ]\n                            ),\n                            source=hl.str(\"gnomad\"),\n                        )\n                    ]\n                ),\n            )\n            .key_by(\"chromosome\", \"position\")\n            .drop(\"locus\", \"alleles\")\n            .select_globals()\n            .to_spark(flatten=False)\n            .withColumns(\n                {\n                    # Generate a variantId that is hashed for long variant ids:\n                    \"variantId\": VariantIndex.hash_long_variant_ids(\n                        f.col(\"variantId\"),\n                        f.col(\"chromosome\"),\n                        f.col(\"position\"),\n                        self.lenght_threshold,\n                    ),\n                    # We are not capturing the most severe consequence from GnomAD, but this column needed for the schema:\n                    \"mostSevereConsequenceId\": f.lit(None).cast(t.StringType()),\n                }\n            )\n        ),\n        _schema=VariantIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/_gwas_catalog/","title":"GWAS Catalog","text":"GWAS Catalog <p>The GWAS Catalog is a comprehensive resource that aims to provide a curated collection of Genome-Wide Association Studies (GWAS) (including harmonized full GWAS summary statistics) across various traits and diseases in humans.</p> <p>It serves as a valuable repository of genetic associations identified in diverse populations, offering insights into the genetic basis of complex traits and diseases.</p> <p>We rely on the GWAS Catalog for a rich source of genetic associations, utilizing the data for analysis and interpretation.</p> <p>For detailed information on specific genetic associations, their significance, and associated studies, refer to the GWAS Catalog.</p> <p>Within our analyses, we leverage two different types of studies from the GWAS Catalog:</p> <ol> <li> <p>Studies with (full) GWAS summary stats</p> </li> <li> <p>Studies with top hits only - GWAS curated studies</p> </li> </ol>"},{"location":"python_api/datasources/gwas_catalog/associations/","title":"Associations","text":""},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser","title":"<code>gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser</code>  <code>dataclass</code>","text":"<p>GWAS Catalog curated associations parser.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@dataclass\nclass GWASCatalogCuratedAssociationsParser:\n    \"\"\"GWAS Catalog curated associations parser.\"\"\"\n\n    @staticmethod\n    def convert_gnomad_position_to_ensembl(\n        position: Column, reference: Column, alternate: Column\n    ) -&gt; Column:\n        \"\"\"Convert GnomAD variant position to Ensembl variant position.\n\n        For indels (the reference or alternate allele is longer than 1), then adding 1 to the position, for SNPs,\n        the position is unchanged. More info about the problem: https://www.biostars.org/p/84686/\n\n        Args:\n            position (Column): Position of the variant in GnomAD's coordinates system.\n            reference (Column): The reference allele in GnomAD's coordinates system.\n            alternate (Column): The alternate allele in GnomAD's coordinates system.\n\n        Returns:\n            Column: The position of the variant in the Ensembl genome.\n\n        Examples:\n            &gt;&gt;&gt; d = [(1, \"A\", \"C\"), (2, \"AA\", \"C\"), (3, \"A\", \"AA\")]\n            &gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"position\", \"reference\", \"alternate\")\n            &gt;&gt;&gt; df.withColumn(\"new_position\", GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl(f.col(\"position\"), f.col(\"reference\"), f.col(\"alternate\"))).show()\n            +--------+---------+---------+------------+\n            |position|reference|alternate|new_position|\n            +--------+---------+---------+------------+\n            |       1|        A|        C|           1|\n            |       2|       AA|        C|           3|\n            |       3|        A|       AA|           4|\n            +--------+---------+---------+------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.when(\n            (f.length(reference) &gt; 1) | (f.length(alternate) &gt; 1), position + 1\n        ).otherwise(position)\n\n    @staticmethod\n    def _split_pvalue_column(pvalue: Column) -&gt; PValComponents:\n        \"\"\"Parse p-value column.\n\n        Args:\n            pvalue (Column): p-value [string]\n\n        Returns:\n            PValComponents: p-value mantissa and exponent\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [(\"1.0\"), (\"0.5\"), (\"1E-20\"), (\"3E-3\"), (\"1E-1000\")]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, t.StringType())\n            &gt;&gt;&gt; df.select('value',*GWASCatalogCuratedAssociationsParser._split_pvalue_column(f.col('value'))).show()\n            +-------+--------------+--------------+\n            |  value|pValueMantissa|pValueExponent|\n            +-------+--------------+--------------+\n            |    1.0|           1.0|             1|\n            |    0.5|           0.5|             1|\n            |  1E-20|           1.0|           -20|\n            |   3E-3|           3.0|            -3|\n            |1E-1000|           1.0|         -1000|\n            +-------+--------------+--------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        split = f.split(pvalue, \"E\")\n        return PValComponents(\n            mantissa=split.getItem(0).cast(\"float\").alias(\"pValueMantissa\"),\n            exponent=f.coalesce(split.getItem(1).cast(\"integer\"), f.lit(1)).alias(\n                \"pValueExponent\"\n            ),\n        )\n\n    @staticmethod\n    def _normalise_pvaluetext(p_value_text: Column) -&gt; Column:\n        \"\"\"Normalised p-value text column to a standardised format.\n\n        For cases where there is no mapping, the value is set to null.\n\n        Args:\n            p_value_text (Column): `pValueText` column from GWASCatalog\n\n        Returns:\n            Column: Array column after using GWAS Catalog mappings. There might be multiple mappings for a single p-value text.\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [(\"European Ancestry\"), (\"African ancestry\"), (\"Alzheimer\u2019s Disease\"), (\"(progression)\"), (\"\"), (None)]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, t.StringType())\n            &gt;&gt;&gt; df.withColumn('normalised', GWASCatalogCuratedAssociationsParser._normalise_pvaluetext(f.col('value'))).show()\n            +-------------------+----------+\n            |              value|normalised|\n            +-------------------+----------+\n            |  European Ancestry|      [EA]|\n            |   African ancestry|      [AA]|\n            |Alzheimer\u2019s Disease|      [AD]|\n            |      (progression)|      NULL|\n            |                   |      NULL|\n            |               NULL|      NULL|\n            +-------------------+----------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        # GWAS Catalog to p-value mapping\n        pkg = pkg_resources.files(data).joinpath(\"gwas_pValueText_map.json\")\n        with pkg.open(encoding=\"utf-8\") as file:\n            json_dict = json.load(file)\n            json_dict = cast(dict[str, str], json_dict)\n\n        map_expr = f.create_map(*[f.lit(x) for x in chain(*json_dict.items())])\n\n        splitted_col = f.split(f.regexp_replace(p_value_text, r\"[\\(\\)]\", \"\"), \",\")\n        mapped_col = f.transform(splitted_col, lambda x: map_expr[x])\n        return f.when(f.forall(mapped_col, lambda x: x.isNull()), None).otherwise(\n            mapped_col\n        )\n\n    @staticmethod\n    def _extract_risk_allele(risk_allele: Column) -&gt; Column:\n        \"\"\"Extract risk allele from provided \"STRONGEST SNP-RISK ALLELE\" input column.\n\n        If multiple risk alleles are present, the first one is returned.\n\n        Args:\n            risk_allele (Column): `riskAllele` column from GWASCatalog\n\n        Returns:\n            Column: mapped using GWAS Catalog mapping\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [(\"rs1234-A-G\"), (\"rs1234-A\"), (\"rs1234-A; rs1235-G\")]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, t.StringType())\n            &gt;&gt;&gt; df.withColumn('normalised', GWASCatalogCuratedAssociationsParser._extract_risk_allele(f.col('value'))).show()\n            +------------------+----------+\n            |             value|normalised|\n            +------------------+----------+\n            |        rs1234-A-G|         A|\n            |          rs1234-A|         A|\n            |rs1234-A; rs1235-G|         A|\n            +------------------+----------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # GWAS Catalog to risk allele mapping\n        return f.split(f.split(risk_allele, \"; \").getItem(0), \"-\").getItem(1)\n\n    @staticmethod\n    def _collect_rsids(\n        snp_id: Column, snp_id_current: Column, risk_allele: Column\n    ) -&gt; Column:\n        \"\"\"It takes three columns, and returns an array of distinct values from those columns.\n\n        Args:\n            snp_id (Column): The original snp id from the GWAS catalog.\n            snp_id_current (Column): The current snp id field is just a number at the moment (stored as a string). Adding 'rs' prefix if looks good.\n            risk_allele (Column): The risk allele for the SNP.\n\n        Returns:\n            Column: An array of distinct values.\n        \"\"\"\n        # The current snp id field is just a number at the moment (stored as a string). Adding 'rs' prefix if looks good.\n        snp_id_current = f.when(\n            snp_id_current.rlike(\"^[0-9]*$\"),\n            f.format_string(\"rs%s\", snp_id_current),\n        )\n        # Cleaning risk allele:\n        risk_allele = f.split(risk_allele, \"-\").getItem(0)\n\n        # Collecting all values:\n        return f.array_distinct(f.array(snp_id, snp_id_current, risk_allele))\n\n    @staticmethod\n    def _map_variants_to_gnomad_variants(\n        gwas_associations: DataFrame, variant_index: VariantIndex\n    ) -&gt; DataFrame:\n        \"\"\"Add variant metadata in associations.\n\n        Args:\n            gwas_associations (DataFrame): raw GWAS Catalog associations.\n            variant_index (VariantIndex): GnomaAD variants dataset with allele frequencies.\n\n        Returns:\n            DataFrame: GWAS Catalog associations data including `variantId`, `referenceAllele`,\n            `alternateAllele`, `chromosome`, `position` with variant metadata\n        \"\"\"\n        # Subset of GWAS Catalog associations required for resolving variant IDs:\n        gwas_associations_subset = gwas_associations.select(\n            \"rowId\",\n            f.col(\"CHR_ID\").alias(\"chromosome\"),\n            # The positions from GWAS Catalog are from ensembl that causes discrepancy for indels:\n            f.col(\"CHR_POS\").cast(IntegerType()).alias(\"ensemblPosition\"),\n            # List of all SNPs associated with the variant\n            GWASCatalogCuratedAssociationsParser._collect_rsids(\n                f.split(f.col(\"SNPS\"), \"; \").getItem(0),\n                f.col(\"SNP_ID_CURRENT\"),\n                f.split(f.col(\"STRONGEST SNP-RISK ALLELE\"), \"; \").getItem(0),\n            ).alias(\"rsIdsGwasCatalog\"),\n            GWASCatalogCuratedAssociationsParser._extract_risk_allele(\n                f.col(\"STRONGEST SNP-RISK ALLELE\")\n            ).alias(\"riskAllele\"),\n        )\n\n        # Subset of variant annotation required for GWAS Catalog annotations:\n        va_subset = variant_index.df.select(\n            \"variantId\",\n            \"chromosome\",\n            # Calculate the position in Ensembl coordinates for indels:\n            GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl(\n                f.col(\"position\"),\n                f.col(\"referenceAllele\"),\n                f.col(\"alternateAllele\"),\n            ).alias(\"ensemblPosition\"),\n            # Keeping GnomAD position:\n            \"position\",\n            f.col(\"rsIds\").alias(\"rsIdsGnomad\"),\n            \"referenceAllele\",\n            \"alternateAllele\",\n            \"alleleFrequencies\",\n            variant_index.max_maf().alias(\"maxMaf\"),\n        ).join(\n            gwas_associations_subset.select(\"chromosome\", \"ensemblPosition\").distinct(),\n            on=[\"chromosome\", \"ensemblPosition\"],\n            how=\"inner\",\n        )\n\n        # Semi-resolved ids (still contains duplicates when conclusion was not possible to make\n        # based on rsIds or allele concordance)\n        filtered_associations = (\n            gwas_associations_subset.join(\n                va_subset,\n                on=[\"chromosome\", \"ensemblPosition\"],\n                how=\"left\",\n            )\n            .withColumn(\n                \"rsIdFilter\",\n                GWASCatalogCuratedAssociationsParser._flag_mappings_to_retain(\n                    f.col(\"rowId\"),\n                    GWASCatalogCuratedAssociationsParser._compare_rsids(\n                        f.col(\"rsIdsGnomad\"), f.col(\"rsIdsGwasCatalog\")\n                    ),\n                ),\n            )\n            .withColumn(\n                \"concordanceFilter\",\n                GWASCatalogCuratedAssociationsParser._flag_mappings_to_retain(\n                    f.col(\"rowId\"),\n                    GWASCatalogCuratedAssociationsParser._check_concordance(\n                        f.col(\"riskAllele\"),\n                        f.col(\"referenceAllele\"),\n                        f.col(\"alternateAllele\"),\n                    ),\n                ),\n            )\n            .filter(\n                # Filter out rows where GWAS Catalog rsId does not match with GnomAD rsId,\n                # but there is corresponding variant for the same association\n                f.col(\"rsIdFilter\")\n                # or filter out rows where GWAS Catalog alleles are not concordant with GnomAD alleles,\n                # but there is corresponding variant for the same association\n                | f.col(\"concordanceFilter\")\n            )\n        )\n\n        # Keep only highest maxMaf variant per rowId\n        fully_mapped_associations = get_record_with_maximum_value(\n            filtered_associations, grouping_col=\"rowId\", sorting_col=\"maxMaf\"\n        ).select(\n            \"rowId\",\n            \"variantId\",\n            \"referenceAllele\",\n            \"alternateAllele\",\n            \"chromosome\",\n            \"position\",\n        )\n\n        return gwas_associations.join(fully_mapped_associations, on=\"rowId\", how=\"left\")\n\n    @staticmethod\n    def _compare_rsids(gnomad: Column, gwas: Column) -&gt; Column:\n        \"\"\"If the intersection of the two arrays is greater than 0, return True, otherwise return False.\n\n        Args:\n            gnomad (Column): rsids from gnomad\n            gwas (Column): rsids from the GWAS Catalog\n\n        Returns:\n            Column: A boolean column that is true if the GnomAD rsIDs can be found in the GWAS rsIDs.\n\n        Examples:\n            &gt;&gt;&gt; d = [\n            ...    (1, [\"rs123\", \"rs523\"], [\"rs123\"]),\n            ...    (2, [], [\"rs123\"]),\n            ...    (3, [\"rs123\", \"rs523\"], []),\n            ...    (4, [], []),\n            ... ]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, ['associationId', 'gnomad', 'gwas'])\n            &gt;&gt;&gt; df.withColumn(\"rsid_matches\", GWASCatalogCuratedAssociationsParser._compare_rsids(f.col(\"gnomad\"),f.col('gwas'))).show()\n            +-------------+--------------+-------+------------+\n            |associationId|        gnomad|   gwas|rsid_matches|\n            +-------------+--------------+-------+------------+\n            |            1|[rs123, rs523]|[rs123]|        true|\n            |            2|            []|[rs123]|       false|\n            |            3|[rs123, rs523]|     []|       false|\n            |            4|            []|     []|       false|\n            +-------------+--------------+-------+------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return f.when(f.size(f.array_intersect(gnomad, gwas)) &gt; 0, True).otherwise(\n            False\n        )\n\n    @staticmethod\n    def _flag_mappings_to_retain(\n        association_id: Column, filter_column: Column\n    ) -&gt; Column:\n        \"\"\"Flagging mappings to drop for each association.\n\n        Some associations have multiple mappings. Some has matching rsId others don't. We only\n        want to drop the non-matching mappings, when a matching is available for the given association.\n        This logic can be generalised for other measures eg. allele concordance.\n\n        Args:\n            association_id (Column): association identifier column\n            filter_column (Column): boolean col indicating to keep a mapping\n\n        Returns:\n            Column: A column with a boolean value.\n\n        Examples:\n        &gt;&gt;&gt; d = [\n        ...    (1, False),\n        ...    (1, False),\n        ...    (2, False),\n        ...    (2, True),\n        ...    (3, True),\n        ...    (3, True),\n        ... ]\n        &gt;&gt;&gt; df = spark.createDataFrame(d, ['associationId', 'filter'])\n        &gt;&gt;&gt; df.withColumn(\"isConcordant\", GWASCatalogCuratedAssociationsParser._flag_mappings_to_retain(f.col(\"associationId\"),f.col('filter'))).show()\n        +-------------+------+------------+\n        |associationId|filter|isConcordant|\n        +-------------+------+------------+\n        |            1| false|        true|\n        |            1| false|        true|\n        |            2| false|       false|\n        |            2|  true|        true|\n        |            3|  true|        true|\n        |            3|  true|        true|\n        +-------------+------+------------+\n        &lt;BLANKLINE&gt;\n\n        \"\"\"\n        w = Window.partitionBy(association_id)\n\n        # Generating a boolean column informing if the filter column contains true anywhere for the association:\n        aggregated_filter = f.when(\n            f.array_contains(f.collect_set(filter_column).over(w), True), True\n        ).otherwise(False)\n\n        # Generate a filter column:\n        return f.when(aggregated_filter &amp; (~filter_column), False).otherwise(True)\n\n    @staticmethod\n    def _check_concordance(\n        risk_allele: Column, reference_allele: Column, alternate_allele: Column\n    ) -&gt; Column:\n        \"\"\"A function to check if the risk allele is concordant with the alt or ref allele.\n\n        If the risk allele is the same as the reference or alternate allele, or if the reverse complement of\n        the risk allele is the same as the reference or alternate allele, then the allele is concordant.\n        If no mapping is available (ref/alt is null), the function returns True.\n\n        Args:\n            risk_allele (Column): The allele that is associated with the risk of the disease.\n            reference_allele (Column): The reference allele from the GWAS catalog\n            alternate_allele (Column): The alternate allele of the variant.\n\n        Returns:\n            Column: A boolean column that is True if the risk allele is the same as the reference or alternate allele,\n            or if the reverse complement of the risk allele is the same as the reference or alternate allele.\n\n        Examples:\n            &gt;&gt;&gt; d = [\n            ...     ('A', 'A', 'G'),\n            ...     ('A', 'T', 'G'),\n            ...     ('A', 'C', 'G'),\n            ...     ('A', 'A', '?'),\n            ...     (None, None, 'A'),\n            ... ]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, ['riskAllele', 'referenceAllele', 'alternateAllele'])\n            &gt;&gt;&gt; df.withColumn(\"isConcordant\", GWASCatalogCuratedAssociationsParser._check_concordance(f.col(\"riskAllele\"),f.col('referenceAllele'), f.col('alternateAllele'))).show()\n            +----------+---------------+---------------+------------+\n            |riskAllele|referenceAllele|alternateAllele|isConcordant|\n            +----------+---------------+---------------+------------+\n            |         A|              A|              G|        true|\n            |         A|              T|              G|        true|\n            |         A|              C|              G|       false|\n            |         A|              A|              ?|        true|\n            |      NULL|           NULL|              A|        true|\n            +----------+---------------+---------------+------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        # Calculating the reverse complement of the risk allele:\n        risk_allele_reverse_complement = f.when(\n            risk_allele.rlike(r\"^[ACTG]+$\"),\n            f.reverse(f.translate(risk_allele, \"ACTG\", \"TGAC\")),\n        ).otherwise(risk_allele)\n\n        # OK, is the risk allele or the reverse complent is the same as the mapped alleles:\n        return (\n            f.when(\n                (risk_allele == reference_allele) | (risk_allele == alternate_allele),\n                True,\n            )\n            # If risk allele is found on the negative strand:\n            .when(\n                (risk_allele_reverse_complement == reference_allele)\n                | (risk_allele_reverse_complement == alternate_allele),\n                True,\n            )\n            # If risk allele is ambiguous, still accepted: &lt; This condition could be reconsidered\n            .when(risk_allele == \"?\", True)\n            # If the association could not be mapped we keep it:\n            .when(reference_allele.isNull(), True)\n            # Allele is discordant:\n            .otherwise(False)\n        )\n\n    @staticmethod\n    def _get_reverse_complement(allele_col: Column) -&gt; Column:\n        \"\"\"A function to return the reverse complement of an allele column.\n\n        It takes a string and returns the reverse complement of that string if it's a DNA sequence,\n        otherwise it returns the original string. Assumes alleles in upper case.\n\n        Args:\n            allele_col (Column): The column containing the allele to reverse complement.\n\n        Returns:\n            Column: A column that is the reverse complement of the allele column.\n\n        Examples:\n            &gt;&gt;&gt; d = [{\"allele\": 'A'}, {\"allele\": 'T'},{\"allele\": 'G'}, {\"allele\": 'C'},{\"allele\": 'AC'}, {\"allele\": 'GTaatc'},{\"allele\": '?'}, {\"allele\": None}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d)\n            &gt;&gt;&gt; df.withColumn(\"revcom_allele\", GWASCatalogCuratedAssociationsParser._get_reverse_complement(f.col(\"allele\"))).show()\n            +------+-------------+\n            |allele|revcom_allele|\n            +------+-------------+\n            |     A|            T|\n            |     T|            A|\n            |     G|            C|\n            |     C|            G|\n            |    AC|           GT|\n            |GTaatc|       GATTAC|\n            |     ?|            ?|\n            |  NULL|         NULL|\n            +------+-------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        allele_col = f.upper(allele_col)\n        return f.when(\n            allele_col.rlike(\"[ACTG]+\"),\n            f.reverse(f.translate(allele_col, \"ACTG\", \"TGAC\")),\n        ).otherwise(allele_col)\n\n    @staticmethod\n    def _effect_needs_harmonisation(\n        risk_allele: Column, reference_allele: Column\n    ) -&gt; Column:\n        \"\"\"A function to check if the effect allele needs to be harmonised.\n\n        Args:\n            risk_allele (Column): Risk allele column\n            reference_allele (Column): Effect allele column\n\n        Returns:\n            Column: A boolean column indicating if the effect allele needs to be harmonised.\n\n        Examples:\n            &gt;&gt;&gt; d = [{\"risk\": 'A', \"reference\": 'A'}, {\"risk\": 'A', \"reference\": 'T'}, {\"risk\": 'AT', \"reference\": 'TA'}, {\"risk\": 'AT', \"reference\": 'AT'}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d)\n            &gt;&gt;&gt; df.withColumn(\"needs_harmonisation\", GWASCatalogCuratedAssociationsParser._effect_needs_harmonisation(f.col(\"risk\"), f.col(\"reference\"))).show()\n            +---------+----+-------------------+\n            |reference|risk|needs_harmonisation|\n            +---------+----+-------------------+\n            |        A|   A|               true|\n            |        T|   A|               true|\n            |       TA|  AT|              false|\n            |       AT|  AT|               true|\n            +---------+----+-------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return (risk_allele == reference_allele) | (\n            risk_allele\n            == GWASCatalogCuratedAssociationsParser._get_reverse_complement(\n                reference_allele\n            )\n        )\n\n    @staticmethod\n    def _are_alleles_palindromic(\n        reference_allele: Column, alternate_allele: Column\n    ) -&gt; Column:\n        \"\"\"A function to check if the alleles are palindromic.\n\n        Args:\n            reference_allele (Column): Reference allele column\n            alternate_allele (Column): Alternate allele column\n\n        Returns:\n            Column: A boolean column indicating if the alleles are palindromic.\n\n        Examples:\n            &gt;&gt;&gt; d = [{\"reference\": 'A', \"alternate\": 'T'}, {\"reference\": 'AT', \"alternate\": 'AG'}, {\"reference\": 'AT', \"alternate\": 'AT'}, {\"reference\": 'CATATG', \"alternate\": 'CATATG'}, {\"reference\": '-', \"alternate\": None}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d)\n            &gt;&gt;&gt; df.withColumn(\"is_palindromic\", GWASCatalogCuratedAssociationsParser._are_alleles_palindromic(f.col(\"reference\"), f.col(\"alternate\"))).show()\n            +---------+---------+--------------+\n            |alternate|reference|is_palindromic|\n            +---------+---------+--------------+\n            |        T|        A|          true|\n            |       AG|       AT|         false|\n            |       AT|       AT|          true|\n            |   CATATG|   CATATG|          true|\n            |     NULL|        -|         false|\n            +---------+---------+--------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        revcomp = GWASCatalogCuratedAssociationsParser._get_reverse_complement(\n            alternate_allele\n        )\n        return (\n            f.when(reference_allele == revcomp, True)\n            .when(revcomp.isNull(), False)\n            .otherwise(False)\n        )\n\n    @staticmethod\n    def _harmonise_beta(\n        effect_size: Column,\n        confidence_interval: Column,\n        flipping_needed: Column,\n    ) -&gt; Column:\n        \"\"\"A function to extract the beta value from the effect size and confidence interval and harmonises for the alternate allele.\n\n        If the confidence interval contains the word \"increase\" or \"decrease\" it indicates, we are dealing with betas.\n        If it's \"increase\" and the effect size needs to be harmonized, then multiply the effect size by -1.\n        The sign of the effect size is flipped if the confidence interval contains \"decrease\".\n\n        eg. if the reported value is 0.5, and the confidence interval tells \"decrease\"? -&gt; beta is -0.5\n\n        Args:\n            effect_size (Column): GWAS Catalog effect size column.\n            confidence_interval (Column): GWAS Catalog confidence interval column to know the direction of the effect.\n            flipping_needed (Column): Boolean flag indicating if effect needs to be flipped based on the alleles.\n\n        Returns:\n            Column: A column containing the beta value.\n\n        Examples:\n            &gt;&gt;&gt; d = [\n            ...    # positive effect -no flipping:\n            ...    (0.5, 'increase', False),\n            ...    # Positive effect - flip:\n            ...    (0.5, 'decrease', False),\n            ...    # Positive effect - flip:\n            ...    (0.5, 'decrease', True),\n            ...    # Negative effect - no flip:\n            ...    (0.5, 'increase', True),\n            ...    # Negative effect - flip:\n            ...    (0.5, 'decrease', False),\n            ... ]\n            &gt;&gt;&gt; (\n            ...    spark.createDataFrame(d, ['effect', 'ci_text', 'flip'])\n            ...    .select(\"effect\", \"ci_text\", 'flip', GWASCatalogCuratedAssociationsParser._harmonise_beta(f.col(\"effect\"), f.col(\"ci_text\"), f.lit(False)).alias(\"beta\"))\n            ...    .show()\n            ... )\n            +------+--------+-----+----+\n            |effect| ci_text| flip|beta|\n            +------+--------+-----+----+\n            |   0.5|increase|false| 0.5|\n            |   0.5|decrease|false|-0.5|\n            |   0.5|decrease| true|-0.5|\n            |   0.5|increase| true| 0.5|\n            |   0.5|decrease|false|-0.5|\n            +------+--------+-----+----+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        return (\n            f.when(\n                (flipping_needed &amp; confidence_interval.contains(\"increase\"))\n                | (~flipping_needed &amp; confidence_interval.contains(\"decrease\")),\n                -effect_size,\n            )\n            .otherwise(effect_size)\n            .cast(DoubleType())\n        )\n\n    @staticmethod\n    def _harmonise_odds_ratio(\n        effect_size: Column,\n        flipping_needed: Column,\n    ) -&gt; Column:\n        \"\"\"Odds ratio is either propagated as is, or flipped if indicated, meaning returning a reciprocal value.\n\n        Args:\n            effect_size (Column): containing effect size,\n            flipping_needed (Column): Boolean flag indicating if effect needs to be flipped\n\n        Returns:\n            Column: A column with the odds ratio, or 1/odds_ratio if harmonization required.\n\n        Examples:\n        &gt;&gt;&gt; d = [(0.5, False), (0.5, True), (0.0, False), (0.0, True)]\n        &gt;&gt;&gt; (\n        ...    spark.createDataFrame(d, ['effect', 'flip'])\n        ...    .select(\"effect\", \"flip\", GWASCatalogCuratedAssociationsParser._harmonise_odds_ratio(f.col(\"effect\"), f.col(\"flip\")).alias(\"odds_ratio\"))\n        ...    .show()\n        ... )\n        +------+-----+----------+\n        |effect| flip|odds_ratio|\n        +------+-----+----------+\n        |   0.5|false|       0.5|\n        |   0.5| true|       2.0|\n        |   0.0|false|       0.0|\n        |   0.0| true|      NULL|\n        +------+-----+----------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        return (\n            # We are not flipping zero effect size:\n            f.when((effect_size.cast(DoubleType()) == 0) &amp; flipping_needed, f.lit(None))\n            .when(\n                flipping_needed,\n                1 / effect_size,\n            )\n            .otherwise(effect_size)\n            .cast(DoubleType())\n        )\n\n    @staticmethod\n    def _concatenate_substudy_description(\n        association_trait: Column, pvalue_text: Column, mapped_trait_uri: Column\n    ) -&gt; Column:\n        \"\"\"Substudy description parsing. Complex string containing metadata about the substudy (e.g. QTL, specific EFO, etc.).\n\n        Args:\n            association_trait (Column): GWAS Catalog association trait column\n            pvalue_text (Column): GWAS Catalog p-value text column\n            mapped_trait_uri (Column): GWAS Catalog mapped trait URI column\n\n        Returns:\n            Column: A column with the substudy description in the shape trait|pvaluetext1_pvaluetext2|EFO1_EFO2.\n\n        Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([\n        ...    (\"Height\", \"http://www.ebi.ac.uk/efo/EFO_0000001,http://www.ebi.ac.uk/efo/EFO_0000002\", \"European Ancestry\"),\n        ...    (\"Schizophrenia\", \"http://www.ebi.ac.uk/efo/MONDO_0005090\", None)],\n        ...    [\"association_trait\", \"mapped_trait_uri\", \"pvalue_text\"]\n        ... )\n        &gt;&gt;&gt; df.withColumn('substudy_description', GWASCatalogCuratedAssociationsParser._concatenate_substudy_description(df.association_trait, df.pvalue_text, df.mapped_trait_uri)).show(truncate=False)\n        +-----------------+-------------------------------------------------------------------------+-----------------+------------------------------------------+\n        |association_trait|mapped_trait_uri                                                         |pvalue_text      |substudy_description                      |\n        +-----------------+-------------------------------------------------------------------------+-----------------+------------------------------------------+\n        |Height           |http://www.ebi.ac.uk/efo/EFO_0000001,http://www.ebi.ac.uk/efo/EFO_0000002|European Ancestry|Height|EA|EFO_0000001/EFO_0000002         |\n        |Schizophrenia    |http://www.ebi.ac.uk/efo/MONDO_0005090                                   |NULL             |Schizophrenia|no_pvalue_text|MONDO_0005090|\n        +-----------------+-------------------------------------------------------------------------+-----------------+------------------------------------------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        p_value_text = f.coalesce(\n            GWASCatalogCuratedAssociationsParser._normalise_pvaluetext(pvalue_text),\n            f.array(f.lit(\"no_pvalue_text\")),\n        )\n        return f.concat_ws(\n            \"|\",\n            association_trait,\n            f.concat_ws(\n                \"/\",\n                p_value_text,\n            ),\n            f.concat_ws(\n                \"/\",\n                parse_efos(mapped_trait_uri),\n            ),\n        )\n\n    @staticmethod\n    def _qc_all(\n        qc: Column,\n        chromosome: Column,\n        position: Column,\n        reference_allele: Column,\n        alternate_allele: Column,\n        strongest_snp_risk_allele: Column,\n        p_value_mantissa: Column,\n        p_value_exponent: Column,\n        p_value_cutoff: float,\n    ) -&gt; Column:\n        \"\"\"Flag associations that fail any QC.\n\n        Args:\n            qc (Column): QC column\n            chromosome (Column): Chromosome column\n            position (Column): Position column\n            reference_allele (Column): Reference allele column\n            alternate_allele (Column): Alternate allele column\n            strongest_snp_risk_allele (Column): Strongest SNP risk allele column\n            p_value_mantissa (Column): P-value mantissa column\n            p_value_exponent (Column): P-value exponent column\n            p_value_cutoff (float): P-value cutoff\n\n        Returns:\n            Column: Updated QC column with flag.\n        \"\"\"\n        qc = GWASCatalogCuratedAssociationsParser._qc_variant_interactions(\n            qc, strongest_snp_risk_allele\n        )\n        qc = StudyLocus._qc_subsignificant_associations(\n            qc, p_value_mantissa, p_value_exponent, p_value_cutoff\n        )\n        qc = GWASCatalogCuratedAssociationsParser._qc_genomic_location(\n            qc, chromosome, position\n        )\n        qc = GWASCatalogCuratedAssociationsParser._qc_variant_inconsistencies(\n            qc, chromosome, position, strongest_snp_risk_allele\n        )\n        qc = GWASCatalogCuratedAssociationsParser._qc_unmapped_variants(\n            qc, alternate_allele\n        )\n        qc = GWASCatalogCuratedAssociationsParser._qc_palindromic_alleles(\n            qc, reference_allele, alternate_allele\n        )\n        return qc\n\n    @staticmethod\n    def _qc_variant_interactions(\n        qc: Column, strongest_snp_risk_allele: Column\n    ) -&gt; Column:\n        \"\"\"Flag associations based on variant x variant interactions.\n\n        Args:\n            qc (Column): QC column\n            strongest_snp_risk_allele (Column): Column with the strongest SNP risk allele\n\n        Returns:\n            Column: Updated QC column with flag.\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            strongest_snp_risk_allele.contains(\";\"),\n            StudyLocusQualityCheck.COMPOSITE_FLAG,\n        )\n\n    @staticmethod\n    def _qc_genomic_location(\n        qc: Column, chromosome: Column, position: Column\n    ) -&gt; Column:\n        \"\"\"Flag associations without genomic location in GWAS Catalog.\n\n        Args:\n            qc (Column): QC column\n            chromosome (Column): Chromosome column in GWAS Catalog\n            position (Column): Position column in GWAS Catalog\n\n        Returns:\n            Column: Updated QC column with flag.\n\n        Examples:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [{'qc': None, 'chromosome': None, 'position': None}, {'qc': None, 'chromosome': '1', 'position': None}, {'qc': None, 'chromosome': None, 'position': 1}, {'qc': None, 'chromosome': '1', 'position': 1}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d, schema=t.StructType([t.StructField('qc', t.ArrayType(t.StringType()), True), t.StructField('chromosome', t.StringType()), t.StructField('position', t.IntegerType())]))\n            &gt;&gt;&gt; df.withColumn('qc', GWASCatalogCuratedAssociationsParser._qc_genomic_location(df.qc, df.chromosome, df.position)).show(truncate=False)\n            +----------------------------+----------+--------+\n            |qc                          |chromosome|position|\n            +----------------------------+----------+--------+\n            |[Incomplete genomic mapping]|NULL      |NULL    |\n            |[Incomplete genomic mapping]|1         |NULL    |\n            |[Incomplete genomic mapping]|NULL      |1       |\n            |[]                          |1         |1       |\n            +----------------------------+----------+--------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            position.isNull() | chromosome.isNull(),\n            StudyLocusQualityCheck.NO_GENOMIC_LOCATION_FLAG,\n        )\n\n    @staticmethod\n    def _qc_variant_inconsistencies(\n        qc: Column,\n        chromosome: Column,\n        position: Column,\n        strongest_snp_risk_allele: Column,\n    ) -&gt; Column:\n        \"\"\"Flag associations with inconsistencies in the variant annotation.\n\n        Args:\n            qc (Column): QC column\n            chromosome (Column): Chromosome column in GWAS Catalog\n            position (Column): Position column in GWAS Catalog\n            strongest_snp_risk_allele (Column): Strongest SNP risk allele column in GWAS Catalog\n\n        Returns:\n            Column: Updated QC column with flag.\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            # Number of chromosomes does not correspond to the number of positions:\n            (f.size(f.split(chromosome, \";\")) != f.size(f.split(position, \";\")))\n            # Number of chromosome values different from riskAllele values:\n            | (\n                f.size(f.split(chromosome, \";\"))\n                != f.size(f.split(strongest_snp_risk_allele, \";\"))\n            ),\n            StudyLocusQualityCheck.INCONSISTENCY_FLAG,\n        )\n\n    @staticmethod\n    def _qc_unmapped_variants(qc: Column, alternate_allele: Column) -&gt; Column:\n        \"\"\"Flag associations with variants not mapped to variantAnnotation.\n\n        Args:\n            qc (Column): QC column\n            alternate_allele (Column): alternate allele\n\n        Returns:\n            Column: Updated QC column with flag.\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; d = [{'alternate_allele': 'A', 'qc': None}, {'alternate_allele': None, 'qc': None}]\n            &gt;&gt;&gt; schema = t.StructType([t.StructField('alternate_allele', t.StringType(), True), t.StructField('qc', t.ArrayType(t.StringType()), True)])\n            &gt;&gt;&gt; df = spark.createDataFrame(data=d, schema=schema)\n            &gt;&gt;&gt; df.withColumn(\"new_qc\", GWASCatalogCuratedAssociationsParser._qc_unmapped_variants(f.col(\"qc\"), f.col(\"alternate_allele\"))).show()\n            +----------------+----+--------------------+\n            |alternate_allele|  qc|              new_qc|\n            +----------------+----+--------------------+\n            |               A|NULL|                  []|\n            |            NULL|NULL|[No mapping in Gn...|\n            +----------------+----+--------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            alternate_allele.isNull(),\n            StudyLocusQualityCheck.NON_MAPPED_VARIANT_FLAG,\n        )\n\n    @staticmethod\n    def _qc_palindromic_alleles(\n        qc: Column, reference_allele: Column, alternate_allele: Column\n    ) -&gt; Column:\n        \"\"\"Flag associations with palindromic variants which effects can not be harmonised.\n\n        Args:\n            qc (Column): QC column\n            reference_allele (Column): reference allele\n            alternate_allele (Column): alternate allele\n\n        Returns:\n            Column: Updated QC column with flag.\n\n        Example:\n            &gt;&gt;&gt; import pyspark.sql.types as t\n            &gt;&gt;&gt; schema = t.StructType([t.StructField('reference_allele', t.StringType(), True), t.StructField('alternate_allele', t.StringType(), True), t.StructField('qc', t.ArrayType(t.StringType()), True)])\n            &gt;&gt;&gt; d = [{'reference_allele': 'A', 'alternate_allele': 'T', 'qc': None}, {'reference_allele': 'AT', 'alternate_allele': 'TA', 'qc': None}, {'reference_allele': 'AT', 'alternate_allele': 'AT', 'qc': None}]\n            &gt;&gt;&gt; df = spark.createDataFrame(data=d, schema=schema)\n            &gt;&gt;&gt; df.withColumn(\"qc\", GWASCatalogCuratedAssociationsParser._qc_palindromic_alleles(f.col(\"qc\"), f.col(\"reference_allele\"), f.col(\"alternate_allele\"))).show(truncate=False)\n            +----------------+----------------+---------------------------------------+\n            |reference_allele|alternate_allele|qc                                     |\n            +----------------+----------------+---------------------------------------+\n            |A               |T               |[Palindrome alleles - cannot harmonize]|\n            |AT              |TA              |[]                                     |\n            |AT              |AT              |[Palindrome alleles - cannot harmonize]|\n            +----------------+----------------+---------------------------------------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            qc,\n            GWASCatalogCuratedAssociationsParser._are_alleles_palindromic(\n                reference_allele, alternate_allele\n            ),\n            StudyLocusQualityCheck.PALINDROMIC_ALLELE_FLAG,\n        )\n\n    @staticmethod\n    def _get_effect_type(ci_text: Column) -&gt; Column:\n        \"\"\"Extracts the effect type from the 95% CI text.\n\n        The GWAS Catalog confidence interval column contains text that can be used to infer the effect type.\n        If the text contains \"increase\" or \"decrease\", the effect type is beta, otherwise it is odds ratio.\n        Null columns return null as the effect type.\n\n        Args:\n            ci_text (Column): Column containing the 95% CI text.\n\n        Returns:\n            Column: A column containing the effect type.\n\n        Examples:\n            &gt;&gt;&gt; data = [{\"ci_text\": \"95% CI: [0.1-0.2]\"}, {\"ci_text\": \"95% CI: [0.1-0.2] increase\"}, {\"ci_text\": \"95% CI: [0.1-0.2] decrease\"}, {\"ci_text\": None}]\n            &gt;&gt;&gt; spark.createDataFrame(data).select('ci_text', GWASCatalogCuratedAssociationsParser._get_effect_type(f.col('ci_text')).alias('effect_type')).show(truncate=False)\n            +--------------------------+-----------+\n            |ci_text                   |effect_type|\n            +--------------------------+-----------+\n            |95% CI: [0.1-0.2]         |odds_ratio |\n            |95% CI: [0.1-0.2] increase|beta       |\n            |95% CI: [0.1-0.2] decrease|beta       |\n            |NULL                      |NULL       |\n            +--------------------------+-----------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return f.when(\n            f.lower(ci_text).contains(\"increase\")\n            | f.lower(ci_text).contains(\"decrease\"),\n            f.lit(\"beta\"),\n        ).when(ci_text.isNotNull(), f.lit(\"odds_ratio\"))\n\n    @staticmethod\n    def harmonise_association_effect_to_beta(\n        df: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"Harmonise effect to beta value.\n\n        The harmonisation process has a number of steps:\n        - Extracting the reported effect allele.\n        - Flagging palindromic alleles.\n        - Flagging associations where the effect direction needs to be flipped.\n        - Flagging the effect type.\n        - Getting the standard error from the beta and neglog p-value or odds ratio confidence intervals.\n        - Harmonising both beta and odds ratio.\n        - Converting the odds ratio to beta.\n\n        Args:\n            df (DataFrame): DataFrame with the following columns:\n\n        Returns:\n            DataFrame: DataFrame with the following columns:\n\n        Raises:\n            ValueError: If any of the required columns are missing.\n\n        Examples:\n        ---\n        &gt;&gt;&gt; data = [\n        ...    # Flagged as palindromic:\n        ...    ('rs123-T', 'A', 'T', '0.1', '[0.08-0.12] unit increase', 21.96),\n        ...    # Not palindromic, beta needs to be flipped:\n        ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12] unit increase', 21.96),\n        ...    # Beta is not flipped:\n        ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12] unit increase', 21.96),\n        ...    # odds ratio:\n        ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12]', 21.96),\n        ...    # odds ratio flipped:\n        ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12]', 21.96),\n        ... ]\n        &gt;&gt;&gt; schema = [\"STRONGEST SNP-RISK ALLELE\", \"referenceAllele\", \"alternateAllele\", \"OR or BETA\", \"95% CI (TEXT)\", \"PVALUE_MLOG\"]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n        &gt;&gt;&gt; GWASCatalogCuratedAssociationsParser.harmonise_association_effect_to_beta(df).show()\n        +-------------------------+---------------+---------------+----------+--------------------+-----------+-------------------+-------------------+\n        |STRONGEST SNP-RISK ALLELE|referenceAllele|alternateAllele|OR or BETA|       95% CI (TEXT)|PVALUE_MLOG|               beta|      standardError|\n        +-------------------------+---------------+---------------+----------+--------------------+-----------+-------------------+-------------------+\n        |                  rs123-T|              A|              T|       0.1|[0.08-0.12] unit ...|      21.96|               NULL|               NULL|\n        |                  rs123-C|              G|              T|       0.1|[0.08-0.12] unit ...|      21.96|               -0.1|0.01020130187396028|\n        |                  rs123-T|              C|              T|       0.1|[0.08-0.12] unit ...|      21.96|                0.1|0.01020130187396028|\n        |                  rs123-T|              C|              T|       0.1|         [0.08-0.12]|      21.96|-2.3025850929940455|0.23489365624113162|\n        |                  rs123-C|              G|              T|       0.1|         [0.08-0.12]|      21.96|  2.302585092994046|0.23489365624113168|\n        +-------------------------+---------------+---------------+----------+--------------------+-----------+-------------------+-------------------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        # Testing if all columns are in the dataframe:\n        required_columns = [\n            \"STRONGEST SNP-RISK ALLELE\",\n            \"referenceAllele\",\n            \"alternateAllele\",\n            \"OR or BETA\",\n            \"95% CI (TEXT)\",\n            \"PVALUE_MLOG\",\n        ]\n\n        for column in required_columns:\n            if column not in df.columns:\n                raise ValueError(\n                    f\"Column {column} is required for harmonising effect to beta value.\"\n                )\n\n        pval_components = pvalue_from_neglogpval(f.col(\"PVALUE_MLOG\"))\n\n        return (\n            df.withColumn(\n                \"reportedRiskAllele\",\n                GWASCatalogCuratedAssociationsParser._extract_risk_allele(\n                    f.col(\"STRONGEST SNP-RISK ALLELE\")\n                ),\n            )\n            .withColumns(\n                {\n                    # Flag palindromic alleles:\n                    \"isAllelePalindromic\": GWASCatalogCuratedAssociationsParser._are_alleles_palindromic(\n                        f.col(\"referenceAllele\"), f.col(\"alternateAllele\")\n                    ),\n                    # Flag associations, where the effect direction needs to be flipped:\n                    \"needsFlipping\": GWASCatalogCuratedAssociationsParser._effect_needs_harmonisation(\n                        f.col(\"reportedRiskAllele\"), f.col(\"referenceAllele\")\n                    ),\n                    # Flag effect type:\n                    \"effectType\": GWASCatalogCuratedAssociationsParser._get_effect_type(\n                        f.col(\"95% CI (TEXT)\")\n                    ),\n                }\n            )\n            # Harmonise both beta and odds ratio:\n            .withColumns(\n                {  # Normalise beta value of the association:\n                    \"effect_beta\": f.when(\n                        (f.col(\"effectType\") == \"beta\")\n                        &amp; (~f.col(\"isAllelePalindromic\")),\n                        GWASCatalogCuratedAssociationsParser._harmonise_beta(\n                            f.col(\"OR or BETA\"),\n                            f.col(\"95% CI (TEXT)\"),\n                            f.col(\"needsFlipping\"),\n                        ),\n                    ),\n                    # Normalise odds ratio of the association:\n                    \"effect_odds_ratio\": f.when(\n                        (f.col(\"effectType\") == \"odds_ratio\")\n                        &amp; (~f.col(\"isAllelePalindromic\")),\n                        GWASCatalogCuratedAssociationsParser._harmonise_odds_ratio(\n                            f.col(\"OR or BETA\"),\n                            f.col(\"needsFlipping\"),\n                        ),\n                    ),\n                },\n            )\n            .select(\n                *df.columns,\n                # Harmonise beta and standard error:\n                *normalise_gwas_statistics(\n                    beta=f.col(\"effect_beta\"),\n                    odds_ratio=f.col(\"effect_odds_ratio\"),\n                    standard_error=f.lit(None).alias(\"standardError\"),\n                    ci_lower=f.regexp_extract(\n                        \"95% CI (TEXT)\", r\"\\[(\\d+\\.*\\d*)-\\d+\\.*\\d*\\]\", 1\n                    ).cast(FloatType()),\n                    ci_upper=f.regexp_extract(\n                        \"95% CI (TEXT)\", r\"\\[\\d+\\.*\\d*-(\\d+\\.*\\d*)\\]\", 1\n                    ).cast(FloatType()),\n                    mantissa=pval_components.mantissa,\n                    exponent=pval_components.exponent,\n                ),\n            )\n        )\n\n    @classmethod\n    def from_source(\n        cls: type[GWASCatalogCuratedAssociationsParser],\n        gwas_associations: DataFrame,\n        gnomad_variants: VariantIndex,\n        pvalue_threshold: float = WindowBasedClumpingStepConfig.gwas_significance,\n    ) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Read GWASCatalog associations.\n\n        It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and\n        applies some pre-defined filters on the data:\n\n        Args:\n            gwas_associations (DataFrame): GWAS Catalog raw associations dataset.\n            gnomad_variants (VariantIndex): Variant dataset from GnomAD, with allele frequencies.\n            pvalue_threshold (float): P-value threshold for flagging associations.\n\n        Returns:\n            StudyLocusGWASCatalog: GWASCatalogAssociations dataset\n\n        pvalue_threshold is kept in sync with the WindowBasedClumpingStep gwas_significance.\n        \"\"\"\n        pvalue_columns = GWASCatalogCuratedAssociationsParser._split_pvalue_column(\n            f.col(\"P-VALUE\")\n        )\n        return StudyLocusGWASCatalog(\n            _df=gwas_associations.withColumn(\n                # temporary column\n                \"rowId\",\n                f.monotonically_increasing_id().cast(StringType()),\n            )\n            .transform(\n                # Map/harmonise variants to variant annotation dataset:\n                # This function adds columns: variantId, referenceAllele, alternateAllele, chromosome, position\n                lambda df: GWASCatalogCuratedAssociationsParser._map_variants_to_gnomad_variants(\n                    df, gnomad_variants\n                )\n            )\n            .withColumns(\n                # Perform all quality control checks:\n                {\n                    \"qualityControls\": GWASCatalogCuratedAssociationsParser._qc_all(\n                        qc=f.array().alias(\"qualityControls\"),\n                        chromosome=f.col(\"CHR_ID\"),\n                        position=f.col(\"CHR_POS\").cast(IntegerType()),\n                        reference_allele=f.col(\"referenceAllele\"),\n                        alternate_allele=f.col(\"alternateAllele\"),\n                        strongest_snp_risk_allele=f.col(\"STRONGEST SNP-RISK ALLELE\"),\n                        p_value_mantissa=pvalue_columns.mantissa,\n                        p_value_exponent=pvalue_columns.exponent,\n                        p_value_cutoff=pvalue_threshold,\n                    )\n                }\n            )\n            # Harmonising effect to beta value and flip effect if needed:\n            .withColumns(\n                {\n                    \"pValueMantissa\": pvalue_columns.mantissa,\n                    \"pValueExponent\": pvalue_columns.exponent,\n                }\n            )\n            .transform(cls.harmonise_association_effect_to_beta)\n            .withColumnRenamed(\"STUDY ACCESSION\", \"studyId\")\n            # Adding study-locus id:\n            .withColumn(\n                \"studyLocusId\",\n                StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n            )\n            .select(\n                # INSIDE STUDY-LOCUS SCHEMA:\n                \"studyLocusId\",\n                \"variantId\",\n                # Mapped genomic location of the variant (; separated list)\n                \"chromosome\",\n                \"position\",\n                \"studyId\",\n                # p-value of the association, string: split into exponent and mantissa.\n                \"pValueMantissa\",\n                \"pValueExponent\",\n                # Capturing phenotype granularity at the association level\n                GWASCatalogCuratedAssociationsParser._concatenate_substudy_description(\n                    f.col(\"DISEASE/TRAIT\"),\n                    f.col(\"P-VALUE (TEXT)\"),\n                    f.col(\"MAPPED_TRAIT_URI\"),\n                ).alias(\"subStudyDescription\"),\n                # Quality controls (array of strings)\n                \"qualityControls\",\n                \"beta\",\n                \"standardError\",\n            ),\n            _schema=StudyLocusGWASCatalog.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl","title":"<code>convert_gnomad_position_to_ensembl(position: Column, reference: Column, alternate: Column) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Convert GnomAD variant position to Ensembl variant position.</p> <p>For indels (the reference or alternate allele is longer than 1), then adding 1 to the position, for SNPs, the position is unchanged. More info about the problem: https://www.biostars.org/p/84686/</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>Column</code> <p>Position of the variant in GnomAD's coordinates system.</p> required <code>reference</code> <code>Column</code> <p>The reference allele in GnomAD's coordinates system.</p> required <code>alternate</code> <code>Column</code> <p>The alternate allele in GnomAD's coordinates system.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>The position of the variant in the Ensembl genome.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = [(1, \"A\", \"C\"), (2, \"AA\", \"C\"), (3, \"A\", \"AA\")]\n&gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"position\", \"reference\", \"alternate\")\n&gt;&gt;&gt; df.withColumn(\"new_position\", GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl(f.col(\"position\"), f.col(\"reference\"), f.col(\"alternate\"))).show()\n+--------+---------+---------+------------+\n|position|reference|alternate|new_position|\n+--------+---------+---------+------------+\n|       1|        A|        C|           1|\n|       2|       AA|        C|           3|\n|       3|        A|       AA|           4|\n+--------+---------+---------+------------+\n</code></pre> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@staticmethod\ndef convert_gnomad_position_to_ensembl(\n    position: Column, reference: Column, alternate: Column\n) -&gt; Column:\n    \"\"\"Convert GnomAD variant position to Ensembl variant position.\n\n    For indels (the reference or alternate allele is longer than 1), then adding 1 to the position, for SNPs,\n    the position is unchanged. More info about the problem: https://www.biostars.org/p/84686/\n\n    Args:\n        position (Column): Position of the variant in GnomAD's coordinates system.\n        reference (Column): The reference allele in GnomAD's coordinates system.\n        alternate (Column): The alternate allele in GnomAD's coordinates system.\n\n    Returns:\n        Column: The position of the variant in the Ensembl genome.\n\n    Examples:\n        &gt;&gt;&gt; d = [(1, \"A\", \"C\"), (2, \"AA\", \"C\"), (3, \"A\", \"AA\")]\n        &gt;&gt;&gt; df = spark.createDataFrame(d).toDF(\"position\", \"reference\", \"alternate\")\n        &gt;&gt;&gt; df.withColumn(\"new_position\", GWASCatalogCuratedAssociationsParser.convert_gnomad_position_to_ensembl(f.col(\"position\"), f.col(\"reference\"), f.col(\"alternate\"))).show()\n        +--------+---------+---------+------------+\n        |position|reference|alternate|new_position|\n        +--------+---------+---------+------------+\n        |       1|        A|        C|           1|\n        |       2|       AA|        C|           3|\n        |       3|        A|       AA|           4|\n        +--------+---------+---------+------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.when(\n        (f.length(reference) &gt; 1) | (f.length(alternate) &gt; 1), position + 1\n    ).otherwise(position)\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser.from_source","title":"<code>from_source(gwas_associations: DataFrame, gnomad_variants: VariantIndex, pvalue_threshold: float = WindowBasedClumpingStepConfig.gwas_significance) -&gt; StudyLocusGWASCatalog</code>  <code>classmethod</code>","text":"<p>Read GWASCatalog associations.</p> <p>It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data:</p> <p>Parameters:</p> Name Type Description Default <code>gwas_associations</code> <code>DataFrame</code> <p>GWAS Catalog raw associations dataset.</p> required <code>gnomad_variants</code> <code>VariantIndex</code> <p>Variant dataset from GnomAD, with allele frequencies.</p> required <code>pvalue_threshold</code> <code>float</code> <p>P-value threshold for flagging associations.</p> <code>gwas_significance</code> <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>GWASCatalogAssociations dataset</p> <p>pvalue_threshold is kept in sync with the WindowBasedClumpingStep gwas_significance.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[GWASCatalogCuratedAssociationsParser],\n    gwas_associations: DataFrame,\n    gnomad_variants: VariantIndex,\n    pvalue_threshold: float = WindowBasedClumpingStepConfig.gwas_significance,\n) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Read GWASCatalog associations.\n\n    It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and\n    applies some pre-defined filters on the data:\n\n    Args:\n        gwas_associations (DataFrame): GWAS Catalog raw associations dataset.\n        gnomad_variants (VariantIndex): Variant dataset from GnomAD, with allele frequencies.\n        pvalue_threshold (float): P-value threshold for flagging associations.\n\n    Returns:\n        StudyLocusGWASCatalog: GWASCatalogAssociations dataset\n\n    pvalue_threshold is kept in sync with the WindowBasedClumpingStep gwas_significance.\n    \"\"\"\n    pvalue_columns = GWASCatalogCuratedAssociationsParser._split_pvalue_column(\n        f.col(\"P-VALUE\")\n    )\n    return StudyLocusGWASCatalog(\n        _df=gwas_associations.withColumn(\n            # temporary column\n            \"rowId\",\n            f.monotonically_increasing_id().cast(StringType()),\n        )\n        .transform(\n            # Map/harmonise variants to variant annotation dataset:\n            # This function adds columns: variantId, referenceAllele, alternateAllele, chromosome, position\n            lambda df: GWASCatalogCuratedAssociationsParser._map_variants_to_gnomad_variants(\n                df, gnomad_variants\n            )\n        )\n        .withColumns(\n            # Perform all quality control checks:\n            {\n                \"qualityControls\": GWASCatalogCuratedAssociationsParser._qc_all(\n                    qc=f.array().alias(\"qualityControls\"),\n                    chromosome=f.col(\"CHR_ID\"),\n                    position=f.col(\"CHR_POS\").cast(IntegerType()),\n                    reference_allele=f.col(\"referenceAllele\"),\n                    alternate_allele=f.col(\"alternateAllele\"),\n                    strongest_snp_risk_allele=f.col(\"STRONGEST SNP-RISK ALLELE\"),\n                    p_value_mantissa=pvalue_columns.mantissa,\n                    p_value_exponent=pvalue_columns.exponent,\n                    p_value_cutoff=pvalue_threshold,\n                )\n            }\n        )\n        # Harmonising effect to beta value and flip effect if needed:\n        .withColumns(\n            {\n                \"pValueMantissa\": pvalue_columns.mantissa,\n                \"pValueExponent\": pvalue_columns.exponent,\n            }\n        )\n        .transform(cls.harmonise_association_effect_to_beta)\n        .withColumnRenamed(\"STUDY ACCESSION\", \"studyId\")\n        # Adding study-locus id:\n        .withColumn(\n            \"studyLocusId\",\n            StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n        )\n        .select(\n            # INSIDE STUDY-LOCUS SCHEMA:\n            \"studyLocusId\",\n            \"variantId\",\n            # Mapped genomic location of the variant (; separated list)\n            \"chromosome\",\n            \"position\",\n            \"studyId\",\n            # p-value of the association, string: split into exponent and mantissa.\n            \"pValueMantissa\",\n            \"pValueExponent\",\n            # Capturing phenotype granularity at the association level\n            GWASCatalogCuratedAssociationsParser._concatenate_substudy_description(\n                f.col(\"DISEASE/TRAIT\"),\n                f.col(\"P-VALUE (TEXT)\"),\n                f.col(\"MAPPED_TRAIT_URI\"),\n            ).alias(\"subStudyDescription\"),\n            # Quality controls (array of strings)\n            \"qualityControls\",\n            \"beta\",\n            \"standardError\",\n        ),\n        _schema=StudyLocusGWASCatalog.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser.harmonise_association_effect_to_beta","title":"<code>harmonise_association_effect_to_beta(df: DataFrame) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>Harmonise effect to beta value.</p> <p>The harmonisation process has a number of steps: - Extracting the reported effect allele. - Flagging palindromic alleles. - Flagging associations where the effect direction needs to be flipped. - Flagging the effect type. - Getting the standard error from the beta and neglog p-value or odds ratio confidence intervals. - Harmonising both beta and odds ratio. - Converting the odds ratio to beta.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with the following columns:</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with the following columns:</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the required columns are missing.</p>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.GWASCatalogCuratedAssociationsParser.harmonise_association_effect_to_beta--examples","title":"Examples:","text":"<p>data = [ ...    # Flagged as palindromic: ...    ('rs123-T', 'A', 'T', '0.1', '[0.08-0.12] unit increase', 21.96), ...    # Not palindromic, beta needs to be flipped: ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12] unit increase', 21.96), ...    # Beta is not flipped: ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12] unit increase', 21.96), ...    # odds ratio: ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12]', 21.96), ...    # odds ratio flipped: ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12]', 21.96), ... ] schema = [\"STRONGEST SNP-RISK ALLELE\", \"referenceAllele\", \"alternateAllele\", \"OR or BETA\", \"95% CI (TEXT)\", \"PVALUE_MLOG\"] df = spark.createDataFrame(data, schema) GWASCatalogCuratedAssociationsParser.harmonise_association_effect_to_beta(df).show() +-------------------------+---------------+---------------+----------+--------------------+-----------+-------------------+-------------------+ |STRONGEST SNP-RISK ALLELE|referenceAllele|alternateAllele|OR or BETA|       95% CI (TEXT)|PVALUE_MLOG|               beta|      standardError| +-------------------------+---------------+---------------+----------+--------------------+-----------+-------------------+-------------------+ |                  rs123-T|              A|              T|       0.1|[0.08-0.12] unit ...|      21.96|               NULL|               NULL| |                  rs123-C|              G|              T|       0.1|[0.08-0.12] unit ...|      21.96|               -0.1|0.01020130187396028| |                  rs123-T|              C|              T|       0.1|[0.08-0.12] unit ...|      21.96|                0.1|0.01020130187396028| |                  rs123-T|              C|              T|       0.1|         [0.08-0.12]|      21.96|-2.3025850929940455|0.23489365624113162| |                  rs123-C|              G|              T|       0.1|         [0.08-0.12]|      21.96|  2.302585092994046|0.23489365624113168| +-------------------------+---------------+---------------+----------+--------------------+-----------+-------------------+-------------------+  Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@staticmethod\ndef harmonise_association_effect_to_beta(\n    df: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"Harmonise effect to beta value.\n\n    The harmonisation process has a number of steps:\n    - Extracting the reported effect allele.\n    - Flagging palindromic alleles.\n    - Flagging associations where the effect direction needs to be flipped.\n    - Flagging the effect type.\n    - Getting the standard error from the beta and neglog p-value or odds ratio confidence intervals.\n    - Harmonising both beta and odds ratio.\n    - Converting the odds ratio to beta.\n\n    Args:\n        df (DataFrame): DataFrame with the following columns:\n\n    Returns:\n        DataFrame: DataFrame with the following columns:\n\n    Raises:\n        ValueError: If any of the required columns are missing.\n\n    Examples:\n    ---\n    &gt;&gt;&gt; data = [\n    ...    # Flagged as palindromic:\n    ...    ('rs123-T', 'A', 'T', '0.1', '[0.08-0.12] unit increase', 21.96),\n    ...    # Not palindromic, beta needs to be flipped:\n    ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12] unit increase', 21.96),\n    ...    # Beta is not flipped:\n    ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12] unit increase', 21.96),\n    ...    # odds ratio:\n    ...    ('rs123-T', 'C', 'T', '0.1', '[0.08-0.12]', 21.96),\n    ...    # odds ratio flipped:\n    ...    ('rs123-C', 'G', 'T', '0.1', '[0.08-0.12]', 21.96),\n    ... ]\n    &gt;&gt;&gt; schema = [\"STRONGEST SNP-RISK ALLELE\", \"referenceAllele\", \"alternateAllele\", \"OR or BETA\", \"95% CI (TEXT)\", \"PVALUE_MLOG\"]\n    &gt;&gt;&gt; df = spark.createDataFrame(data, schema)\n    &gt;&gt;&gt; GWASCatalogCuratedAssociationsParser.harmonise_association_effect_to_beta(df).show()\n    +-------------------------+---------------+---------------+----------+--------------------+-----------+-------------------+-------------------+\n    |STRONGEST SNP-RISK ALLELE|referenceAllele|alternateAllele|OR or BETA|       95% CI (TEXT)|PVALUE_MLOG|               beta|      standardError|\n    +-------------------------+---------------+---------------+----------+--------------------+-----------+-------------------+-------------------+\n    |                  rs123-T|              A|              T|       0.1|[0.08-0.12] unit ...|      21.96|               NULL|               NULL|\n    |                  rs123-C|              G|              T|       0.1|[0.08-0.12] unit ...|      21.96|               -0.1|0.01020130187396028|\n    |                  rs123-T|              C|              T|       0.1|[0.08-0.12] unit ...|      21.96|                0.1|0.01020130187396028|\n    |                  rs123-T|              C|              T|       0.1|         [0.08-0.12]|      21.96|-2.3025850929940455|0.23489365624113162|\n    |                  rs123-C|              G|              T|       0.1|         [0.08-0.12]|      21.96|  2.302585092994046|0.23489365624113168|\n    +-------------------------+---------------+---------------+----------+--------------------+-----------+-------------------+-------------------+\n    &lt;BLANKLINE&gt;\n    \"\"\"\n    # Testing if all columns are in the dataframe:\n    required_columns = [\n        \"STRONGEST SNP-RISK ALLELE\",\n        \"referenceAllele\",\n        \"alternateAllele\",\n        \"OR or BETA\",\n        \"95% CI (TEXT)\",\n        \"PVALUE_MLOG\",\n    ]\n\n    for column in required_columns:\n        if column not in df.columns:\n            raise ValueError(\n                f\"Column {column} is required for harmonising effect to beta value.\"\n            )\n\n    pval_components = pvalue_from_neglogpval(f.col(\"PVALUE_MLOG\"))\n\n    return (\n        df.withColumn(\n            \"reportedRiskAllele\",\n            GWASCatalogCuratedAssociationsParser._extract_risk_allele(\n                f.col(\"STRONGEST SNP-RISK ALLELE\")\n            ),\n        )\n        .withColumns(\n            {\n                # Flag palindromic alleles:\n                \"isAllelePalindromic\": GWASCatalogCuratedAssociationsParser._are_alleles_palindromic(\n                    f.col(\"referenceAllele\"), f.col(\"alternateAllele\")\n                ),\n                # Flag associations, where the effect direction needs to be flipped:\n                \"needsFlipping\": GWASCatalogCuratedAssociationsParser._effect_needs_harmonisation(\n                    f.col(\"reportedRiskAllele\"), f.col(\"referenceAllele\")\n                ),\n                # Flag effect type:\n                \"effectType\": GWASCatalogCuratedAssociationsParser._get_effect_type(\n                    f.col(\"95% CI (TEXT)\")\n                ),\n            }\n        )\n        # Harmonise both beta and odds ratio:\n        .withColumns(\n            {  # Normalise beta value of the association:\n                \"effect_beta\": f.when(\n                    (f.col(\"effectType\") == \"beta\")\n                    &amp; (~f.col(\"isAllelePalindromic\")),\n                    GWASCatalogCuratedAssociationsParser._harmonise_beta(\n                        f.col(\"OR or BETA\"),\n                        f.col(\"95% CI (TEXT)\"),\n                        f.col(\"needsFlipping\"),\n                    ),\n                ),\n                # Normalise odds ratio of the association:\n                \"effect_odds_ratio\": f.when(\n                    (f.col(\"effectType\") == \"odds_ratio\")\n                    &amp; (~f.col(\"isAllelePalindromic\")),\n                    GWASCatalogCuratedAssociationsParser._harmonise_odds_ratio(\n                        f.col(\"OR or BETA\"),\n                        f.col(\"needsFlipping\"),\n                    ),\n                ),\n            },\n        )\n        .select(\n            *df.columns,\n            # Harmonise beta and standard error:\n            *normalise_gwas_statistics(\n                beta=f.col(\"effect_beta\"),\n                odds_ratio=f.col(\"effect_odds_ratio\"),\n                standard_error=f.lit(None).alias(\"standardError\"),\n                ci_lower=f.regexp_extract(\n                    \"95% CI (TEXT)\", r\"\\[(\\d+\\.*\\d*)-\\d+\\.*\\d*\\]\", 1\n                ).cast(FloatType()),\n                ci_upper=f.regexp_extract(\n                    \"95% CI (TEXT)\", r\"\\[\\d+\\.*\\d*-(\\d+\\.*\\d*)\\]\", 1\n                ).cast(FloatType()),\n                mantissa=pval_components.mantissa,\n                exponent=pval_components.exponent,\n            ),\n        )\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog","title":"<code>gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog</code>  <code>dataclass</code>","text":"<p>               Bases: <code>StudyLocus</code></p> <p>Study locus Dataset for GWAS Catalog curated associations.</p> <p>A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>@dataclass\nclass StudyLocusGWASCatalog(StudyLocus):\n    \"\"\"Study locus Dataset for GWAS Catalog curated associations.\n\n    A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.\n    \"\"\"\n\n    def update_study_id(\n        self: StudyLocusGWASCatalog, study_annotation: DataFrame\n    ) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Update final studyId and studyLocusId with a dataframe containing study annotation.\n\n        Args:\n            study_annotation (DataFrame): Dataframe containing `updatedStudyId` and key columns `studyId` and `subStudyDescription`.\n\n        Returns:\n            StudyLocusGWASCatalog: Updated study locus with new `studyId` and `studyLocusId`.\n        \"\"\"\n        self.df = (\n            self._df.join(\n                study_annotation, on=[\"studyId\", \"subStudyDescription\"], how=\"left\"\n            )\n            .withColumn(\"studyId\", f.coalesce(\"updatedStudyId\", \"studyId\"))\n            .drop(\"subStudyDescription\", \"updatedStudyId\")\n        ).withColumn(\n            \"studyLocusId\",\n            StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n        )\n        return self\n\n    def qc_ambiguous_study(self: StudyLocusGWASCatalog) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Flag associations with variants that can not be unambiguously associated with one study.\n\n        Returns:\n            StudyLocusGWASCatalog: Updated study locus.\n        \"\"\"\n        assoc_ambiguity_window = Window.partitionBy(\n            f.col(\"studyId\"), f.col(\"variantId\")\n        )\n\n        self._df.withColumn(\n            \"qualityControls\",\n            StudyLocus.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.count(f.col(\"variantId\")).over(assoc_ambiguity_window) &gt; 1,\n                StudyLocusQualityCheck.AMBIGUOUS_STUDY,\n            ),\n        )\n        return self\n\n    def qc_flag_all_tophits(self: StudyLocusGWASCatalog) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Flag all associations as top hits.\n\n        Returns:\n            StudyLocusGWASCatalog: Updated study locus.\n        \"\"\"\n        return StudyLocusGWASCatalog(\n            _df=self._df.withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.lit(True),\n                    StudyLocusQualityCheck.TOP_HIT,\n                ),\n            ),\n            _schema=StudyLocusGWASCatalog.get_schema(),\n        )\n\n    def apply_inclusion_list(\n        self: StudyLocusGWASCatalog, inclusion_list: DataFrame\n    ) -&gt; StudyLocusGWASCatalog:\n        \"\"\"Restricting GWAS Catalog studies based on a list of accpected study ids.\n\n        Args:\n            inclusion_list (DataFrame): List of accepted GWAS Catalog study identifiers\n\n        Returns:\n            StudyLocusGWASCatalog: Filtered dataset.\n        \"\"\"\n        return StudyLocusGWASCatalog(\n            _df=self.df.join(inclusion_list, on=\"studyId\", how=\"inner\"),\n            _schema=StudyLocusGWASCatalog.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog.apply_inclusion_list","title":"<code>apply_inclusion_list(inclusion_list: DataFrame) -&gt; StudyLocusGWASCatalog</code>","text":"<p>Restricting GWAS Catalog studies based on a list of accpected study ids.</p> <p>Parameters:</p> Name Type Description Default <code>inclusion_list</code> <code>DataFrame</code> <p>List of accepted GWAS Catalog study identifiers</p> required <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>Filtered dataset.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>def apply_inclusion_list(\n    self: StudyLocusGWASCatalog, inclusion_list: DataFrame\n) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Restricting GWAS Catalog studies based on a list of accpected study ids.\n\n    Args:\n        inclusion_list (DataFrame): List of accepted GWAS Catalog study identifiers\n\n    Returns:\n        StudyLocusGWASCatalog: Filtered dataset.\n    \"\"\"\n    return StudyLocusGWASCatalog(\n        _df=self.df.join(inclusion_list, on=\"studyId\", how=\"inner\"),\n        _schema=StudyLocusGWASCatalog.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog.qc_ambiguous_study","title":"<code>qc_ambiguous_study() -&gt; StudyLocusGWASCatalog</code>","text":"<p>Flag associations with variants that can not be unambiguously associated with one study.</p> <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>Updated study locus.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>def qc_ambiguous_study(self: StudyLocusGWASCatalog) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Flag associations with variants that can not be unambiguously associated with one study.\n\n    Returns:\n        StudyLocusGWASCatalog: Updated study locus.\n    \"\"\"\n    assoc_ambiguity_window = Window.partitionBy(\n        f.col(\"studyId\"), f.col(\"variantId\")\n    )\n\n    self._df.withColumn(\n        \"qualityControls\",\n        StudyLocus.update_quality_flag(\n            f.col(\"qualityControls\"),\n            f.count(f.col(\"variantId\")).over(assoc_ambiguity_window) &gt; 1,\n            StudyLocusQualityCheck.AMBIGUOUS_STUDY,\n        ),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog.qc_flag_all_tophits","title":"<code>qc_flag_all_tophits() -&gt; StudyLocusGWASCatalog</code>","text":"<p>Flag all associations as top hits.</p> <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>Updated study locus.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>def qc_flag_all_tophits(self: StudyLocusGWASCatalog) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Flag all associations as top hits.\n\n    Returns:\n        StudyLocusGWASCatalog: Updated study locus.\n    \"\"\"\n    return StudyLocusGWASCatalog(\n        _df=self._df.withColumn(\n            \"qualityControls\",\n            StudyLocus.update_quality_flag(\n                f.col(\"qualityControls\"),\n                f.lit(True),\n                StudyLocusQualityCheck.TOP_HIT,\n            ),\n        ),\n        _schema=StudyLocusGWASCatalog.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/associations/#gentropy.datasource.gwas_catalog.associations.StudyLocusGWASCatalog.update_study_id","title":"<code>update_study_id(study_annotation: DataFrame) -&gt; StudyLocusGWASCatalog</code>","text":"<p>Update final studyId and studyLocusId with a dataframe containing study annotation.</p> <p>Parameters:</p> Name Type Description Default <code>study_annotation</code> <code>DataFrame</code> <p>Dataframe containing <code>updatedStudyId</code> and key columns <code>studyId</code> and <code>subStudyDescription</code>.</p> required <p>Returns:</p> Name Type Description <code>StudyLocusGWASCatalog</code> <code>StudyLocusGWASCatalog</code> <p>Updated study locus with new <code>studyId</code> and <code>studyLocusId</code>.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/associations.py</code> <pre><code>def update_study_id(\n    self: StudyLocusGWASCatalog, study_annotation: DataFrame\n) -&gt; StudyLocusGWASCatalog:\n    \"\"\"Update final studyId and studyLocusId with a dataframe containing study annotation.\n\n    Args:\n        study_annotation (DataFrame): Dataframe containing `updatedStudyId` and key columns `studyId` and `subStudyDescription`.\n\n    Returns:\n        StudyLocusGWASCatalog: Updated study locus with new `studyId` and `studyLocusId`.\n    \"\"\"\n    self.df = (\n        self._df.join(\n            study_annotation, on=[\"studyId\", \"subStudyDescription\"], how=\"left\"\n        )\n        .withColumn(\"studyId\", f.coalesce(\"updatedStudyId\", \"studyId\"))\n        .drop(\"subStudyDescription\", \"updatedStudyId\")\n    ).withColumn(\n        \"studyLocusId\",\n        StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/","title":"Study Index","text":""},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalogParser","title":"<code>gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalogParser</code>  <code>dataclass</code>","text":"<p>GWAS Catalog study index parser.</p> <p>The following information is harmonised from the GWAS Catalog:</p> <ul> <li>All publication related information retained.</li> <li>Mapped measured and background traits parsed.</li> <li>Flagged if harmonized summary statistics datasets available.</li> <li>If available, the ftp path to these files presented.</li> <li>Ancestries from the discovery and replication stages are structured with sample counts.</li> <li>Case/control counts extracted.</li> <li>The number of samples with European ancestry extracted.</li> </ul> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>@dataclass\nclass StudyIndexGWASCatalogParser:\n    \"\"\"GWAS Catalog study index parser.\n\n    The following information is harmonised from the GWAS Catalog:\n\n    - All publication related information retained.\n    - Mapped measured and background traits parsed.\n    - Flagged if harmonized summary statistics datasets available.\n    - If available, the ftp path to these files presented.\n    - Ancestries from the discovery and replication stages are structured with sample counts.\n    - Case/control counts extracted.\n    - The number of samples with European ancestry extracted.\n\n    \"\"\"\n\n    @staticmethod\n    def _parse_discovery_samples(discovery_samples: Column) -&gt; Column:\n        \"\"\"Parse discovery sample sizes from GWAS Catalog.\n\n        This is a curated field. From publication sometimes it is not clear how the samples were split\n        across the reported ancestries. In such cases we are assuming the ancestries were evenly presented\n        and the total sample size is split:\n\n        [\"European, African\", 100] -&gt; [\"European, 50], [\"African\", 50]\n\n        Args:\n            discovery_samples (Column): Raw discovery sample sizes\n\n        Returns:\n            Column: Parsed and de-duplicated list of discovery ancestries with sample size.\n\n        Examples:\n            &gt;&gt;&gt; data = [('s1', \"European\", 10), ('s1', \"African\", 10), ('s2', \"European, African, Asian\", 100), ('s2', \"European\", 50)]\n            &gt;&gt;&gt; df = (\n            ...    spark.createDataFrame(data, ['studyId', 'ancestry', 'sampleSize'])\n            ...    .groupBy('studyId')\n            ...    .agg(\n            ...        f.collect_set(\n            ...            f.struct('ancestry', 'sampleSize')\n            ...        ).alias('discoverySampleSize')\n            ...    )\n            ...    .orderBy('studyId')\n            ...    .withColumn('discoverySampleSize', StudyIndexGWASCatalogParser._parse_discovery_samples(f.col('discoverySampleSize')))\n            ...    .select('discoverySampleSize')\n            ...    .show(truncate=False)\n            ... )\n            +--------------------------------------------+\n            |discoverySampleSize                         |\n            +--------------------------------------------+\n            |[{African, 10}, {European, 10}]             |\n            |[{European, 83}, {African, 33}, {Asian, 33}]|\n            +--------------------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # To initialize return objects for aggregate functions, schema has to be defined:\n        schema = t.ArrayType(\n            t.StructType(\n                [\n                    t.StructField(\"ancestry\", t.StringType(), True),\n                    t.StructField(\"sampleSize\", t.IntegerType(), True),\n                ]\n            )\n        )\n\n        # Splitting comma separated ancestries:\n        exploded_ancestries = f.transform(\n            discovery_samples,\n            lambda sample: f.split(sample.ancestry, r\",\\s(?![^()]*\\))\"),\n        )\n\n        # Initialize discoverySample object from unique list of ancestries:\n        unique_ancestries = f.transform(\n            f.aggregate(\n                exploded_ancestries,\n                f.array().cast(t.ArrayType(t.StringType())),\n                lambda x, y: f.array_union(x, y),\n                f.array_distinct,\n            ),\n            lambda ancestry: f.struct(\n                ancestry.alias(\"ancestry\"),\n                f.lit(0).alias(\"sampleSize\"),\n            ),\n        )\n\n        # Computing sample sizes for ancestries when splitting is needed:\n        resolved_sample_count = f.transform(\n            f.arrays_zip(\n                f.transform(exploded_ancestries, lambda pop: f.size(pop)).alias(\n                    \"pop_size\"\n                ),\n                f.transform(discovery_samples, lambda pop: pop.sampleSize).alias(\n                    \"pop_count\"\n                ),\n            ),\n            lambda pop: (pop.pop_count / pop.pop_size).cast(t.IntegerType()),\n        )\n\n        # Flattening out ancestries with sample sizes:\n        parsed_sample_size = f.aggregate(\n            f.transform(\n                f.arrays_zip(\n                    exploded_ancestries.alias(\"ancestries\"),\n                    resolved_sample_count.alias(\"sample_count\"),\n                ),\n                StudyIndexGWASCatalogParser._merge_ancestries_and_counts,\n            ),\n            f.array().cast(schema),\n            lambda x, y: f.array_union(x, y),\n        )\n\n        # Normalize ancestries:\n        return f.aggregate(\n            parsed_sample_size,\n            unique_ancestries,\n            StudyIndexGWASCatalogParser._normalize_ancestries,\n        )\n\n    @staticmethod\n    def _normalize_ancestries(merged: Column, ancestry: Column) -&gt; Column:\n        \"\"\"Normalize ancestries from a list of structs.\n\n        As some ancestry label might be repeated with different sample counts,\n        these counts need to be collected.\n\n        Args:\n            merged (Column): Resulting list of struct with unique ancestries.\n            ancestry (Column): One ancestry object coming from raw.\n\n        Returns:\n            Column: Unique list of ancestries with the sample counts.\n        \"\"\"\n        # Iterating over the list of unique ancestries and adding the sample size if label matches:\n        return f.transform(\n            merged,\n            lambda a: f.when(\n                a.ancestry == ancestry.ancestry,\n                f.struct(\n                    a.ancestry.alias(\"ancestry\"),\n                    (a.sampleSize + ancestry.sampleSize)\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                ),\n            ).otherwise(a),\n        )\n\n    @staticmethod\n    def _merge_ancestries_and_counts(ancestry_group: Column) -&gt; Column:\n        \"\"\"Merge ancestries with sample sizes.\n\n        After splitting ancestry annotations, all resulting ancestries needs to be assigned\n        with the proper sample size.\n\n        Args:\n            ancestry_group (Column): Each element is a struct with `sample_count` (int) and `ancestries` (list)\n\n        Returns:\n            Column: a list of structs with `ancestry` and `sampleSize` fields.\n\n        Examples:\n            &gt;&gt;&gt; data = [(12, ['African', 'European']),(12, ['African'])]\n            &gt;&gt;&gt; (\n            ...     spark.createDataFrame(data, ['sample_count', 'ancestries'])\n            ...     .select(StudyIndexGWASCatalogParser._merge_ancestries_and_counts(f.struct('sample_count', 'ancestries')).alias('test'))\n            ...     .show(truncate=False)\n            ... )\n            +-------------------------------+\n            |test                           |\n            +-------------------------------+\n            |[{African, 12}, {European, 12}]|\n            |[{African, 12}]                |\n            +-------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        # Extract sample size for the ancestry group:\n        count = ancestry_group.sample_count\n\n        # We need to loop through the ancestries:\n        return f.transform(\n            ancestry_group.ancestries,\n            lambda ancestry: f.struct(\n                ancestry.alias(\"ancestry\"),\n                count.alias(\"sampleSize\"),\n            ),\n        )\n\n    @staticmethod\n    def parse_cohorts(raw_cohort: Column) -&gt; Column:\n        \"\"\"Return a list of unique cohort labels from pipe separated list if provided.\n\n        Args:\n            raw_cohort (Column): Cohort list column, where labels are separated by `|` sign.\n\n        Returns:\n            Column: an array colun with string elements.\n\n        Examples:\n        &gt;&gt;&gt; data = [('BioME|CaPS|Estonia|FHS|UKB|GERA|GERA|GERA',),(None,),]\n        &gt;&gt;&gt; spark.createDataFrame(data, ['cohorts']).select(StudyIndexGWASCatalogParser.parse_cohorts(f.col('cohorts')).alias('parsedCohorts')).show(truncate=False)\n        +--------------------------------------+\n        |parsedCohorts                         |\n        +--------------------------------------+\n        |[BioME, CaPS, Estonia, FHS, UKB, GERA]|\n        |NULL                                  |\n        +--------------------------------------+\n        &lt;BLANKLINE&gt;\n        \"\"\"\n        return f.when(\n            (raw_cohort.isNotNull()) &amp; (raw_cohort != \"\"),\n            f.array_distinct(f.split(raw_cohort, r\"\\|\")),\n        )\n\n    @classmethod\n    def _parse_study_table(\n        cls: type[StudyIndexGWASCatalogParser], catalog_studies: DataFrame\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Harmonise GWASCatalog study table with `StudyIndex` schema.\n\n        Args:\n            catalog_studies (DataFrame): GWAS Catalog study table\n\n        Returns:\n            StudyIndexGWASCatalog: Parsed and annotated GWAS Catalog study table.\n        \"\"\"\n        return StudyIndexGWASCatalog(\n            _df=catalog_studies.select(\n                f.coalesce(\n                    f.col(\"STUDY ACCESSION\"), f.monotonically_increasing_id()\n                ).alias(\"studyId\"),\n                f.lit(\"GCST\").alias(\"projectId\"),\n                f.lit(\"gwas\").alias(\"studyType\"),\n                f.col(\"PUBMED ID\").alias(\"pubmedId\"),\n                f.col(\"FIRST AUTHOR\").alias(\"publicationFirstAuthor\"),\n                f.col(\"DATE\").alias(\"publicationDate\"),\n                f.col(\"JOURNAL\").alias(\"publicationJournal\"),\n                f.col(\"STUDY\").alias(\"publicationTitle\"),\n                f.coalesce(f.col(\"DISEASE/TRAIT\"), f.lit(\"Unreported\")).alias(\n                    \"traitFromSource\"\n                ),\n                f.col(\"INITIAL SAMPLE SIZE\").alias(\"initialSampleSize\"),\n                parse_efos(f.col(\"MAPPED_TRAIT_URI\")).alias(\"traitFromSourceMappedIds\"),\n                parse_efos(f.col(\"MAPPED BACKGROUND TRAIT URI\")).alias(\n                    \"backgroundTraitFromSourceMappedIds\"\n                ),\n                cls.parse_cohorts(f.col(\"COHORT\")).alias(\"cohorts\"),\n            ),\n            _schema=StudyIndexGWASCatalog.get_schema(),\n        )\n\n    @classmethod\n    def from_source(\n        cls: type[StudyIndexGWASCatalogParser],\n        catalog_studies: DataFrame,\n        ancestry_file: DataFrame,\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Ingests study level metadata from the GWAS Catalog.\n\n        Args:\n            catalog_studies (DataFrame): GWAS Catalog raw study table\n            ancestry_file (DataFrame): GWAS Catalog ancestry table.\n\n        Returns:\n            StudyIndexGWASCatalog: Parsed and annotated GWAS Catalog study table.\n        \"\"\"\n        # Read GWAS Catalogue raw data\n        return (\n            cls._parse_study_table(catalog_studies)\n            .annotate_ancestries(ancestry_file)\n            .annotate_discovery_sample_sizes()\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalogParser.from_source","title":"<code>from_source(catalog_studies: DataFrame, ancestry_file: DataFrame) -&gt; StudyIndexGWASCatalog</code>  <code>classmethod</code>","text":"<p>Ingests study level metadata from the GWAS Catalog.</p> <p>Parameters:</p> Name Type Description Default <code>catalog_studies</code> <code>DataFrame</code> <p>GWAS Catalog raw study table</p> required <code>ancestry_file</code> <code>DataFrame</code> <p>GWAS Catalog ancestry table.</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Parsed and annotated GWAS Catalog study table.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[StudyIndexGWASCatalogParser],\n    catalog_studies: DataFrame,\n    ancestry_file: DataFrame,\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Ingests study level metadata from the GWAS Catalog.\n\n    Args:\n        catalog_studies (DataFrame): GWAS Catalog raw study table\n        ancestry_file (DataFrame): GWAS Catalog ancestry table.\n\n    Returns:\n        StudyIndexGWASCatalog: Parsed and annotated GWAS Catalog study table.\n    \"\"\"\n    # Read GWAS Catalogue raw data\n    return (\n        cls._parse_study_table(catalog_studies)\n        .annotate_ancestries(ancestry_file)\n        .annotate_discovery_sample_sizes()\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalogParser.parse_cohorts","title":"<code>parse_cohorts(raw_cohort: Column) -&gt; Column</code>  <code>staticmethod</code>","text":"<p>Return a list of unique cohort labels from pipe separated list if provided.</p> <p>Parameters:</p> Name Type Description Default <code>raw_cohort</code> <code>Column</code> <p>Cohort list column, where labels are separated by <code>|</code> sign.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>an array colun with string elements.</p> <p>Examples:</p> <p>data = [('BioME|CaPS|Estonia|FHS|UKB|GERA|GERA|GERA',),(None,),] spark.createDataFrame(data, ['cohorts']).select(StudyIndexGWASCatalogParser.parse_cohorts(f.col('cohorts')).alias('parsedCohorts')).show(truncate=False) +--------------------------------------+ |parsedCohorts                         | +--------------------------------------+ |[BioME, CaPS, Estonia, FHS, UKB, GERA]| |NULL                                  | +--------------------------------------+  Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>@staticmethod\ndef parse_cohorts(raw_cohort: Column) -&gt; Column:\n    \"\"\"Return a list of unique cohort labels from pipe separated list if provided.\n\n    Args:\n        raw_cohort (Column): Cohort list column, where labels are separated by `|` sign.\n\n    Returns:\n        Column: an array colun with string elements.\n\n    Examples:\n    &gt;&gt;&gt; data = [('BioME|CaPS|Estonia|FHS|UKB|GERA|GERA|GERA',),(None,),]\n    &gt;&gt;&gt; spark.createDataFrame(data, ['cohorts']).select(StudyIndexGWASCatalogParser.parse_cohorts(f.col('cohorts')).alias('parsedCohorts')).show(truncate=False)\n    +--------------------------------------+\n    |parsedCohorts                         |\n    +--------------------------------------+\n    |[BioME, CaPS, Estonia, FHS, UKB, GERA]|\n    |NULL                                  |\n    +--------------------------------------+\n    &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.when(\n        (raw_cohort.isNotNull()) &amp; (raw_cohort != \"\"),\n        f.array_distinct(f.split(raw_cohort, r\"\\|\")),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog","title":"<code>gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog</code>  <code>dataclass</code>","text":"<p>               Bases: <code>StudyIndex</code></p> <p>Study index dataset from GWAS Catalog.</p> <p>A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>@dataclass\nclass StudyIndexGWASCatalog(StudyIndex):\n    \"\"\"Study index dataset from GWAS Catalog.\n\n    A study index dataset captures all the metadata for all studies including GWAS and Molecular QTL.\n    \"\"\"\n\n    def update_study_id(\n        self: StudyIndexGWASCatalog, study_annotation: DataFrame\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Update studyId with a dataframe containing study.\n\n        Args:\n            study_annotation (DataFrame): Dataframe containing `updatedStudyId`, `traitFromSource`, `traitFromSourceMappedIds` and key column `studyId`.\n\n        Returns:\n            StudyIndexGWASCatalog: Updated study table.\n        \"\"\"\n        self.df = (\n            self._df.join(\n                study_annotation.select(\n                    *[\n                        f.col(c).alias(f\"updated{c}\")\n                        if c not in [\"studyId\", \"updatedStudyId\"]\n                        else f.col(c)\n                        for c in study_annotation.columns\n                    ]\n                ),\n                on=\"studyId\",\n                how=\"left\",\n            )\n            .withColumn(\n                \"studyId\",\n                f.coalesce(f.col(\"updatedStudyId\"), f.col(\"studyId\")),\n            )\n            .withColumn(\n                \"traitFromSource\",\n                f.coalesce(f.col(\"updatedtraitFromSource\"), f.col(\"traitFromSource\")),\n            )\n            .withColumn(\n                \"traitFromSourceMappedIds\",\n                f.coalesce(\n                    f.col(\"updatedtraitFromSourceMappedIds\"),\n                    f.col(\"traitFromSourceMappedIds\"),\n                ),\n            )\n            .select(self._df.columns)\n        )\n\n        return self\n\n    def annotate_from_study_curation(\n        self: StudyIndexGWASCatalog, curation_table: DataFrame | None\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Annotating study index with curation.\n\n        Args:\n            curation_table (DataFrame | None): Curated GWAS Catalog studies with summary statistics\n\n        Returns:\n            StudyIndexGWASCatalog: Updated study index\n        \"\"\"\n        studies = self.df\n\n        if \"qualityControls\" not in studies.columns:\n            studies = studies.withColumn(\n                \"qualityControls\", f.array().cast(ArrayType(StringType()))\n            )\n\n        if \"analysisFlags\" not in studies.columns:\n            studies = studies.withColumn(\n                \"analysisFlags\", f.array().cast(ArrayType(StringType()))\n            )\n\n        # Providing curation table is optional. However once this method is called, the quality and studyFlag columns are added.\n        if curation_table is None:\n            return StudyIndexGWASCatalog(\n                _df=studies, _schema=StudyIndexGWASCatalog.get_schema()\n            )\n\n        # Adding prefix to columns in the curation table:\n        curation_table = curation_table.select(\n            *[\n                f.col(column).alias(f\"curation_{column}\")\n                if column != \"studyId\"\n                else f.col(column)\n                for column in curation_table.columns\n            ]\n        )\n\n        # Based on the curation table, columns needs to be updated:\n        curated_df = (\n            studies.join(\n                curation_table.withColumn(\"isCurated\", f.lit(True)),\n                on=\"studyId\",\n                how=\"left\",\n            )\n            .withColumn(\"isCurated\", f.coalesce(f.col(\"isCurated\"), f.lit(False)))\n            # Updating study type:\n            .withColumn(\n                \"studyType\", f.coalesce(f.col(\"curation_studyType\"), f.col(\"studyType\"))\n            )\n            # Updating study annotation flags:\n            .withColumn(\n                \"analysisFlags\",\n                f.array_union(f.col(\"analysisFlags\"), f.col(\"curation_analysisFlags\")),\n            )\n            .withColumn(\"analysisFlags\", f.coalesce(f.col(\"analysisFlags\"), f.array()))\n            .withColumn(\n                \"qualityControls\",\n                StudyIndex.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    ~f.col(\"isCurated\"),\n                    StudyQualityCheck.NO_OT_CURATION,\n                ),\n            )\n            # Dropping columns coming from the curation table:\n            .select(*studies.columns)\n        )\n        return StudyIndexGWASCatalog(\n            _df=curated_df, _schema=StudyIndexGWASCatalog.get_schema()\n        )\n\n    def extract_studies_for_curation(\n        self: StudyIndexGWASCatalog, curation: DataFrame | None\n    ) -&gt; DataFrame:\n        \"\"\"Extract studies for curation.\n\n        Args:\n            curation (DataFrame | None): Dataframe with curation.\n\n        Returns:\n            DataFrame: Updated curation table. New studies are have the `isCurated` False.\n        \"\"\"\n        # If no curation table provided, assume all studies needs curation:\n        if curation is None:\n            return (\n                self.df\n                # Curation only applyed on studies with summary statistics:\n                .filter(f.col(\"hasSumstats\"))\n                # Adding columns expected in the curation table - array columns aready flattened:\n                .withColumn(\"studyType\", f.lit(None).cast(t.StringType()))\n                .withColumn(\"analysisFlag\", f.lit(None).cast(t.StringType()))\n                .withColumn(\"qualityControl\", f.lit(None).cast(t.StringType()))\n                .withColumn(\"isCurated\", f.lit(False).cast(t.StringType()))\n            )\n\n        # Adding prefix to columns in the curation table:\n        curation = curation.select(\n            *[\n                f.col(column).alias(f\"curation_{column}\")\n                if column != \"studyId\"\n                else f.col(column)\n                for column in curation.columns\n            ]\n        )\n\n        return (\n            self.df\n            # Curation only applyed on studies with summary statistics:\n            .filter(f.col(\"hasSumstats\"))\n            .join(curation, on=\"studyId\", how=\"left\")\n            .select(\n                \"studyId\",\n                # Propagate existing curation - array columns are being flattened:\n                f.col(\"curation_studyType\").alias(\"studyType\"),\n                f.array_join(f.col(\"curation_analysisFlags\"), \"|\").alias(\n                    \"analysisFlag\"\n                ),\n                f.array_join(f.col(\"curation_qualityControls\"), \"|\").alias(\n                    \"qualityControl\"\n                ),\n                # This boolean flag needs to be casted to string, because saving to tsv would fail otherwise:\n                f.coalesce(f.col(\"curation_isCurated\"), f.lit(False))\n                .cast(t.StringType())\n                .alias(\"isCurated\"),\n                # The following columns are propagated to make curation easier:\n                \"pubmedId\",\n                \"publicationTitle\",\n                \"traitFromSource\",\n            )\n        )\n\n    def annotate_ancestries(\n        self: StudyIndexGWASCatalog, ancestry_lut: DataFrame\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Extracting sample sizes and ancestry information.\n\n        This function parses the ancestry data. Also get counts for the europeans in the same\n        discovery stage.\n\n        Args:\n            ancestry_lut (DataFrame): Ancestry table as downloaded from the GWAS Catalog\n\n        Returns:\n            StudyIndexGWASCatalog: Slimmed and cleaned version of the ancestry annotation.\n        \"\"\"\n        from gentropy.datasource.gwas_catalog.study_index import (\n            StudyIndexGWASCatalogParser as GWASCatalogStudyIndexParser,\n        )\n\n        ancestry = (\n            ancestry_lut\n            # Convert column headers to camelcase:\n            .transform(\n                lambda df: df.select(\n                    *[f.expr(column2camel_case(x)) for x in df.columns]\n                )\n            ).withColumnRenamed(\n                \"studyAccession\", \"studyId\"\n            )  # studyId has not been split yet\n        )\n\n        # Get a high resolution dataset on experimental stage:\n        ancestry_stages = (\n            ancestry.groupBy(\"studyId\")\n            .pivot(\"stage\")\n            .agg(\n                f.collect_set(\n                    f.struct(\n                        f.col(\"broadAncestralCategory\").alias(\"ancestry\"),\n                        f.col(\"numberOfIndividuals\")\n                        .cast(t.IntegerType())\n                        .alias(\"sampleSize\"),\n                    )\n                )\n            )\n            .withColumn(\n                \"discoverySamples\",\n                GWASCatalogStudyIndexParser._parse_discovery_samples(f.col(\"initial\")),\n            )\n            .withColumnRenamed(\"replication\", \"replicationSamples\")\n            # Mapping discovery stage ancestries to LD reference:\n            .withColumn(\n                \"ldPopulationStructure\",\n                self.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n            )\n            .drop(\"initial\")\n            .persist()\n        )\n\n        # Generate information on the ancestry composition of the discovery stage, and calculate\n        # the proportion of the Europeans:\n        europeans_deconvoluted = (\n            ancestry\n            # Focus on discovery stage:\n            .filter(f.col(\"stage\") == \"initial\")\n            # Sorting ancestries if European:\n            .withColumn(\n                \"ancestryFlag\",\n                # Excluding finnish:\n                f.when(\n                    f.col(\"initialSampleDescription\").contains(\"Finnish\"),\n                    f.lit(\"other\"),\n                )\n                # Excluding Icelandic population:\n                .when(\n                    f.col(\"initialSampleDescription\").contains(\"Icelandic\"),\n                    f.lit(\"other\"),\n                )\n                # Including European ancestry:\n                .when(f.col(\"broadAncestralCategory\") == \"European\", f.lit(\"european\"))\n                # Exclude all other population:\n                .otherwise(\"other\"),\n            )\n            # Grouping by study accession and initial sample description:\n            .groupBy(\"studyId\")\n            .pivot(\"ancestryFlag\")\n            .agg(\n                # Summarizing sample sizes for all ancestries:\n                f.sum(f.col(\"numberOfIndividuals\"))\n            )\n            # Do arithmetics to make sure we have the right proportion of european in the set:\n            .withColumn(\n                \"initialSampleCountEuropean\",\n                f.when(f.col(\"european\").isNull(), f.lit(0)).otherwise(\n                    f.col(\"european\")\n                ),\n            )\n            .withColumn(\n                \"initialSampleCountOther\",\n                f.when(f.col(\"other\").isNull(), f.lit(0)).otherwise(f.col(\"other\")),\n            )\n            .withColumn(\n                \"initialSampleCount\",\n                f.col(\"initialSampleCountEuropean\") + f.col(\"other\"),\n            )\n            .drop(\n                \"european\",\n                \"other\",\n                \"initialSampleCount\",\n                \"initialSampleCountEuropean\",\n                \"initialSampleCountOther\",\n            )\n        )\n\n        parsed_ancestry_lut = ancestry_stages.join(\n            europeans_deconvoluted, on=\"studyId\", how=\"outer\"\n        ).select(\n            \"studyId\", \"discoverySamples\", \"ldPopulationStructure\", \"replicationSamples\"\n        )\n        self.df = self.df.join(parsed_ancestry_lut, on=\"studyId\", how=\"left\")\n        return self\n\n    def annotate_discovery_sample_sizes(\n        self: StudyIndexGWASCatalog,\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog.\n\n        For some studies that measure quantitative traits, nCases and nControls can't be extracted. Therefore, we assume these are 0.\n\n        Returns:\n            StudyIndexGWASCatalog: object with columns `nCases`, `nControls`, and `nSamples` per `studyId` correctly extracted.\n        \"\"\"\n        sample_size_lut = (\n            self.df.select(\n                \"studyId\",\n                f.explode_outer(f.split(f.col(\"initialSampleSize\"), r\",\\s+\")).alias(\n                    \"samples\"\n                ),\n            )\n            # Extracting the sample size from the string:\n            .withColumn(\n                \"sampleSize\",\n                f.regexp_extract(\n                    f.regexp_replace(f.col(\"samples\"), \",\", \"\"), r\"[0-9,]+\", 0\n                ).cast(t.IntegerType()),\n            )\n            .select(\n                \"studyId\",\n                \"sampleSize\",\n                f.when(f.col(\"samples\").contains(\"cases\"), f.col(\"sampleSize\"))\n                .otherwise(f.lit(0))\n                .alias(\"nCases\"),\n                f.when(f.col(\"samples\").contains(\"controls\"), f.col(\"sampleSize\"))\n                .otherwise(f.lit(0))\n                .alias(\"nControls\"),\n            )\n            # Aggregating sample sizes for all ancestries:\n            .groupBy(\"studyId\")  # studyId has not been split yet\n            .agg(\n                f.sum(\"nCases\").cast(\"integer\").alias(\"nCases\"),\n                f.sum(\"nControls\").cast(\"integer\").alias(\"nControls\"),\n                f.sum(\"sampleSize\").cast(\"integer\").alias(\"nSamples\"),\n            )\n        )\n        self.df = self.df.join(sample_size_lut, on=\"studyId\", how=\"left\")\n        return self\n\n    def apply_inclusion_list(\n        self: StudyIndexGWASCatalog, inclusion_list: DataFrame\n    ) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Restricting GWAS Catalog studies based on a list of accepted study identifiers.\n\n        Args:\n            inclusion_list (DataFrame): List of accepted GWAS Catalog study identifiers\n\n        Returns:\n            StudyIndexGWASCatalog: Filtered dataset.\n        \"\"\"\n        return StudyIndexGWASCatalog(\n            _df=self.df.join(inclusion_list, on=\"studyId\", how=\"inner\"),\n            _schema=StudyIndexGWASCatalog.get_schema(),\n        )\n\n    def add_no_sumstats_flag(self: StudyIndexGWASCatalog) -&gt; StudyIndexGWASCatalog:\n        \"\"\"Add a flag to the study index if no summary statistics are available.\n\n        Returns:\n            StudyIndexGWASCatalog: Updated study index.\n        \"\"\"\n        self.df = self.df.withColumn(\n            \"qualityControls\",\n            f.array(f.lit(StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value)),\n        )\n        return self\n\n    @staticmethod\n    def _parse_gwas_catalog_study_id(sumstats_path_column: str) -&gt; Column:\n        \"\"\"Extract GWAS Catalog study accession from the summary statistics path.\n\n        Args:\n            sumstats_path_column (str): column *name* for the summary statistics path\n\n        Returns:\n            Column: GWAS Catalog study accession.\n\n        Examples:\n            &gt;&gt;&gt; data = [\n            ... ('./GCST90086001-GCST90087000/GCST90086758/harmonised/35078996-GCST90086758-EFO_0007937.h.tsv.gz',),\n            ...    ('gs://open-targets-gwas-summary-stats/harmonised/GCST000568.parquet/',),\n            ...    (None,)\n            ... ]\n            &gt;&gt;&gt; spark.createDataFrame(data, ['testColumn']).select(StudyIndexGWASCatalog._parse_gwas_catalog_study_id('testColumn').alias('accessions')).collect()\n            [Row(accessions='GCST90086758'), Row(accessions='GCST000568'), Row(accessions=None)]\n        \"\"\"\n        accesions = f.expr(rf\"regexp_extract_all({sumstats_path_column}, '(GCST\\\\d+)')\")\n        return accesions[f.size(accesions) - 1]\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.add_no_sumstats_flag","title":"<code>add_no_sumstats_flag() -&gt; StudyIndexGWASCatalog</code>","text":"<p>Add a flag to the study index if no summary statistics are available.</p> <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Updated study index.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def add_no_sumstats_flag(self: StudyIndexGWASCatalog) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Add a flag to the study index if no summary statistics are available.\n\n    Returns:\n        StudyIndexGWASCatalog: Updated study index.\n    \"\"\"\n    self.df = self.df.withColumn(\n        \"qualityControls\",\n        f.array(f.lit(StudyQualityCheck.SUMSTATS_NOT_AVAILABLE.value)),\n    )\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.annotate_ancestries","title":"<code>annotate_ancestries(ancestry_lut: DataFrame) -&gt; StudyIndexGWASCatalog</code>","text":"<p>Extracting sample sizes and ancestry information.</p> <p>This function parses the ancestry data. Also get counts for the europeans in the same discovery stage.</p> <p>Parameters:</p> Name Type Description Default <code>ancestry_lut</code> <code>DataFrame</code> <p>Ancestry table as downloaded from the GWAS Catalog</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Slimmed and cleaned version of the ancestry annotation.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def annotate_ancestries(\n    self: StudyIndexGWASCatalog, ancestry_lut: DataFrame\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Extracting sample sizes and ancestry information.\n\n    This function parses the ancestry data. Also get counts for the europeans in the same\n    discovery stage.\n\n    Args:\n        ancestry_lut (DataFrame): Ancestry table as downloaded from the GWAS Catalog\n\n    Returns:\n        StudyIndexGWASCatalog: Slimmed and cleaned version of the ancestry annotation.\n    \"\"\"\n    from gentropy.datasource.gwas_catalog.study_index import (\n        StudyIndexGWASCatalogParser as GWASCatalogStudyIndexParser,\n    )\n\n    ancestry = (\n        ancestry_lut\n        # Convert column headers to camelcase:\n        .transform(\n            lambda df: df.select(\n                *[f.expr(column2camel_case(x)) for x in df.columns]\n            )\n        ).withColumnRenamed(\n            \"studyAccession\", \"studyId\"\n        )  # studyId has not been split yet\n    )\n\n    # Get a high resolution dataset on experimental stage:\n    ancestry_stages = (\n        ancestry.groupBy(\"studyId\")\n        .pivot(\"stage\")\n        .agg(\n            f.collect_set(\n                f.struct(\n                    f.col(\"broadAncestralCategory\").alias(\"ancestry\"),\n                    f.col(\"numberOfIndividuals\")\n                    .cast(t.IntegerType())\n                    .alias(\"sampleSize\"),\n                )\n            )\n        )\n        .withColumn(\n            \"discoverySamples\",\n            GWASCatalogStudyIndexParser._parse_discovery_samples(f.col(\"initial\")),\n        )\n        .withColumnRenamed(\"replication\", \"replicationSamples\")\n        # Mapping discovery stage ancestries to LD reference:\n        .withColumn(\n            \"ldPopulationStructure\",\n            self.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n        )\n        .drop(\"initial\")\n        .persist()\n    )\n\n    # Generate information on the ancestry composition of the discovery stage, and calculate\n    # the proportion of the Europeans:\n    europeans_deconvoluted = (\n        ancestry\n        # Focus on discovery stage:\n        .filter(f.col(\"stage\") == \"initial\")\n        # Sorting ancestries if European:\n        .withColumn(\n            \"ancestryFlag\",\n            # Excluding finnish:\n            f.when(\n                f.col(\"initialSampleDescription\").contains(\"Finnish\"),\n                f.lit(\"other\"),\n            )\n            # Excluding Icelandic population:\n            .when(\n                f.col(\"initialSampleDescription\").contains(\"Icelandic\"),\n                f.lit(\"other\"),\n            )\n            # Including European ancestry:\n            .when(f.col(\"broadAncestralCategory\") == \"European\", f.lit(\"european\"))\n            # Exclude all other population:\n            .otherwise(\"other\"),\n        )\n        # Grouping by study accession and initial sample description:\n        .groupBy(\"studyId\")\n        .pivot(\"ancestryFlag\")\n        .agg(\n            # Summarizing sample sizes for all ancestries:\n            f.sum(f.col(\"numberOfIndividuals\"))\n        )\n        # Do arithmetics to make sure we have the right proportion of european in the set:\n        .withColumn(\n            \"initialSampleCountEuropean\",\n            f.when(f.col(\"european\").isNull(), f.lit(0)).otherwise(\n                f.col(\"european\")\n            ),\n        )\n        .withColumn(\n            \"initialSampleCountOther\",\n            f.when(f.col(\"other\").isNull(), f.lit(0)).otherwise(f.col(\"other\")),\n        )\n        .withColumn(\n            \"initialSampleCount\",\n            f.col(\"initialSampleCountEuropean\") + f.col(\"other\"),\n        )\n        .drop(\n            \"european\",\n            \"other\",\n            \"initialSampleCount\",\n            \"initialSampleCountEuropean\",\n            \"initialSampleCountOther\",\n        )\n    )\n\n    parsed_ancestry_lut = ancestry_stages.join(\n        europeans_deconvoluted, on=\"studyId\", how=\"outer\"\n    ).select(\n        \"studyId\", \"discoverySamples\", \"ldPopulationStructure\", \"replicationSamples\"\n    )\n    self.df = self.df.join(parsed_ancestry_lut, on=\"studyId\", how=\"left\")\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.annotate_discovery_sample_sizes","title":"<code>annotate_discovery_sample_sizes() -&gt; StudyIndexGWASCatalog</code>","text":"<p>Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog.</p> <p>For some studies that measure quantitative traits, nCases and nControls can't be extracted. Therefore, we assume these are 0.</p> <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>object with columns <code>nCases</code>, <code>nControls</code>, and <code>nSamples</code> per <code>studyId</code> correctly extracted.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def annotate_discovery_sample_sizes(\n    self: StudyIndexGWASCatalog,\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog.\n\n    For some studies that measure quantitative traits, nCases and nControls can't be extracted. Therefore, we assume these are 0.\n\n    Returns:\n        StudyIndexGWASCatalog: object with columns `nCases`, `nControls`, and `nSamples` per `studyId` correctly extracted.\n    \"\"\"\n    sample_size_lut = (\n        self.df.select(\n            \"studyId\",\n            f.explode_outer(f.split(f.col(\"initialSampleSize\"), r\",\\s+\")).alias(\n                \"samples\"\n            ),\n        )\n        # Extracting the sample size from the string:\n        .withColumn(\n            \"sampleSize\",\n            f.regexp_extract(\n                f.regexp_replace(f.col(\"samples\"), \",\", \"\"), r\"[0-9,]+\", 0\n            ).cast(t.IntegerType()),\n        )\n        .select(\n            \"studyId\",\n            \"sampleSize\",\n            f.when(f.col(\"samples\").contains(\"cases\"), f.col(\"sampleSize\"))\n            .otherwise(f.lit(0))\n            .alias(\"nCases\"),\n            f.when(f.col(\"samples\").contains(\"controls\"), f.col(\"sampleSize\"))\n            .otherwise(f.lit(0))\n            .alias(\"nControls\"),\n        )\n        # Aggregating sample sizes for all ancestries:\n        .groupBy(\"studyId\")  # studyId has not been split yet\n        .agg(\n            f.sum(\"nCases\").cast(\"integer\").alias(\"nCases\"),\n            f.sum(\"nControls\").cast(\"integer\").alias(\"nControls\"),\n            f.sum(\"sampleSize\").cast(\"integer\").alias(\"nSamples\"),\n        )\n    )\n    self.df = self.df.join(sample_size_lut, on=\"studyId\", how=\"left\")\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.annotate_from_study_curation","title":"<code>annotate_from_study_curation(curation_table: DataFrame | None) -&gt; StudyIndexGWASCatalog</code>","text":"<p>Annotating study index with curation.</p> <p>Parameters:</p> Name Type Description Default <code>curation_table</code> <code>DataFrame | None</code> <p>Curated GWAS Catalog studies with summary statistics</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Updated study index</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def annotate_from_study_curation(\n    self: StudyIndexGWASCatalog, curation_table: DataFrame | None\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Annotating study index with curation.\n\n    Args:\n        curation_table (DataFrame | None): Curated GWAS Catalog studies with summary statistics\n\n    Returns:\n        StudyIndexGWASCatalog: Updated study index\n    \"\"\"\n    studies = self.df\n\n    if \"qualityControls\" not in studies.columns:\n        studies = studies.withColumn(\n            \"qualityControls\", f.array().cast(ArrayType(StringType()))\n        )\n\n    if \"analysisFlags\" not in studies.columns:\n        studies = studies.withColumn(\n            \"analysisFlags\", f.array().cast(ArrayType(StringType()))\n        )\n\n    # Providing curation table is optional. However once this method is called, the quality and studyFlag columns are added.\n    if curation_table is None:\n        return StudyIndexGWASCatalog(\n            _df=studies, _schema=StudyIndexGWASCatalog.get_schema()\n        )\n\n    # Adding prefix to columns in the curation table:\n    curation_table = curation_table.select(\n        *[\n            f.col(column).alias(f\"curation_{column}\")\n            if column != \"studyId\"\n            else f.col(column)\n            for column in curation_table.columns\n        ]\n    )\n\n    # Based on the curation table, columns needs to be updated:\n    curated_df = (\n        studies.join(\n            curation_table.withColumn(\"isCurated\", f.lit(True)),\n            on=\"studyId\",\n            how=\"left\",\n        )\n        .withColumn(\"isCurated\", f.coalesce(f.col(\"isCurated\"), f.lit(False)))\n        # Updating study type:\n        .withColumn(\n            \"studyType\", f.coalesce(f.col(\"curation_studyType\"), f.col(\"studyType\"))\n        )\n        # Updating study annotation flags:\n        .withColumn(\n            \"analysisFlags\",\n            f.array_union(f.col(\"analysisFlags\"), f.col(\"curation_analysisFlags\")),\n        )\n        .withColumn(\"analysisFlags\", f.coalesce(f.col(\"analysisFlags\"), f.array()))\n        .withColumn(\n            \"qualityControls\",\n            StudyIndex.update_quality_flag(\n                f.col(\"qualityControls\"),\n                ~f.col(\"isCurated\"),\n                StudyQualityCheck.NO_OT_CURATION,\n            ),\n        )\n        # Dropping columns coming from the curation table:\n        .select(*studies.columns)\n    )\n    return StudyIndexGWASCatalog(\n        _df=curated_df, _schema=StudyIndexGWASCatalog.get_schema()\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.apply_inclusion_list","title":"<code>apply_inclusion_list(inclusion_list: DataFrame) -&gt; StudyIndexGWASCatalog</code>","text":"<p>Restricting GWAS Catalog studies based on a list of accepted study identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>inclusion_list</code> <code>DataFrame</code> <p>List of accepted GWAS Catalog study identifiers</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Filtered dataset.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def apply_inclusion_list(\n    self: StudyIndexGWASCatalog, inclusion_list: DataFrame\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Restricting GWAS Catalog studies based on a list of accepted study identifiers.\n\n    Args:\n        inclusion_list (DataFrame): List of accepted GWAS Catalog study identifiers\n\n    Returns:\n        StudyIndexGWASCatalog: Filtered dataset.\n    \"\"\"\n    return StudyIndexGWASCatalog(\n        _df=self.df.join(inclusion_list, on=\"studyId\", how=\"inner\"),\n        _schema=StudyIndexGWASCatalog.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.extract_studies_for_curation","title":"<code>extract_studies_for_curation(curation: DataFrame | None) -&gt; DataFrame</code>","text":"<p>Extract studies for curation.</p> <p>Parameters:</p> Name Type Description Default <code>curation</code> <code>DataFrame | None</code> <p>Dataframe with curation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Updated curation table. New studies are have the <code>isCurated</code> False.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def extract_studies_for_curation(\n    self: StudyIndexGWASCatalog, curation: DataFrame | None\n) -&gt; DataFrame:\n    \"\"\"Extract studies for curation.\n\n    Args:\n        curation (DataFrame | None): Dataframe with curation.\n\n    Returns:\n        DataFrame: Updated curation table. New studies are have the `isCurated` False.\n    \"\"\"\n    # If no curation table provided, assume all studies needs curation:\n    if curation is None:\n        return (\n            self.df\n            # Curation only applyed on studies with summary statistics:\n            .filter(f.col(\"hasSumstats\"))\n            # Adding columns expected in the curation table - array columns aready flattened:\n            .withColumn(\"studyType\", f.lit(None).cast(t.StringType()))\n            .withColumn(\"analysisFlag\", f.lit(None).cast(t.StringType()))\n            .withColumn(\"qualityControl\", f.lit(None).cast(t.StringType()))\n            .withColumn(\"isCurated\", f.lit(False).cast(t.StringType()))\n        )\n\n    # Adding prefix to columns in the curation table:\n    curation = curation.select(\n        *[\n            f.col(column).alias(f\"curation_{column}\")\n            if column != \"studyId\"\n            else f.col(column)\n            for column in curation.columns\n        ]\n    )\n\n    return (\n        self.df\n        # Curation only applyed on studies with summary statistics:\n        .filter(f.col(\"hasSumstats\"))\n        .join(curation, on=\"studyId\", how=\"left\")\n        .select(\n            \"studyId\",\n            # Propagate existing curation - array columns are being flattened:\n            f.col(\"curation_studyType\").alias(\"studyType\"),\n            f.array_join(f.col(\"curation_analysisFlags\"), \"|\").alias(\n                \"analysisFlag\"\n            ),\n            f.array_join(f.col(\"curation_qualityControls\"), \"|\").alias(\n                \"qualityControl\"\n            ),\n            # This boolean flag needs to be casted to string, because saving to tsv would fail otherwise:\n            f.coalesce(f.col(\"curation_isCurated\"), f.lit(False))\n            .cast(t.StringType())\n            .alias(\"isCurated\"),\n            # The following columns are propagated to make curation easier:\n            \"pubmedId\",\n            \"publicationTitle\",\n            \"traitFromSource\",\n        )\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_index/#gentropy.datasource.gwas_catalog.study_index.StudyIndexGWASCatalog.update_study_id","title":"<code>update_study_id(study_annotation: DataFrame) -&gt; StudyIndexGWASCatalog</code>","text":"<p>Update studyId with a dataframe containing study.</p> <p>Parameters:</p> Name Type Description Default <code>study_annotation</code> <code>DataFrame</code> <p>Dataframe containing <code>updatedStudyId</code>, <code>traitFromSource</code>, <code>traitFromSourceMappedIds</code> and key column <code>studyId</code>.</p> required <p>Returns:</p> Name Type Description <code>StudyIndexGWASCatalog</code> <code>StudyIndexGWASCatalog</code> <p>Updated study table.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_index.py</code> <pre><code>def update_study_id(\n    self: StudyIndexGWASCatalog, study_annotation: DataFrame\n) -&gt; StudyIndexGWASCatalog:\n    \"\"\"Update studyId with a dataframe containing study.\n\n    Args:\n        study_annotation (DataFrame): Dataframe containing `updatedStudyId`, `traitFromSource`, `traitFromSourceMappedIds` and key column `studyId`.\n\n    Returns:\n        StudyIndexGWASCatalog: Updated study table.\n    \"\"\"\n    self.df = (\n        self._df.join(\n            study_annotation.select(\n                *[\n                    f.col(c).alias(f\"updated{c}\")\n                    if c not in [\"studyId\", \"updatedStudyId\"]\n                    else f.col(c)\n                    for c in study_annotation.columns\n                ]\n            ),\n            on=\"studyId\",\n            how=\"left\",\n        )\n        .withColumn(\n            \"studyId\",\n            f.coalesce(f.col(\"updatedStudyId\"), f.col(\"studyId\")),\n        )\n        .withColumn(\n            \"traitFromSource\",\n            f.coalesce(f.col(\"updatedtraitFromSource\"), f.col(\"traitFromSource\")),\n        )\n        .withColumn(\n            \"traitFromSourceMappedIds\",\n            f.coalesce(\n                f.col(\"updatedtraitFromSourceMappedIds\"),\n                f.col(\"traitFromSourceMappedIds\"),\n            ),\n        )\n        .select(self._df.columns)\n    )\n\n    return self\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_splitter/","title":"Study Splitter","text":""},{"location":"python_api/datasources/gwas_catalog/study_splitter/#gentropy.datasource.gwas_catalog.study_splitter.GWASCatalogStudySplitter","title":"<code>gentropy.datasource.gwas_catalog.study_splitter.GWASCatalogStudySplitter</code>","text":"<p>Splitting multi-trait GWAS Catalog studies.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_splitter.py</code> <pre><code>class GWASCatalogStudySplitter:\n    \"\"\"Splitting multi-trait GWAS Catalog studies.\"\"\"\n\n    @staticmethod\n    def _resolve_trait(\n        study_trait: Column, association_trait: Column, p_value_text: Column\n    ) -&gt; Column:\n        \"\"\"Resolve trait names by consolidating association-level and study-level trait names.\n\n        Args:\n            study_trait (Column): Study-level trait name.\n            association_trait (Column): Association-level trait name.\n            p_value_text (Column): P-value text.\n\n        Returns:\n            Column: Resolved trait name.\n        \"\"\"\n        return (\n            f.when(\n                (p_value_text.isNotNull()) &amp; (p_value_text != (\"no_pvalue_text\")),\n                f.concat(\n                    association_trait,\n                    f.lit(\" [\"),\n                    p_value_text,\n                    f.lit(\"]\"),\n                ),\n            )\n            .when(\n                association_trait.isNotNull(),\n                association_trait,\n            )\n            .otherwise(study_trait)\n        )\n\n    @staticmethod\n    def _resolve_efo(association_efo: Column, study_efo: Column) -&gt; Column:\n        \"\"\"Resolve EFOs by consolidating association-level and study-level EFOs.\n\n        Args:\n            association_efo (Column): EFO column from the association table.\n            study_efo (Column): EFO column from the study table.\n\n        Returns:\n            Column: Consolidated EFO column.\n        \"\"\"\n        return f.coalesce(f.split(association_efo, r\"\\/\"), study_efo)\n\n    @staticmethod\n    def _resolve_study_id(study_id: Column, sub_study_description: Column) -&gt; Column:\n        \"\"\"Resolve study IDs by exploding association-level information (e.g. pvalue_text, EFO).\n\n        Args:\n            study_id (Column): Study ID column.\n            sub_study_description (Column): Sub-study description column from the association table.\n\n        Returns:\n            Column: Resolved study ID column.\n        \"\"\"\n        split_w = Window.partitionBy(study_id).orderBy(sub_study_description)\n        row_number = f.dense_rank().over(split_w)\n        substudy_count = f.approx_count_distinct(row_number).over(split_w)\n        return f.when(substudy_count == 1, study_id).otherwise(\n            f.concat_ws(\"_\", study_id, row_number)\n        )\n\n    @classmethod\n    def split(\n        cls: type[GWASCatalogStudySplitter],\n        studies: StudyIndexGWASCatalog,\n        associations: StudyLocusGWASCatalog,\n    ) -&gt; tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]:\n        \"\"\"Splitting multi-trait GWAS Catalog studies.\n\n        If assigned disease of the study and the association don't agree, we assume the study needs to be split.\n        Then disease EFOs, trait names and study ID are consolidated\n\n        Args:\n            studies (StudyIndexGWASCatalog): GWAS Catalog studies.\n            associations (StudyLocusGWASCatalog): GWAS Catalog associations.\n\n        Returns:\n            tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]: Split studies and associations.\n        \"\"\"\n        # Composite of studies and associations to resolve scattered information\n        st_ass = (\n            associations.df.join(f.broadcast(studies.df), on=\"studyId\", how=\"inner\")\n            .select(\n                \"studyId\",\n                \"subStudyDescription\",\n                cls._resolve_study_id(\n                    f.col(\"studyId\"), f.col(\"subStudyDescription\")\n                ).alias(\"updatedStudyId\"),\n                cls._resolve_trait(\n                    f.col(\"traitFromSource\"),\n                    f.split(\"subStudyDescription\", r\"\\|\").getItem(0),\n                    f.split(\"subStudyDescription\", r\"\\|\").getItem(1),\n                ).alias(\"traitFromSource\"),\n                cls._resolve_efo(\n                    f.split(\"subStudyDescription\", r\"\\|\").getItem(2),\n                    f.col(\"traitFromSourceMappedIds\"),\n                ).alias(\"traitFromSourceMappedIds\"),\n            )\n            .persist()\n        )\n\n        return (\n            studies.update_study_id(\n                st_ass.select(\n                    \"studyId\",\n                    \"updatedStudyId\",\n                    \"traitFromSource\",\n                    \"traitFromSourceMappedIds\",\n                ).distinct()\n            ),\n            associations.update_study_id(\n                st_ass.select(\n                    \"updatedStudyId\", \"studyId\", \"subStudyDescription\"\n                ).distinct()\n            )\n            .qc_ambiguous_study()\n            .qc_flag_all_tophits(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/study_splitter/#gentropy.datasource.gwas_catalog.study_splitter.GWASCatalogStudySplitter.split","title":"<code>split(studies: StudyIndexGWASCatalog, associations: StudyLocusGWASCatalog) -&gt; tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]</code>  <code>classmethod</code>","text":"<p>Splitting multi-trait GWAS Catalog studies.</p> <p>If assigned disease of the study and the association don't agree, we assume the study needs to be split. Then disease EFOs, trait names and study ID are consolidated</p> <p>Parameters:</p> Name Type Description Default <code>studies</code> <code>StudyIndexGWASCatalog</code> <p>GWAS Catalog studies.</p> required <code>associations</code> <code>StudyLocusGWASCatalog</code> <p>GWAS Catalog associations.</p> required <p>Returns:</p> Type Description <code>tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]</code> <p>tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]: Split studies and associations.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/study_splitter.py</code> <pre><code>@classmethod\ndef split(\n    cls: type[GWASCatalogStudySplitter],\n    studies: StudyIndexGWASCatalog,\n    associations: StudyLocusGWASCatalog,\n) -&gt; tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]:\n    \"\"\"Splitting multi-trait GWAS Catalog studies.\n\n    If assigned disease of the study and the association don't agree, we assume the study needs to be split.\n    Then disease EFOs, trait names and study ID are consolidated\n\n    Args:\n        studies (StudyIndexGWASCatalog): GWAS Catalog studies.\n        associations (StudyLocusGWASCatalog): GWAS Catalog associations.\n\n    Returns:\n        tuple[StudyIndexGWASCatalog, StudyLocusGWASCatalog]: Split studies and associations.\n    \"\"\"\n    # Composite of studies and associations to resolve scattered information\n    st_ass = (\n        associations.df.join(f.broadcast(studies.df), on=\"studyId\", how=\"inner\")\n        .select(\n            \"studyId\",\n            \"subStudyDescription\",\n            cls._resolve_study_id(\n                f.col(\"studyId\"), f.col(\"subStudyDescription\")\n            ).alias(\"updatedStudyId\"),\n            cls._resolve_trait(\n                f.col(\"traitFromSource\"),\n                f.split(\"subStudyDescription\", r\"\\|\").getItem(0),\n                f.split(\"subStudyDescription\", r\"\\|\").getItem(1),\n            ).alias(\"traitFromSource\"),\n            cls._resolve_efo(\n                f.split(\"subStudyDescription\", r\"\\|\").getItem(2),\n                f.col(\"traitFromSourceMappedIds\"),\n            ).alias(\"traitFromSourceMappedIds\"),\n        )\n        .persist()\n    )\n\n    return (\n        studies.update_study_id(\n            st_ass.select(\n                \"studyId\",\n                \"updatedStudyId\",\n                \"traitFromSource\",\n                \"traitFromSourceMappedIds\",\n            ).distinct()\n        ),\n        associations.update_study_id(\n            st_ass.select(\n                \"updatedStudyId\", \"studyId\", \"subStudyDescription\"\n            ).distinct()\n        )\n        .qc_ambiguous_study()\n        .qc_flag_all_tophits(),\n    )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/summary_statistics/","title":"Summary statistics","text":""},{"location":"python_api/datasources/gwas_catalog/summary_statistics/#gentropy.datasource.gwas_catalog.summary_statistics.GWASCatalogSummaryStatistics","title":"<code>gentropy.datasource.gwas_catalog.summary_statistics.GWASCatalogSummaryStatistics</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SummaryStatistics</code></p> <p>GWAS Catalog Summary Statistics reader.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/summary_statistics.py</code> <pre><code>@dataclass\nclass GWASCatalogSummaryStatistics(SummaryStatistics):\n    \"\"\"GWAS Catalog Summary Statistics reader.\"\"\"\n\n    @classmethod\n    def from_gwas_harmonized_summary_stats(\n        cls: type[GWASCatalogSummaryStatistics],\n        spark: SparkSession,\n        sumstats_file: str,\n    ) -&gt; GWASCatalogSummaryStatistics:\n        \"\"\"Create summary statistics object from summary statistics flatfile, harmonized by the GWAS Catalog.\n\n        Things got slightly complicated given the GWAS Catalog harmonization pipelines changed recently so we had to accomodate to\n        both formats.\n\n        Args:\n            spark (SparkSession): spark session\n            sumstats_file (str): list of GWAS Catalog summary stat files, with study ids in them.\n\n        Returns:\n            GWASCatalogSummaryStatistics: Summary statistics object.\n        \"\"\"\n        sumstats_df = spark.read.csv(sumstats_file, sep=\"\\t\", header=True)\n        sumstats_df = sumstats_df.withColumn(\n            # Parsing GWAS Catalog study identifier from filename:\n            \"studyId\",\n            f.lit(filename_to_study_identifier(sumstats_file)),\n        )\n\n        # Parsing variant id fields:\n        chromosome = (\n            f.col(\"hm_chrom\")\n            if \"hm_chrom\" in sumstats_df.columns\n            else f.col(\"chromosome\")\n        ).cast(t.StringType())\n        position = (\n            f.col(\"hm_pos\")\n            if \"hm_pos\" in sumstats_df.columns\n            else f.col(\"base_pair_location\")\n        ).cast(t.IntegerType())\n        ref_allele = (\n            f.col(\"hm_other_allele\")\n            if \"hm_other_allele\" in sumstats_df.columns\n            else f.col(\"other_allele\")\n        )\n        alt_allele = (\n            f.col(\"hm_effect_allele\")\n            if \"hm_effect_allele\" in sumstats_df.columns\n            else f.col(\"effect_allele\")\n        )\n\n        # Parsing p-value (get a tuple with mantissa and exponent):\n        p_value_expression = (\n            split_pvalue_column(f.col(\"p_value\"))\n            if \"p_value\" in sumstats_df.columns\n            else pvalue_from_neglogpval(f.col(\"neg_log_10_p_value\"))\n        )\n\n        # The effect allele frequency is an optional column, we have to test if it is there:\n        allele_frequency = (\n            f.col(\"effect_allele_frequency\")\n            if \"effect_allele_frequency\" in sumstats_df.columns\n            else f.lit(None)\n        ).cast(t.FloatType())\n\n        # Do we have sample size? This expression captures 99.7% of sample size columns.\n        sample_size = (f.col(\"n\") if \"n\" in sumstats_df.columns else f.lit(None)).cast(\n            t.IntegerType()\n        )\n\n        # Depending on the input, we might have beta, but the column might not be there at all also old format calls differently:\n        beta_expression = (\n            f.col(\"hm_beta\")\n            if \"hm_beta\" in sumstats_df.columns\n            else f.col(\"beta\")\n            if \"beta\" in sumstats_df.columns\n            # If no column, create one:\n            else f.lit(None)\n        ).cast(t.DoubleType())\n\n        # We might have odds ratio or hazard ratio, which are basically the same:\n        odds_ratio_expression = (\n            f.col(\"hm_odds_ratio\")\n            if \"hm_odds_ratio\" in sumstats_df.columns\n            else f.col(\"odds_ratio\")\n            if \"odds_ratio\" in sumstats_df.columns\n            else f.col(\"hazard_ratio\")\n            if \"hazard_ratio\" in sumstats_df.columns\n            # If no column, create one:\n            else f.lit(None)\n        ).cast(t.DoubleType())\n\n        # Does the file have standard error column?\n        standard_error = (\n            f.col(\"standard_error\")\n            if \"standard_error\" in sumstats_df.columns\n            else f.lit(None)\n        ).cast(t.DoubleType())\n\n        ci_upper = (\n            f.col(\"ci_upper\") if \"ci_upper\" in sumstats_df.columns else f.lit(None)\n        ).cast(t.DoubleType())\n\n        ci_lower = (\n            f.col(\"ci_lower\") if \"ci_lower\" in sumstats_df.columns else f.lit(None)\n        ).cast(t.DoubleType())\n\n        # Processing columns of interest:\n        processed_sumstats_df = (\n            sumstats_df\n            # Dropping rows which doesn't have proper position:\n            .select(\n                \"studyId\",\n                # Adding variant identifier:\n                f.concat_ws(\n                    \"_\",\n                    chromosome,\n                    position,\n                    ref_allele,\n                    alt_allele,\n                ).alias(\"variantId\"),\n                chromosome.alias(\"chromosome\"),\n                position.alias(\"position\"),\n                # Parsing p-value mantissa and exponent:\n                *p_value_expression,\n                # Converting/calculating effect and confidence interval:\n                *normalise_gwas_statistics(\n                    beta=beta_expression,\n                    odds_ratio=odds_ratio_expression,\n                    standard_error=standard_error,\n                    ci_upper=ci_upper,\n                    ci_lower=ci_lower,\n                    mantissa=p_value_expression.mantissa,\n                    exponent=p_value_expression.exponent,\n                ),\n                allele_frequency.alias(\"effectAlleleFrequencyFromSource\"),\n                sample_size.alias(\"sampleSize\"),\n            )\n            # Make sure the former select statement are executed before the filtering.\n            .persist()\n            # Dropping associations where no harmonized position is available:\n            .filter(f.col(\"position\").isNotNull())\n            # We are not interested in associations empty beta values:\n            .filter(f.col(\"beta\").isNotNull())\n            # We are not interested in associations with zero effect:\n            .filter(f.col(\"beta\") != 0)\n            .orderBy(f.col(\"chromosome\"), f.col(\"position\"))\n            # median study size is 200Mb, max is 2.6Gb\n            .repartition(20)\n        )\n\n        # Initializing summary statistics object:\n        return cls(\n            _df=processed_sumstats_df,\n            _schema=cls.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/gwas_catalog/summary_statistics/#gentropy.datasource.gwas_catalog.summary_statistics.GWASCatalogSummaryStatistics.from_gwas_harmonized_summary_stats","title":"<code>from_gwas_harmonized_summary_stats(spark: SparkSession, sumstats_file: str) -&gt; GWASCatalogSummaryStatistics</code>  <code>classmethod</code>","text":"<p>Create summary statistics object from summary statistics flatfile, harmonized by the GWAS Catalog.</p> <p>Things got slightly complicated given the GWAS Catalog harmonization pipelines changed recently so we had to accomodate to both formats.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>spark session</p> required <code>sumstats_file</code> <code>str</code> <p>list of GWAS Catalog summary stat files, with study ids in them.</p> required <p>Returns:</p> Name Type Description <code>GWASCatalogSummaryStatistics</code> <code>GWASCatalogSummaryStatistics</code> <p>Summary statistics object.</p> Source code in <code>src/gentropy/datasource/gwas_catalog/summary_statistics.py</code> <pre><code>@classmethod\ndef from_gwas_harmonized_summary_stats(\n    cls: type[GWASCatalogSummaryStatistics],\n    spark: SparkSession,\n    sumstats_file: str,\n) -&gt; GWASCatalogSummaryStatistics:\n    \"\"\"Create summary statistics object from summary statistics flatfile, harmonized by the GWAS Catalog.\n\n    Things got slightly complicated given the GWAS Catalog harmonization pipelines changed recently so we had to accomodate to\n    both formats.\n\n    Args:\n        spark (SparkSession): spark session\n        sumstats_file (str): list of GWAS Catalog summary stat files, with study ids in them.\n\n    Returns:\n        GWASCatalogSummaryStatistics: Summary statistics object.\n    \"\"\"\n    sumstats_df = spark.read.csv(sumstats_file, sep=\"\\t\", header=True)\n    sumstats_df = sumstats_df.withColumn(\n        # Parsing GWAS Catalog study identifier from filename:\n        \"studyId\",\n        f.lit(filename_to_study_identifier(sumstats_file)),\n    )\n\n    # Parsing variant id fields:\n    chromosome = (\n        f.col(\"hm_chrom\")\n        if \"hm_chrom\" in sumstats_df.columns\n        else f.col(\"chromosome\")\n    ).cast(t.StringType())\n    position = (\n        f.col(\"hm_pos\")\n        if \"hm_pos\" in sumstats_df.columns\n        else f.col(\"base_pair_location\")\n    ).cast(t.IntegerType())\n    ref_allele = (\n        f.col(\"hm_other_allele\")\n        if \"hm_other_allele\" in sumstats_df.columns\n        else f.col(\"other_allele\")\n    )\n    alt_allele = (\n        f.col(\"hm_effect_allele\")\n        if \"hm_effect_allele\" in sumstats_df.columns\n        else f.col(\"effect_allele\")\n    )\n\n    # Parsing p-value (get a tuple with mantissa and exponent):\n    p_value_expression = (\n        split_pvalue_column(f.col(\"p_value\"))\n        if \"p_value\" in sumstats_df.columns\n        else pvalue_from_neglogpval(f.col(\"neg_log_10_p_value\"))\n    )\n\n    # The effect allele frequency is an optional column, we have to test if it is there:\n    allele_frequency = (\n        f.col(\"effect_allele_frequency\")\n        if \"effect_allele_frequency\" in sumstats_df.columns\n        else f.lit(None)\n    ).cast(t.FloatType())\n\n    # Do we have sample size? This expression captures 99.7% of sample size columns.\n    sample_size = (f.col(\"n\") if \"n\" in sumstats_df.columns else f.lit(None)).cast(\n        t.IntegerType()\n    )\n\n    # Depending on the input, we might have beta, but the column might not be there at all also old format calls differently:\n    beta_expression = (\n        f.col(\"hm_beta\")\n        if \"hm_beta\" in sumstats_df.columns\n        else f.col(\"beta\")\n        if \"beta\" in sumstats_df.columns\n        # If no column, create one:\n        else f.lit(None)\n    ).cast(t.DoubleType())\n\n    # We might have odds ratio or hazard ratio, which are basically the same:\n    odds_ratio_expression = (\n        f.col(\"hm_odds_ratio\")\n        if \"hm_odds_ratio\" in sumstats_df.columns\n        else f.col(\"odds_ratio\")\n        if \"odds_ratio\" in sumstats_df.columns\n        else f.col(\"hazard_ratio\")\n        if \"hazard_ratio\" in sumstats_df.columns\n        # If no column, create one:\n        else f.lit(None)\n    ).cast(t.DoubleType())\n\n    # Does the file have standard error column?\n    standard_error = (\n        f.col(\"standard_error\")\n        if \"standard_error\" in sumstats_df.columns\n        else f.lit(None)\n    ).cast(t.DoubleType())\n\n    ci_upper = (\n        f.col(\"ci_upper\") if \"ci_upper\" in sumstats_df.columns else f.lit(None)\n    ).cast(t.DoubleType())\n\n    ci_lower = (\n        f.col(\"ci_lower\") if \"ci_lower\" in sumstats_df.columns else f.lit(None)\n    ).cast(t.DoubleType())\n\n    # Processing columns of interest:\n    processed_sumstats_df = (\n        sumstats_df\n        # Dropping rows which doesn't have proper position:\n        .select(\n            \"studyId\",\n            # Adding variant identifier:\n            f.concat_ws(\n                \"_\",\n                chromosome,\n                position,\n                ref_allele,\n                alt_allele,\n            ).alias(\"variantId\"),\n            chromosome.alias(\"chromosome\"),\n            position.alias(\"position\"),\n            # Parsing p-value mantissa and exponent:\n            *p_value_expression,\n            # Converting/calculating effect and confidence interval:\n            *normalise_gwas_statistics(\n                beta=beta_expression,\n                odds_ratio=odds_ratio_expression,\n                standard_error=standard_error,\n                ci_upper=ci_upper,\n                ci_lower=ci_lower,\n                mantissa=p_value_expression.mantissa,\n                exponent=p_value_expression.exponent,\n            ),\n            allele_frequency.alias(\"effectAlleleFrequencyFromSource\"),\n            sample_size.alias(\"sampleSize\"),\n        )\n        # Make sure the former select statement are executed before the filtering.\n        .persist()\n        # Dropping associations where no harmonized position is available:\n        .filter(f.col(\"position\").isNotNull())\n        # We are not interested in associations empty beta values:\n        .filter(f.col(\"beta\").isNotNull())\n        # We are not interested in associations with zero effect:\n        .filter(f.col(\"beta\") != 0)\n        .orderBy(f.col(\"chromosome\"), f.col(\"position\"))\n        # median study size is 200Mb, max is 2.6Gb\n        .repartition(20)\n    )\n\n    # Initializing summary statistics object:\n    return cls(\n        _df=processed_sumstats_df,\n        _schema=cls.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/_intervals/","title":"List of Interaction and Interval-based Studies","text":"<p>In this section, we provide a list of studies that focus on interaction and interval-based investigations, shedding light on the intricate relationships between genetic elements and their functional implications.</p> <ol> <li>E2G (Gschwind et al., Nov 2023): Title: \"An encyclopedia of enhancer-gene regulatory interactions in the human genome\".    This study comprises of a large, curated compendium of enhancer\u2192gene links built by integrating multiple evidence types (epigenomic signals, 3D contacts, expression correlations, and CRISPR perturbations) across many biosamples. The resource reports confidence/score per enhancer\u2013gene pair and is organised by biosample/cell type.</li> </ol> <p>DOI: 10.1101/2023.11.09.563812</p> <ol> <li>EPIraction (Nurtdinov et al., Feb 2025): Title: \"EPIraction - an atlas of candidate enhancer-gene interactions in human tissues and cell lines\".    This study is a genome-wide atlas of candidate enhancer\u2013gene links inferred primarily from H3K27ac ChIP-seq (enhancer activity) integrated with Hi-C contact probabilities, scored per tissue/cell line\u2014methodologically similar in spirit to ABC-style scoring.</li> </ol> <p>DOI: 10.1101/2025.02.18.638885</p> <p>For in-depth details on each study, you may refer to the respective publications.</p>"},{"location":"python_api/datasources/intervals/e2g/","title":"ENCODE-rE2G","text":""},{"location":"python_api/datasources/intervals/e2g/#gentropy.datasource.intervals.e2g.IntervalsE2G","title":"<code>gentropy.datasource.intervals.e2g.IntervalsE2G</code>","text":"<p>Interval dataset from E2G.</p> Source code in <code>src/gentropy/datasource/intervals/e2g.py</code> <pre><code>class IntervalsE2G:\n    \"\"\"Interval dataset from E2G.\"\"\"\n\n    DATASET_NAME: ClassVar[str] = \"E2G\"\n    PMID: ClassVar[str] = \"38014075\"  # PMID for the E2G paper\n    VALID_INTERVAL_TYPES: ClassVar[list[str]] = [\"promoter\", \"genic\", \"intergenic\"]\n\n    @staticmethod\n    def read(spark: SparkSession, path: str) -&gt; DataFrame:\n        \"\"\"Read E2G dataset.\n\n        Args:\n            spark (SparkSession): Spark session\n            path (str): Path to the E2G dataset (tsv.gz files)\n\n        Returns:\n            DataFrame: Raw E2G DataFrame.\n        \"\"\"\n        return (\n            spark.read.option(\"delimiter\", \"\\t\")\n            .option(\"header\", \"true\")\n            .csv(path)\n            .withColumn(\"file_path\", f.input_file_name())\n        )\n\n    @classmethod\n    def parse(\n        cls: type[IntervalsE2G],\n        raw_e2g_df: DataFrame,\n        biosample_mapping: DataFrame,\n        target_index: TargetIndex,\n        biosample_index: BiosampleIndex,\n    ) -&gt; Intervals:\n        \"\"\"Parse E2G dataset.\n\n        Args:\n            raw_e2g_df (DataFrame): Raw E2G DataFrame.\n            biosample_mapping (DataFrame): Biosample mapping DataFrame.\n            target_index (TargetIndex): Target index.\n            biosample_index (BiosampleIndex): Biosample index.\n\n        Returns:\n            Intervals: Parsed Intervals dataset.\n        \"\"\"\n        base = (\n            raw_e2g_df.withColumn(\n                \"studyId\", f.regexp_extract(f.col(\"file_path\"), r\"([^/]+)\\.bed\\.gz$\", 1)\n            )\n            .withColumn(\"chromosome\", f.regexp_replace(f.col(\"chr\"), \"^chr\", \"\"))\n            .withColumnRenamed(\"TargetGeneEnsemblID\", \"geneId\")\n            .withColumnRenamed(\"CellType\", \"biosampleName\")\n            .withColumnRenamed(\"Score\", \"score\")\n            .withColumnRenamed(\"class\", \"intervalType\")\n            .withColumn(\n                \"resourceScore\",\n                f.array(\n                    f.struct(\n                        f.lit(\"DNase\").alias(\"name\"),\n                        f.col(\"`normalizedDNase_prom.Feature`\")\n                        .cast(\"float\")\n                        .alias(\"value\"),\n                    ),\n                    f.struct(\n                        f.lit(\"HiC_contacts\").alias(\"name\"),\n                        f.col(\"`3DContact.Feature`\").cast(\"float\").alias(\"value\"),\n                    ),\n                ),\n            )\n            .withColumn(\"start\", f.col(\"start\").cast(\"long\"))\n            .withColumn(\"end\", f.col(\"end\").cast(\"long\"))\n        )\n\n        base = base.withColumn(\n            \"intervalType\", f.lower(f.trim(f.col(\"intervalType\")))\n        ).filter(f.col(\"intervalType\").isin(cls.VALID_INTERVAL_TYPES))\n\n        # Target Index: preferred TSS + fallbacks (canonical transcript, genomicLocation)\n        ti = target_index._df.select(\n            f.col(\"id\").alias(\"geneId\"),\n            f.col(\"tss\").cast(\"long\").alias(\"tss_primary\"),\n            f.col(\"canonicalTranscript.chromosome\").alias(\"ct_chr\"),\n            f.col(\"canonicalTranscript.start\").cast(\"long\").alias(\"ct_start\"),\n            f.col(\"canonicalTranscript.end\").cast(\"long\").alias(\"ct_end\"),\n            f.col(\"canonicalTranscript.strand\").alias(\"ct_strand\"),\n            f.col(\"genomicLocation.chromosome\").alias(\"gl_chr\"),\n            f.col(\"genomicLocation.start\").cast(\"long\").alias(\"gl_start\"),\n            f.col(\"genomicLocation.end\").cast(\"long\").alias(\"gl_end\"),\n            f.col(\"genomicLocation.strand\").cast(\"int\").alias(\"gl_strand\"),\n        )\n\n        ct_tss = f.when(f.col(\"ct_strand\") == \"+\", f.col(\"ct_start\")).when(\n            f.col(\"ct_strand\") == \"-\", f.col(\"ct_end\")\n        )\n        gl_tss = f.when(f.col(\"gl_strand\") == 1, f.col(\"gl_start\")).when(\n            f.col(\"gl_strand\") == -1, f.col(\"gl_end\")\n        )\n\n        ti_with_tss = ti.withColumn(\n            \"tss_coalesced\", f.coalesce(f.col(\"tss_primary\"), ct_tss, gl_tss)\n        )\n\n        # Join &amp; recompute distanceToTss\n        joined = base.alias(\"iv\").join(\n            ti_with_tss.alias(\"ti\"), on=\"geneId\", how=\"inner\"\n        )\n        tss = f.col(\"ti.tss_coalesced\")\n\n        dist_core = f.when(\n            (tss &gt;= f.col(\"iv.start\")) &amp; (tss &lt;= f.col(\"iv.end\")), f.lit(0)\n        ).otherwise(\n            f.least(f.abs(tss - f.col(\"iv.start\")), f.abs(tss - f.col(\"iv.end\")))\n        )\n        distance_expr = (\n            f.when(f.col(\"iv.intervalType\") == \"promoter\", f.lit(0))\n            .when(tss.isNull(), f.lit(None).cast(\"long\"))\n            .otherwise(dist_core)\n        )\n\n        parsed = (\n            joined.withColumn(\"distanceToTss\", distance_expr.cast(\"double\"))\n            .withColumn(\n                \"intervalId\",\n                f.sha1(\n                    f.concat_ws(\"_\", \"chromosome\", \"start\", \"end\", \"geneId\", \"studyId\")\n                ),\n            )\n            .join(\n                biosample_mapping.select(\"biosampleName\", \"biosampleId\"),\n                on=\"biosampleName\",\n                how=\"left\",\n            )\n            .join(\n                biosample_index.df.select(\"biosampleId\"), on=\"biosampleId\", how=\"inner\"\n            )\n        )\n\n        return Intervals(\n            _df=(\n                parsed.select(\n                    f.col(\"chromosome\"),\n                    f.col(\"start\").cast(\"string\"),\n                    f.col(\"end\").cast(\"string\"),\n                    f.col(\"geneId\"),\n                    f.col(\"biosampleName\"),\n                    f.col(\"intervalType\"),\n                    f.col(\"distanceToTss\").cast(\"integer\"),\n                    f.col(\"score\").cast(\"double\"),\n                    f.col(\"resourceScore\"),\n                    f.lit(cls.DATASET_NAME).alias(\"datasourceId\"),\n                    f.lit(cls.PMID).alias(\"pmid\"),\n                    f.col(\"studyId\"),\n                    f.col(\"biosampleId\"),\n                    f.col(\"intervalId\"),\n                )\n            ),\n            _schema=Intervals.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/intervals/e2g/#gentropy.datasource.intervals.e2g.IntervalsE2G.parse","title":"<code>parse(raw_e2g_df: DataFrame, biosample_mapping: DataFrame, target_index: TargetIndex, biosample_index: BiosampleIndex) -&gt; Intervals</code>  <code>classmethod</code>","text":"<p>Parse E2G dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raw_e2g_df</code> <code>DataFrame</code> <p>Raw E2G DataFrame.</p> required <code>biosample_mapping</code> <code>DataFrame</code> <p>Biosample mapping DataFrame.</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index.</p> required <code>biosample_index</code> <code>BiosampleIndex</code> <p>Biosample index.</p> required <p>Returns:</p> Name Type Description <code>Intervals</code> <code>Intervals</code> <p>Parsed Intervals dataset.</p> Source code in <code>src/gentropy/datasource/intervals/e2g.py</code> <pre><code>@classmethod\ndef parse(\n    cls: type[IntervalsE2G],\n    raw_e2g_df: DataFrame,\n    biosample_mapping: DataFrame,\n    target_index: TargetIndex,\n    biosample_index: BiosampleIndex,\n) -&gt; Intervals:\n    \"\"\"Parse E2G dataset.\n\n    Args:\n        raw_e2g_df (DataFrame): Raw E2G DataFrame.\n        biosample_mapping (DataFrame): Biosample mapping DataFrame.\n        target_index (TargetIndex): Target index.\n        biosample_index (BiosampleIndex): Biosample index.\n\n    Returns:\n        Intervals: Parsed Intervals dataset.\n    \"\"\"\n    base = (\n        raw_e2g_df.withColumn(\n            \"studyId\", f.regexp_extract(f.col(\"file_path\"), r\"([^/]+)\\.bed\\.gz$\", 1)\n        )\n        .withColumn(\"chromosome\", f.regexp_replace(f.col(\"chr\"), \"^chr\", \"\"))\n        .withColumnRenamed(\"TargetGeneEnsemblID\", \"geneId\")\n        .withColumnRenamed(\"CellType\", \"biosampleName\")\n        .withColumnRenamed(\"Score\", \"score\")\n        .withColumnRenamed(\"class\", \"intervalType\")\n        .withColumn(\n            \"resourceScore\",\n            f.array(\n                f.struct(\n                    f.lit(\"DNase\").alias(\"name\"),\n                    f.col(\"`normalizedDNase_prom.Feature`\")\n                    .cast(\"float\")\n                    .alias(\"value\"),\n                ),\n                f.struct(\n                    f.lit(\"HiC_contacts\").alias(\"name\"),\n                    f.col(\"`3DContact.Feature`\").cast(\"float\").alias(\"value\"),\n                ),\n            ),\n        )\n        .withColumn(\"start\", f.col(\"start\").cast(\"long\"))\n        .withColumn(\"end\", f.col(\"end\").cast(\"long\"))\n    )\n\n    base = base.withColumn(\n        \"intervalType\", f.lower(f.trim(f.col(\"intervalType\")))\n    ).filter(f.col(\"intervalType\").isin(cls.VALID_INTERVAL_TYPES))\n\n    # Target Index: preferred TSS + fallbacks (canonical transcript, genomicLocation)\n    ti = target_index._df.select(\n        f.col(\"id\").alias(\"geneId\"),\n        f.col(\"tss\").cast(\"long\").alias(\"tss_primary\"),\n        f.col(\"canonicalTranscript.chromosome\").alias(\"ct_chr\"),\n        f.col(\"canonicalTranscript.start\").cast(\"long\").alias(\"ct_start\"),\n        f.col(\"canonicalTranscript.end\").cast(\"long\").alias(\"ct_end\"),\n        f.col(\"canonicalTranscript.strand\").alias(\"ct_strand\"),\n        f.col(\"genomicLocation.chromosome\").alias(\"gl_chr\"),\n        f.col(\"genomicLocation.start\").cast(\"long\").alias(\"gl_start\"),\n        f.col(\"genomicLocation.end\").cast(\"long\").alias(\"gl_end\"),\n        f.col(\"genomicLocation.strand\").cast(\"int\").alias(\"gl_strand\"),\n    )\n\n    ct_tss = f.when(f.col(\"ct_strand\") == \"+\", f.col(\"ct_start\")).when(\n        f.col(\"ct_strand\") == \"-\", f.col(\"ct_end\")\n    )\n    gl_tss = f.when(f.col(\"gl_strand\") == 1, f.col(\"gl_start\")).when(\n        f.col(\"gl_strand\") == -1, f.col(\"gl_end\")\n    )\n\n    ti_with_tss = ti.withColumn(\n        \"tss_coalesced\", f.coalesce(f.col(\"tss_primary\"), ct_tss, gl_tss)\n    )\n\n    # Join &amp; recompute distanceToTss\n    joined = base.alias(\"iv\").join(\n        ti_with_tss.alias(\"ti\"), on=\"geneId\", how=\"inner\"\n    )\n    tss = f.col(\"ti.tss_coalesced\")\n\n    dist_core = f.when(\n        (tss &gt;= f.col(\"iv.start\")) &amp; (tss &lt;= f.col(\"iv.end\")), f.lit(0)\n    ).otherwise(\n        f.least(f.abs(tss - f.col(\"iv.start\")), f.abs(tss - f.col(\"iv.end\")))\n    )\n    distance_expr = (\n        f.when(f.col(\"iv.intervalType\") == \"promoter\", f.lit(0))\n        .when(tss.isNull(), f.lit(None).cast(\"long\"))\n        .otherwise(dist_core)\n    )\n\n    parsed = (\n        joined.withColumn(\"distanceToTss\", distance_expr.cast(\"double\"))\n        .withColumn(\n            \"intervalId\",\n            f.sha1(\n                f.concat_ws(\"_\", \"chromosome\", \"start\", \"end\", \"geneId\", \"studyId\")\n            ),\n        )\n        .join(\n            biosample_mapping.select(\"biosampleName\", \"biosampleId\"),\n            on=\"biosampleName\",\n            how=\"left\",\n        )\n        .join(\n            biosample_index.df.select(\"biosampleId\"), on=\"biosampleId\", how=\"inner\"\n        )\n    )\n\n    return Intervals(\n        _df=(\n            parsed.select(\n                f.col(\"chromosome\"),\n                f.col(\"start\").cast(\"string\"),\n                f.col(\"end\").cast(\"string\"),\n                f.col(\"geneId\"),\n                f.col(\"biosampleName\"),\n                f.col(\"intervalType\"),\n                f.col(\"distanceToTss\").cast(\"integer\"),\n                f.col(\"score\").cast(\"double\"),\n                f.col(\"resourceScore\"),\n                f.lit(cls.DATASET_NAME).alias(\"datasourceId\"),\n                f.lit(cls.PMID).alias(\"pmid\"),\n                f.col(\"studyId\"),\n                f.col(\"biosampleId\"),\n                f.col(\"intervalId\"),\n            )\n        ),\n        _schema=Intervals.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/e2g/#gentropy.datasource.intervals.e2g.IntervalsE2G.read","title":"<code>read(spark: SparkSession, path: str) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>Read E2G dataset.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>path</code> <code>str</code> <p>Path to the E2G dataset (tsv.gz files)</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Raw E2G DataFrame.</p> Source code in <code>src/gentropy/datasource/intervals/e2g.py</code> <pre><code>@staticmethod\ndef read(spark: SparkSession, path: str) -&gt; DataFrame:\n    \"\"\"Read E2G dataset.\n\n    Args:\n        spark (SparkSession): Spark session\n        path (str): Path to the E2G dataset (tsv.gz files)\n\n    Returns:\n        DataFrame: Raw E2G DataFrame.\n    \"\"\"\n    return (\n        spark.read.option(\"delimiter\", \"\\t\")\n        .option(\"header\", \"true\")\n        .csv(path)\n        .withColumn(\"file_path\", f.input_file_name())\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/epiraction/","title":"EPIraction","text":""},{"location":"python_api/datasources/intervals/epiraction/#gentropy.datasource.intervals.epiraction.IntervalsEpiraction","title":"<code>gentropy.datasource.intervals.epiraction.IntervalsEpiraction</code>","text":"<p>Interval dataset from EPIraction.</p> Source code in <code>src/gentropy/datasource/intervals/epiraction.py</code> <pre><code>class IntervalsEpiraction:\n    \"\"\"Interval dataset from EPIraction.\"\"\"\n\n    DATASET_NAME: ClassVar[str] = \"epiraction\"\n    PMID: ClassVar[str] = \"40027634\"\n    VALID_INTERVAL_TYPES: ClassVar[list[str]] = [\"promoter\", \"enhancer\"]\n\n    @staticmethod\n    def read(spark: SparkSession, path: str) -&gt; DataFrame:\n        \"\"\"Read EPIraction dataset (tsv with header).\n\n        Args:\n            spark (SparkSession): Spark session\n            path (str): Path to the EPIraction dataset (tsv files)\n\n        Returns:\n            DataFrame: Raw EPIraction DataFrame.\n        \"\"\"\n        return (\n            spark.read.option(\"delimiter\", \"\\t\")\n            .option(\"mode\", \"DROPMALFORMED\")\n            .option(\"header\", \"true\")\n            .csv(path)\n        )\n\n    @classmethod\n    def parse(\n        cls: type[IntervalsEpiraction],\n        raw_epiraction_df: DataFrame,\n        target_index: TargetIndex,\n    ) -&gt; Intervals:\n        \"\"\"Parse EPIraction Intervals.\n\n        Args:\n            raw_epiraction_df (DataFrame): Raw EPIraction DataFrame.\n            target_index (TargetIndex): Target index.\n\n        Returns:\n            Intervals: Parsed Intervals dataset.\n        \"\"\"\n        base = (\n            raw_epiraction_df.filter(f.col(\"class\").isin(cls.VALID_INTERVAL_TYPES))\n            .withColumn(\"chromosome\", f.regexp_replace(f.col(\"chr\"), r\"^chr\", \"\"))\n            .withColumnRenamed(\"TargetGeneEnsemblID\", \"geneId\")\n            .withColumnRenamed(\"CellType\", \"biosampleName\")\n            .withColumnRenamed(\"Score\", \"score\")\n            .withColumnRenamed(\"class\", \"intervalType\")\n            .withColumn(\n                \"resourceScore\",\n                f.array(\n                    f.struct(\n                        f.lit(\"H3K27ac\").alias(\"name\"),\n                        f.col(\"H3K27ac\").cast(\"float\").alias(\"value\"),\n                    ),\n                    f.struct(\n                        f.lit(\"Open\").alias(\"name\"),\n                        f.col(\"Open\").cast(\"float\").alias(\"value\"),\n                    ),\n                    f.struct(\n                        f.lit(\"Cofactor\").alias(\"name\"),\n                        f.col(\"Cofactor\").cast(\"float\").alias(\"value\"),\n                    ),\n                    f.struct(\n                        f.lit(\"CTCF\").alias(\"name\"),\n                        f.col(\"CTCF\").cast(\"float\").alias(\"value\"),\n                    ),\n                    f.struct(\n                        f.lit(\"HiC_contacts\").alias(\"name\"),\n                        f.col(\"HiC_contacts\").cast(\"float\").alias(\"value\"),\n                    ),\n                    f.struct(\n                        f.lit(\"abc_tissue\").alias(\"name\"),\n                        f.col(\"abc_tissue\").cast(\"float\").alias(\"value\"),\n                    ),\n                ),\n            )\n            .withColumn(\"start\", f.col(\"start\").cast(\"long\"))\n            .withColumn(\"end\", f.col(\"end\").cast(\"long\"))\n            .withColumn(\"intervalType\", f.lower(f.trim(f.col(\"intervalType\"))))\n        )\n\n        # Target Index: preferred TSS (+ fallbacks)\n        ti = target_index._df.select(\n            f.col(\"id\").alias(\"geneId\"),\n            f.col(\"tss\").cast(\"long\").alias(\"tss_primary\"),\n            f.col(\"canonicalTranscript.start\").cast(\"long\").alias(\"ct_start\"),\n            f.col(\"canonicalTranscript.end\").cast(\"long\").alias(\"ct_end\"),\n            f.col(\"canonicalTranscript.strand\").alias(\"ct_strand\"),\n            f.col(\"genomicLocation.start\").cast(\"long\").alias(\"gl_start\"),\n            f.col(\"genomicLocation.end\").cast(\"long\").alias(\"gl_end\"),\n            f.col(\"genomicLocation.strand\").cast(\"int\").alias(\"gl_strand\"),\n        )\n\n        ct_tss = f.when(f.col(\"ct_strand\") == \"+\", f.col(\"ct_start\")).when(\n            f.col(\"ct_strand\") == \"-\", f.col(\"ct_end\")\n        )\n        gl_tss = f.when(f.col(\"gl_strand\") == 1, f.col(\"gl_start\")).when(\n            f.col(\"gl_strand\") == -1, f.col(\"gl_end\")\n        )\n\n        ti_with_tss = ti.withColumn(\n            \"tss_from_target_index\", f.coalesce(f.col(\"tss_primary\"), ct_tss, gl_tss)\n        )\n\n        has_input_tss = \"distanceToTSS\" in base.columns\n        base_with_fallback = (\n            base.withColumn(\"tss_from_input\", f.col(\"distanceToTSS\").cast(\"long\"))\n            if has_input_tss\n            else base.withColumn(\"tss_from_input\", f.lit(None).cast(\"long\"))\n        )\n\n        joined = base_with_fallback.alias(\"iv\").join(\n            ti_with_tss.alias(\"ti\"), on=\"geneId\", how=\"inner\"\n        )\n\n        tss = f.coalesce(f.col(\"ti.tss_from_target_index\"), f.col(\"iv.tss_from_input\"))\n\n        dist_core = f.when(\n            (tss &gt;= f.col(\"iv.start\")) &amp; (tss &lt;= f.col(\"iv.end\")), f.lit(0)\n        ).otherwise(\n            f.least(f.abs(tss - f.col(\"iv.start\")), f.abs(tss - f.col(\"iv.end\")))\n        )\n        distance_expr = (\n            f.when(f.col(\"iv.intervalType\") == \"promoter\", f.lit(0))\n            .when(tss.isNull(), f.lit(None).cast(\"long\"))\n            .otherwise(dist_core)\n        )\n\n        parsed = joined.withColumn(\n            \"distanceToTss\", distance_expr.cast(\"double\")\n        ).withColumn(\n            \"intervalId\",\n            f.sha1(\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"iv.chromosome\"),\n                    f.col(\"iv.start\"),\n                    f.col(\"iv.end\"),\n                    f.col(\"iv.geneId\"),\n                    f.lit(cls.DATASET_NAME),\n                )\n            ),\n        )\n\n        return Intervals(\n            _df=(\n                parsed.select(\n                    f.col(\"iv.chromosome\").alias(\"chromosome\"),\n                    f.col(\"iv.start\").cast(\"string\").alias(\"start\"),\n                    f.col(\"iv.end\").cast(\"string\").alias(\"end\"),\n                    f.col(\"iv.geneId\").alias(\"geneId\"),\n                    f.col(\"iv.biosampleName\").alias(\"biosampleName\"),\n                    f.col(\"iv.intervalType\").alias(\"intervalType\"),\n                    f.col(\"distanceToTss\").cast(\"integer\").alias(\"distanceToTss\"),\n                    f.col(\"iv.score\").cast(\"double\").alias(\"score\"),\n                    f.col(\"iv.resourceScore\").alias(\"resourceScore\"),\n                    f.lit(cls.DATASET_NAME).alias(\"datasourceId\"),\n                    f.lit(cls.PMID).alias(\"pmid\"),\n                )\n            ),\n            _schema=Intervals.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/intervals/epiraction/#gentropy.datasource.intervals.epiraction.IntervalsEpiraction.parse","title":"<code>parse(raw_epiraction_df: DataFrame, target_index: TargetIndex) -&gt; Intervals</code>  <code>classmethod</code>","text":"<p>Parse EPIraction Intervals.</p> <p>Parameters:</p> Name Type Description Default <code>raw_epiraction_df</code> <code>DataFrame</code> <p>Raw EPIraction DataFrame.</p> required <code>target_index</code> <code>TargetIndex</code> <p>Target index.</p> required <p>Returns:</p> Name Type Description <code>Intervals</code> <code>Intervals</code> <p>Parsed Intervals dataset.</p> Source code in <code>src/gentropy/datasource/intervals/epiraction.py</code> <pre><code>@classmethod\ndef parse(\n    cls: type[IntervalsEpiraction],\n    raw_epiraction_df: DataFrame,\n    target_index: TargetIndex,\n) -&gt; Intervals:\n    \"\"\"Parse EPIraction Intervals.\n\n    Args:\n        raw_epiraction_df (DataFrame): Raw EPIraction DataFrame.\n        target_index (TargetIndex): Target index.\n\n    Returns:\n        Intervals: Parsed Intervals dataset.\n    \"\"\"\n    base = (\n        raw_epiraction_df.filter(f.col(\"class\").isin(cls.VALID_INTERVAL_TYPES))\n        .withColumn(\"chromosome\", f.regexp_replace(f.col(\"chr\"), r\"^chr\", \"\"))\n        .withColumnRenamed(\"TargetGeneEnsemblID\", \"geneId\")\n        .withColumnRenamed(\"CellType\", \"biosampleName\")\n        .withColumnRenamed(\"Score\", \"score\")\n        .withColumnRenamed(\"class\", \"intervalType\")\n        .withColumn(\n            \"resourceScore\",\n            f.array(\n                f.struct(\n                    f.lit(\"H3K27ac\").alias(\"name\"),\n                    f.col(\"H3K27ac\").cast(\"float\").alias(\"value\"),\n                ),\n                f.struct(\n                    f.lit(\"Open\").alias(\"name\"),\n                    f.col(\"Open\").cast(\"float\").alias(\"value\"),\n                ),\n                f.struct(\n                    f.lit(\"Cofactor\").alias(\"name\"),\n                    f.col(\"Cofactor\").cast(\"float\").alias(\"value\"),\n                ),\n                f.struct(\n                    f.lit(\"CTCF\").alias(\"name\"),\n                    f.col(\"CTCF\").cast(\"float\").alias(\"value\"),\n                ),\n                f.struct(\n                    f.lit(\"HiC_contacts\").alias(\"name\"),\n                    f.col(\"HiC_contacts\").cast(\"float\").alias(\"value\"),\n                ),\n                f.struct(\n                    f.lit(\"abc_tissue\").alias(\"name\"),\n                    f.col(\"abc_tissue\").cast(\"float\").alias(\"value\"),\n                ),\n            ),\n        )\n        .withColumn(\"start\", f.col(\"start\").cast(\"long\"))\n        .withColumn(\"end\", f.col(\"end\").cast(\"long\"))\n        .withColumn(\"intervalType\", f.lower(f.trim(f.col(\"intervalType\"))))\n    )\n\n    # Target Index: preferred TSS (+ fallbacks)\n    ti = target_index._df.select(\n        f.col(\"id\").alias(\"geneId\"),\n        f.col(\"tss\").cast(\"long\").alias(\"tss_primary\"),\n        f.col(\"canonicalTranscript.start\").cast(\"long\").alias(\"ct_start\"),\n        f.col(\"canonicalTranscript.end\").cast(\"long\").alias(\"ct_end\"),\n        f.col(\"canonicalTranscript.strand\").alias(\"ct_strand\"),\n        f.col(\"genomicLocation.start\").cast(\"long\").alias(\"gl_start\"),\n        f.col(\"genomicLocation.end\").cast(\"long\").alias(\"gl_end\"),\n        f.col(\"genomicLocation.strand\").cast(\"int\").alias(\"gl_strand\"),\n    )\n\n    ct_tss = f.when(f.col(\"ct_strand\") == \"+\", f.col(\"ct_start\")).when(\n        f.col(\"ct_strand\") == \"-\", f.col(\"ct_end\")\n    )\n    gl_tss = f.when(f.col(\"gl_strand\") == 1, f.col(\"gl_start\")).when(\n        f.col(\"gl_strand\") == -1, f.col(\"gl_end\")\n    )\n\n    ti_with_tss = ti.withColumn(\n        \"tss_from_target_index\", f.coalesce(f.col(\"tss_primary\"), ct_tss, gl_tss)\n    )\n\n    has_input_tss = \"distanceToTSS\" in base.columns\n    base_with_fallback = (\n        base.withColumn(\"tss_from_input\", f.col(\"distanceToTSS\").cast(\"long\"))\n        if has_input_tss\n        else base.withColumn(\"tss_from_input\", f.lit(None).cast(\"long\"))\n    )\n\n    joined = base_with_fallback.alias(\"iv\").join(\n        ti_with_tss.alias(\"ti\"), on=\"geneId\", how=\"inner\"\n    )\n\n    tss = f.coalesce(f.col(\"ti.tss_from_target_index\"), f.col(\"iv.tss_from_input\"))\n\n    dist_core = f.when(\n        (tss &gt;= f.col(\"iv.start\")) &amp; (tss &lt;= f.col(\"iv.end\")), f.lit(0)\n    ).otherwise(\n        f.least(f.abs(tss - f.col(\"iv.start\")), f.abs(tss - f.col(\"iv.end\")))\n    )\n    distance_expr = (\n        f.when(f.col(\"iv.intervalType\") == \"promoter\", f.lit(0))\n        .when(tss.isNull(), f.lit(None).cast(\"long\"))\n        .otherwise(dist_core)\n    )\n\n    parsed = joined.withColumn(\n        \"distanceToTss\", distance_expr.cast(\"double\")\n    ).withColumn(\n        \"intervalId\",\n        f.sha1(\n            f.concat_ws(\n                \"_\",\n                f.col(\"iv.chromosome\"),\n                f.col(\"iv.start\"),\n                f.col(\"iv.end\"),\n                f.col(\"iv.geneId\"),\n                f.lit(cls.DATASET_NAME),\n            )\n        ),\n    )\n\n    return Intervals(\n        _df=(\n            parsed.select(\n                f.col(\"iv.chromosome\").alias(\"chromosome\"),\n                f.col(\"iv.start\").cast(\"string\").alias(\"start\"),\n                f.col(\"iv.end\").cast(\"string\").alias(\"end\"),\n                f.col(\"iv.geneId\").alias(\"geneId\"),\n                f.col(\"iv.biosampleName\").alias(\"biosampleName\"),\n                f.col(\"iv.intervalType\").alias(\"intervalType\"),\n                f.col(\"distanceToTss\").cast(\"integer\").alias(\"distanceToTss\"),\n                f.col(\"iv.score\").cast(\"double\").alias(\"score\"),\n                f.col(\"iv.resourceScore\").alias(\"resourceScore\"),\n                f.lit(cls.DATASET_NAME).alias(\"datasourceId\"),\n                f.lit(cls.PMID).alias(\"pmid\"),\n            )\n        ),\n        _schema=Intervals.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/intervals/epiraction/#gentropy.datasource.intervals.epiraction.IntervalsEpiraction.read","title":"<code>read(spark: SparkSession, path: str) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>Read EPIraction dataset (tsv with header).</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>path</code> <code>str</code> <p>Path to the EPIraction dataset (tsv files)</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Raw EPIraction DataFrame.</p> Source code in <code>src/gentropy/datasource/intervals/epiraction.py</code> <pre><code>@staticmethod\ndef read(spark: SparkSession, path: str) -&gt; DataFrame:\n    \"\"\"Read EPIraction dataset (tsv with header).\n\n    Args:\n        spark (SparkSession): Spark session\n        path (str): Path to the EPIraction dataset (tsv files)\n\n    Returns:\n        DataFrame: Raw EPIraction DataFrame.\n    \"\"\"\n    return (\n        spark.read.option(\"delimiter\", \"\\t\")\n        .option(\"mode\", \"DROPMALFORMED\")\n        .option(\"header\", \"true\")\n        .csv(path)\n    )\n</code></pre>"},{"location":"python_api/datasources/open_targets/_open_targets/","title":"Open Targets","text":"<p>The Open Targets Platform is a comprehensive resource that aims to aggregate and harmonize various types of data to facilitate the identification, prioritization, and validation of drug targets. By integrating publicly available datasets, including data generated by the Open Targets consortium, the Platform builds and scores target-disease associations to assist in drug target identification and prioritization. It also integrates relevant annotation information about targets, diseases, phenotypes, and drugs, as well as their most relevant relationships.</p> <p>Within our analyses, we utilize Open Targets to infer two datasets:</p> <ol> <li> <p>The list of targets:    This dataset provides a compilation of targets. In the Open Targets Platform, a target is understood as any naturally-occurring molecule that can be targeted by a medicinal product. The EMBL-EBI Ensembl database serves as the source for human targets in the Platform, with the Ensembl gene ID as the primary identifier. For more details, refer to this link.</p> </li> <li> <p>The list of Gold Standard Positives:    We use this dataset for training the Locus-to-Gene model. The current list contains 496 Gold Standard Positives.</p> </li> </ol>"},{"location":"python_api/datasources/open_targets/l2g_gold_standard/","title":"L2G Gold Standard","text":""},{"location":"python_api/datasources/open_targets/l2g_gold_standard/#gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard","title":"<code>gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard</code>","text":"<p>Parser for OTGenetics locus to gene gold standards curation.</p> The curation is processed to generate a dataset with 2 labels <ul> <li>Gold Standard Positive (GSP): When the lead variant is part of a curated list of GWAS loci with known gene-trait associations.</li> <li>Gold Standard Negative (GSN): When the lead variant is not part of a curated list of GWAS loci with known gene-trait associations but is in the vicinity of a gene's TSS.</li> </ul> Source code in <code>src/gentropy/datasource/open_targets/l2g_gold_standard.py</code> <pre><code>class OpenTargetsL2GGoldStandard:\n    \"\"\"Parser for OTGenetics locus to gene gold standards curation.\n\n    The curation is processed to generate a dataset with 2 labels:\n        - Gold Standard Positive (GSP): When the lead variant is part of a curated list of GWAS loci with known gene-trait associations.\n        - Gold Standard Negative (GSN): When the lead variant is not part of a curated list of GWAS loci with known gene-trait associations but is in the vicinity of a gene's TSS.\n    \"\"\"\n\n    LOCUS_TO_GENE_WINDOW = 500_000\n\n    @classmethod\n    def parse_positive_curation(\n        cls: type[OpenTargetsL2GGoldStandard], gold_standard_curation: DataFrame\n    ) -&gt; DataFrame:\n        \"\"\"Parse positive set from gold standard curation.\n\n        Args:\n            gold_standard_curation (DataFrame): Gold standard curation dataframe\n\n        Returns:\n            DataFrame: Positive set\n        \"\"\"\n        return (\n            gold_standard_curation.filter(\n                f.col(\"gold_standard_info.highest_confidence\").isin([\"High\", \"Medium\"])\n            )\n            .select(\n                f.col(\"association_info.otg_id\").alias(\"studyId\"),\n                f.col(\"gold_standard_info.gene_id\").alias(\"geneId\"),\n                f.concat_ws(\n                    \"_\",\n                    f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n                    f.col(\"sentinel_variant.locus_GRCh38.position\"),\n                    f.col(\"sentinel_variant.alleles.reference\"),\n                    f.col(\"sentinel_variant.alleles.alternative\"),\n                ).alias(\"variantId\"),\n                f.col(\"metadata.set_label\").alias(\"source\"),\n            )\n            .withColumn(\n                \"studyLocusId\",\n                StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n            )\n            .groupBy(\"studyLocusId\", \"studyId\", \"variantId\", \"geneId\")\n            .agg(f.collect_set(\"source\").alias(\"sources\"))\n        )\n\n    @classmethod\n    def expand_gold_standard_with_negatives(\n        cls: type[OpenTargetsL2GGoldStandard],\n        positive_set: DataFrame,\n        variant_index: VariantIndex,\n    ) -&gt; DataFrame:\n        \"\"\"Create full set of positive and negative evidence of locus to gene associations.\n\n        Negative evidence consists of all genes within a window of 500kb of the lead variant that are not in the positive set.\n\n        Args:\n            positive_set (DataFrame): Positive set from curation\n            variant_index (VariantIndex): Variant index to get distance to gene\n\n        Returns:\n            DataFrame: Full set of positive and negative evidence of locus to gene associations\n        \"\"\"\n        return (\n            positive_set.withColumnRenamed(\"geneId\", \"curated_geneId\")\n            .join(\n                variant_index.get_distance_to_gene()\n                .selectExpr(\n                    \"variantId\",\n                    \"targetId as non_curated_geneId\",\n                    \"distanceFromTss\",\n                )\n                .filter(f.col(\"distanceFromTss\") &lt;= cls.LOCUS_TO_GENE_WINDOW),\n                on=\"variantId\",\n                how=\"left\",\n            )\n            .withColumn(\n                \"goldStandardSet\",\n                f.when(\n                    (f.col(\"curated_geneId\") == f.col(\"non_curated_geneId\"))\n                    # to keep the positives that are not part of the variant index\n                    | (f.col(\"non_curated_geneId\").isNull()),\n                    f.lit(L2GGoldStandard.GS_POSITIVE_LABEL),\n                ).otherwise(L2GGoldStandard.GS_NEGATIVE_LABEL),\n            )\n            .withColumn(\n                \"geneId\",\n                f.when(\n                    f.col(\"goldStandardSet\") == L2GGoldStandard.GS_POSITIVE_LABEL,\n                    f.col(\"curated_geneId\"),\n                ).otherwise(f.col(\"non_curated_geneId\")),\n            )\n            .drop(\"distanceFromTss\", \"curated_geneId\", \"non_curated_geneId\")\n        )\n\n    @classmethod\n    def as_l2g_gold_standard(\n        cls: type[OpenTargetsL2GGoldStandard],\n        gold_standard_curation: DataFrame,\n        variant_index: VariantIndex,\n    ) -&gt; L2GGoldStandard:\n        \"\"\"Initialise L2GGoldStandard from source dataset.\n\n        Args:\n            gold_standard_curation (DataFrame): Gold standard curation dataframe, extracted from https://github.com/opentargets/genetics-gold-standards\n            variant_index (VariantIndex): Dataset to bring distance between a variant and a gene's footprint\n\n        Returns:\n            L2GGoldStandard: L2G Gold Standard dataset. False negatives have not yet been removed.\n        \"\"\"\n        return L2GGoldStandard(\n            _df=cls.parse_positive_curation(gold_standard_curation).transform(\n                cls.expand_gold_standard_with_negatives, variant_index\n            ),\n            _schema=L2GGoldStandard.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/open_targets/l2g_gold_standard/#gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard.as_l2g_gold_standard","title":"<code>as_l2g_gold_standard(gold_standard_curation: DataFrame, variant_index: VariantIndex) -&gt; L2GGoldStandard</code>  <code>classmethod</code>","text":"<p>Initialise L2GGoldStandard from source dataset.</p> <p>Parameters:</p> Name Type Description Default <code>gold_standard_curation</code> <code>DataFrame</code> <p>Gold standard curation dataframe, extracted from https://github.com/opentargets/genetics-gold-standards</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Dataset to bring distance between a variant and a gene's footprint</p> required <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>L2G Gold Standard dataset. False negatives have not yet been removed.</p> Source code in <code>src/gentropy/datasource/open_targets/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef as_l2g_gold_standard(\n    cls: type[OpenTargetsL2GGoldStandard],\n    gold_standard_curation: DataFrame,\n    variant_index: VariantIndex,\n) -&gt; L2GGoldStandard:\n    \"\"\"Initialise L2GGoldStandard from source dataset.\n\n    Args:\n        gold_standard_curation (DataFrame): Gold standard curation dataframe, extracted from https://github.com/opentargets/genetics-gold-standards\n        variant_index (VariantIndex): Dataset to bring distance between a variant and a gene's footprint\n\n    Returns:\n        L2GGoldStandard: L2G Gold Standard dataset. False negatives have not yet been removed.\n    \"\"\"\n    return L2GGoldStandard(\n        _df=cls.parse_positive_curation(gold_standard_curation).transform(\n            cls.expand_gold_standard_with_negatives, variant_index\n        ),\n        _schema=L2GGoldStandard.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/open_targets/l2g_gold_standard/#gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard.expand_gold_standard_with_negatives","title":"<code>expand_gold_standard_with_negatives(positive_set: DataFrame, variant_index: VariantIndex) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Create full set of positive and negative evidence of locus to gene associations.</p> <p>Negative evidence consists of all genes within a window of 500kb of the lead variant that are not in the positive set.</p> <p>Parameters:</p> Name Type Description Default <code>positive_set</code> <code>DataFrame</code> <p>Positive set from curation</p> required <code>variant_index</code> <code>VariantIndex</code> <p>Variant index to get distance to gene</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Full set of positive and negative evidence of locus to gene associations</p> Source code in <code>src/gentropy/datasource/open_targets/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef expand_gold_standard_with_negatives(\n    cls: type[OpenTargetsL2GGoldStandard],\n    positive_set: DataFrame,\n    variant_index: VariantIndex,\n) -&gt; DataFrame:\n    \"\"\"Create full set of positive and negative evidence of locus to gene associations.\n\n    Negative evidence consists of all genes within a window of 500kb of the lead variant that are not in the positive set.\n\n    Args:\n        positive_set (DataFrame): Positive set from curation\n        variant_index (VariantIndex): Variant index to get distance to gene\n\n    Returns:\n        DataFrame: Full set of positive and negative evidence of locus to gene associations\n    \"\"\"\n    return (\n        positive_set.withColumnRenamed(\"geneId\", \"curated_geneId\")\n        .join(\n            variant_index.get_distance_to_gene()\n            .selectExpr(\n                \"variantId\",\n                \"targetId as non_curated_geneId\",\n                \"distanceFromTss\",\n            )\n            .filter(f.col(\"distanceFromTss\") &lt;= cls.LOCUS_TO_GENE_WINDOW),\n            on=\"variantId\",\n            how=\"left\",\n        )\n        .withColumn(\n            \"goldStandardSet\",\n            f.when(\n                (f.col(\"curated_geneId\") == f.col(\"non_curated_geneId\"))\n                # to keep the positives that are not part of the variant index\n                | (f.col(\"non_curated_geneId\").isNull()),\n                f.lit(L2GGoldStandard.GS_POSITIVE_LABEL),\n            ).otherwise(L2GGoldStandard.GS_NEGATIVE_LABEL),\n        )\n        .withColumn(\n            \"geneId\",\n            f.when(\n                f.col(\"goldStandardSet\") == L2GGoldStandard.GS_POSITIVE_LABEL,\n                f.col(\"curated_geneId\"),\n            ).otherwise(f.col(\"non_curated_geneId\")),\n        )\n        .drop(\"distanceFromTss\", \"curated_geneId\", \"non_curated_geneId\")\n    )\n</code></pre>"},{"location":"python_api/datasources/open_targets/l2g_gold_standard/#gentropy.datasource.open_targets.l2g_gold_standard.OpenTargetsL2GGoldStandard.parse_positive_curation","title":"<code>parse_positive_curation(gold_standard_curation: DataFrame) -&gt; DataFrame</code>  <code>classmethod</code>","text":"<p>Parse positive set from gold standard curation.</p> <p>Parameters:</p> Name Type Description Default <code>gold_standard_curation</code> <code>DataFrame</code> <p>Gold standard curation dataframe</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Positive set</p> Source code in <code>src/gentropy/datasource/open_targets/l2g_gold_standard.py</code> <pre><code>@classmethod\ndef parse_positive_curation(\n    cls: type[OpenTargetsL2GGoldStandard], gold_standard_curation: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Parse positive set from gold standard curation.\n\n    Args:\n        gold_standard_curation (DataFrame): Gold standard curation dataframe\n\n    Returns:\n        DataFrame: Positive set\n    \"\"\"\n    return (\n        gold_standard_curation.filter(\n            f.col(\"gold_standard_info.highest_confidence\").isin([\"High\", \"Medium\"])\n        )\n        .select(\n            f.col(\"association_info.otg_id\").alias(\"studyId\"),\n            f.col(\"gold_standard_info.gene_id\").alias(\"geneId\"),\n            f.concat_ws(\n                \"_\",\n                f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n                f.col(\"sentinel_variant.locus_GRCh38.position\"),\n                f.col(\"sentinel_variant.alleles.reference\"),\n                f.col(\"sentinel_variant.alleles.alternative\"),\n            ).alias(\"variantId\"),\n            f.col(\"metadata.set_label\").alias(\"source\"),\n        )\n        .withColumn(\n            \"studyLocusId\",\n            StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n        )\n        .groupBy(\"studyLocusId\", \"studyId\", \"variantId\", \"geneId\")\n        .agg(f.collect_set(\"source\").alias(\"sources\"))\n    )\n</code></pre>"},{"location":"python_api/datasources/ukb_ppp_eur/_ukb_ppp_eur/","title":"UK Biobank Pharma Proteomics Project (UKB-PPP) (EUR)","text":"<p>The UKB-PPP is a collaboration between the UK Biobank (UKB) and thirteen biopharmaceutical companies characterising the plasma proteomic profiles of 54,219 UKB participants.</p> <p>The original data is available here: https://www.synapse.org/Synapse:syn51364943/wiki/622119. The associated paper is https://www.nature.com/articles/s41586-023-06592-6.</p>"},{"location":"python_api/datasources/ukb_ppp_eur/study_index/","title":"Study Index","text":""},{"location":"python_api/datasources/ukb_ppp_eur/study_index/#gentropy.datasource.ukb_ppp_eur.study_index.UkbPppEurStudyIndex","title":"<code>gentropy.datasource.ukb_ppp_eur.study_index.UkbPppEurStudyIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>StudyIndex</code></p> <p>Study index dataset from UKB PPP (EUR).</p> Source code in <code>src/gentropy/datasource/ukb_ppp_eur/study_index.py</code> <pre><code>class UkbPppEurStudyIndex(StudyIndex):\n    \"\"\"Study index dataset from UKB PPP (EUR).\"\"\"\n\n    @classmethod\n    def from_source(\n        cls: type[UkbPppEurStudyIndex],\n        spark: SparkSession,\n        raw_study_index_path_from_tsv: str,\n        raw_summary_stats_path: str,\n    ) -&gt; StudyIndex:\n        \"\"\"This function ingests study level metadata from UKB PPP (EUR).\n\n        Args:\n            spark (SparkSession): Spark session object.\n            raw_study_index_path_from_tsv (str): Raw study index path.\n            raw_summary_stats_path (str): Raw summary stats path.\n\n        Returns:\n            StudyIndex: Parsed and annotated UKB PPP (EUR) study table.\n        \"\"\"\n        # In order to populate the nSamples column, we need to peek inside the summary stats dataframe.\n        num_of_samples = (\n            spark.read.parquet(raw_summary_stats_path)\n            .filter(f.col(\"chromosome\") == \"22\")\n            .groupBy(\"studyId\")\n            .agg(f.first(\"N\").cast(\"integer\").alias(\"nSamples\"))\n            .select(\"*\")\n        )\n        # Now we can read the raw study index and complete the processing.\n        study_index_df = (\n            spark.read.csv(raw_study_index_path_from_tsv, sep=\"\\t\", header=True)\n            .select(\n                f.lit(\"pqtl\").alias(\"studyType\"),\n                f.lit(\"UKB_PPP_EUR\").alias(\"projectId\"),\n                f.col(\"_gentropy_study_id\").alias(\"studyId\"),\n                f.col(\"UKBPPP_ProteinID\").alias(\"traitFromSource\"),\n                f.lit(\"UBERON_0001969\").alias(\"biosampleFromSourceId\"),\n                f.col(\"ensembl_id\").alias(\"geneId\"),\n                f.lit(True).alias(\"hasSumstats\"),\n                f.col(\"_gentropy_summary_stats_link\").alias(\"summarystatsLocation\"),\n            )\n            .join(num_of_samples, \"studyId\", \"inner\")\n        )\n        # Add population structure.\n        study_index_df = (\n            study_index_df.withColumn(\n                \"discoverySamples\",\n                f.array(\n                    f.struct(\n                        f.col(\"nSamples\").cast(\"integer\").alias(\"sampleSize\"),\n                        f.lit(\"European\").alias(\"ancestry\"),\n                    )\n                ),\n            )\n            .withColumn(\n                \"ldPopulationStructure\",\n                cls.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n            )\n            .withColumn(\"biosampleFromSourceId\", f.lit(\"UBERON_0001969\"))\n        )\n\n        return StudyIndex(\n            _df=study_index_df,\n            _schema=StudyIndex.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/datasources/ukb_ppp_eur/study_index/#gentropy.datasource.ukb_ppp_eur.study_index.UkbPppEurStudyIndex.from_source","title":"<code>from_source(spark: SparkSession, raw_study_index_path_from_tsv: str, raw_summary_stats_path: str) -&gt; StudyIndex</code>  <code>classmethod</code>","text":"<p>This function ingests study level metadata from UKB PPP (EUR).</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>raw_study_index_path_from_tsv</code> <code>str</code> <p>Raw study index path.</p> required <code>raw_summary_stats_path</code> <code>str</code> <p>Raw summary stats path.</p> required <p>Returns:</p> Name Type Description <code>StudyIndex</code> <code>StudyIndex</code> <p>Parsed and annotated UKB PPP (EUR) study table.</p> Source code in <code>src/gentropy/datasource/ukb_ppp_eur/study_index.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[UkbPppEurStudyIndex],\n    spark: SparkSession,\n    raw_study_index_path_from_tsv: str,\n    raw_summary_stats_path: str,\n) -&gt; StudyIndex:\n    \"\"\"This function ingests study level metadata from UKB PPP (EUR).\n\n    Args:\n        spark (SparkSession): Spark session object.\n        raw_study_index_path_from_tsv (str): Raw study index path.\n        raw_summary_stats_path (str): Raw summary stats path.\n\n    Returns:\n        StudyIndex: Parsed and annotated UKB PPP (EUR) study table.\n    \"\"\"\n    # In order to populate the nSamples column, we need to peek inside the summary stats dataframe.\n    num_of_samples = (\n        spark.read.parquet(raw_summary_stats_path)\n        .filter(f.col(\"chromosome\") == \"22\")\n        .groupBy(\"studyId\")\n        .agg(f.first(\"N\").cast(\"integer\").alias(\"nSamples\"))\n        .select(\"*\")\n    )\n    # Now we can read the raw study index and complete the processing.\n    study_index_df = (\n        spark.read.csv(raw_study_index_path_from_tsv, sep=\"\\t\", header=True)\n        .select(\n            f.lit(\"pqtl\").alias(\"studyType\"),\n            f.lit(\"UKB_PPP_EUR\").alias(\"projectId\"),\n            f.col(\"_gentropy_study_id\").alias(\"studyId\"),\n            f.col(\"UKBPPP_ProteinID\").alias(\"traitFromSource\"),\n            f.lit(\"UBERON_0001969\").alias(\"biosampleFromSourceId\"),\n            f.col(\"ensembl_id\").alias(\"geneId\"),\n            f.lit(True).alias(\"hasSumstats\"),\n            f.col(\"_gentropy_summary_stats_link\").alias(\"summarystatsLocation\"),\n        )\n        .join(num_of_samples, \"studyId\", \"inner\")\n    )\n    # Add population structure.\n    study_index_df = (\n        study_index_df.withColumn(\n            \"discoverySamples\",\n            f.array(\n                f.struct(\n                    f.col(\"nSamples\").cast(\"integer\").alias(\"sampleSize\"),\n                    f.lit(\"European\").alias(\"ancestry\"),\n                )\n            ),\n        )\n        .withColumn(\n            \"ldPopulationStructure\",\n            cls.aggregate_and_map_ancestries(f.col(\"discoverySamples\")),\n        )\n        .withColumn(\"biosampleFromSourceId\", f.lit(\"UBERON_0001969\"))\n    )\n\n    return StudyIndex(\n        _df=study_index_df,\n        _schema=StudyIndex.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/ukb_ppp_eur/summary_stats/","title":"Summary Statistics","text":""},{"location":"python_api/datasources/ukb_ppp_eur/summary_stats/#gentropy.datasource.ukb_ppp_eur.summary_stats.UkbPppEurSummaryStats","title":"<code>gentropy.datasource.ukb_ppp_eur.summary_stats.UkbPppEurSummaryStats</code>  <code>dataclass</code>","text":"<p>Summary statistics dataset for UKB PPP (EUR).</p> Source code in <code>src/gentropy/datasource/ukb_ppp_eur/summary_stats.py</code> <pre><code>@dataclass\nclass UkbPppEurSummaryStats:\n    \"\"\"Summary statistics dataset for UKB PPP (EUR).\"\"\"\n\n    @classmethod\n    def from_source(\n        cls: type[UkbPppEurSummaryStats],\n        spark: SparkSession,\n        raw_summary_stats_path: str,\n        tmp_variant_annotation_path: str,\n        chromosome: str,\n        study_index_path: str,\n    ) -&gt; SummaryStatistics:\n        \"\"\"Ingest and harmonise all summary stats for UKB PPP (EUR) data.\n\n        Args:\n            spark (SparkSession): Spark session object.\n            raw_summary_stats_path (str): Input raw summary stats path.\n            tmp_variant_annotation_path (str): Input variant annotation dataset path.\n            chromosome (str): Which chromosome to process.\n            study_index_path (str): The path to study index, which is necessary in some cases to populate the sample size column.\n\n        Returns:\n            SummaryStatistics: Processed summary statistics dataset for a given chromosome.\n        \"\"\"\n        df = harmonise_summary_stats(\n            spark,\n            raw_summary_stats_path,\n            tmp_variant_annotation_path,\n            chromosome,\n            colname_position=\"GENPOS\",\n            colname_allele0=\"ALLELE0\",\n            colname_allele1=\"ALLELE1\",\n            colname_a1freq=\"A1FREQ\",\n            colname_info=\"INFO\",\n            colname_beta=\"BETA\",\n            colname_se=\"SE\",\n            colname_mlog10p=\"LOG10P\",\n            colname_n=\"N\",\n        )\n\n        # Create the summary statistics object.\n        return SummaryStatistics(\n            _df=df,\n            _schema=SummaryStatistics.get_schema(),\n        )\n\n    @classmethod\n    def process_summary_stats_per_chromosome(\n        cls,\n        session: Session,\n        raw_summary_stats_path: str,\n        tmp_variant_annotation_path: str,\n        summary_stats_output_path: str,\n        study_index_path: str,\n    ) -&gt; None:\n        \"\"\"Processes summary statistics for each chromosome, partitioning and writing results.\n\n        Args:\n            session (Session): The Gentropy session session to use for distributed data processing.\n            raw_summary_stats_path (str): The path to the raw summary statistics files.\n            tmp_variant_annotation_path (str): The path to temporary variant annotation data, used for chromosome joins.\n            summary_stats_output_path (str): The output path to write processed summary statistics as parquet files.\n            study_index_path (str): The path to study index, which is necessary in some cases to populate the sample size column.\n        \"\"\"\n        # Set mode to overwrite for processing the first chromosome.\n        write_mode = \"overwrite\"\n        # Chromosome 23 is X, this is handled downstream.\n        for chromosome in list(range(1, 24)):\n            logging_message = f\"  Processing chromosome {chromosome}\"\n            session.logger.info(logging_message)\n            (\n                cls.from_source(\n                    spark=session.spark,\n                    raw_summary_stats_path=raw_summary_stats_path,\n                    tmp_variant_annotation_path=tmp_variant_annotation_path,\n                    chromosome=str(chromosome),\n                    study_index_path=study_index_path,\n                )\n                .df.coalesce(1)\n                .repartition(\"studyId\", \"chromosome\")\n                .write.partitionBy(\"studyId\", \"chromosome\")\n                .mode(write_mode)\n                .parquet(summary_stats_output_path)\n            )\n            # Now that we have written the first chromosome, change mode to append for subsequent operations.\n            write_mode = \"append\"\n</code></pre>"},{"location":"python_api/datasources/ukb_ppp_eur/summary_stats/#gentropy.datasource.ukb_ppp_eur.summary_stats.UkbPppEurSummaryStats.from_source","title":"<code>from_source(spark: SparkSession, raw_summary_stats_path: str, tmp_variant_annotation_path: str, chromosome: str, study_index_path: str) -&gt; SummaryStatistics</code>  <code>classmethod</code>","text":"<p>Ingest and harmonise all summary stats for UKB PPP (EUR) data.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>raw_summary_stats_path</code> <code>str</code> <p>Input raw summary stats path.</p> required <code>tmp_variant_annotation_path</code> <code>str</code> <p>Input variant annotation dataset path.</p> required <code>chromosome</code> <code>str</code> <p>Which chromosome to process.</p> required <code>study_index_path</code> <code>str</code> <p>The path to study index, which is necessary in some cases to populate the sample size column.</p> required <p>Returns:</p> Name Type Description <code>SummaryStatistics</code> <code>SummaryStatistics</code> <p>Processed summary statistics dataset for a given chromosome.</p> Source code in <code>src/gentropy/datasource/ukb_ppp_eur/summary_stats.py</code> <pre><code>@classmethod\ndef from_source(\n    cls: type[UkbPppEurSummaryStats],\n    spark: SparkSession,\n    raw_summary_stats_path: str,\n    tmp_variant_annotation_path: str,\n    chromosome: str,\n    study_index_path: str,\n) -&gt; SummaryStatistics:\n    \"\"\"Ingest and harmonise all summary stats for UKB PPP (EUR) data.\n\n    Args:\n        spark (SparkSession): Spark session object.\n        raw_summary_stats_path (str): Input raw summary stats path.\n        tmp_variant_annotation_path (str): Input variant annotation dataset path.\n        chromosome (str): Which chromosome to process.\n        study_index_path (str): The path to study index, which is necessary in some cases to populate the sample size column.\n\n    Returns:\n        SummaryStatistics: Processed summary statistics dataset for a given chromosome.\n    \"\"\"\n    df = harmonise_summary_stats(\n        spark,\n        raw_summary_stats_path,\n        tmp_variant_annotation_path,\n        chromosome,\n        colname_position=\"GENPOS\",\n        colname_allele0=\"ALLELE0\",\n        colname_allele1=\"ALLELE1\",\n        colname_a1freq=\"A1FREQ\",\n        colname_info=\"INFO\",\n        colname_beta=\"BETA\",\n        colname_se=\"SE\",\n        colname_mlog10p=\"LOG10P\",\n        colname_n=\"N\",\n    )\n\n    # Create the summary statistics object.\n    return SummaryStatistics(\n        _df=df,\n        _schema=SummaryStatistics.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/datasources/ukb_ppp_eur/summary_stats/#gentropy.datasource.ukb_ppp_eur.summary_stats.UkbPppEurSummaryStats.process_summary_stats_per_chromosome","title":"<code>process_summary_stats_per_chromosome(session: Session, raw_summary_stats_path: str, tmp_variant_annotation_path: str, summary_stats_output_path: str, study_index_path: str) -&gt; None</code>  <code>classmethod</code>","text":"<p>Processes summary statistics for each chromosome, partitioning and writing results.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>The Gentropy session session to use for distributed data processing.</p> required <code>raw_summary_stats_path</code> <code>str</code> <p>The path to the raw summary statistics files.</p> required <code>tmp_variant_annotation_path</code> <code>str</code> <p>The path to temporary variant annotation data, used for chromosome joins.</p> required <code>summary_stats_output_path</code> <code>str</code> <p>The output path to write processed summary statistics as parquet files.</p> required <code>study_index_path</code> <code>str</code> <p>The path to study index, which is necessary in some cases to populate the sample size column.</p> required Source code in <code>src/gentropy/datasource/ukb_ppp_eur/summary_stats.py</code> <pre><code>@classmethod\ndef process_summary_stats_per_chromosome(\n    cls,\n    session: Session,\n    raw_summary_stats_path: str,\n    tmp_variant_annotation_path: str,\n    summary_stats_output_path: str,\n    study_index_path: str,\n) -&gt; None:\n    \"\"\"Processes summary statistics for each chromosome, partitioning and writing results.\n\n    Args:\n        session (Session): The Gentropy session session to use for distributed data processing.\n        raw_summary_stats_path (str): The path to the raw summary statistics files.\n        tmp_variant_annotation_path (str): The path to temporary variant annotation data, used for chromosome joins.\n        summary_stats_output_path (str): The output path to write processed summary statistics as parquet files.\n        study_index_path (str): The path to study index, which is necessary in some cases to populate the sample size column.\n    \"\"\"\n    # Set mode to overwrite for processing the first chromosome.\n    write_mode = \"overwrite\"\n    # Chromosome 23 is X, this is handled downstream.\n    for chromosome in list(range(1, 24)):\n        logging_message = f\"  Processing chromosome {chromosome}\"\n        session.logger.info(logging_message)\n        (\n            cls.from_source(\n                spark=session.spark,\n                raw_summary_stats_path=raw_summary_stats_path,\n                tmp_variant_annotation_path=tmp_variant_annotation_path,\n                chromosome=str(chromosome),\n                study_index_path=study_index_path,\n            )\n            .df.coalesce(1)\n            .repartition(\"studyId\", \"chromosome\")\n            .write.partitionBy(\"studyId\", \"chromosome\")\n            .mode(write_mode)\n            .parquet(summary_stats_output_path)\n        )\n        # Now that we have written the first chromosome, change mode to append for subsequent operations.\n        write_mode = \"append\"\n</code></pre>"},{"location":"python_api/methods/_methods/","title":"Methods","text":"<p>This section consists of all the methods available in the package. It provides detailed explanations and usage examples for each method. Developers can refer to this section to understand how to use the methods effectively in their code. The list of methods is constantly updated.</p>"},{"location":"python_api/methods/carma/","title":"CARMA","text":"<p>CARMA is the method of the fine-mapping and outlier detection, originally implemented in R (CARMA on GitHub).</p> <p>The full repository for the reimplementation of CARMA in Python can be found here.</p> <p>This is a simplified version of CARMA with the following features:</p> <ol> <li>It uses only Spike-slab effect size priors and Poisson model priors.</li> <li>C++ is re-implemented in Python.</li> <li>The way of storing the configuration list is changed. It uses a string with the list of indexes for causal SNPs instead of a sparse matrix.</li> <li>Fixed bugs in PIP calculation.</li> <li>No credible models.</li> <li>No credible sets, only PIPs.</li> <li>No functional annotations.</li> <li>Removed unnecessary parameters.</li> </ol>"},{"location":"python_api/methods/carma/#gentropy.method.carma.CARMA","title":"<code>gentropy.method.carma.CARMA</code>","text":"<p>Implementation of CARMA outlier detection method.</p> Source code in <code>src/gentropy/method/carma.py</code> <pre><code>class CARMA:\n    \"\"\"Implementation of CARMA outlier detection method.\"\"\"\n\n    @staticmethod\n    def time_limited_CARMA_spike_slab_noEM(\n        z: np.ndarray,\n        ld: np.ndarray,\n        sec_threshold: float = 600,\n        tau: float = 0.04,\n    ) -&gt; dict[str, Any]:\n        \"\"\"The wrapper for the CARMA_spike_slab_noEM function that runs the function in a separate thread and terminates it if it takes too long.\n\n        Args:\n            z (np.ndarray): Numeric vector representing z-scores.\n            ld (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n            sec_threshold (float): The time threshold in seconds.\n            tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix\n\n        Returns:\n            dict[str, Any]: A dictionary containing the following results:\n                - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs or None.\n                - B_list: A dataframe containing the marginal likelihoods and the corresponding model space or None.\n                - Outliers: A list of outlier SNPs or None.\n        \"\"\"\n        # Ignore pandas future warnings\n        warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n        try:\n            # Execute CARMA.CARMA_spike_slab_noEM with a timeout\n            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n                future = executor.submit(\n                    CARMA.CARMA_spike_slab_noEM, z=z, ld=ld, tau=tau\n                )\n                result = future.result(timeout=sec_threshold)\n        except concurrent.futures.TimeoutError:\n            # If execution exceeds the timeout, return None\n            result = {\"PIPs\": None, \"B_list\": None, \"Outliers\": None}\n\n        return result\n\n    @staticmethod\n    def CARMA_spike_slab_noEM(\n        z: np.ndarray,\n        ld: np.ndarray,\n        lambda_val: float = 1,\n        Max_Model_Dim: int = 200_000,\n        all_iter: int = 1,\n        all_inner_iter: int = 10,\n        epsilon_threshold: float = 1e-5,\n        num_causal: int = 10,\n        tau: float = 0.04,\n        outlier_switch: bool = True,\n        outlier_BF_index: float = 1 / 3.2,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Perform CARMA analysis using a Spike-and-Slab prior without Expectation-Maximization (EM).\n\n        Args:\n            z (np.ndarray): Numeric vector representing z-scores.\n            ld (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n            lambda_val (float): Regularization parameter controlling the strength of the L1 penalty.\n            Max_Model_Dim (int): Maximum allowed dimension for the causal models.\n            all_iter (int): The total number of iterations to run the CARMA analysis.\n            all_inner_iter (int): The number of inner iterations in each CARMA iteration.\n            epsilon_threshold (float): Threshold for convergence in CARMA iterations.\n            num_causal (int): Maximal number of causal variants to be selected in the final model.\n            tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix.\n            outlier_switch (bool): Whether to consider outlier detection in the analysis.\n            outlier_BF_index (float): Bayes Factor threshold for identifying outliers.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the following results:\n                - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs.\n                - B_list: A dataframe containing the marginal likelihoods and the corresponding model space.\n                - Outliers: A list of outlier SNPs.\n        \"\"\"\n        p_snp = len(z)\n        epsilon_list = epsilon_threshold * p_snp\n        all_epsilon_threshold = epsilon_threshold * p_snp\n\n        # Zero step\n        all_C_list = CARMA._MCS_modified(\n            z=z,\n            ld_matrix=ld,\n            epsilon=epsilon_list,\n            Max_Model_Dim=Max_Model_Dim,\n            lambda_val=lambda_val,\n            outlier_switch=outlier_switch,\n            tau=tau,\n            num_causal=num_causal,\n            inner_all_iter=all_inner_iter,\n            outlier_BF_index=outlier_BF_index,\n        )\n\n        # Main steps\n        for _ in range(0, all_iter):\n            ac1 = all_C_list[\"B_list\"][\"set_gamma_margin\"]\n            previous_result = np.mean(ac1[0 : round(len(ac1) / 4)])\n\n            all_C_list = CARMA._MCS_modified(\n                z=z,\n                ld_matrix=ld,\n                input_conditional_S_list=all_C_list[\"conditional_S_list\"],\n                Max_Model_Dim=Max_Model_Dim,\n                num_causal=num_causal,\n                epsilon=epsilon_list,\n                outlier_switch=outlier_switch,\n                tau=tau,\n                lambda_val=lambda_val,\n                inner_all_iter=all_inner_iter,\n                outlier_BF_index=outlier_BF_index,\n            )\n\n            ac1 = all_C_list[\"B_list\"][\"set_gamma_margin\"]\n            difference = np.abs(previous_result - np.mean(ac1[0 : round(len(ac1) / 4)]))\n            if difference &lt; all_epsilon_threshold:\n                break\n\n        # Calculate PIPs and Credible Set\n        pip = CARMA._PIP_func(\n            likeli=all_C_list[\"B_list\"][\"set_gamma_margin\"],\n            model_space=all_C_list[\"B_list\"][\"matrix_gamma\"],\n            p=p_snp,\n            num_causal=num_causal,\n        )\n\n        results_list = {\n            \"PIPs\": pip,\n            \"B_list\": all_C_list[\"B_list\"],\n            \"Outliers\": all_C_list[\"conditional_S_list\"],\n        }\n\n        return results_list\n\n    @staticmethod\n    def _ind_normal_sigma_fixed_marginal_fun_indi(\n        zSigmaz_S: np.ndarray, tau: float, p_S: int, det_S: float\n    ) -&gt; float:\n        \"\"\"Internal function for calculating the marginal likelihood of configuration model.\n\n        Args:\n            zSigmaz_S (np.ndarray): The zSigmaz_S value.\n            tau (float): The tau value.\n            p_S (int): The number of SNPs.\n            det_S (float): The det_S value.\n\n        Returns:\n            float: The marginal likelihood of a model.\n\n        Examples:\n            &gt;&gt;&gt; zSigmaz_S = 0.1\n            &gt;&gt;&gt; tau = 1 / 0.05**2\n            &gt;&gt;&gt; p_S = 3\n            &gt;&gt;&gt; det_S = 0.1\n            &gt;&gt;&gt; np.round(CARMA._ind_normal_sigma_fixed_marginal_fun_indi(zSigmaz_S, tau, p_S, det_S),decimals=5)\n            np.float64(10.18849)\n        \"\"\"\n        return p_S / 2.0 * np.log(tau) - 0.5 * np.log(det_S) + zSigmaz_S / 2.0\n\n    @staticmethod\n    def _ind_Normal_fixed_sigma_marginal_external(\n        index_vec_input: np.ndarray,\n        Sigma: np.ndarray,\n        z: np.ndarray,\n        tau: float,\n        p_S: int,\n    ) -&gt; float:\n        \"\"\"Marginal likelihood of configuration model.\n\n        Args:\n            index_vec_input (np.ndarray): The index vector.\n            Sigma (np.ndarray): The Sigma matrix.\n            z (np.ndarray): The z vector.\n            tau (float): The tau value.\n            p_S (int): The number of SNPs.\n\n        Returns:\n            float: The marginal likelihood of a model.\n\n        Examples:\n            &gt;&gt;&gt; index_vec_input = np.array([1, 2])\n            &gt;&gt;&gt; Sigma = np.array([[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]])\n            &gt;&gt;&gt; z = np.array([10, 11, 10])\n            &gt;&gt;&gt; tau = 1\n            &gt;&gt;&gt; p_S = 2\n            &gt;&gt;&gt; np.round(CARMA._ind_Normal_fixed_sigma_marginal_external(index_vec_input, Sigma, z, tau, p_S),decimals=5)\n            np.float64(43.60579)\n        \"\"\"\n        index_vec = index_vec_input - 1\n        Sigma_S = Sigma[np.ix_(index_vec, index_vec)]\n        A = tau * np.eye(p_S)\n\n        det_S = det(Sigma_S + A)\n        Sigma_S_inv = inv(Sigma_S + A)\n\n        sub_z = z[index_vec]\n        zSigmaz_S = np.dot(sub_z.T, np.dot(Sigma_S_inv, sub_z))\n\n        b = CARMA._ind_normal_sigma_fixed_marginal_fun_indi(zSigmaz_S, tau, p_S, det_S)\n\n        results = b\n\n        return results\n\n    @staticmethod\n    def _outlier_ind_Normal_marginal_external(\n        index_vec_input: np.ndarray,\n        Sigma: np.ndarray,\n        z: np.ndarray,\n        tau: float,\n        p_S: int,\n    ) -&gt; float:\n        \"\"\"Likehood of outlier model.\n\n        Args:\n            index_vec_input (np.ndarray): The index vector.\n            Sigma (np.ndarray): The Sigma matrix.\n            z (np.ndarray): The z vector.\n            tau (float): The tau value.\n            p_S (int): The number of SNPs.\n\n        Returns:\n            float: The likelihood of a model.\n\n        Examples:\n            &gt;&gt;&gt; index_vec_input = np.array([1, 2, 3])\n            &gt;&gt;&gt; Sigma = np.array([[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]])\n            &gt;&gt;&gt; z = np.array([0.1, 0.2, 0.3])\n            &gt;&gt;&gt; tau = 1 / 0.05**2\n            &gt;&gt;&gt; p_S = 3\n            &gt;&gt;&gt; np.round(CARMA._outlier_ind_Normal_marginal_external(index_vec_input, Sigma, z, tau, p_S),decimals=5)\n            np.float64(-8.8497)\n        \"\"\"\n        index_vec = index_vec_input - 1\n\n        Sigma_S = Sigma[np.ix_(index_vec, index_vec)]\n        A = tau * np.eye(p_S)\n\n        Sigma_S_I_inv = pinv(Sigma_S + A, rtol=0.00001)\n        Sigma_S_inv = pinv(Sigma_S, rtol=0.00001)\n\n        det_S = np.abs(det(Sigma_S_inv))\n        det_I_S = np.abs(det(Sigma_S_I_inv))\n\n        sub_z = z[index_vec]\n        zSigmaz_S = np.dot(sub_z, np.dot(Sigma_S_inv, sub_z))\n        zSigmaz_I_S = np.dot(sub_z, np.dot(Sigma_S_I_inv, sub_z))\n\n        b = 0.5 * (np.log(det_S) + np.log(det_I_S)) - 0.5 * (zSigmaz_S - zSigmaz_I_S)\n        results = b\n\n        return results\n\n    @staticmethod\n    def _add_function(S_sub: np.ndarray, y: Any) -&gt; np.ndarray:\n        \"\"\"Concatenate two arrays and sort the result.\n\n        Args:\n            S_sub (np.ndarray): The first array.\n            y (Any): The second array.\n\n        Returns:\n            np.ndarray: The concatenated and sorted array.\n\n        Examples:\n            &gt;&gt;&gt; S_sub = np.array([3, 4])\n            &gt;&gt;&gt; y = np.array([1, 2])\n            &gt;&gt;&gt; CARMA._add_function(S_sub, y)\n            array([[1, 2, 3],\n                   [1, 2, 4]])\n        \"\"\"\n        return np.array([np.sort(np.concatenate(([x], y))) for x in S_sub])\n\n    @staticmethod\n    def _set_gamma_func_base(S: Any, p: int) -&gt; dict[int, np.ndarray]:\n        \"\"\"Creates a dictionary of sets of configurations assuming no conditional set.\n\n        Args:\n            S (Any): The input set.\n            p (int): The number of SNPs.\n\n        Returns:\n            dict[int, np.ndarray]: A dictionary of sets of configurations.\n\n        Examples:\n        &gt;&gt;&gt; S = [0,1]\n        &gt;&gt;&gt; p = 4\n        &gt;&gt;&gt; CARMA._set_gamma_func_base(S, p)\n        {0: array([[0],\n               [1]]), 1: array([[0, 1, 2],\n               [0, 1, 3]]), 2: array([[0, 2],\n               [0, 3],\n               [1, 2],\n               [1, 3]])}\n\n        &gt;&gt;&gt; S = [0]\n        &gt;&gt;&gt; p = 2\n        &gt;&gt;&gt; CARMA._set_gamma_func_base(S, p)\n        {0: None, 1: array([[0, 1]]), 2: array([[1]])}\n\n        &gt;&gt;&gt; S = []\n        &gt;&gt;&gt; p = 2\n        &gt;&gt;&gt; CARMA._set_gamma_func_base(S, p)\n        {0: None, 1: array([[0],\n               [1]]), 2: None}\n        \"\"\"\n        set_gamma: dict[int, Any] = {}\n\n        if len(S) == 0:\n            set_gamma[0] = None\n            set_gamma[1] = np.arange(0, p).reshape(-1, 1)\n            set_gamma[2] = None\n\n        if len(S) == 1:\n            S_sub = np.setdiff1d(np.arange(0, p), S)\n            set_gamma[0] = None\n            set_gamma[1] = CARMA._add_function(S_sub, S)\n            set_gamma[2] = S_sub.reshape(-1, 1)\n\n        if len(S) &gt; 1:\n            S_sub = np.setdiff1d(np.arange(0, p), S)\n            S = np.sort(S)\n            set_gamma[0] = np.array(list(combinations(S, len(S) - 1)))\n            set_gamma[1] = CARMA._add_function(S_sub, S)\n            xs = np.vstack([CARMA._add_function(S_sub, row) for row in set_gamma[0]])\n            set_gamma[2] = xs\n\n        return set_gamma\n\n    @staticmethod\n    def _set_gamma_func_conditional(\n        input_S: Any, condition_index: list[int], p: int\n    ) -&gt; dict[int, np.ndarray]:\n        \"\"\"Creates a dictionary of sets of configurations assuming conditional set.\n\n        Args:\n            input_S (Any): The input set.\n            condition_index (list[int]): The conditional set.\n            p (int): The number of SNPs.\n\n        Returns:\n            dict[int, np.ndarray]: A dictionary of sets of configurations.\n\n        Examples:\n        &gt;&gt;&gt; input_S = [0,1,2]\n        &gt;&gt;&gt; condition_index = [2]\n        &gt;&gt;&gt; p = 4\n        &gt;&gt;&gt; CARMA._set_gamma_func_conditional(input_S, condition_index, p)\n        {0: array([[0],\n               [1]]), 1: array([[0, 1, 3]]), 2: array([[0, 3],\n               [1, 3]])}\n        \"\"\"\n        set_gamma: dict[int, Any] = {}\n        S = np.setdiff1d(input_S, condition_index)\n\n        # set of gamma-\n        if len(S) == 0:\n            S_sub = np.setdiff1d(np.arange(0, p), condition_index)\n            set_gamma[0] = None\n            set_gamma[1] = S_sub.reshape(-1, 1)\n            set_gamma[2] = None\n\n        if len(S) == 1:\n            S_sub = np.setdiff1d(np.arange(0, p), input_S)\n            set_gamma[0] = None\n            set_gamma[1] = CARMA._add_function(S_sub, S)\n            set_gamma[2] = S_sub.reshape(-1, 1)\n\n        if len(S) &gt; 1:\n            S_sub = np.setdiff1d(np.arange(0, p), input_S)\n            S = np.sort(S)\n            set_gamma[0] = np.array(list(combinations(S, len(S) - 1)))\n            set_gamma[1] = CARMA._add_function(S_sub, S)\n            xs = np.vstack([CARMA._add_function(S_sub, row) for row in set_gamma[0]])\n            set_gamma[2] = xs\n\n        return set_gamma\n\n    @staticmethod\n    def _set_gamma_func(\n        input_S: Any, p: int, condition_index: list[int] | None = None\n    ) -&gt; dict[int, np.ndarray]:\n        \"\"\"Creates a dictionary of sets of configurations.\n\n        Args:\n            input_S (Any): The input set.\n            p (int): The number of SNPs.\n            condition_index (list[int] | None): The conditional set. Defaults to None.\n\n        Returns:\n            dict[int, np.ndarray]: A dictionary of sets of configurations.\n\n        Examples:\n        &gt;&gt;&gt; input_S = [0,1,2]\n        &gt;&gt;&gt; condition_index=[2]\n        &gt;&gt;&gt; p = 4\n        &gt;&gt;&gt; CARMA._set_gamma_func(input_S, p, condition_index)\n        {0: array([[0],\n               [1]]), 1: array([[0, 1, 3]]), 2: array([[0, 3],\n               [1, 3]])}\n        \"\"\"\n        if condition_index is None:\n            results = CARMA._set_gamma_func_base(input_S, p)\n        else:\n            results = CARMA._set_gamma_func_conditional(input_S, condition_index, p)\n        return results\n\n    @staticmethod\n    def _index_fun_internal(x: np.ndarray) -&gt; str:\n        \"\"\"Convert an array of causal SNP indexes to comma-separated string.\n\n        Args:\n            x (np.ndarray): The input array.\n\n        Returns:\n            str: The comma-separated string.\n\n        Examples:\n        &gt;&gt;&gt; x = np.array([1,2,3])\n        &gt;&gt;&gt; CARMA._index_fun_internal(x)\n        '1,2,3'\n        \"\"\"\n        y = np.sort(x)\n        y = y.astype(str)\n        return \",\".join(y)\n\n    @staticmethod\n    def _index_fun(y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Convert an array of causal SNP indexes to comma-separated string.\n\n        Args:\n            y (np.ndarray): The input array.\n\n        Returns:\n            np.ndarray: The comma-separated string.\n\n        Examples:\n        &gt;&gt;&gt; y = np.array([[1,2,3],[4,5,6]])\n        &gt;&gt;&gt; CARMA._index_fun(y)\n        array(['1,2,3', '4,5,6'], dtype='&lt;U5')\n        \"\"\"\n        return np.array([CARMA._index_fun_internal(x) for x in y])\n\n    @staticmethod\n    def _ridge_fun(\n        x: float,\n        Sigma: np.ndarray,\n        modi_ld_S: np.ndarray,\n        test_S: np.ndarray,\n        z: np.ndarray,\n        outlier_tau: float,\n        outlier_likelihood: Any,\n    ) -&gt; float:\n        \"\"\"Estimate the matrix shrinkage parameter for outlier detection.\n\n        Args:\n            x (float): The input parameter.\n            Sigma (np.ndarray): The Sigma matrix.\n            modi_ld_S (np.ndarray): The modi_ld_S matrix.\n            test_S (np.ndarray): The test_S matrix.\n            z (np.ndarray): The z vector.\n            outlier_tau (float): The outlier_tau value.\n            outlier_likelihood (Any): The outlier_likelihood function.\n\n        Returns:\n            float: The estimated matrix shrinkage parameter.\n\n        Examples:\n        &gt;&gt;&gt; x = 0.5\n        &gt;&gt;&gt; Sigma = np.array([[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]])\n        &gt;&gt;&gt; modi_ld_S = np.array([[1, 0.5], [0.5, 1]])\n        &gt;&gt;&gt; test_S = np.array([1, 2])\n        &gt;&gt;&gt; z = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; outlier_tau = 1 / 0.05**2\n        &gt;&gt;&gt; outlier_likelihood = CARMA._outlier_ind_Normal_marginal_external\n        &gt;&gt;&gt; np.round(CARMA._ridge_fun(x, Sigma, modi_ld_S, test_S, z, outlier_tau, outlier_likelihood),decimals=5)\n        np.float64(6.01486)\n        \"\"\"\n        temp_Sigma = Sigma.copy()\n        temp_ld_S = x * modi_ld_S + (1 - x) * np.eye(len(modi_ld_S))\n        temp_Sigma[np.ix_(test_S, test_S)] = temp_ld_S\n        return -outlier_likelihood(\n            index_vec_input=test_S + 1,\n            Sigma=temp_Sigma,\n            z=z,\n            tau=outlier_tau,\n            p_S=len(test_S),\n        )\n\n    @staticmethod\n    def _prior_dist(t: str, lambda_val: float, p: int) -&gt; float:\n        \"\"\"Estimate the priors for the given configurations.\n\n        Args:\n            t (str): The input string for the given configuration.\n            lambda_val (float): The lambda value.\n            p (int): The number of SNPs.\n\n        Returns:\n            float: The estimated prior.\n\n        Examples:\n        &gt;&gt;&gt; t = \"1,2,3\"\n        &gt;&gt;&gt; lambda_val = 1\n        &gt;&gt;&gt; p = 4\n        &gt;&gt;&gt; np.round(CARMA._prior_dist(t, lambda_val, p),decimals=5)\n        np.float64(-3.17805)\n        \"\"\"\n        index_array = t.split(\",\")\n        dim_model = len(index_array)\n        if t == \"\":\n            dim_model = 0\n        return (\n            dim_model * np.log(lambda_val) + lgamma(p - dim_model + 1) - lgamma(p + 1)\n        )\n\n    @staticmethod\n    def _PIP_func(\n        likeli: pd.DataFrame, model_space: pd.DataFrame, p: int, num_causal: int\n    ) -&gt; np.ndarray:\n        \"\"\"Estimates the posterior inclusion probabilities (PIPs) for all SNPs.\n\n        Args:\n            likeli (pd.DataFrame): The marginal likelihoods.\n            model_space (pd.DataFrame): The corresponding model space.\n            p (int): The number of SNPs.\n            num_causal (int): The maximal number of causal SNPs.\n\n        Returns:\n            np.ndarray: The posterior inclusion probabilities (PIPs) for all SNPs.\n\n        Examples:\n        &gt;&gt;&gt; likeli = pd.DataFrame([10, 10, 5,11,0], columns=['likeli']).squeeze()\n        &gt;&gt;&gt; model_space = pd.DataFrame(['0', '1', '2','0,1',''], columns=['config']).squeeze()\n        &gt;&gt;&gt; p = 3\n        &gt;&gt;&gt; num_causal = 2\n        &gt;&gt;&gt; CARMA._PIP_func(likeli, model_space, p, num_causal)\n        array([0.7869271, 0.7869271, 0.001426 ])\n        \"\"\"\n        likeli = likeli.reset_index(drop=True)\n        model_space = model_space.reset_index(drop=True)\n\n        model_space_matrix = np.zeros((len(model_space), p), dtype=int)\n\n        for i in range(len(model_space)):\n            if model_space.iloc[i] != \"\":\n                ind = list(map(int, model_space.iloc[i].split(\",\")))\n                if len(ind) &gt; 0:\n                    model_space_matrix[i, ind] = 1\n\n        infi_index = np.where(np.isinf(likeli))[0]\n        if len(infi_index) != 0:\n            likeli = likeli.drop(infi_index).reset_index(drop=True)\n            model_space_matrix = np.delete(model_space_matrix, infi_index, axis=0)\n\n        na_index = np.where(np.isnan(likeli))[0]\n        if len(na_index) != 0:\n            likeli = likeli.drop(na_index).reset_index(drop=True)\n            model_space_matrix = np.delete(model_space_matrix, na_index, axis=0)\n\n        row_sums = np.sum(model_space_matrix, axis=1)\n        model_space_matrix = model_space_matrix[row_sums &lt;= num_causal]\n        likeli = likeli[row_sums &lt;= num_causal]\n\n        aa = likeli - max(likeli)\n        prob_sum = np.sum(np.exp(aa))\n\n        result_prob = np.zeros(p)\n        for i in range(p):\n            result_prob[i] = (\n                np.sum(np.exp(aa[model_space_matrix[:, i] == 1])) / prob_sum\n            )\n\n        return result_prob\n\n    @staticmethod\n    def _MCS_modified(  # noqa: C901\n        z: np.ndarray,\n        ld_matrix: np.ndarray,\n        Max_Model_Dim: int = 10_000,\n        lambda_val: float = 1,\n        num_causal: int = 10,\n        outlier_switch: bool = True,\n        input_conditional_S_list: list[int] | None = None,\n        tau: float = 1 / 0.05**2,\n        epsilon: float = 1e-3,\n        inner_all_iter: int = 10,\n        outlier_BF_index: float | None = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Modified Monte Carlo shotgun sampling (MCS) algorithm.\n\n        Args:\n            z (np.ndarray): Numeric vector representing z-scores.\n            ld_matrix (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n            Max_Model_Dim (int): Maximum allowed dimension for the causal models.\n            lambda_val (float): Regularization parameter controlling the strength of the L1 penalty.\n            num_causal (int): Maximal number of causal variants to be selected in the final model.\n            outlier_switch (bool): Whether to consider outlier detection in the analysis.\n            input_conditional_S_list (list[int] | None): The conditional set. Defaults to None.\n            tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix.\n            epsilon (float): Threshold for convergence in CARMA iterations.\n            inner_all_iter (int): The number of inner iterations in each CARMA iteration.\n            outlier_BF_index (float | None): Bayes Factor threshold for identifying outliers. Defaults to None.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the following results:\n                - B_list: A dataframe containing the marginal likelihoods and the corresponding model space.\n                - conditional_S_list: A list of outliers.\n\n        Examples:\n        &gt;&gt;&gt; z = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; ld_matrix = np.array([[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]])\n        &gt;&gt;&gt; Max_Model_Dim = 10_000\n        &gt;&gt;&gt; lambda_val = 1\n\n        &gt;&gt;&gt; num_causal = 10\n        &gt;&gt;&gt; outlier_switch = True\n        \"\"\"\n        p = len(z)\n        marginal_likelihood = CARMA._ind_Normal_fixed_sigma_marginal_external\n        tau_sample = tau\n        if outlier_switch:\n            outlier_likelihood = CARMA._outlier_ind_Normal_marginal_external\n            outlier_tau = tau\n\n        B = Max_Model_Dim\n        stored_bf = 0\n        Sigma = ld_matrix\n\n        S = []\n\n        null_model = \"\"\n        null_margin = CARMA._prior_dist(null_model, lambda_val=lambda_val, p=p)\n\n        B_list = pd.DataFrame({\"set_gamma_margin\": [null_margin], \"matrix_gamma\": [\"\"]})\n\n        if input_conditional_S_list is None:\n            conditional_S = []\n        else:\n            conditional_S = input_conditional_S_list\n            S = conditional_S\n\n        for _i in range(0, inner_all_iter):\n            for _j in range(0, 10):\n                set_gamma = CARMA._set_gamma_func(\n                    input_S=S, p=p, condition_index=conditional_S\n                )\n\n                if conditional_S is None:\n                    working_S = S\n                else:\n                    working_S = np.sort(np.setdiff1d(S, conditional_S)).astype(int)\n\n                set_gamma_margin: list[Any] = [None, None, None]\n                set_gamma_prior: list[Any] = [None, None, None]\n                matrix_gamma: list[Any] = [None, None, None]\n\n                for i in range(0, len(set_gamma)):\n                    if set_gamma[i] is not None:\n                        matrix_gamma[i] = CARMA._index_fun(set_gamma[i])\n                        p_S = set_gamma[i].shape[1]\n                        set_gamma_margin[i] = np.apply_along_axis(\n                            marginal_likelihood,\n                            1,\n                            set_gamma[i] + 1,\n                            Sigma=Sigma,\n                            z=z,\n                            tau=tau_sample,\n                            p_S=p_S,\n                        )\n                        set_gamma_prior[i] = np.array(\n                            [\n                                CARMA._prior_dist(model, lambda_val=lambda_val, p=p)\n                                for model in matrix_gamma[i]\n                            ]\n                        )\n                        set_gamma_margin[i] = set_gamma_prior[i] + set_gamma_margin[i]\n                    else:\n                        set_gamma_margin[i] = np.array(null_margin)\n                        set_gamma_prior[i] = 0\n                        matrix_gamma[i] = np.array(null_model)\n\n                columns = [\"set_gamma_margin\", \"matrix_gamma\"]\n                add_B = pd.DataFrame(columns=columns)\n\n                for i in range(len(set_gamma)):\n                    if isinstance(set_gamma_margin[i].tolist(), list):\n                        new_row = pd.DataFrame(\n                            {\n                                \"set_gamma_margin\": set_gamma_margin[i].tolist(),\n                                \"matrix_gamma\": matrix_gamma[i].tolist(),\n                            }\n                        )\n                        add_B = pd.concat([add_B, new_row], ignore_index=True)\n                    else:\n                        new_row = pd.DataFrame(\n                            {\n                                \"set_gamma_margin\": [set_gamma_margin[i].tolist()],\n                                \"matrix_gamma\": [matrix_gamma[i].tolist()],\n                            }\n                        )\n                        add_B = pd.concat([add_B, new_row], ignore_index=True)\n\n                # Add visited models into the storage space of models\n                B_list = pd.concat([B_list, add_B], ignore_index=True)\n                B_list = B_list.drop_duplicates(\n                    subset=\"matrix_gamma\", ignore_index=True\n                )\n                B_list = B_list.sort_values(\n                    by=\"set_gamma_margin\", ignore_index=True, ascending=False\n                )\n\n                if len(working_S) == 0:\n                    # Create a DataFrame set.star\n                    set_star = pd.DataFrame(\n                        {\n                            \"set_index\": [0, 1, 2],\n                            \"gamma_set_index\": [np.nan, np.nan, np.nan],\n                            \"margin\": [np.nan, np.nan, np.nan],\n                        }\n                    )\n\n                    # Assuming set.gamma.margin and current.log.margin are defined\n                    aa = set_gamma_margin[1]\n                    aa = aa - aa[np.argmax(aa)]\n\n                    min_half_len = min(len(aa), floor(p / 2))\n                    decr_ind = np.argsort(np.exp(aa))[::-1]\n                    decr_half_ind = decr_ind[:min_half_len]\n\n                    probs = np.exp(aa)[decr_half_ind]\n\n                    chosen_index = np.random.choice(\n                        decr_half_ind, 1, p=probs / np.sum(probs)\n                    )\n                    set_star.at[1, \"gamma_set_index\"] = chosen_index[0]\n                    set_star.at[1, \"margin\"] = set_gamma_margin[1][chosen_index[0]]\n\n                    S = set_gamma[1][chosen_index[0]].tolist()\n\n                else:\n                    set_star = pd.DataFrame(\n                        {\n                            \"set_index\": [0, 1, 2],\n                            \"gamma_set_index\": [np.nan, np.nan, np.nan],\n                            \"margin\": [np.nan, np.nan, np.nan],\n                        }\n                    )\n                    for i in range(0, 3):\n                        aa = set_gamma_margin[i]\n                        if np.size(aa) &gt; 1:\n                            aa = aa - aa[np.argmax(aa)]\n                            chosen_index = np.random.choice(\n                                range(0, np.size(set_gamma_margin[i])),\n                                1,\n                                p=np.exp(aa) / np.sum(np.exp(aa)),\n                            )\n                            set_star.at[i, \"gamma_set_index\"] = chosen_index\n                            set_star.at[i, \"margin\"] = set_gamma_margin[i][chosen_index]\n                        else:\n                            set_star.at[i, \"gamma_set_index\"] = 0\n                            set_star.at[i, \"margin\"] = set_gamma_margin[i]\n\n                    if outlier_switch:\n                        for i in range(1, len(set_gamma)):\n                            test_log_BF: float = 100\n                            while True:\n                                aa = set_gamma_margin[i]\n                                aa = aa - aa[np.argmax(aa)]\n                                chosen_index = np.random.choice(\n                                    range(0, np.size(set_gamma_margin[i])),\n                                    1,\n                                    p=np.exp(aa) / np.sum(np.exp(aa)),\n                                )\n                                set_star.at[i, \"gamma_set_index\"] = chosen_index\n                                set_star.at[i, \"margin\"] = set_gamma_margin[i][\n                                    chosen_index\n                                ]\n\n                                test_S = set_gamma[i][int(chosen_index), :]\n\n                                modi_Sigma = Sigma.copy()\n                                if np.size(test_S) &gt; 1:\n                                    modi_ld_S = modi_Sigma[test_S][:, test_S]\n\n                                    result = minimize_scalar(\n                                        CARMA._ridge_fun,\n                                        bounds=(0, 1),\n                                        args=(\n                                            Sigma,\n                                            modi_ld_S,\n                                            test_S,\n                                            z,\n                                            outlier_tau,\n                                            outlier_likelihood,\n                                        ),\n                                        method=\"bounded\",\n                                    )\n                                    modi_ld_S = result.x * modi_ld_S + (\n                                        1 - result.x\n                                    ) * np.eye(len(modi_ld_S))\n\n                                    modi_Sigma[np.ix_(test_S, test_S)] = modi_ld_S\n\n                                    test_log_BF = outlier_likelihood(\n                                        test_S + 1, Sigma, z, outlier_tau, len(test_S)\n                                    ) - outlier_likelihood(\n                                        test_S + 1,\n                                        modi_Sigma,\n                                        z,\n                                        outlier_tau,\n                                        len(test_S),\n                                    )\n                                    test_log_BF = -np.abs(test_log_BF)\n\n                                if np.exp(test_log_BF) &lt; outlier_BF_index:\n                                    set_gamma[i] = np.delete(\n                                        set_gamma[i],\n                                        int(set_star[\"gamma_set_index\"][i]),\n                                        axis=0,\n                                    )\n                                    set_gamma_margin[i] = np.delete(\n                                        set_gamma_margin[i],\n                                        int(set_star[\"gamma_set_index\"][i]),\n                                        axis=0,\n                                    )\n                                    conditional_S = np.concatenate(\n                                        [conditional_S, np.setdiff1d(test_S, working_S)]\n                                    )\n                                    conditional_S = (\n                                        np.unique(conditional_S).astype(int).tolist()\n                                    )\n                                else:\n                                    break\n\n                    if len(working_S) == num_causal:\n                        set_star = set_star.drop(1)\n                        aa = set_star[\"margin\"] - max(set_star[\"margin\"])\n                        sec_sample = np.random.choice(\n                            [0, 2], 1, p=np.exp(aa) / np.sum(np.exp(aa))\n                        )\n                        ind_sec = int(\n                            set_star[\"gamma_set_index\"][\n                                set_star[\"set_index\"] == int(sec_sample)\n                            ]\n                        )\n                        S = set_gamma[sec_sample[0]][ind_sec].tolist()\n                    else:\n                        aa = set_star[\"margin\"] - max(set_star[\"margin\"])\n                        sec_sample = np.random.choice(\n                            range(0, 3), 1, p=np.exp(aa) / np.sum(np.exp(aa))\n                        )\n                        if set_gamma[sec_sample[0]] is not None:\n                            S = set_gamma[sec_sample[0]][\n                                int(set_star[\"gamma_set_index\"][sec_sample[0]])\n                            ].tolist()\n                        else:\n                            sec_sample = np.random.choice(\n                                range(1, 3),\n                                1,\n                                p=np.exp(aa)[[1, 2]] / np.sum(np.exp(aa)[[1, 2]]),\n                            )\n                            S = set_gamma[sec_sample[0]][\n                                int(set_star[\"gamma_set_index\"][sec_sample[0]])\n                            ].tolist()\n\n                for item in conditional_S:\n                    if item not in S:\n                        S.append(item)\n            # END h_ind loop\n            #\n            if conditional_S is not None:\n                all_c_index = []\n                index_array = [s.split(\",\") for s in B_list[\"matrix_gamma\"]]\n                for tt in conditional_S:\n                    tt_str = str(tt)\n                    ind = [\n                        i for i, sublist in enumerate(index_array) if tt_str in sublist\n                    ]\n                    all_c_index.extend(ind)\n\n                all_c_index = list(set(all_c_index))\n\n                if len(all_c_index) &gt; 0:\n                    temp_B_list = B_list.copy()\n                    temp_B_list = B_list.drop(all_c_index)\n                else:\n                    temp_B_list = B_list.copy()\n            else:\n                temp_B_list = B_list.copy()\n\n            result_B_list = temp_B_list[: min(int(B), len(temp_B_list))]\n\n            rb1 = result_B_list[\"set_gamma_margin\"]\n\n            difference = abs(rb1[: (len(rb1) // 4)].mean() - stored_bf)\n\n            if difference &lt; epsilon:\n                break\n            else:\n                stored_bf = rb1[: (len(rb1) // 4)].mean()\n\n        out = {\"B_list\": result_B_list, \"conditional_S_list\": conditional_S}\n\n        return out\n</code></pre>"},{"location":"python_api/methods/carma/#gentropy.method.carma.CARMA.CARMA_spike_slab_noEM","title":"<code>CARMA_spike_slab_noEM(z: np.ndarray, ld: np.ndarray, lambda_val: float = 1, Max_Model_Dim: int = 200000, all_iter: int = 1, all_inner_iter: int = 10, epsilon_threshold: float = 1e-05, num_causal: int = 10, tau: float = 0.04, outlier_switch: bool = True, outlier_BF_index: float = 1 / 3.2) -&gt; dict[str, Any]</code>  <code>staticmethod</code>","text":"<p>Perform CARMA analysis using a Spike-and-Slab prior without Expectation-Maximization (EM).</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>ndarray</code> <p>Numeric vector representing z-scores.</p> required <code>ld</code> <code>ndarray</code> <p>Numeric matrix representing the linkage disequilibrium (LD) matrix.</p> required <code>lambda_val</code> <code>float</code> <p>Regularization parameter controlling the strength of the L1 penalty.</p> <code>1</code> <code>Max_Model_Dim</code> <code>int</code> <p>Maximum allowed dimension for the causal models.</p> <code>200000</code> <code>all_iter</code> <code>int</code> <p>The total number of iterations to run the CARMA analysis.</p> <code>1</code> <code>all_inner_iter</code> <code>int</code> <p>The number of inner iterations in each CARMA iteration.</p> <code>10</code> <code>epsilon_threshold</code> <code>float</code> <p>Threshold for convergence in CARMA iterations.</p> <code>1e-05</code> <code>num_causal</code> <code>int</code> <p>Maximal number of causal variants to be selected in the final model.</p> <code>10</code> <code>tau</code> <code>float</code> <p>Tuning parameter controlling the level of shrinkage of the LD matrix.</p> <code>0.04</code> <code>outlier_switch</code> <code>bool</code> <p>Whether to consider outlier detection in the analysis.</p> <code>True</code> <code>outlier_BF_index</code> <code>float</code> <p>Bayes Factor threshold for identifying outliers.</p> <code>1 / 3.2</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the following results: - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs. - B_list: A dataframe containing the marginal likelihoods and the corresponding model space. - Outliers: A list of outlier SNPs.</p> Source code in <code>src/gentropy/method/carma.py</code> <pre><code>@staticmethod\ndef CARMA_spike_slab_noEM(\n    z: np.ndarray,\n    ld: np.ndarray,\n    lambda_val: float = 1,\n    Max_Model_Dim: int = 200_000,\n    all_iter: int = 1,\n    all_inner_iter: int = 10,\n    epsilon_threshold: float = 1e-5,\n    num_causal: int = 10,\n    tau: float = 0.04,\n    outlier_switch: bool = True,\n    outlier_BF_index: float = 1 / 3.2,\n) -&gt; dict[str, Any]:\n    \"\"\"Perform CARMA analysis using a Spike-and-Slab prior without Expectation-Maximization (EM).\n\n    Args:\n        z (np.ndarray): Numeric vector representing z-scores.\n        ld (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n        lambda_val (float): Regularization parameter controlling the strength of the L1 penalty.\n        Max_Model_Dim (int): Maximum allowed dimension for the causal models.\n        all_iter (int): The total number of iterations to run the CARMA analysis.\n        all_inner_iter (int): The number of inner iterations in each CARMA iteration.\n        epsilon_threshold (float): Threshold for convergence in CARMA iterations.\n        num_causal (int): Maximal number of causal variants to be selected in the final model.\n        tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix.\n        outlier_switch (bool): Whether to consider outlier detection in the analysis.\n        outlier_BF_index (float): Bayes Factor threshold for identifying outliers.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the following results:\n            - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs.\n            - B_list: A dataframe containing the marginal likelihoods and the corresponding model space.\n            - Outliers: A list of outlier SNPs.\n    \"\"\"\n    p_snp = len(z)\n    epsilon_list = epsilon_threshold * p_snp\n    all_epsilon_threshold = epsilon_threshold * p_snp\n\n    # Zero step\n    all_C_list = CARMA._MCS_modified(\n        z=z,\n        ld_matrix=ld,\n        epsilon=epsilon_list,\n        Max_Model_Dim=Max_Model_Dim,\n        lambda_val=lambda_val,\n        outlier_switch=outlier_switch,\n        tau=tau,\n        num_causal=num_causal,\n        inner_all_iter=all_inner_iter,\n        outlier_BF_index=outlier_BF_index,\n    )\n\n    # Main steps\n    for _ in range(0, all_iter):\n        ac1 = all_C_list[\"B_list\"][\"set_gamma_margin\"]\n        previous_result = np.mean(ac1[0 : round(len(ac1) / 4)])\n\n        all_C_list = CARMA._MCS_modified(\n            z=z,\n            ld_matrix=ld,\n            input_conditional_S_list=all_C_list[\"conditional_S_list\"],\n            Max_Model_Dim=Max_Model_Dim,\n            num_causal=num_causal,\n            epsilon=epsilon_list,\n            outlier_switch=outlier_switch,\n            tau=tau,\n            lambda_val=lambda_val,\n            inner_all_iter=all_inner_iter,\n            outlier_BF_index=outlier_BF_index,\n        )\n\n        ac1 = all_C_list[\"B_list\"][\"set_gamma_margin\"]\n        difference = np.abs(previous_result - np.mean(ac1[0 : round(len(ac1) / 4)]))\n        if difference &lt; all_epsilon_threshold:\n            break\n\n    # Calculate PIPs and Credible Set\n    pip = CARMA._PIP_func(\n        likeli=all_C_list[\"B_list\"][\"set_gamma_margin\"],\n        model_space=all_C_list[\"B_list\"][\"matrix_gamma\"],\n        p=p_snp,\n        num_causal=num_causal,\n    )\n\n    results_list = {\n        \"PIPs\": pip,\n        \"B_list\": all_C_list[\"B_list\"],\n        \"Outliers\": all_C_list[\"conditional_S_list\"],\n    }\n\n    return results_list\n</code></pre>"},{"location":"python_api/methods/carma/#gentropy.method.carma.CARMA.time_limited_CARMA_spike_slab_noEM","title":"<code>time_limited_CARMA_spike_slab_noEM(z: np.ndarray, ld: np.ndarray, sec_threshold: float = 600, tau: float = 0.04) -&gt; dict[str, Any]</code>  <code>staticmethod</code>","text":"<p>The wrapper for the CARMA_spike_slab_noEM function that runs the function in a separate thread and terminates it if it takes too long.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>ndarray</code> <p>Numeric vector representing z-scores.</p> required <code>ld</code> <code>ndarray</code> <p>Numeric matrix representing the linkage disequilibrium (LD) matrix.</p> required <code>sec_threshold</code> <code>float</code> <p>The time threshold in seconds.</p> <code>600</code> <code>tau</code> <code>float</code> <p>Tuning parameter controlling the level of shrinkage of the LD matrix</p> <code>0.04</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the following results: - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs or None. - B_list: A dataframe containing the marginal likelihoods and the corresponding model space or None. - Outliers: A list of outlier SNPs or None.</p> Source code in <code>src/gentropy/method/carma.py</code> <pre><code>@staticmethod\ndef time_limited_CARMA_spike_slab_noEM(\n    z: np.ndarray,\n    ld: np.ndarray,\n    sec_threshold: float = 600,\n    tau: float = 0.04,\n) -&gt; dict[str, Any]:\n    \"\"\"The wrapper for the CARMA_spike_slab_noEM function that runs the function in a separate thread and terminates it if it takes too long.\n\n    Args:\n        z (np.ndarray): Numeric vector representing z-scores.\n        ld (np.ndarray): Numeric matrix representing the linkage disequilibrium (LD) matrix.\n        sec_threshold (float): The time threshold in seconds.\n        tau (float): Tuning parameter controlling the level of shrinkage of the LD matrix\n\n    Returns:\n        dict[str, Any]: A dictionary containing the following results:\n            - PIPs: A numeric vector of posterior inclusion probabilities (PIPs) for all SNPs or None.\n            - B_list: A dataframe containing the marginal likelihoods and the corresponding model space or None.\n            - Outliers: A list of outlier SNPs or None.\n    \"\"\"\n    # Ignore pandas future warnings\n    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n    try:\n        # Execute CARMA.CARMA_spike_slab_noEM with a timeout\n        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n            future = executor.submit(\n                CARMA.CARMA_spike_slab_noEM, z=z, ld=ld, tau=tau\n            )\n            result = future.result(timeout=sec_threshold)\n    except concurrent.futures.TimeoutError:\n        # If execution exceeds the timeout, return None\n        result = {\"PIPs\": None, \"B_list\": None, \"Outliers\": None}\n\n    return result\n</code></pre>"},{"location":"python_api/methods/clumping/","title":"Clumping","text":"<p>Clumping is a commonly used post-processing method that allows for the identification of independent association signals from GWAS summary statistics and curated associations. This process is critical because of the complex linkage disequilibrium (LD) structure in human populations, which can result in multiple statistically significant associations within the same genomic region. Clumping methods help reduce redundancy in GWAS results and ensure that each reported association represents an independent signal.</p> <p>We have implemented two clumping methods:</p> <ol> <li>Distance-based clumping: Uses genomic window to clump the significant SNPs into one hit.</li> <li>LD-based clumping: Uses genomic window and LD to clump the significant SNPs into one hit.</li> <li>Locus-breaker clumping: Applies a distance cutoff between baseline significant SNPs. Returns the start and end position of the locus as well.</li> </ol> <p>The algorithmic logic is similar to classic clumping approaches from PLINK (Reference: PLINK Clump Documentation). See details below:</p>"},{"location":"python_api/methods/clumping/#distance-based-clumping","title":"Distance-based clumping","text":""},{"location":"python_api/methods/clumping/#gentropy.method.window_based_clumping.WindowBasedClumping","title":"<code>gentropy.method.window_based_clumping.WindowBasedClumping</code>","text":"<p>Get semi-lead snps from summary statistics using a window based function.</p> Source code in <code>src/gentropy/method/window_based_clumping.py</code> <pre><code>class WindowBasedClumping:\n    \"\"\"Get semi-lead snps from summary statistics using a window based function.\"\"\"\n\n    @staticmethod\n    def _cluster_peaks(\n        study: Column, chromosome: Column, position: Column, window_length: int\n    ) -&gt; Column:\n        \"\"\"Cluster GWAS significant variants, were clusters are separated by a defined distance.\n\n        !! Important to note that the length of the clusters can be arbitrarily big.\n\n        Args:\n            study (Column): study identifier\n            chromosome (Column): chromosome identifier\n            position (Column): position of the variant\n            window_length (int): window length in basepair\n\n        Returns:\n            Column: containing cluster identifier\n\n        Examples:\n            &gt;&gt;&gt; data = [\n            ...     # Cluster 1:\n            ...     ('s1', 'chr1', 2),\n            ...     ('s1', 'chr1', 4),\n            ...     ('s1', 'chr1', 12),\n            ...     # Cluster 2 - Same chromosome:\n            ...     ('s1', 'chr1', 31),\n            ...     ('s1', 'chr1', 38),\n            ...     ('s1', 'chr1', 42),\n            ...     # Cluster 3 - New chromosome:\n            ...     ('s1', 'chr2', 41),\n            ...     ('s1', 'chr2', 44),\n            ...     ('s1', 'chr2', 50),\n            ...     # Cluster 4 - other study:\n            ...     ('s2', 'chr2', 55),\n            ...     ('s2', 'chr2', 62),\n            ...     ('s2', 'chr2', 70),\n            ... ]\n            &gt;&gt;&gt; window_length = 10\n            &gt;&gt;&gt; (\n            ...     spark.createDataFrame(data, ['studyId', 'chromosome', 'position'])\n            ...     .withColumn(\"cluster_id\",\n            ...         WindowBasedClumping._cluster_peaks(\n            ...             f.col('studyId'),\n            ...             f.col('chromosome'),\n            ...             f.col('position'),\n            ...             window_length\n            ...         )\n            ...     ).show()\n            ... )\n            +-------+----------+--------+----------+\n            |studyId|chromosome|position|cluster_id|\n            +-------+----------+--------+----------+\n            |     s1|      chr1|       2| s1_chr1_2|\n            |     s1|      chr1|       4| s1_chr1_2|\n            |     s1|      chr1|      12| s1_chr1_2|\n            |     s1|      chr1|      31|s1_chr1_31|\n            |     s1|      chr1|      38|s1_chr1_31|\n            |     s1|      chr1|      42|s1_chr1_31|\n            |     s1|      chr2|      41|s1_chr2_41|\n            |     s1|      chr2|      44|s1_chr2_41|\n            |     s1|      chr2|      50|s1_chr2_41|\n            |     s2|      chr2|      55|s2_chr2_55|\n            |     s2|      chr2|      62|s2_chr2_55|\n            |     s2|      chr2|      70|s2_chr2_55|\n            +-------+----------+--------+----------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        # By adding previous position, the cluster boundary can be identified:\n        previous_position = f.lag(position).over(\n            Window.partitionBy(study, chromosome).orderBy(position)\n        )\n        # We consider a cluster boudary if subsequent snps are further than the defined window:\n        cluster_id = f.when(\n            (previous_position.isNull())\n            | (position - previous_position &gt; window_length),\n            f.concat_ws(\"_\", study, chromosome, position),\n        )\n        # The cluster identifier is propagated across every variant of the cluster:\n        return f.when(\n            cluster_id.isNull(),\n            f.last(cluster_id, ignorenulls=True).over(\n                Window.partitionBy(study, chromosome)\n                .orderBy(position)\n                .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n            ),\n        ).otherwise(cluster_id)\n\n    @staticmethod\n    def _prune_peak(position: NDArray[np.float64], window_size: int) -&gt; DenseVector:\n        \"\"\"Establish lead snps based on their positions listed by p-value.\n\n        The function `find_peak` assigns lead SNPs based on their positions listed by p-value within a specified window size.\n\n        Args:\n            position (NDArray[np.float64]): positions of the SNPs sorted by p-value.\n            window_size (int): the distance in bp within which associations are clumped together around the lead snp.\n\n        Returns:\n            DenseVector: binary vector where 1 indicates a lead SNP and 0 indicates a non-lead SNP.\n\n        Examples:\n            &gt;&gt;&gt; from pyspark.ml import functions as fml\n            &gt;&gt;&gt; from pyspark.ml.linalg import DenseVector\n            &gt;&gt;&gt; WindowBasedClumping._prune_peak(np.array((3, 9, 8, 4, 6)), 2)\n            DenseVector([1.0, 1.0, 0.0, 0.0, 1.0])\n\n        \"\"\"\n        # Initializing the lead list with zeroes:\n        is_lead = np.zeros(len(position))\n\n        # List containing indices of leads:\n        lead_indices: list[int] = []\n\n        # Looping through all positions:\n        for index in range(len(position)):\n            # Looping through leads to find out if they are within a window:\n            for lead_index in lead_indices:\n                # If any of the leads within the window:\n                if abs(position[lead_index] - position[index]) &lt; window_size:\n                    # Skipping further checks:\n                    break\n            else:\n                # None of the leads were within the window:\n                lead_indices.append(index)\n                is_lead[index] = 1\n\n        return DenseVector(is_lead)\n\n    @staticmethod\n    def clump(\n        unclumped_associations: SummaryStatistics | StudyLocus,\n        distance: int = WindowBasedClumpingStepConfig().distance,\n    ) -&gt; StudyLocus:\n        \"\"\"Clump single point associations from summary statistics or study locus dataset based on window.\n\n        Args:\n            unclumped_associations (SummaryStatistics | StudyLocus): Input dataset to be used for clumping. Assumes that the input dataset is already filtered for significant variants.\n            distance (int): Distance in base pairs to be used for clumping. Defaults to 500_000.\n\n        Returns:\n            StudyLocus: clumped associations, where the clumped variants are flagged.\n        \"\"\"\n        # Quality check expression that flags variants that are not considered lead variant:\n        qc_check = f.col(\"semiIndices\")[f.col(\"pvRank\") - 1] &lt;= 0\n\n        # The quality control expression will depend on the input dataset, as the column might be already present:\n        qc_expression = (\n            # When the column is already present and the condition is met, the value is appended to the array, otherwise keep as is:\n            f.when(\n                qc_check,\n                f.array_union(\n                    f.col(\"qualityControls\"),\n                    f.array(f.lit(StudyLocusQualityCheck.WINDOW_CLUMPED.value)),\n                ),\n            ).otherwise(f.col(\"qualityControls\"))\n            if \"qualityControls\" in unclumped_associations.df.columns\n            # If column is not there yet, initialize it with the flag value, or an empty array:\n            else f.when(\n                qc_check, f.array(f.lit(StudyLocusQualityCheck.WINDOW_CLUMPED.value))\n            ).otherwise(f.array().cast(t.ArrayType(t.StringType())))\n        )\n\n        # Create window for locus clusters\n        # - variants where the distance between subsequent variants is below the defined threshold.\n        # - Variants are sorted by descending significance\n        cluster_window = Window.partitionBy(\n            \"studyId\", \"chromosome\", \"cluster_id\"\n        ).orderBy(f.col(\"pValueExponent\").asc(), f.col(\"pValueMantissa\").asc())\n\n        return StudyLocus(\n            _df=(\n                unclumped_associations.df\n                # Clustering variants for efficient windowing (complexity reduction):\n                .withColumn(\n                    \"cluster_id\",\n                    WindowBasedClumping._cluster_peaks(\n                        f.col(\"studyId\"),\n                        f.col(\"chromosome\"),\n                        f.col(\"position\"),\n                        distance,\n                    ),\n                )\n                # Within each cluster variants are ranked by significance:\n                .withColumn(\"pvRank\", f.row_number().over(cluster_window))\n                # Collect positions in cluster for the most significant variant (complexity reduction):\n                .withColumn(\n                    \"collectedPositions\",\n                    f.when(\n                        f.col(\"pvRank\") == 1,\n                        f.collect_list(f.col(\"position\")).over(\n                            cluster_window.rowsBetween(\n                                Window.currentRow, Window.unboundedFollowing\n                            )\n                        ),\n                    ).otherwise(f.array()),\n                )\n                # Collect top loci per cluster:\n                .withColumn(\n                    \"semiIndices\",\n                    f.when(\n                        f.size(f.col(\"collectedPositions\")) &gt; 0,\n                        fml.vector_to_array(\n                            f.udf(WindowBasedClumping._prune_peak, VectorUDT())(\n                                fml.array_to_vector(f.col(\"collectedPositions\")),\n                                f.lit(distance),\n                            )\n                        ),\n                    ),\n                )\n                # Propagating the result of the above calculation for all rows:\n                .withColumn(\n                    \"semiIndices\",\n                    f.when(\n                        f.col(\"semiIndices\").isNull(),\n                        f.first(f.col(\"semiIndices\"), ignorenulls=True).over(\n                            cluster_window\n                        ),\n                    ).otherwise(f.col(\"semiIndices\")),\n                )\n                # Adding study-locus id:\n                .withColumn(\n                    \"studyLocusId\",\n                    StudyLocus.assign_study_locus_id(\n                        [\"studyId\", \"variantId\"]\n                    ),\n                )\n                # Initialize QC column as array of strings:\n                .withColumn(\"qualityControls\", qc_expression)\n                .drop(\"pvRank\", \"collectedPositions\", \"semiIndices\", \"cluster_id\")\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/clumping/#gentropy.method.window_based_clumping.WindowBasedClumping.clump","title":"<code>clump(unclumped_associations: SummaryStatistics | StudyLocus, distance: int = WindowBasedClumpingStepConfig().distance) -&gt; StudyLocus</code>  <code>staticmethod</code>","text":"<p>Clump single point associations from summary statistics or study locus dataset based on window.</p> <p>Parameters:</p> Name Type Description Default <code>unclumped_associations</code> <code>SummaryStatistics | StudyLocus</code> <p>Input dataset to be used for clumping. Assumes that the input dataset is already filtered for significant variants.</p> required <code>distance</code> <code>int</code> <p>Distance in base pairs to be used for clumping. Defaults to 500_000.</p> <code>distance</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>clumped associations, where the clumped variants are flagged.</p> Source code in <code>src/gentropy/method/window_based_clumping.py</code> <pre><code>@staticmethod\ndef clump(\n    unclumped_associations: SummaryStatistics | StudyLocus,\n    distance: int = WindowBasedClumpingStepConfig().distance,\n) -&gt; StudyLocus:\n    \"\"\"Clump single point associations from summary statistics or study locus dataset based on window.\n\n    Args:\n        unclumped_associations (SummaryStatistics | StudyLocus): Input dataset to be used for clumping. Assumes that the input dataset is already filtered for significant variants.\n        distance (int): Distance in base pairs to be used for clumping. Defaults to 500_000.\n\n    Returns:\n        StudyLocus: clumped associations, where the clumped variants are flagged.\n    \"\"\"\n    # Quality check expression that flags variants that are not considered lead variant:\n    qc_check = f.col(\"semiIndices\")[f.col(\"pvRank\") - 1] &lt;= 0\n\n    # The quality control expression will depend on the input dataset, as the column might be already present:\n    qc_expression = (\n        # When the column is already present and the condition is met, the value is appended to the array, otherwise keep as is:\n        f.when(\n            qc_check,\n            f.array_union(\n                f.col(\"qualityControls\"),\n                f.array(f.lit(StudyLocusQualityCheck.WINDOW_CLUMPED.value)),\n            ),\n        ).otherwise(f.col(\"qualityControls\"))\n        if \"qualityControls\" in unclumped_associations.df.columns\n        # If column is not there yet, initialize it with the flag value, or an empty array:\n        else f.when(\n            qc_check, f.array(f.lit(StudyLocusQualityCheck.WINDOW_CLUMPED.value))\n        ).otherwise(f.array().cast(t.ArrayType(t.StringType())))\n    )\n\n    # Create window for locus clusters\n    # - variants where the distance between subsequent variants is below the defined threshold.\n    # - Variants are sorted by descending significance\n    cluster_window = Window.partitionBy(\n        \"studyId\", \"chromosome\", \"cluster_id\"\n    ).orderBy(f.col(\"pValueExponent\").asc(), f.col(\"pValueMantissa\").asc())\n\n    return StudyLocus(\n        _df=(\n            unclumped_associations.df\n            # Clustering variants for efficient windowing (complexity reduction):\n            .withColumn(\n                \"cluster_id\",\n                WindowBasedClumping._cluster_peaks(\n                    f.col(\"studyId\"),\n                    f.col(\"chromosome\"),\n                    f.col(\"position\"),\n                    distance,\n                ),\n            )\n            # Within each cluster variants are ranked by significance:\n            .withColumn(\"pvRank\", f.row_number().over(cluster_window))\n            # Collect positions in cluster for the most significant variant (complexity reduction):\n            .withColumn(\n                \"collectedPositions\",\n                f.when(\n                    f.col(\"pvRank\") == 1,\n                    f.collect_list(f.col(\"position\")).over(\n                        cluster_window.rowsBetween(\n                            Window.currentRow, Window.unboundedFollowing\n                        )\n                    ),\n                ).otherwise(f.array()),\n            )\n            # Collect top loci per cluster:\n            .withColumn(\n                \"semiIndices\",\n                f.when(\n                    f.size(f.col(\"collectedPositions\")) &gt; 0,\n                    fml.vector_to_array(\n                        f.udf(WindowBasedClumping._prune_peak, VectorUDT())(\n                            fml.array_to_vector(f.col(\"collectedPositions\")),\n                            f.lit(distance),\n                        )\n                    ),\n                ),\n            )\n            # Propagating the result of the above calculation for all rows:\n            .withColumn(\n                \"semiIndices\",\n                f.when(\n                    f.col(\"semiIndices\").isNull(),\n                    f.first(f.col(\"semiIndices\"), ignorenulls=True).over(\n                        cluster_window\n                    ),\n                ).otherwise(f.col(\"semiIndices\")),\n            )\n            # Adding study-locus id:\n            .withColumn(\n                \"studyLocusId\",\n                StudyLocus.assign_study_locus_id(\n                    [\"studyId\", \"variantId\"]\n                ),\n            )\n            # Initialize QC column as array of strings:\n            .withColumn(\"qualityControls\", qc_expression)\n            .drop(\"pvRank\", \"collectedPositions\", \"semiIndices\", \"cluster_id\")\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/clumping/#ld-based-clumping","title":"LD-based clumping:","text":""},{"location":"python_api/methods/clumping/#gentropy.method.clump.LDclumping","title":"<code>gentropy.method.clump.LDclumping</code>","text":"<p>LD clumping reports the most significant genetic associations in a region in terms of a smaller number of \u201cclumps\u201d of genetically linked SNPs.</p> Source code in <code>src/gentropy/method/clump.py</code> <pre><code>class LDclumping:\n    \"\"\"LD clumping reports the most significant genetic associations in a region in terms of a smaller number of \u201cclumps\u201d of genetically linked SNPs.\"\"\"\n\n    @staticmethod\n    def _is_lead_linked(\n        study_id: Column,\n        chromosome: Column,\n        variant_id: Column,\n        p_value_exponent: Column,\n        p_value_mantissa: Column,\n        ld_set: Column,\n    ) -&gt; Column:\n        \"\"\"Evaluates whether a lead variant is linked to a tag (with lowest p-value) in the same studyLocus dataset.\n\n        Args:\n            study_id (Column): studyId\n            chromosome (Column): chromosome\n            variant_id (Column): Lead variant id\n            p_value_exponent (Column): p-value exponent\n            p_value_mantissa (Column): p-value mantissa\n            ld_set (Column): Array of variants in LD with the lead variant\n\n        Returns:\n            Column: Boolean in which True indicates that the lead is linked to another tag in the same dataset.\n        \"\"\"\n        # Partitoning data by study and chromosome - this is the scope for looking for linked loci.\n        # Within the partition, we order the data by increasing p-value, and we collect the more significant lead variants in the window.\n        windowspec = (\n            Window.partitionBy(study_id, chromosome)\n            .orderBy(p_value_exponent.asc(), p_value_mantissa.asc())\n            .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n        )\n        more_significant_leads = f.collect_set(variant_id).over(windowspec)\n\n        # Collect all variants from the ld_set + adding the lead variant to the list to make sure that the lead is always in the list.\n        tags_in_studylocus = f.array_distinct(\n            f.array_union(\n                f.array(variant_id),\n                f.transform(ld_set, lambda x: x.getField(\"tagVariantId\")),\n            )\n        )\n\n        # If more than one tags of the ld_set can be found in the list of the more significant leads, the lead is linked.\n        # Study loci without variantId is considered as not linked.\n        # Also leads that were not found in the LD index is also considered as not linked.\n        return f.when(\n            variant_id.isNotNull(),\n            f.size(f.array_intersect(more_significant_leads, tags_in_studylocus)) &gt; 1,\n        ).otherwise(f.lit(False))\n\n    @classmethod\n    def clump(cls: type[LDclumping], associations: StudyLocus) -&gt; StudyLocus:\n        \"\"\"Perform clumping on studyLocus dataset.\n\n        Args:\n            associations (StudyLocus): StudyLocus dataset\n\n        Returns:\n            StudyLocus: including flag and removing locus information for LD clumped loci.\n        \"\"\"\n        return associations.clump()\n</code></pre>"},{"location":"python_api/methods/clumping/#gentropy.method.clump.LDclumping.clump","title":"<code>clump(associations: StudyLocus) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Perform clumping on studyLocus dataset.</p> <p>Parameters:</p> Name Type Description Default <code>associations</code> <code>StudyLocus</code> <p>StudyLocus dataset</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>including flag and removing locus information for LD clumped loci.</p> Source code in <code>src/gentropy/method/clump.py</code> <pre><code>@classmethod\ndef clump(cls: type[LDclumping], associations: StudyLocus) -&gt; StudyLocus:\n    \"\"\"Perform clumping on studyLocus dataset.\n\n    Args:\n        associations (StudyLocus): StudyLocus dataset\n\n    Returns:\n        StudyLocus: including flag and removing locus information for LD clumped loci.\n    \"\"\"\n    return associations.clump()\n</code></pre>"},{"location":"python_api/methods/clumping/#locus-breaker-clumping","title":"Locus-breaker clumping","text":""},{"location":"python_api/methods/clumping/#gentropy.method.locus_breaker_clumping.LocusBreakerClumping","title":"<code>gentropy.method.locus_breaker_clumping.LocusBreakerClumping</code>","text":"<p>Locus-breaker clumping method.</p> Source code in <code>src/gentropy/method/locus_breaker_clumping.py</code> <pre><code>class LocusBreakerClumping:\n    \"\"\"Locus-breaker clumping method.\"\"\"\n\n    @staticmethod\n    def locus_breaker(\n        summary_statistics: SummaryStatistics,\n        baseline_pvalue_cutoff: float,\n        distance_cutoff: int,\n        pvalue_cutoff: float,\n        flanking_distance: int,\n    ) -&gt; StudyLocus:\n        \"\"\"Identify GWAS associated loci based on the provided p-value and distance cutoff.\n\n        - The GWAS associated loci identified by this method have a varying width, and are separated by a distance greater than the provided distance cutoff.\n        - The distance is only calculted between single point associations that reach the baseline p-value cutoff.\n        - As the width of the selected genomic region dynamically depends on the loci, the resulting StudyLocus object will contain the locus start and end position.\n        - To ensure completeness, the locus is extended by a flanking distance in both ends.\n\n        Args:\n            summary_statistics (SummaryStatistics): Input summary statistics dataset.\n            baseline_pvalue_cutoff (float): baseline significance we consider for the locus.\n            distance_cutoff (int): minimum distance that separates two loci.\n            pvalue_cutoff (float): the minimum significance the locus should have.\n            flanking_distance (int): the distance to extend the locus in both directions.\n\n        Returns:\n            StudyLocus: clumped study loci with locus start and end positions + lead variant from the locus.\n        \"\"\"\n        # Extract columns from the summary statistics:\n        columns_sumstats_columns = summary_statistics.df.columns\n        # Convert pvalue_cutoff to neglog scale:\n        neglog_pv_cutoff = -np.log10(pvalue_cutoff)\n\n        # First window to calculate the distance between consecutive positions:\n        w1 = Window.partitionBy(\"studyId\", \"chromosome\").orderBy(\"position\")\n\n        # Second window to calculate the locus start and end:\n        w2 = (\n            Window.partitionBy(\"studyId\", \"chromosome\", \"locusStart\")\n            .orderBy(\"position\")\n            .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n        )\n\n        # Third window to rank the variants within the locus based on neglog p-value to find top loci:\n        w3 = Window.partitionBy(\n            \"studyId\", \"chromosome\", \"locusStart\", \"locusEnd\"\n        ).orderBy(f.col(\"negLogPValue\").desc())\n\n        return StudyLocus(\n            _df=(\n                # Applying the baseline p-value cutoff:\n                summary_statistics.pvalue_filter(baseline_pvalue_cutoff)\n                # Calculating the neglog p-value for easier sorting:\n                .df.withColumn(\n                    \"negLogPValue\",\n                    neglogpval_from_pvalue(\n                        f.col(\"pValueMantissa\"), f.col(\"pValueExponent\")\n                    ),\n                )\n                # Calculating the distance between consecutive positions, then identifying the locus start and end:\n                .withColumn(\"next_position\", f.lag(f.col(\"position\")).over(w1))\n                .withColumn(\"distance\", f.col(\"position\") - f.col(\"next_position\"))\n                .withColumn(\n                    \"locusStart\",\n                    f.when(\n                        (f.col(\"distance\") &gt; distance_cutoff)\n                        | f.col(\"distance\").isNull(),\n                        f.col(\"position\"),\n                    ),\n                )\n                .withColumn(\n                    \"locusStart\",\n                    f.when(\n                        f.last(f.col(\"locusStart\") - flanking_distance, True).over(\n                            w1.rowsBetween(-sys.maxsize, 0)\n                        )\n                        &gt; 0,\n                        f.last(f.col(\"locusStart\") - flanking_distance, True).over(\n                            w1.rowsBetween(-sys.maxsize, 0)\n                        ),\n                    ).otherwise(f.lit(0)),\n                )\n                .withColumn(\n                    \"locusEnd\", f.max(f.col(\"position\") + flanking_distance).over(w2)\n                )\n                .withColumn(\"rank\", f.rank().over(w3))\n                .filter(\n                    (f.col(\"rank\") == 1) &amp; (f.col(\"negLogPValue\") &gt; neglog_pv_cutoff)\n                )\n                .select(\n                    *columns_sumstats_columns,\n                    # To make sure that the type of locusStart and locusEnd follows schema of StudyLocus:\n                    f.col(\"locusStart\").cast(t.IntegerType()).alias(\"locusStart\"),\n                    f.col(\"locusEnd\").cast(t.IntegerType()).alias(\"locusEnd\"),\n                    f.lit(None)\n                    .cast(t.ArrayType(t.StringType()))\n                    .alias(\"qualityControls\"),\n                    StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n                )\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n\n    @staticmethod\n    def process_locus_breaker_output(\n        lbc: StudyLocus,\n        wbc: StudyLocus,\n        large_loci_size: int,\n    ) -&gt; StudyLocus:\n        \"\"\"Process the locus breaker method result, and run window-based clumping on large loci.\n\n        Args:\n            lbc (StudyLocus): StudyLocus object from locus-breaker clumping.\n            wbc (StudyLocus): StudyLocus object from window-based clumping.\n            large_loci_size (int): the size to define large loci which should be broken with wbc.\n\n        Returns:\n            StudyLocus: clumped study loci with large loci broken by window-based clumping.\n        \"\"\"\n        large_loci_size = int(large_loci_size)\n        small_loci = lbc.filter(\n            (f.col(\"locusEnd\") - f.col(\"locusStart\")) &lt;= large_loci_size\n        )\n        large_loci = lbc.filter(\n            (f.col(\"locusEnd\") - f.col(\"locusStart\")) &gt; large_loci_size\n        )\n        large_loci_wbc = StudyLocus(\n            wbc.df.alias(\"wbc\")\n            .join(\n                large_loci.df.alias(\"ll\"),\n                (f.col(\"wbc.studyId\") == f.col(\"ll.studyId\"))\n                &amp; (f.col(\"wbc.chromosome\") == f.col(\"ll.chromosome\"))\n                &amp; (\n                    f.col(\"wbc.position\").between(\n                        f.col(\"ll.locusStart\"), f.col(\"ll.locusEnd\")\n                    )\n                ),\n                \"semi\",\n            )\n            .withColumns(\n                {\n                    \"locusStart\": f.col(\"position\") - large_loci_size // 2,\n                    \"locusEnd\": f.col(\"position\") + large_loci_size // 2,\n                }\n            ),\n            StudyLocus.get_schema(),\n        )\n        return StudyLocus(\n            large_loci_wbc.df.unionByName(small_loci.df),\n            StudyLocus.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/clumping/#gentropy.method.locus_breaker_clumping.LocusBreakerClumping.locus_breaker","title":"<code>locus_breaker(summary_statistics: SummaryStatistics, baseline_pvalue_cutoff: float, distance_cutoff: int, pvalue_cutoff: float, flanking_distance: int) -&gt; StudyLocus</code>  <code>staticmethod</code>","text":"<p>Identify GWAS associated loci based on the provided p-value and distance cutoff.</p> <ul> <li>The GWAS associated loci identified by this method have a varying width, and are separated by a distance greater than the provided distance cutoff.</li> <li>The distance is only calculted between single point associations that reach the baseline p-value cutoff.</li> <li>As the width of the selected genomic region dynamically depends on the loci, the resulting StudyLocus object will contain the locus start and end position.</li> <li>To ensure completeness, the locus is extended by a flanking distance in both ends.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>summary_statistics</code> <code>SummaryStatistics</code> <p>Input summary statistics dataset.</p> required <code>baseline_pvalue_cutoff</code> <code>float</code> <p>baseline significance we consider for the locus.</p> required <code>distance_cutoff</code> <code>int</code> <p>minimum distance that separates two loci.</p> required <code>pvalue_cutoff</code> <code>float</code> <p>the minimum significance the locus should have.</p> required <code>flanking_distance</code> <code>int</code> <p>the distance to extend the locus in both directions.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>clumped study loci with locus start and end positions + lead variant from the locus.</p> Source code in <code>src/gentropy/method/locus_breaker_clumping.py</code> <pre><code>@staticmethod\ndef locus_breaker(\n    summary_statistics: SummaryStatistics,\n    baseline_pvalue_cutoff: float,\n    distance_cutoff: int,\n    pvalue_cutoff: float,\n    flanking_distance: int,\n) -&gt; StudyLocus:\n    \"\"\"Identify GWAS associated loci based on the provided p-value and distance cutoff.\n\n    - The GWAS associated loci identified by this method have a varying width, and are separated by a distance greater than the provided distance cutoff.\n    - The distance is only calculted between single point associations that reach the baseline p-value cutoff.\n    - As the width of the selected genomic region dynamically depends on the loci, the resulting StudyLocus object will contain the locus start and end position.\n    - To ensure completeness, the locus is extended by a flanking distance in both ends.\n\n    Args:\n        summary_statistics (SummaryStatistics): Input summary statistics dataset.\n        baseline_pvalue_cutoff (float): baseline significance we consider for the locus.\n        distance_cutoff (int): minimum distance that separates two loci.\n        pvalue_cutoff (float): the minimum significance the locus should have.\n        flanking_distance (int): the distance to extend the locus in both directions.\n\n    Returns:\n        StudyLocus: clumped study loci with locus start and end positions + lead variant from the locus.\n    \"\"\"\n    # Extract columns from the summary statistics:\n    columns_sumstats_columns = summary_statistics.df.columns\n    # Convert pvalue_cutoff to neglog scale:\n    neglog_pv_cutoff = -np.log10(pvalue_cutoff)\n\n    # First window to calculate the distance between consecutive positions:\n    w1 = Window.partitionBy(\"studyId\", \"chromosome\").orderBy(\"position\")\n\n    # Second window to calculate the locus start and end:\n    w2 = (\n        Window.partitionBy(\"studyId\", \"chromosome\", \"locusStart\")\n        .orderBy(\"position\")\n        .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n    )\n\n    # Third window to rank the variants within the locus based on neglog p-value to find top loci:\n    w3 = Window.partitionBy(\n        \"studyId\", \"chromosome\", \"locusStart\", \"locusEnd\"\n    ).orderBy(f.col(\"negLogPValue\").desc())\n\n    return StudyLocus(\n        _df=(\n            # Applying the baseline p-value cutoff:\n            summary_statistics.pvalue_filter(baseline_pvalue_cutoff)\n            # Calculating the neglog p-value for easier sorting:\n            .df.withColumn(\n                \"negLogPValue\",\n                neglogpval_from_pvalue(\n                    f.col(\"pValueMantissa\"), f.col(\"pValueExponent\")\n                ),\n            )\n            # Calculating the distance between consecutive positions, then identifying the locus start and end:\n            .withColumn(\"next_position\", f.lag(f.col(\"position\")).over(w1))\n            .withColumn(\"distance\", f.col(\"position\") - f.col(\"next_position\"))\n            .withColumn(\n                \"locusStart\",\n                f.when(\n                    (f.col(\"distance\") &gt; distance_cutoff)\n                    | f.col(\"distance\").isNull(),\n                    f.col(\"position\"),\n                ),\n            )\n            .withColumn(\n                \"locusStart\",\n                f.when(\n                    f.last(f.col(\"locusStart\") - flanking_distance, True).over(\n                        w1.rowsBetween(-sys.maxsize, 0)\n                    )\n                    &gt; 0,\n                    f.last(f.col(\"locusStart\") - flanking_distance, True).over(\n                        w1.rowsBetween(-sys.maxsize, 0)\n                    ),\n                ).otherwise(f.lit(0)),\n            )\n            .withColumn(\n                \"locusEnd\", f.max(f.col(\"position\") + flanking_distance).over(w2)\n            )\n            .withColumn(\"rank\", f.rank().over(w3))\n            .filter(\n                (f.col(\"rank\") == 1) &amp; (f.col(\"negLogPValue\") &gt; neglog_pv_cutoff)\n            )\n            .select(\n                *columns_sumstats_columns,\n                # To make sure that the type of locusStart and locusEnd follows schema of StudyLocus:\n                f.col(\"locusStart\").cast(t.IntegerType()).alias(\"locusStart\"),\n                f.col(\"locusEnd\").cast(t.IntegerType()).alias(\"locusEnd\"),\n                f.lit(None)\n                .cast(t.ArrayType(t.StringType()))\n                .alias(\"qualityControls\"),\n                StudyLocus.assign_study_locus_id([\"studyId\", \"variantId\"]),\n            )\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/clumping/#gentropy.method.locus_breaker_clumping.LocusBreakerClumping.process_locus_breaker_output","title":"<code>process_locus_breaker_output(lbc: StudyLocus, wbc: StudyLocus, large_loci_size: int) -&gt; StudyLocus</code>  <code>staticmethod</code>","text":"<p>Process the locus breaker method result, and run window-based clumping on large loci.</p> <p>Parameters:</p> Name Type Description Default <code>lbc</code> <code>StudyLocus</code> <p>StudyLocus object from locus-breaker clumping.</p> required <code>wbc</code> <code>StudyLocus</code> <p>StudyLocus object from window-based clumping.</p> required <code>large_loci_size</code> <code>int</code> <p>the size to define large loci which should be broken with wbc.</p> required <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>clumped study loci with large loci broken by window-based clumping.</p> Source code in <code>src/gentropy/method/locus_breaker_clumping.py</code> <pre><code>@staticmethod\ndef process_locus_breaker_output(\n    lbc: StudyLocus,\n    wbc: StudyLocus,\n    large_loci_size: int,\n) -&gt; StudyLocus:\n    \"\"\"Process the locus breaker method result, and run window-based clumping on large loci.\n\n    Args:\n        lbc (StudyLocus): StudyLocus object from locus-breaker clumping.\n        wbc (StudyLocus): StudyLocus object from window-based clumping.\n        large_loci_size (int): the size to define large loci which should be broken with wbc.\n\n    Returns:\n        StudyLocus: clumped study loci with large loci broken by window-based clumping.\n    \"\"\"\n    large_loci_size = int(large_loci_size)\n    small_loci = lbc.filter(\n        (f.col(\"locusEnd\") - f.col(\"locusStart\")) &lt;= large_loci_size\n    )\n    large_loci = lbc.filter(\n        (f.col(\"locusEnd\") - f.col(\"locusStart\")) &gt; large_loci_size\n    )\n    large_loci_wbc = StudyLocus(\n        wbc.df.alias(\"wbc\")\n        .join(\n            large_loci.df.alias(\"ll\"),\n            (f.col(\"wbc.studyId\") == f.col(\"ll.studyId\"))\n            &amp; (f.col(\"wbc.chromosome\") == f.col(\"ll.chromosome\"))\n            &amp; (\n                f.col(\"wbc.position\").between(\n                    f.col(\"ll.locusStart\"), f.col(\"ll.locusEnd\")\n                )\n            ),\n            \"semi\",\n        )\n        .withColumns(\n            {\n                \"locusStart\": f.col(\"position\") - large_loci_size // 2,\n                \"locusEnd\": f.col(\"position\") + large_loci_size // 2,\n            }\n        ),\n        StudyLocus.get_schema(),\n    )\n    return StudyLocus(\n        large_loci_wbc.df.unionByName(small_loci.df),\n        StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/coloc/","title":"Coloc","text":""},{"location":"python_api/methods/coloc/#gentropy.method.colocalisation.Coloc","title":"<code>gentropy.method.colocalisation.Coloc</code>","text":"<p>               Bases: <code>ColocalisationMethodInterface</code></p> <p>Calculate bayesian colocalisation based on overlapping signals from credible sets.</p> <p>Based on the R COLOC package, which uses the Bayes factors from the credible set to estimate the posterior probability of colocalisation. This method makes the simplifying assumption that only one single causal variant exists for any given trait in any genomic region.</p> Hypothesis Description H<sub>0</sub> no association with either trait in the region H<sub>1</sub> association with trait 1 only H<sub>2</sub> association with trait 2 only H<sub>3</sub> both traits are associated, but have different single causal variants H<sub>4</sub> both traits are associated and share the same single causal variant <p>Bayes factors required</p> <p>Coloc requires the availability of Bayes factors (BF) for each variant in the credible set (<code>logBF</code> column).</p> <p>Attributes:</p> Name Type Description <code>PSEUDOCOUNT</code> <code>float</code> <p>Pseudocount to avoid log(0). Defaults to 1e-10.</p> <code>OVERLAP_SIZE_CUTOFF</code> <code>int</code> <p>Minimum number of overlapping variants bfore filtering. Defaults to 5.</p> <code>POSTERIOR_CUTOFF</code> <code>float</code> <p>Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.5.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>class Coloc(ColocalisationMethodInterface):\n    \"\"\"Calculate bayesian colocalisation based on overlapping signals from credible sets.\n\n    Based on the [R COLOC package](https://github.com/chr1swallace/coloc/blob/main/R/claudia.R), which uses the Bayes factors from the credible set to estimate the posterior probability of colocalisation. This method makes the simplifying assumption that **only one single causal variant** exists for any given trait in any genomic region.\n\n    | Hypothesis    | Description                                                           |\n    | ------------- | --------------------------------------------------------------------- |\n    | H&lt;sub&gt;0&lt;/sub&gt; | no association with either trait in the region                        |\n    | H&lt;sub&gt;1&lt;/sub&gt; | association with trait 1 only                                         |\n    | H&lt;sub&gt;2&lt;/sub&gt; | association with trait 2 only                                         |\n    | H&lt;sub&gt;3&lt;/sub&gt; | both traits are associated, but have different single causal variants |\n    | H&lt;sub&gt;4&lt;/sub&gt; | both traits are associated and share the same single causal variant   |\n\n    !!! warning \"Bayes factors required\"\n\n        Coloc requires the availability of Bayes factors (BF) for each variant in the credible set (`logBF` column).\n\n    Attributes:\n        PSEUDOCOUNT (float): Pseudocount to avoid log(0). Defaults to 1e-10.\n        OVERLAP_SIZE_CUTOFF (int): Minimum number of overlapping variants bfore filtering. Defaults to 5.\n        POSTERIOR_CUTOFF (float): Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.5.\n    \"\"\"\n\n    METHOD_NAME: str = \"COLOC\"\n    METHOD_METRIC: str = \"h4\"\n    PSEUDOCOUNT: float = 1e-10\n\n    @staticmethod\n    def _get_posteriors(all_bfs: NDArray[np.float64]) -&gt; DenseVector:\n        \"\"\"Calculate posterior probabilities for each hypothesis.\n\n        Args:\n            all_bfs (NDArray[np.float64]): h0-h4 bayes factors\n\n        Returns:\n            DenseVector: Posterior\n\n        Example:\n            &gt;&gt;&gt; l = np.array([0.2, 0.1, 0.05, 0])\n            &gt;&gt;&gt; Coloc._get_posteriors(l)\n            DenseVector([0.279, 0.2524, 0.2401, 0.2284])\n        \"\"\"\n        diff = all_bfs - get_logsum(all_bfs)\n        bfs_posteriors = np.exp(diff)\n        return Vectors.dense(bfs_posteriors)\n\n    @classmethod\n    def colocalise(\n        cls: type[Coloc],\n        overlapping_signals: StudyLocusOverlap,\n        **kwargs: Any,\n    ) -&gt; Colocalisation:\n        \"\"\"Calculate bayesian colocalisation based on overlapping signals.\n\n        Args:\n            overlapping_signals (StudyLocusOverlap): overlapping peaks\n            **kwargs (Any): Additional parameters passed to the colocalise method.\n\n        Keyword Args:\n            priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4.\n            priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4.\n            priorc12 (float): Prior on variant being causal for traits 1 and 2. Defaults to 1e-5.\n            overlap_size_cutoff (int): Minimum number of overlapping variants before filtering. Defaults to 0.\n            posterior_cutoff (float): Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.0.\n\n        Returns:\n            Colocalisation: Colocalisation results\n\n        Raises:\n            TypeError: When passed incorrect prior argument types.\n        \"\"\"\n        # Get kwargs for overlap size and posterior cutoff\n        overlap_size_cutoff = kwargs.get(\"overlap_size_cutoff\") or 0\n        posterior_cutoff = kwargs.get(\"posterior_cutoff\") or 0.0\n        # Ensure priors are always present, even if not passed\n        priorc1 = kwargs.get(\"priorc1\") or 1e-4\n        priorc2 = kwargs.get(\"priorc2\") or 1e-4\n        priorc12 = kwargs.get(\"priorc12\") or 1e-5\n        priors = [priorc1, priorc2, priorc12]\n        if any(not isinstance(prior, float) for prior in priors):\n            raise TypeError(\n                \"Passed incorrect type(s) for prior parameters. got %s\",\n                {type(p): p for p in priors},\n            )\n\n        # register udfs\n        logsum = f.udf(get_logsum, DoubleType())\n        posteriors = f.udf(Coloc._get_posteriors, VectorUDT())\n        return Colocalisation(\n            _df=(\n                overlapping_signals.df.withColumn(\n                    \"tagVariantSource\", get_tag_variant_source(f.col(\"statistics\"))\n                )\n                .select(\"*\", \"statistics.*\")\n                # Before summing log_BF columns nulls need to be filled with 0:\n                .fillna(\n                    0,\n                    subset=[\n                        \"left_logBF\",\n                        \"right_logBF\",\n                        \"left_posteriorProbability\",\n                        \"right_posteriorProbability\",\n                    ],\n                )\n                # Sum of log_BFs for each pair of signals\n                .withColumn(\n                    \"sum_log_bf\",\n                    f.col(\"left_logBF\") + f.col(\"right_logBF\"),\n                )\n                # Group by overlapping peak and generating dense vectors of log_BF:\n                .groupBy(\n                    \"chromosome\",\n                    \"leftStudyLocusId\",\n                    \"rightStudyLocusId\",\n                    \"rightStudyType\",\n                )\n                .agg(\n                    f.size(\n                        f.filter(\n                            f.collect_list(f.col(\"tagVariantSource\")),\n                            lambda x: x == \"both\",\n                        )\n                    )\n                    .cast(t.LongType())\n                    .alias(\"numberColocalisingVariants\"),\n                    fml.array_to_vector(f.collect_list(f.col(\"left_logBF\"))).alias(\n                        \"left_logBF\"\n                    ),\n                    fml.array_to_vector(f.collect_list(f.col(\"right_logBF\"))).alias(\n                        \"right_logBF\"\n                    ),\n                    fml.array_to_vector(\n                        f.collect_list(f.col(\"left_posteriorProbability\"))\n                    ).alias(\"left_posteriorProbability\"),\n                    fml.array_to_vector(\n                        f.collect_list(f.col(\"right_posteriorProbability\"))\n                    ).alias(\"right_posteriorProbability\"),\n                    fml.array_to_vector(f.collect_list(f.col(\"sum_log_bf\"))).alias(\n                        \"sum_log_bf\"\n                    ),\n                    f.collect_list(f.col(\"tagVariantSource\")).alias(\n                        \"tagVariantSourceList\"\n                    ),\n                )\n                .withColumn(\"logsum1\", logsum(f.col(\"left_logBF\")))\n                .withColumn(\"logsum2\", logsum(f.col(\"right_logBF\")))\n                .withColumn(\"logsum12\", logsum(f.col(\"sum_log_bf\")))\n                .drop(\"left_logBF\", \"right_logBF\", \"sum_log_bf\")\n                # Add priors\n                # priorc1 Prior on variant being causal for trait 1\n                .withColumn(\"priorc1\", f.lit(priorc1))\n                # priorc2 Prior on variant being causal for trait 2\n                .withColumn(\"priorc2\", f.lit(priorc2))\n                # priorc12 Prior on variant being causal for traits 1 and 2\n                .withColumn(\"priorc12\", f.lit(priorc12))\n                # h0-h2\n                .withColumn(\"lH0bf\", f.lit(0))\n                .withColumn(\"lH1bf\", f.log(f.col(\"priorc1\")) + f.col(\"logsum1\"))\n                .withColumn(\"lH2bf\", f.log(f.col(\"priorc2\")) + f.col(\"logsum2\"))\n                # h3\n                .withColumn(\"sumlogsum\", f.col(\"logsum1\") + f.col(\"logsum2\"))\n                .withColumn(\"max\", f.greatest(\"sumlogsum\", \"logsum12\"))\n                .withColumn(\n                    \"anySnpBothSidesHigh\",\n                    f.aggregate(\n                        f.transform(\n                            f.arrays_zip(\n                                fml.vector_to_array(f.col(\"left_posteriorProbability\")),\n                                fml.vector_to_array(\n                                    f.col(\"right_posteriorProbability\")\n                                ),\n                                f.col(\"tagVariantSourceList\"),\n                            ),\n                            # row[\"0\"] = left PP, row[\"1\"] = right PP, row[\"tagVariantSourceList\"]\n                            lambda row: f.when(\n                                (row[\"tagVariantSourceList\"] == \"both\")\n                                &amp; (row[\"0\"] &gt; posterior_cutoff)\n                                &amp; (row[\"1\"] &gt; posterior_cutoff),\n                                1.0,\n                            ).otherwise(0.0),\n                        ),\n                        f.lit(0.0),\n                        lambda acc, x: acc + x,\n                    )\n                    &gt; 0,  # True if sum of these 1.0's &gt; 0\n                )\n                .filter(\n                    (f.col(\"numberColocalisingVariants\") &gt; overlap_size_cutoff)\n                    | (f.col(\"anySnpBothSidesHigh\"))\n                )\n                .withColumn(\n                    \"logdiff\",\n                    f.when(\n                        (f.col(\"sumlogsum\") == f.col(\"logsum12\")),\n                        Coloc.PSEUDOCOUNT,\n                    ).otherwise(\n                        f.col(\"max\")\n                        + f.log(\n                            f.exp(f.col(\"sumlogsum\") - f.col(\"max\"))\n                            - f.exp(f.col(\"logsum12\") - f.col(\"max\"))\n                        )\n                    ),\n                )\n                .withColumn(\n                    \"lH3bf\",\n                    f.log(f.col(\"priorc1\"))\n                    + f.log(f.col(\"priorc2\"))\n                    + f.col(\"logdiff\"),\n                )\n                .drop(\"right_logsum\", \"left_logsum\", \"sumlogsum\", \"max\", \"logdiff\")\n                # h4\n                .withColumn(\"lH4bf\", f.log(f.col(\"priorc12\")) + f.col(\"logsum12\"))\n                # cleaning\n                .drop(\n                    \"priorc1\", \"priorc2\", \"priorc12\", \"logsum1\", \"logsum2\", \"logsum12\"\n                )\n                # posteriors\n                .withColumn(\n                    \"allBF\",\n                    fml.array_to_vector(\n                        f.array(\n                            f.col(\"lH0bf\"),\n                            f.col(\"lH1bf\"),\n                            f.col(\"lH2bf\"),\n                            f.col(\"lH3bf\"),\n                            f.col(\"lH4bf\"),\n                        )\n                    ),\n                )\n                .withColumn(\n                    \"posteriors\", fml.vector_to_array(posteriors(f.col(\"allBF\")))\n                )\n                .withColumn(\"h0\", f.col(\"posteriors\").getItem(0))\n                .withColumn(\"h1\", f.col(\"posteriors\").getItem(1))\n                .withColumn(\"h2\", f.col(\"posteriors\").getItem(2))\n                .withColumn(\"h3\", f.col(\"posteriors\").getItem(3))\n                .withColumn(\"h4\", f.col(\"posteriors\").getItem(4))\n                # clean up\n                .drop(\n                    \"posteriors\",\n                    \"allBF\",\n                    \"lH0bf\",\n                    \"lH1bf\",\n                    \"lH2bf\",\n                    \"lH3bf\",\n                    \"lH4bf\",\n                    \"left_posteriorProbability\",\n                    \"right_posteriorProbability\",\n                    \"tagVariantSourceList\",\n                    \"anySnpBothSidesHigh\",\n                )\n                .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n                .join(\n                    overlapping_signals.calculate_beta_ratio(),\n                    on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                    how=\"left\",\n                )\n            ),\n            _schema=Colocalisation.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/coloc/#gentropy.method.colocalisation.Coloc.colocalise","title":"<code>colocalise(overlapping_signals: StudyLocusOverlap, **kwargs: Any) -&gt; Colocalisation</code>  <code>classmethod</code>","text":"<p>Calculate bayesian colocalisation based on overlapping signals.</p> <p>Parameters:</p> Name Type Description Default <code>overlapping_signals</code> <code>StudyLocusOverlap</code> <p>overlapping peaks</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to the colocalise method.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>priorc1</code> <code>float</code> <p>Prior on variant being causal for trait 1. Defaults to 1e-4.</p> <code>priorc2</code> <code>float</code> <p>Prior on variant being causal for trait 2. Defaults to 1e-4.</p> <code>priorc12</code> <code>float</code> <p>Prior on variant being causal for traits 1 and 2. Defaults to 1e-5.</p> <code>overlap_size_cutoff</code> <code>int</code> <p>Minimum number of overlapping variants before filtering. Defaults to 0.</p> <code>posterior_cutoff</code> <code>float</code> <p>Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.0.</p> <p>Returns:</p> Name Type Description <code>Colocalisation</code> <code>Colocalisation</code> <p>Colocalisation results</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>When passed incorrect prior argument types.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>@classmethod\ndef colocalise(\n    cls: type[Coloc],\n    overlapping_signals: StudyLocusOverlap,\n    **kwargs: Any,\n) -&gt; Colocalisation:\n    \"\"\"Calculate bayesian colocalisation based on overlapping signals.\n\n    Args:\n        overlapping_signals (StudyLocusOverlap): overlapping peaks\n        **kwargs (Any): Additional parameters passed to the colocalise method.\n\n    Keyword Args:\n        priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4.\n        priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4.\n        priorc12 (float): Prior on variant being causal for traits 1 and 2. Defaults to 1e-5.\n        overlap_size_cutoff (int): Minimum number of overlapping variants before filtering. Defaults to 0.\n        posterior_cutoff (float): Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.0.\n\n    Returns:\n        Colocalisation: Colocalisation results\n\n    Raises:\n        TypeError: When passed incorrect prior argument types.\n    \"\"\"\n    # Get kwargs for overlap size and posterior cutoff\n    overlap_size_cutoff = kwargs.get(\"overlap_size_cutoff\") or 0\n    posterior_cutoff = kwargs.get(\"posterior_cutoff\") or 0.0\n    # Ensure priors are always present, even if not passed\n    priorc1 = kwargs.get(\"priorc1\") or 1e-4\n    priorc2 = kwargs.get(\"priorc2\") or 1e-4\n    priorc12 = kwargs.get(\"priorc12\") or 1e-5\n    priors = [priorc1, priorc2, priorc12]\n    if any(not isinstance(prior, float) for prior in priors):\n        raise TypeError(\n            \"Passed incorrect type(s) for prior parameters. got %s\",\n            {type(p): p for p in priors},\n        )\n\n    # register udfs\n    logsum = f.udf(get_logsum, DoubleType())\n    posteriors = f.udf(Coloc._get_posteriors, VectorUDT())\n    return Colocalisation(\n        _df=(\n            overlapping_signals.df.withColumn(\n                \"tagVariantSource\", get_tag_variant_source(f.col(\"statistics\"))\n            )\n            .select(\"*\", \"statistics.*\")\n            # Before summing log_BF columns nulls need to be filled with 0:\n            .fillna(\n                0,\n                subset=[\n                    \"left_logBF\",\n                    \"right_logBF\",\n                    \"left_posteriorProbability\",\n                    \"right_posteriorProbability\",\n                ],\n            )\n            # Sum of log_BFs for each pair of signals\n            .withColumn(\n                \"sum_log_bf\",\n                f.col(\"left_logBF\") + f.col(\"right_logBF\"),\n            )\n            # Group by overlapping peak and generating dense vectors of log_BF:\n            .groupBy(\n                \"chromosome\",\n                \"leftStudyLocusId\",\n                \"rightStudyLocusId\",\n                \"rightStudyType\",\n            )\n            .agg(\n                f.size(\n                    f.filter(\n                        f.collect_list(f.col(\"tagVariantSource\")),\n                        lambda x: x == \"both\",\n                    )\n                )\n                .cast(t.LongType())\n                .alias(\"numberColocalisingVariants\"),\n                fml.array_to_vector(f.collect_list(f.col(\"left_logBF\"))).alias(\n                    \"left_logBF\"\n                ),\n                fml.array_to_vector(f.collect_list(f.col(\"right_logBF\"))).alias(\n                    \"right_logBF\"\n                ),\n                fml.array_to_vector(\n                    f.collect_list(f.col(\"left_posteriorProbability\"))\n                ).alias(\"left_posteriorProbability\"),\n                fml.array_to_vector(\n                    f.collect_list(f.col(\"right_posteriorProbability\"))\n                ).alias(\"right_posteriorProbability\"),\n                fml.array_to_vector(f.collect_list(f.col(\"sum_log_bf\"))).alias(\n                    \"sum_log_bf\"\n                ),\n                f.collect_list(f.col(\"tagVariantSource\")).alias(\n                    \"tagVariantSourceList\"\n                ),\n            )\n            .withColumn(\"logsum1\", logsum(f.col(\"left_logBF\")))\n            .withColumn(\"logsum2\", logsum(f.col(\"right_logBF\")))\n            .withColumn(\"logsum12\", logsum(f.col(\"sum_log_bf\")))\n            .drop(\"left_logBF\", \"right_logBF\", \"sum_log_bf\")\n            # Add priors\n            # priorc1 Prior on variant being causal for trait 1\n            .withColumn(\"priorc1\", f.lit(priorc1))\n            # priorc2 Prior on variant being causal for trait 2\n            .withColumn(\"priorc2\", f.lit(priorc2))\n            # priorc12 Prior on variant being causal for traits 1 and 2\n            .withColumn(\"priorc12\", f.lit(priorc12))\n            # h0-h2\n            .withColumn(\"lH0bf\", f.lit(0))\n            .withColumn(\"lH1bf\", f.log(f.col(\"priorc1\")) + f.col(\"logsum1\"))\n            .withColumn(\"lH2bf\", f.log(f.col(\"priorc2\")) + f.col(\"logsum2\"))\n            # h3\n            .withColumn(\"sumlogsum\", f.col(\"logsum1\") + f.col(\"logsum2\"))\n            .withColumn(\"max\", f.greatest(\"sumlogsum\", \"logsum12\"))\n            .withColumn(\n                \"anySnpBothSidesHigh\",\n                f.aggregate(\n                    f.transform(\n                        f.arrays_zip(\n                            fml.vector_to_array(f.col(\"left_posteriorProbability\")),\n                            fml.vector_to_array(\n                                f.col(\"right_posteriorProbability\")\n                            ),\n                            f.col(\"tagVariantSourceList\"),\n                        ),\n                        # row[\"0\"] = left PP, row[\"1\"] = right PP, row[\"tagVariantSourceList\"]\n                        lambda row: f.when(\n                            (row[\"tagVariantSourceList\"] == \"both\")\n                            &amp; (row[\"0\"] &gt; posterior_cutoff)\n                            &amp; (row[\"1\"] &gt; posterior_cutoff),\n                            1.0,\n                        ).otherwise(0.0),\n                    ),\n                    f.lit(0.0),\n                    lambda acc, x: acc + x,\n                )\n                &gt; 0,  # True if sum of these 1.0's &gt; 0\n            )\n            .filter(\n                (f.col(\"numberColocalisingVariants\") &gt; overlap_size_cutoff)\n                | (f.col(\"anySnpBothSidesHigh\"))\n            )\n            .withColumn(\n                \"logdiff\",\n                f.when(\n                    (f.col(\"sumlogsum\") == f.col(\"logsum12\")),\n                    Coloc.PSEUDOCOUNT,\n                ).otherwise(\n                    f.col(\"max\")\n                    + f.log(\n                        f.exp(f.col(\"sumlogsum\") - f.col(\"max\"))\n                        - f.exp(f.col(\"logsum12\") - f.col(\"max\"))\n                    )\n                ),\n            )\n            .withColumn(\n                \"lH3bf\",\n                f.log(f.col(\"priorc1\"))\n                + f.log(f.col(\"priorc2\"))\n                + f.col(\"logdiff\"),\n            )\n            .drop(\"right_logsum\", \"left_logsum\", \"sumlogsum\", \"max\", \"logdiff\")\n            # h4\n            .withColumn(\"lH4bf\", f.log(f.col(\"priorc12\")) + f.col(\"logsum12\"))\n            # cleaning\n            .drop(\n                \"priorc1\", \"priorc2\", \"priorc12\", \"logsum1\", \"logsum2\", \"logsum12\"\n            )\n            # posteriors\n            .withColumn(\n                \"allBF\",\n                fml.array_to_vector(\n                    f.array(\n                        f.col(\"lH0bf\"),\n                        f.col(\"lH1bf\"),\n                        f.col(\"lH2bf\"),\n                        f.col(\"lH3bf\"),\n                        f.col(\"lH4bf\"),\n                    )\n                ),\n            )\n            .withColumn(\n                \"posteriors\", fml.vector_to_array(posteriors(f.col(\"allBF\")))\n            )\n            .withColumn(\"h0\", f.col(\"posteriors\").getItem(0))\n            .withColumn(\"h1\", f.col(\"posteriors\").getItem(1))\n            .withColumn(\"h2\", f.col(\"posteriors\").getItem(2))\n            .withColumn(\"h3\", f.col(\"posteriors\").getItem(3))\n            .withColumn(\"h4\", f.col(\"posteriors\").getItem(4))\n            # clean up\n            .drop(\n                \"posteriors\",\n                \"allBF\",\n                \"lH0bf\",\n                \"lH1bf\",\n                \"lH2bf\",\n                \"lH3bf\",\n                \"lH4bf\",\n                \"left_posteriorProbability\",\n                \"right_posteriorProbability\",\n                \"tagVariantSourceList\",\n                \"anySnpBothSidesHigh\",\n            )\n            .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n            .join(\n                overlapping_signals.calculate_beta_ratio(),\n                on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                how=\"left\",\n            )\n        ),\n        _schema=Colocalisation.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/coloc_pip/","title":"Coloc using Posterior Inclusion Probabilities (PIP)","text":""},{"location":"python_api/methods/coloc_pip/#gentropy.method.colocalisation.ColocPIP","title":"<code>gentropy.method.colocalisation.ColocPIP</code>","text":"<p>               Bases: <code>ColocalisationMethodInterface</code></p> <p>Calculate bayesian colocalisation based on overlapping signals from credible sets using PIPs.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>class ColocPIP(ColocalisationMethodInterface):\n    \"\"\"Calculate bayesian colocalisation based on overlapping signals from credible sets using PIPs.\"\"\"\n\n    METHOD_NAME: str = \"COLOC_PIP\"\n    METHOD_METRIC: str = \"h4\"\n\n    @staticmethod\n    def _get_posteriors(\n        pip1_variants: NDArray[np.str_],\n        pip1_values: NDArray[np.float64],\n        pip2_variants: NDArray[np.str_],\n        pip2_values: NDArray[np.float64],\n        p1: float,\n        p2: float,\n        p12: float,\n    ) -&gt; DenseVector:\n        \"\"\"Approximate coloc posteriors using only PIPs, following the R coloc.pp logic.\n\n        Args:\n            pip1_variants (NDArray[np.str_]): Array of variant names for trait 1\n            pip1_values (NDArray[np.float64]): Array of PIP values for trait 1\n            pip2_variants (NDArray[np.str_]): Array of variant names for trait 2\n            pip2_values (NDArray[np.float64]): Array of PIP values for trait 2\n            p1 (float): Prior on variant being causal for trait 1\n            p2 (float): Prior on variant being causal for trait 2\n            p12 (float): Prior on variant being causal for traits 1 and 2\n\n        Returns:\n            DenseVector: [H0, H1, H2, H3, H4] posteriors\n        \"\"\"\n        # Union of SNPs\n        snp_names = np.unique(np.concatenate((pip1_variants, pip2_variants)))\n        pip1_dict = dict(zip(pip1_variants, pip1_values))\n        pip2_dict = dict(zip(pip2_variants, pip2_values))\n\n        # Ensure priors are never zero to avoid log(0)\n        pseudocount = 1e-16\n        p1 = max(p1, pseudocount)\n        p2 = max(p2, pseudocount)\n        p12 = max(p12, pseudocount)\n        pip1_vec = np.array([pip1_dict.get(snp, np.nan) for snp in snp_names])\n        pip2_vec = np.array([pip2_dict.get(snp, np.nan) for snp in snp_names])\n\n        # Ensure no PIPs are exactly zero by adding pseudocount\n        pip1_vec = np.maximum(pip1_vec, pseudocount)\n        pip2_vec = np.maximum(pip2_vec, pseudocount)\n\n        # Work in log-space\n        log_pip1 = np.log(pip1_vec)\n        log_pip2 = np.log(pip2_vec)\n        sum_log_pip1 = get_logsum(log_pip1)\n        sum_log_pip2 = get_logsum(log_pip2)\n        log_sum_both = get_logsum((log_pip1 + log_pip2).astype(np.float64))\n\n        # Compute logdiff as in R coloc:::logdiff\n        x = sum_log_pip1 + sum_log_pip2\n        y = log_sum_both\n        my_max = max(x, y)\n        diff_arg = max(np.exp(x - my_max) - np.exp(y - my_max), 0)\n        if diff_arg == 0:\n            logdiff = -np.inf\n        else:\n            logdiff = my_max + np.log(diff_arg)\n\n        # H3: distinct causal\n        PP3 = np.log(p1) + np.log(p2) + logdiff\n\n        # H4: shared causal\n        PP4 = np.log(p12) + log_sum_both\n\n        # Normalize\n        denom = get_logsum(np.array([PP3, PP4]))\n        PP4 = np.exp(PP4 - denom)\n        PP3 = np.exp(PP3 - denom)\n\n        return Vectors.dense([0.0, 0.0, 0.0, PP3, PP4])\n\n    @classmethod\n    def colocalise(\n        cls: type[ColocPIP],\n        overlapping_signals: StudyLocusOverlap,\n        **kwargs: Any,\n    ) -&gt; Colocalisation:\n        \"\"\"Calculate approximate bayesian colocalisation based on overlapping signals with PIPs.\n\n        Args:\n            overlapping_signals (StudyLocusOverlap): overlapping peaks\n            **kwargs (Any): Additional parameters passed to the colocalise method.\n\n        Keyword Args:\n            priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4.\n            priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4.\n            priorc12 (float): Prior on variant being causal for traits 1 and 2. Defaults to 1e-5.\n\n        Returns:\n            Colocalisation: Colocalisation results\n\n        Raises:\n            TypeError: When passed incorrect prior argument types.\n        \"\"\"\n        # Ensure priors are always present, even if not passed\n        priorc1 = kwargs.get(\"priorc1\") or 1e-4\n        priorc2 = kwargs.get(\"priorc2\") or 1e-4\n        priorc12 = kwargs.get(\"priorc12\") or 1e-5\n        priors = [priorc1, priorc2, priorc12]\n        if any(not isinstance(prior, float) for prior in priors):\n            raise TypeError(\n                \"Passed incorrect type(s) for prior parameters. got %s\",\n                {type(p): p for p in priors},\n            )\n\n        # Register UDF for calculating posteriors from PIPs\n        posteriors_udf = f.udf(cls._get_posteriors, VectorUDT())\n\n        return Colocalisation(\n            _df=(\n                overlapping_signals.df.withColumn(\n                    \"tagVariantSource\", get_tag_variant_source(f.col(\"statistics\"))\n                )\n                .select(\"*\", \"statistics.*\")\n                # Before processing, ensure posterior probabilities are not null\n                .fillna(\n                    0,\n                    subset=[\n                        \"left_posteriorProbability\",\n                        \"right_posteriorProbability\",\n                    ],\n                )\n                # Group by overlapping peaks and collect PIPs as arrays\n                .groupBy(\n                    \"chromosome\",\n                    \"leftStudyLocusId\",\n                    \"rightStudyLocusId\",\n                    \"rightStudyType\",\n                )\n                .agg(\n                    f.size(\n                        f.filter(\n                            f.collect_list(f.col(\"tagVariantSource\")),\n                            lambda x: x == \"both\",\n                        )\n                    )\n                    .cast(t.LongType())\n                    .alias(\"numberColocalisingVariants\"),\n                    # Collect variant IDs and PIPs for left study\n                    f.collect_list(f.col(\"tagVariantId\")).alias(\"left_variants\"),\n                    f.collect_list(f.col(\"left_posteriorProbability\")).alias(\n                        \"left_pips\"\n                    ),\n                    # Collect variant IDs and PIPs for right study\n                    f.collect_list(f.col(\"tagVariantId\")).alias(\"right_variants\"),\n                    f.collect_list(f.col(\"right_posteriorProbability\")).alias(\n                        \"right_pips\"\n                    ),\n                )\n                # Calculate posteriors using the PIP approximation\n                .withColumn(\n                    \"posteriors\",\n                    posteriors_udf(\n                        f.col(\"left_variants\").cast(\"array&lt;string&gt;\"),\n                        f.col(\"left_pips\").cast(\"array&lt;double&gt;\"),\n                        f.col(\"right_variants\").cast(\"array&lt;string&gt;\"),\n                        f.col(\"right_pips\").cast(\"array&lt;double&gt;\"),\n                        f.lit(priorc1),\n                        f.lit(priorc2),\n                        f.lit(priorc12),\n                    ),\n                )\n                # Extract individual hypothesis posteriors\n                .withColumn(\"h0\", fml.vector_to_array(f.col(\"posteriors\")).getItem(0))\n                .withColumn(\"h1\", fml.vector_to_array(f.col(\"posteriors\")).getItem(1))\n                .withColumn(\"h2\", fml.vector_to_array(f.col(\"posteriors\")).getItem(2))\n                .withColumn(\"h3\", fml.vector_to_array(f.col(\"posteriors\")).getItem(3))\n                .withColumn(\"h4\", fml.vector_to_array(f.col(\"posteriors\")).getItem(4))\n                # Clean up intermediate columns\n                .drop(\n                    \"posteriors\",\n                    \"left_variants\",\n                    \"left_pips\",\n                    \"right_variants\",\n                    \"right_pips\",\n                )\n                .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n                .join(\n                    overlapping_signals.calculate_beta_ratio(),\n                    on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                    how=\"left\",\n                )\n            ),\n            _schema=Colocalisation.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/coloc_pip/#gentropy.method.colocalisation.ColocPIP.colocalise","title":"<code>colocalise(overlapping_signals: StudyLocusOverlap, **kwargs: Any) -&gt; Colocalisation</code>  <code>classmethod</code>","text":"<p>Calculate approximate bayesian colocalisation based on overlapping signals with PIPs.</p> <p>Parameters:</p> Name Type Description Default <code>overlapping_signals</code> <code>StudyLocusOverlap</code> <p>overlapping peaks</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to the colocalise method.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>priorc1</code> <code>float</code> <p>Prior on variant being causal for trait 1. Defaults to 1e-4.</p> <code>priorc2</code> <code>float</code> <p>Prior on variant being causal for trait 2. Defaults to 1e-4.</p> <code>priorc12</code> <code>float</code> <p>Prior on variant being causal for traits 1 and 2. Defaults to 1e-5.</p> <p>Returns:</p> Name Type Description <code>Colocalisation</code> <code>Colocalisation</code> <p>Colocalisation results</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>When passed incorrect prior argument types.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>@classmethod\ndef colocalise(\n    cls: type[ColocPIP],\n    overlapping_signals: StudyLocusOverlap,\n    **kwargs: Any,\n) -&gt; Colocalisation:\n    \"\"\"Calculate approximate bayesian colocalisation based on overlapping signals with PIPs.\n\n    Args:\n        overlapping_signals (StudyLocusOverlap): overlapping peaks\n        **kwargs (Any): Additional parameters passed to the colocalise method.\n\n    Keyword Args:\n        priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4.\n        priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4.\n        priorc12 (float): Prior on variant being causal for traits 1 and 2. Defaults to 1e-5.\n\n    Returns:\n        Colocalisation: Colocalisation results\n\n    Raises:\n        TypeError: When passed incorrect prior argument types.\n    \"\"\"\n    # Ensure priors are always present, even if not passed\n    priorc1 = kwargs.get(\"priorc1\") or 1e-4\n    priorc2 = kwargs.get(\"priorc2\") or 1e-4\n    priorc12 = kwargs.get(\"priorc12\") or 1e-5\n    priors = [priorc1, priorc2, priorc12]\n    if any(not isinstance(prior, float) for prior in priors):\n        raise TypeError(\n            \"Passed incorrect type(s) for prior parameters. got %s\",\n            {type(p): p for p in priors},\n        )\n\n    # Register UDF for calculating posteriors from PIPs\n    posteriors_udf = f.udf(cls._get_posteriors, VectorUDT())\n\n    return Colocalisation(\n        _df=(\n            overlapping_signals.df.withColumn(\n                \"tagVariantSource\", get_tag_variant_source(f.col(\"statistics\"))\n            )\n            .select(\"*\", \"statistics.*\")\n            # Before processing, ensure posterior probabilities are not null\n            .fillna(\n                0,\n                subset=[\n                    \"left_posteriorProbability\",\n                    \"right_posteriorProbability\",\n                ],\n            )\n            # Group by overlapping peaks and collect PIPs as arrays\n            .groupBy(\n                \"chromosome\",\n                \"leftStudyLocusId\",\n                \"rightStudyLocusId\",\n                \"rightStudyType\",\n            )\n            .agg(\n                f.size(\n                    f.filter(\n                        f.collect_list(f.col(\"tagVariantSource\")),\n                        lambda x: x == \"both\",\n                    )\n                )\n                .cast(t.LongType())\n                .alias(\"numberColocalisingVariants\"),\n                # Collect variant IDs and PIPs for left study\n                f.collect_list(f.col(\"tagVariantId\")).alias(\"left_variants\"),\n                f.collect_list(f.col(\"left_posteriorProbability\")).alias(\n                    \"left_pips\"\n                ),\n                # Collect variant IDs and PIPs for right study\n                f.collect_list(f.col(\"tagVariantId\")).alias(\"right_variants\"),\n                f.collect_list(f.col(\"right_posteriorProbability\")).alias(\n                    \"right_pips\"\n                ),\n            )\n            # Calculate posteriors using the PIP approximation\n            .withColumn(\n                \"posteriors\",\n                posteriors_udf(\n                    f.col(\"left_variants\").cast(\"array&lt;string&gt;\"),\n                    f.col(\"left_pips\").cast(\"array&lt;double&gt;\"),\n                    f.col(\"right_variants\").cast(\"array&lt;string&gt;\"),\n                    f.col(\"right_pips\").cast(\"array&lt;double&gt;\"),\n                    f.lit(priorc1),\n                    f.lit(priorc2),\n                    f.lit(priorc12),\n                ),\n            )\n            # Extract individual hypothesis posteriors\n            .withColumn(\"h0\", fml.vector_to_array(f.col(\"posteriors\")).getItem(0))\n            .withColumn(\"h1\", fml.vector_to_array(f.col(\"posteriors\")).getItem(1))\n            .withColumn(\"h2\", fml.vector_to_array(f.col(\"posteriors\")).getItem(2))\n            .withColumn(\"h3\", fml.vector_to_array(f.col(\"posteriors\")).getItem(3))\n            .withColumn(\"h4\", fml.vector_to_array(f.col(\"posteriors\")).getItem(4))\n            # Clean up intermediate columns\n            .drop(\n                \"posteriors\",\n                \"left_variants\",\n                \"left_pips\",\n                \"right_variants\",\n                \"right_pips\",\n            )\n            .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n            .join(\n                overlapping_signals.calculate_beta_ratio(),\n                on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                how=\"left\",\n            )\n        ),\n        _schema=Colocalisation.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/ecaviar/","title":"eCAVIAR","text":""},{"location":"python_api/methods/ecaviar/#gentropy.method.colocalisation.ECaviar","title":"<code>gentropy.method.colocalisation.ECaviar</code>","text":"<p>               Bases: <code>ColocalisationMethodInterface</code></p> <p>ECaviar-based colocalisation analysis.</p> <p>It extends CAVIAR framework to explicitly estimate the posterior probability that the same variant is causal in 2 studies while accounting for the uncertainty of LD. eCAVIAR computes the colocalization posterior probability (CLPP) by utilizing the marginal posterior probabilities. This framework allows for multiple variants to be causal in a single locus.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>class ECaviar(ColocalisationMethodInterface):\n    \"\"\"ECaviar-based colocalisation analysis.\n\n    It extends [CAVIAR](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5142122/#bib18) framework to explicitly estimate the posterior probability that the same variant is causal in 2 studies while accounting for the uncertainty of LD. eCAVIAR computes the colocalization posterior probability (**CLPP**) by utilizing the marginal posterior probabilities. This framework allows for **multiple variants to be causal** in a single locus.\n    \"\"\"\n\n    METHOD_NAME: str = \"eCAVIAR\"\n    METHOD_METRIC: str = \"clpp\"\n\n    @staticmethod\n    def _get_clpp(left_pp: Column, right_pp: Column) -&gt; Column:\n        \"\"\"Calculate the colocalisation posterior probability (CLPP).\n\n        If the fact that the same variant is found causal for two studies are independent events,\n        CLPP is defined as the product of posterior porbabilities that a variant is causal in both studies.\n\n        Args:\n            left_pp (Column): left posterior probability\n            right_pp (Column): right posterior probability\n\n        Returns:\n            Column: CLPP\n\n        Examples:\n            &gt;&gt;&gt; d = [{\"left_pp\": 0.5, \"right_pp\": 0.5}, {\"left_pp\": 0.25, \"right_pp\": 0.75}]\n            &gt;&gt;&gt; df = spark.createDataFrame(d)\n            &gt;&gt;&gt; df.withColumn(\"clpp\", ECaviar._get_clpp(f.col(\"left_pp\"), f.col(\"right_pp\"))).show()\n            +-------+--------+------+\n            |left_pp|right_pp|  clpp|\n            +-------+--------+------+\n            |    0.5|     0.5|  0.25|\n            |   0.25|    0.75|0.1875|\n            +-------+--------+------+\n            &lt;BLANKLINE&gt;\n\n        \"\"\"\n        return left_pp * right_pp\n\n    @classmethod\n    def colocalise(\n        cls: type[ECaviar],\n        overlapping_signals: StudyLocusOverlap,\n        **kwargs: Any,\n    ) -&gt; Colocalisation:\n        \"\"\"Calculate bayesian colocalisation based on overlapping signals.\n\n        Args:\n            overlapping_signals (StudyLocusOverlap): overlapping signals.\n            **kwargs (Any): Additional parameters passed to the colocalise method.\n\n        Returns:\n            Colocalisation: colocalisation results based on eCAVIAR.\n        \"\"\"\n        return Colocalisation(\n            _df=(\n                overlapping_signals.df.withColumns(\n                    {\n                        \"clpp\": ECaviar._get_clpp(\n                            f.col(\"statistics.left_posteriorProbability\"),\n                            f.col(\"statistics.right_posteriorProbability\"),\n                        ),\n                        \"tagVariantSource\": get_tag_variant_source(f.col(\"statistics\")),\n                    }\n                )\n                .groupBy(\n                    \"leftStudyLocusId\",\n                    \"rightStudyLocusId\",\n                    \"rightStudyType\",\n                    \"chromosome\",\n                )\n                .agg(\n                    # Count the number of tag variants that can be found in both loci:\n                    f.size(\n                        f.filter(\n                            f.collect_list(f.col(\"tagVariantSource\")),\n                            lambda x: x == \"both\",\n                        )\n                    )\n                    .cast(t.LongType())\n                    .alias(\"numberColocalisingVariants\"),\n                    f.sum(f.col(\"clpp\")).alias(\"clpp\"),\n                )\n                .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n                .join(\n                    overlapping_signals.calculate_beta_ratio(),\n                    on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                    how=\"left\",\n                )\n            ),\n            _schema=Colocalisation.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/ecaviar/#gentropy.method.colocalisation.ECaviar.colocalise","title":"<code>colocalise(overlapping_signals: StudyLocusOverlap, **kwargs: Any) -&gt; Colocalisation</code>  <code>classmethod</code>","text":"<p>Calculate bayesian colocalisation based on overlapping signals.</p> <p>Parameters:</p> Name Type Description Default <code>overlapping_signals</code> <code>StudyLocusOverlap</code> <p>overlapping signals.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to the colocalise method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Colocalisation</code> <code>Colocalisation</code> <p>colocalisation results based on eCAVIAR.</p> Source code in <code>src/gentropy/method/colocalisation.py</code> <pre><code>@classmethod\ndef colocalise(\n    cls: type[ECaviar],\n    overlapping_signals: StudyLocusOverlap,\n    **kwargs: Any,\n) -&gt; Colocalisation:\n    \"\"\"Calculate bayesian colocalisation based on overlapping signals.\n\n    Args:\n        overlapping_signals (StudyLocusOverlap): overlapping signals.\n        **kwargs (Any): Additional parameters passed to the colocalise method.\n\n    Returns:\n        Colocalisation: colocalisation results based on eCAVIAR.\n    \"\"\"\n    return Colocalisation(\n        _df=(\n            overlapping_signals.df.withColumns(\n                {\n                    \"clpp\": ECaviar._get_clpp(\n                        f.col(\"statistics.left_posteriorProbability\"),\n                        f.col(\"statistics.right_posteriorProbability\"),\n                    ),\n                    \"tagVariantSource\": get_tag_variant_source(f.col(\"statistics\")),\n                }\n            )\n            .groupBy(\n                \"leftStudyLocusId\",\n                \"rightStudyLocusId\",\n                \"rightStudyType\",\n                \"chromosome\",\n            )\n            .agg(\n                # Count the number of tag variants that can be found in both loci:\n                f.size(\n                    f.filter(\n                        f.collect_list(f.col(\"tagVariantSource\")),\n                        lambda x: x == \"both\",\n                    )\n                )\n                .cast(t.LongType())\n                .alias(\"numberColocalisingVariants\"),\n                f.sum(f.col(\"clpp\")).alias(\"clpp\"),\n            )\n            .withColumn(\"colocalisationMethod\", f.lit(cls.METHOD_NAME))\n            .join(\n                overlapping_signals.calculate_beta_ratio(),\n                on=[\"leftStudyLocusId\", \"rightStudyLocusId\", \"chromosome\"],\n                how=\"left\",\n            )\n        ),\n        _schema=Colocalisation.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/ld_annotator/","title":"LDAnnotator","text":""},{"location":"python_api/methods/ld_annotator/#gentropy.method.ld.LDAnnotator","title":"<code>gentropy.method.ld.LDAnnotator</code>","text":"<p>Class to annotate linkage disequilibrium (LD) operations from GnomAD.</p> Source code in <code>src/gentropy/method/ld.py</code> <pre><code>class LDAnnotator:\n    \"\"\"Class to annotate linkage disequilibrium (LD) operations from GnomAD.\"\"\"\n\n    @staticmethod\n    def _get_major_population(ordered_populations: Column) -&gt; Column:\n        \"\"\"Get major population based on an ldPopulationStructure array ordered by relativeSampleSize.\n\n        If there is a tie for the major population, nfe is selected if it is one of the major populations.\n        The first population in the array is selected if there is no tie for the major population, or there is a tie but nfe is not one of the major populations.\n\n        Args:\n            ordered_populations (Column): ldPopulationStructure array ordered by relativeSampleSize\n\n        Returns:\n            Column: major population\n        \"\"\"\n        major_population_size = ordered_populations[\"relativeSampleSize\"][0]\n        major_populations = f.filter(\n            ordered_populations,\n            lambda x: x[\"relativeSampleSize\"] == major_population_size,\n        )\n        # Check if nfe (Non-Finnish European) is one of the major populations\n        has_nfe = f.filter(major_populations, lambda x: x[\"ldPopulation\"] == \"nfe\")\n        return f.when(\n            (f.size(major_populations) &gt; 1) &amp; (f.size(has_nfe) == 1), f.lit(\"nfe\")\n        ).otherwise(ordered_populations[\"ldPopulation\"][0])\n\n    @staticmethod\n    def _calculate_r2_major(ld_set: Column, major_population: Column) -&gt; Column:\n        \"\"\"Calculate R2 using R of the major population in the study.\n\n        Args:\n            ld_set (Column): LD set\n            major_population (Column): Major population of the study\n\n        Returns:\n            Column: LD set with added 'r2Overall' field\n        \"\"\"\n        ld_set_with_major_pop = f.transform(\n            ld_set,\n            lambda x: f.struct(\n                x[\"tagVariantId\"].alias(\"tagVariantId\"),\n                f.filter(\n                    x[\"rValues\"], lambda y: y[\"population\"] == major_population\n                ).alias(\"rValues\"),\n            ),\n        )\n        return f.transform(\n            ld_set_with_major_pop,\n            lambda x: f.struct(\n                x[\"tagVariantId\"].alias(\"tagVariantId\"),\n                f.coalesce(f.pow(x[\"rValues\"][\"r\"][0], 2), f.lit(0.0)).alias(\n                    \"r2Overall\"\n                ),\n            ),\n        )\n\n    @staticmethod\n    def _qc_unresolved_ld(ld_set: Column, quality_controls: Column) -&gt; Column:\n        \"\"\"Flag associations with unresolved LD.\n\n        Args:\n            ld_set (Column): LD set\n            quality_controls (Column): Quality controls\n\n        Returns:\n            Column: Quality controls with added 'UNRESOLVED_LD' field\n        \"\"\"\n        return StudyLocus.update_quality_flag(\n            quality_controls,\n            ld_set.isNull(),\n            StudyLocusQualityCheck.UNRESOLVED_LD,\n        )\n\n    @staticmethod\n    def _rescue_lead_variant(ld_set: Column, variant_id: Column) -&gt; Column:\n        \"\"\"Rescue lead variant.\n\n        In cases in which no LD information is available but a lead variant is available, we include the lead as the only variant in the ldSet.\n\n        Args:\n            ld_set (Column): LD set\n            variant_id (Column): Variant ID\n\n        Returns:\n            Column: LD set with added 'tagVariantId' field\n        \"\"\"\n        return f.when(\n            ((ld_set.isNull() | (f.size(ld_set) == 0)) &amp; variant_id.isNotNull()),\n            f.array(\n                f.struct(\n                    variant_id.alias(\"tagVariantId\"),\n                    f.lit(1).alias(\"r2Overall\"),\n                )\n            ),\n        ).otherwise(ld_set)\n\n    @classmethod\n    def ld_annotate(\n        cls: type[LDAnnotator],\n        associations: StudyLocus,\n        studies: StudyIndex,\n        ld_index: LDIndex,\n        r2_threshold: float = 0.5,\n    ) -&gt; StudyLocus:\n        \"\"\"Annotate linkage disequilibrium (LD) information to a set of studyLocus.\n\n        This function:\n            1. Annotates study locus with population structure information ordered by relativeSampleSize from the study index\n            2. Joins the LD index to the StudyLocus\n            3. Gets the major population from the population structure\n            4. Calculates R2 by using the R of the major ancestry\n            5. Flags associations with variants that are not found in the LD reference\n            6. Rescues lead variant when no LD information is available but lead variant is available\n\n        !!! note\n            Because the LD index has a pre-set threshold of R2 = 0.5, this is the minimum threshold for the LD information to be included in the ldSet.\n\n        Args:\n            associations (StudyLocus): Dataset to be LD annotated\n            studies (StudyIndex): Dataset with study information\n            ld_index (LDIndex): Dataset with LD information for every variant present in LD matrix\n            r2_threshold (float): R2 threshold to filter the LD set on. Default is 0.5.\n\n        Returns:\n            StudyLocus: including additional column with LD information.\n        \"\"\"\n        return StudyLocus(\n            _df=(\n                associations.df\n                # Drop ldSet column if already available\n                .select(*[col for col in associations.df.columns if col != \"ldSet\"])\n                # Annotate study locus with population structure ordered by relativeSampleSize from study index\n                .join(\n                    studies.df.select(\n                        \"studyId\",\n                        order_array_of_structs_by_field(\n                            \"ldPopulationStructure\", \"relativeSampleSize\"\n                        ).alias(\"ldPopulationStructure\"),\n                    ),\n                    on=\"studyId\",\n                    how=\"left\",\n                )\n                # Bring LD information from LD Index\n                .join(\n                    ld_index.df,\n                    on=[\"variantId\", \"chromosome\"],\n                    how=\"left\",\n                )\n                # Get major population from population structure if population structure available\n                .withColumn(\n                    \"majorPopulation\",\n                    f.when(\n                        f.col(\"ldPopulationStructure\").isNotNull(),\n                        cls._get_major_population(f.col(\"ldPopulationStructure\")),\n                    ),\n                )\n                # Calculate R2 using R of the major population\n                .withColumn(\n                    \"ldSet\",\n                    f.when(\n                        f.col(\"ldPopulationStructure\").isNotNull(),\n                        cls._calculate_r2_major(\n                            f.col(\"ldSet\"), f.col(\"majorPopulation\")\n                        ),\n                    ),\n                )\n                .drop(\"ldPopulationStructure\", \"majorPopulation\")\n                # Filter the LD set by the R2 threshold and set to null if no LD information passes the threshold\n                .withColumn(\n                    \"ldSet\",\n                    StudyLocus.filter_ld_set(f.col(\"ldSet\"), r2_threshold),\n                )\n                .withColumn(\"ldSet\", f.when(f.size(\"ldSet\") &gt; 0, f.col(\"ldSet\")))\n                # QC: Flag associations with variants that are not found in the LD reference\n                .withColumn(\n                    \"qualityControls\",\n                    cls._qc_unresolved_ld(f.col(\"ldSet\"), f.col(\"qualityControls\")),\n                )\n                # Add lead variant to empty ldSet when no LD information is available but lead variant is available\n                .withColumn(\n                    \"ldSet\",\n                    cls._rescue_lead_variant(f.col(\"ldSet\"), f.col(\"variantId\")),\n                )\n                # Ensure that the lead varaitn is always with r2==1\n                .withColumn(\n                    \"ldSet\",\n                    f.expr(\n                        \"\"\"\n                        transform(ldSet, x -&gt;\n                            IF(x.tagVariantId == variantId,\n                                named_struct('tagVariantId', x.tagVariantId, 'r2Overall', 1.0),\n                                x\n                            )\n                        )\n                        \"\"\"\n                    ),\n                )\n            ),\n            _schema=StudyLocus.get_schema(),\n        )._qc_no_population()\n</code></pre>"},{"location":"python_api/methods/ld_annotator/#gentropy.method.ld.LDAnnotator.ld_annotate","title":"<code>ld_annotate(associations: StudyLocus, studies: StudyIndex, ld_index: LDIndex, r2_threshold: float = 0.5) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Annotate linkage disequilibrium (LD) information to a set of studyLocus.</p> This function <ol> <li>Annotates study locus with population structure information ordered by relativeSampleSize from the study index</li> <li>Joins the LD index to the StudyLocus</li> <li>Gets the major population from the population structure</li> <li>Calculates R2 by using the R of the major ancestry</li> <li>Flags associations with variants that are not found in the LD reference</li> <li>Rescues lead variant when no LD information is available but lead variant is available</li> </ol> <p>Note</p> <p>Because the LD index has a pre-set threshold of R2 = 0.5, this is the minimum threshold for the LD information to be included in the ldSet.</p> <p>Parameters:</p> Name Type Description Default <code>associations</code> <code>StudyLocus</code> <p>Dataset to be LD annotated</p> required <code>studies</code> <code>StudyIndex</code> <p>Dataset with study information</p> required <code>ld_index</code> <code>LDIndex</code> <p>Dataset with LD information for every variant present in LD matrix</p> required <code>r2_threshold</code> <code>float</code> <p>R2 threshold to filter the LD set on. Default is 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>including additional column with LD information.</p> Source code in <code>src/gentropy/method/ld.py</code> <pre><code>@classmethod\ndef ld_annotate(\n    cls: type[LDAnnotator],\n    associations: StudyLocus,\n    studies: StudyIndex,\n    ld_index: LDIndex,\n    r2_threshold: float = 0.5,\n) -&gt; StudyLocus:\n    \"\"\"Annotate linkage disequilibrium (LD) information to a set of studyLocus.\n\n    This function:\n        1. Annotates study locus with population structure information ordered by relativeSampleSize from the study index\n        2. Joins the LD index to the StudyLocus\n        3. Gets the major population from the population structure\n        4. Calculates R2 by using the R of the major ancestry\n        5. Flags associations with variants that are not found in the LD reference\n        6. Rescues lead variant when no LD information is available but lead variant is available\n\n    !!! note\n        Because the LD index has a pre-set threshold of R2 = 0.5, this is the minimum threshold for the LD information to be included in the ldSet.\n\n    Args:\n        associations (StudyLocus): Dataset to be LD annotated\n        studies (StudyIndex): Dataset with study information\n        ld_index (LDIndex): Dataset with LD information for every variant present in LD matrix\n        r2_threshold (float): R2 threshold to filter the LD set on. Default is 0.5.\n\n    Returns:\n        StudyLocus: including additional column with LD information.\n    \"\"\"\n    return StudyLocus(\n        _df=(\n            associations.df\n            # Drop ldSet column if already available\n            .select(*[col for col in associations.df.columns if col != \"ldSet\"])\n            # Annotate study locus with population structure ordered by relativeSampleSize from study index\n            .join(\n                studies.df.select(\n                    \"studyId\",\n                    order_array_of_structs_by_field(\n                        \"ldPopulationStructure\", \"relativeSampleSize\"\n                    ).alias(\"ldPopulationStructure\"),\n                ),\n                on=\"studyId\",\n                how=\"left\",\n            )\n            # Bring LD information from LD Index\n            .join(\n                ld_index.df,\n                on=[\"variantId\", \"chromosome\"],\n                how=\"left\",\n            )\n            # Get major population from population structure if population structure available\n            .withColumn(\n                \"majorPopulation\",\n                f.when(\n                    f.col(\"ldPopulationStructure\").isNotNull(),\n                    cls._get_major_population(f.col(\"ldPopulationStructure\")),\n                ),\n            )\n            # Calculate R2 using R of the major population\n            .withColumn(\n                \"ldSet\",\n                f.when(\n                    f.col(\"ldPopulationStructure\").isNotNull(),\n                    cls._calculate_r2_major(\n                        f.col(\"ldSet\"), f.col(\"majorPopulation\")\n                    ),\n                ),\n            )\n            .drop(\"ldPopulationStructure\", \"majorPopulation\")\n            # Filter the LD set by the R2 threshold and set to null if no LD information passes the threshold\n            .withColumn(\n                \"ldSet\",\n                StudyLocus.filter_ld_set(f.col(\"ldSet\"), r2_threshold),\n            )\n            .withColumn(\"ldSet\", f.when(f.size(\"ldSet\") &gt; 0, f.col(\"ldSet\")))\n            # QC: Flag associations with variants that are not found in the LD reference\n            .withColumn(\n                \"qualityControls\",\n                cls._qc_unresolved_ld(f.col(\"ldSet\"), f.col(\"qualityControls\")),\n            )\n            # Add lead variant to empty ldSet when no LD information is available but lead variant is available\n            .withColumn(\n                \"ldSet\",\n                cls._rescue_lead_variant(f.col(\"ldSet\"), f.col(\"variantId\")),\n            )\n            # Ensure that the lead varaitn is always with r2==1\n            .withColumn(\n                \"ldSet\",\n                f.expr(\n                    \"\"\"\n                    transform(ldSet, x -&gt;\n                        IF(x.tagVariantId == variantId,\n                            named_struct('tagVariantId', x.tagVariantId, 'r2Overall', 1.0),\n                            x\n                        )\n                    )\n                    \"\"\"\n                ),\n            )\n        ),\n        _schema=StudyLocus.get_schema(),\n    )._qc_no_population()\n</code></pre>"},{"location":"python_api/methods/pics/","title":"PICS","text":"<p>PICS Overview:</p> <p>PICS is a fine-mapping method designed to identify the most likely causal SNPs associated with a trait or disease within a genomic region. It leverages both haplotype information and the observed association patterns from genome-wide association studies (GWAS).</p> <p>Please refer to the original publication for in-depth details: PICS Publication.</p> <p>We use PICS for both GWAS clumping results and GWAS curated studies.</p>"},{"location":"python_api/methods/pics/#gentropy.method.pics.PICS","title":"<code>gentropy.method.pics.PICS</code>","text":"<p>Probabilistic Identification of Causal SNPs (PICS), an algorithm estimating the probability that an individual variant is causal considering the haplotype structure and observed pattern of association at the genetic locus.</p> Source code in <code>src/gentropy/method/pics.py</code> <pre><code>class PICS:\n    \"\"\"Probabilistic Identification of Causal SNPs (PICS), an algorithm estimating the probability that an individual variant is causal considering the haplotype structure and observed pattern of association at the genetic locus.\"\"\"\n\n    # The fields for the picsed locus + ldSet tagVariantId is renamed to variantId:\n    PICSED_LOCUS_SCHEMA = t.ArrayType(\n        t.StructType(\n            [\n                t.StructField(\"variantId\", t.StringType(), True),\n                t.StructField(\"r2Overall\", t.DoubleType(), True),\n                t.StructField(\"posteriorProbability\", t.DoubleType(), True),\n                t.StructField(\"standardError\", t.DoubleType(), True),\n            ]\n        )\n    )\n\n    @staticmethod\n    def _pics_relative_posterior_probability(\n        neglog_p: float, pics_snp_mu: float, pics_snp_std: float\n    ) -&gt; float:\n        \"\"\"Compute the PICS posterior probability for a given SNP.\n\n        !!! info \"This probability needs to be scaled to take into account the probabilities of the other variants in the locus.\"\n\n        Args:\n            neglog_p (float): Negative log p-value of the lead variant\n            pics_snp_mu (float): Mean P value of the association between a SNP and a trait\n            pics_snp_std (float): Standard deviation for the P value of the association between a SNP and a trait\n\n        Returns:\n            float: Posterior probability of the association between a SNP and a trait\n\n        Examples:\n            &gt;&gt;&gt; rel_prob = PICS._pics_relative_posterior_probability(neglog_p=10.0, pics_snp_mu=1.0, pics_snp_std=10.0)\n            &gt;&gt;&gt; round(rel_prob, 3)\n            0.368\n        \"\"\"\n        return float(norm(pics_snp_mu, pics_snp_std).sf(neglog_p) * 2)\n\n    @staticmethod\n    def _pics_standard_deviation(neglog_p: float, r2: float, k: float) -&gt; float | None:\n        \"\"\"Compute the PICS standard deviation.\n\n        This distribution is obtained after a series of permutation tests described in the PICS method, and it is only\n        valid when the SNP is highly linked with the lead (r2 &gt; 0.5).\n\n        Args:\n            neglog_p (float): Negative log p-value of the lead variant\n            r2 (float): LD score between a given SNP and the lead variant\n            k (float): Empiric constant that can be adjusted to fit the curve, 6.4 recommended.\n\n        Returns:\n            float | None: Standard deviation for the P value of the association between a SNP and a trait\n\n        Examples:\n            &gt;&gt;&gt; PICS._pics_standard_deviation(neglog_p=1.0, r2=1.0, k=6.4)\n            0.0\n            &gt;&gt;&gt; round(PICS._pics_standard_deviation(neglog_p=10.0, r2=0.5, k=6.4), 3)\n            1.493\n            &gt;&gt;&gt; print(PICS._pics_standard_deviation(neglog_p=1.0, r2=0.0, k=6.4))\n            None\n        \"\"\"\n        return (\n            abs(((1 - (r2**0.5) ** k) ** 0.5) * (neglog_p**0.5) / 2)\n            if r2 &gt;= 0.5\n            else None\n        )\n\n    @staticmethod\n    def _pics_mu(neglog_p: float, r2: float) -&gt; float | None:\n        \"\"\"Compute the PICS mu that estimates the probability of association between a given SNP and the trait.\n\n        This distribution is obtained after a series of permutation tests described in the PICS method, and it is only\n        valid when the SNP is highly linked with the lead (r2 &gt; 0.5).\n\n        Args:\n            neglog_p (float): Negative log p-value of the lead variant\n            r2 (float): LD score between a given SNP and the lead variant\n\n        Returns:\n            float | None: Mean P value of the association between a SNP and a trait\n\n        Examples:\n            &gt;&gt;&gt; PICS._pics_mu(neglog_p=1.0, r2=1.0)\n            1.0\n            &gt;&gt;&gt; PICS._pics_mu(neglog_p=10.0, r2=0.5)\n            5.0\n            &gt;&gt;&gt; print(PICS._pics_mu(neglog_p=10.0, r2=0.3))\n            None\n        \"\"\"\n        return neglog_p * r2 if r2 &gt;= 0.5 else None\n\n    @staticmethod\n    def _finemap(\n        ld_set: list[Row], lead_neglog_p: float, k: float\n    ) -&gt; list[dict[str, Any]] | None:\n        \"\"\"Calculates the probability of a variant being causal in a study-locus context by applying the PICS method.\n\n        It is intended to be applied as an UDF in `PICS.finemap`, where each row is a StudyLocus association.\n        The function iterates over every SNP in the `ldSet` array, and it returns an updated locus with\n        its association signal and causality probability as of PICS.\n\n        Args:\n            ld_set (list[Row]): list of tagging variants after expanding the locus\n            lead_neglog_p (float): P value of the association signal between the lead variant and the study in the form of -log10.\n            k (float): Empiric constant that can be adjusted to fit the curve, 6.4 recommended.\n\n        Returns:\n            list[dict[str, Any]] | None: List of tagging variants with an estimation of the association signal and their posterior probability as of PICS.\n\n        Examples:\n            &gt;&gt;&gt; from pyspark.sql import Row\n            &gt;&gt;&gt; ld_set = [\n            ...     Row(variantId=\"var1\", r2Overall=0.8),\n            ...     Row(variantId=\"var2\", r2Overall=1),\n            ... ]\n            &gt;&gt;&gt; result = PICS._finemap(ld_set, lead_neglog_p=10.0, k=6.4)\n            &gt;&gt;&gt; [list(v.keys()) for v in result]\n            [['variantId', 'r2Overall', 'standardError', 'posteriorProbability'], ['variantId', 'r2Overall', 'standardError', 'posteriorProbability']]\n            &gt;&gt;&gt; [v[\"variantId\"] for v in result]\n            ['var1', 'var2']\n            &gt;&gt;&gt; [round(v[\"posteriorProbability\"], 3) for v in result]\n            [0.071, 0.929]\n            &gt;&gt;&gt; [v[\"r2Overall\"] for v in result]\n            [0.8, 1]\n            &gt;&gt;&gt; [round(v[\"standardError\"], 3) for v in result]\n            [0.074, 0.998]\n            &gt;&gt;&gt; empty_ld_set = []\n            &gt;&gt;&gt; PICS._finemap(empty_ld_set, lead_neglog_p=10.0, k=6.4)\n            []\n            &gt;&gt;&gt; ld_set_with_no_r2 = [\n            ...     Row(variantId=\"var1\", r2Overall=None),\n            ...     Row(variantId=\"var2\", r2Overall=None),\n            ... ]\n            &gt;&gt;&gt; PICS._finemap(ld_set_with_no_r2, lead_neglog_p=10.0, k=6.4)\n            []\n        \"\"\"\n        if ld_set is None:\n            return None\n        elif not ld_set:\n            return []\n        tmp_credible_set = []\n        new_credible_set = []\n        # First iteration: calculation of mu, standard deviation, and the relative posterior probability\n        for tag_struct in ld_set:\n            tag_dict = (\n                tag_struct.asDict()\n            )  # tag_struct is of type pyspark.Row, we'll represent it as a dict\n            if (\n                not tag_dict[\"r2Overall\"]\n                or tag_dict[\"r2Overall\"] &lt; 0.5\n                or not lead_neglog_p\n            ):\n                # If PICS cannot be calculated, we drop the variant from the credible set\n                continue\n\n            # Chaing chema:\n            if \"tagVariantId\" in tag_dict:\n                tag_dict[\"variantId\"] = tag_dict.pop(\"tagVariantId\")\n\n            pics_snp_mu = PICS._pics_mu(lead_neglog_p, tag_dict[\"r2Overall\"])\n            pics_snp_std = PICS._pics_standard_deviation(\n                lead_neglog_p, tag_dict[\"r2Overall\"], k\n            )\n            pics_snp_std = 0.001 if pics_snp_std == 0 else pics_snp_std\n            if pics_snp_mu is not None and pics_snp_std is not None:\n                posterior_probability = PICS._pics_relative_posterior_probability(\n                    lead_neglog_p, pics_snp_mu, pics_snp_std\n                )\n                tag_dict[\"standardError\"] = 10**-pics_snp_std\n                tag_dict[\"relativePosteriorProbability\"] = posterior_probability\n\n                tmp_credible_set.append(tag_dict)\n\n        # Second iteration: calculation of the sum of all the posteriors in each study-locus, so that we scale them between 0-1\n        total_posteriors = sum(\n            tag_dict.get(\"relativePosteriorProbability\", 0)\n            for tag_dict in tmp_credible_set\n        )\n\n        # Third iteration: calculation of the final posteriorProbability\n        for tag_dict in tmp_credible_set:\n            if total_posteriors != 0:\n                tag_dict[\"posteriorProbability\"] = float(\n                    tag_dict.get(\"relativePosteriorProbability\", 0) / total_posteriors\n                )\n            tag_dict.pop(\"relativePosteriorProbability\")\n            new_credible_set.append(tag_dict)\n        return new_credible_set\n\n    @classmethod\n    def finemap(\n        cls: type[PICS], associations: StudyLocus, k: float = 6.4\n    ) -&gt; StudyLocus:\n        \"\"\"Run PICS on a study locus.\n\n        !!! info \"Study locus needs to be LD annotated\"\n\n            The study locus needs to be LD annotated before PICS can be calculated.\n\n        Args:\n            associations (StudyLocus): Study locus to finemap using PICS\n            k (float): Empiric constant that can be adjusted to fit the curve, 6.4 recommended.\n\n        Returns:\n            StudyLocus: Study locus with PICS results\n        \"\"\"\n        # Finemapping method is an optional column:\n        finemapping_method_expression = (\n            f.lit(FinemappingMethod.PICS.value)\n            if \"finemappingMethod\" not in associations.df.columns\n            else f.coalesce(\n                f.col(\"finemappingMethod\"), f.lit(FinemappingMethod.PICS.value)\n            )\n        )\n\n        # Registering the UDF to be used in the pipeline:\n        finemap_udf = f.udf(\n            lambda ld_set, neglog_p: cls._finemap(ld_set, neglog_p, k),\n            cls.PICSED_LOCUS_SCHEMA,\n        )\n\n        return StudyLocus(\n            _df=(\n                associations.df\n                # Old locus column will be dropped if available\n                .select(*[col for col in associations.df.columns if col != \"locus\"])\n                # Estimate neglog_pvalue for the lead variant\n                .withColumn(\"neglog_pvalue\", associations.neglog_pvalue())\n                # New locus containing the PICS results\n                .withColumn(\n                    \"locus\",\n                    f.when(\n                        f.col(\"ldSet\").isNotNull(),\n                        finemap_udf(f.col(\"ldSet\"), f.col(\"neglog_pvalue\")),\n                    ),\n                )\n                # Updating single point statistics in the locus object for the lead variant:\n                .withColumn(\n                    \"locus\",\n                    f.transform(\n                        f.col(\"locus\"),\n                        lambda tag: f.when(\n                            f.col(\"variantId\") == tag[\"variantId\"],\n                            tag.withField(\"pValueMantissa\", f.col(\"pValueMantissa\"))\n                            .withField(\"pValueExponent\", f.col(\"pValueExponent\"))\n                            .withField(\"beta\", f.col(\"beta\")),\n                        ).otherwise(\n                            tag.withField(\n                                \"pValueMantissa\", f.lit(None).cast(t.FloatType())\n                            )\n                            .withField(\n                                \"pValueExponent\", f.lit(None).cast(t.IntegerType())\n                            )\n                            .withField(\"beta\", f.lit(None).cast(t.DoubleType()))\n                        ),\n                    ),\n                )\n                # Flagging all PICS loci with OUT_OF_SAMPLE_LD flag:\n                .withColumn(\n                    \"qualityControls\",\n                    StudyLocus.update_quality_flag(\n                        f.col(\"qualityControls\"),\n                        f.lit(True),\n                        StudyLocusQualityCheck.OUT_OF_SAMPLE_LD,\n                    ),\n                )\n                .withColumn(\n                    \"finemappingMethod\",\n                    finemapping_method_expression,\n                )\n                .withColumn(\n                    \"studyLocusId\",\n                    StudyLocus.assign_study_locus_id(\n                        [\"studyId\", \"variantId\", \"finemappingMethod\"]\n                    ),\n                )\n                .drop(\"neglog_pvalue\")\n            ),\n            _schema=StudyLocus.get_schema(),\n        )\n</code></pre>"},{"location":"python_api/methods/pics/#gentropy.method.pics.PICS.finemap","title":"<code>finemap(associations: StudyLocus, k: float = 6.4) -&gt; StudyLocus</code>  <code>classmethod</code>","text":"<p>Run PICS on a study locus.</p> <p>Study locus needs to be LD annotated</p> <p>The study locus needs to be LD annotated before PICS can be calculated.</p> <p>Parameters:</p> Name Type Description Default <code>associations</code> <code>StudyLocus</code> <p>Study locus to finemap using PICS</p> required <code>k</code> <code>float</code> <p>Empiric constant that can be adjusted to fit the curve, 6.4 recommended.</p> <code>6.4</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Study locus with PICS results</p> Source code in <code>src/gentropy/method/pics.py</code> <pre><code>@classmethod\ndef finemap(\n    cls: type[PICS], associations: StudyLocus, k: float = 6.4\n) -&gt; StudyLocus:\n    \"\"\"Run PICS on a study locus.\n\n    !!! info \"Study locus needs to be LD annotated\"\n\n        The study locus needs to be LD annotated before PICS can be calculated.\n\n    Args:\n        associations (StudyLocus): Study locus to finemap using PICS\n        k (float): Empiric constant that can be adjusted to fit the curve, 6.4 recommended.\n\n    Returns:\n        StudyLocus: Study locus with PICS results\n    \"\"\"\n    # Finemapping method is an optional column:\n    finemapping_method_expression = (\n        f.lit(FinemappingMethod.PICS.value)\n        if \"finemappingMethod\" not in associations.df.columns\n        else f.coalesce(\n            f.col(\"finemappingMethod\"), f.lit(FinemappingMethod.PICS.value)\n        )\n    )\n\n    # Registering the UDF to be used in the pipeline:\n    finemap_udf = f.udf(\n        lambda ld_set, neglog_p: cls._finemap(ld_set, neglog_p, k),\n        cls.PICSED_LOCUS_SCHEMA,\n    )\n\n    return StudyLocus(\n        _df=(\n            associations.df\n            # Old locus column will be dropped if available\n            .select(*[col for col in associations.df.columns if col != \"locus\"])\n            # Estimate neglog_pvalue for the lead variant\n            .withColumn(\"neglog_pvalue\", associations.neglog_pvalue())\n            # New locus containing the PICS results\n            .withColumn(\n                \"locus\",\n                f.when(\n                    f.col(\"ldSet\").isNotNull(),\n                    finemap_udf(f.col(\"ldSet\"), f.col(\"neglog_pvalue\")),\n                ),\n            )\n            # Updating single point statistics in the locus object for the lead variant:\n            .withColumn(\n                \"locus\",\n                f.transform(\n                    f.col(\"locus\"),\n                    lambda tag: f.when(\n                        f.col(\"variantId\") == tag[\"variantId\"],\n                        tag.withField(\"pValueMantissa\", f.col(\"pValueMantissa\"))\n                        .withField(\"pValueExponent\", f.col(\"pValueExponent\"))\n                        .withField(\"beta\", f.col(\"beta\")),\n                    ).otherwise(\n                        tag.withField(\n                            \"pValueMantissa\", f.lit(None).cast(t.FloatType())\n                        )\n                        .withField(\n                            \"pValueExponent\", f.lit(None).cast(t.IntegerType())\n                        )\n                        .withField(\"beta\", f.lit(None).cast(t.DoubleType()))\n                    ),\n                ),\n            )\n            # Flagging all PICS loci with OUT_OF_SAMPLE_LD flag:\n            .withColumn(\n                \"qualityControls\",\n                StudyLocus.update_quality_flag(\n                    f.col(\"qualityControls\"),\n                    f.lit(True),\n                    StudyLocusQualityCheck.OUT_OF_SAMPLE_LD,\n                ),\n            )\n            .withColumn(\n                \"finemappingMethod\",\n                finemapping_method_expression,\n            )\n            .withColumn(\n                \"studyLocusId\",\n                StudyLocus.assign_study_locus_id(\n                    [\"studyId\", \"variantId\", \"finemappingMethod\"]\n                ),\n            )\n            .drop(\"neglog_pvalue\")\n        ),\n        _schema=StudyLocus.get_schema(),\n    )\n</code></pre>"},{"location":"python_api/methods/sumstat_imputation/","title":"Summary Statistics Imputation","text":"<p>Summary statistics imputation leverages linkage disequilibrium (LD) information to compute Z-scores of missing SNPs from neighbouring observed SNPs SNPs by taking advantage of the Linkage Disequilibrium.</p> <p>We implemented the basic model from RAISS (Robust and Accurate Imputation from Summary Statistics) package (see the original paper).</p> <p>The full repository for the RAISS package can be found here.</p> <p>The original model was suggested in 2014 by Bogdan Pasaniuc et al. here.</p> <p>It represents the following formula:</p> <p>E(zi|z_t) = M{i,t} \\cdot (M_{t,t})^{-1} \\cdot z_t</p> <p>Where:</p> <ul> <li> <p>E(z_i|z_t) represents the expected z-score of SNP 'i' given the observed z-scores at known SNP indexes 't'.</p> </li> <li> <p>M_{i,t} represents the LD (Linkage Disequilibrium) matrix between SNP 'i' and the known SNPs at indexes 't'.</p> </li> <li> <p>(M_{t,t})^{-1} represents the inverse of the LD matrix of the known SNPs at indexes 't'.</p> </li> <li> <p>z_t represents the vector of observed z-scores at the known SNP indexes 't'.</p> </li> </ul>"},{"location":"python_api/methods/sumstat_imputation/#gentropy.method.sumstat_imputation.SummaryStatisticsImputation","title":"<code>gentropy.method.sumstat_imputation.SummaryStatisticsImputation</code>","text":"<p>Implementation of RAISS summary statstics imputation model.</p> Source code in <code>src/gentropy/method/sumstat_imputation.py</code> <pre><code>class SummaryStatisticsImputation:\n    \"\"\"Implementation of RAISS summary statstics imputation model.\"\"\"\n\n    @staticmethod\n    def raiss_model(\n        z_scores_known: np.ndarray,\n        ld_matrix_known: np.ndarray,\n        ld_matrix_known_missing: np.ndarray,\n        lamb: float = 0.01,\n        rtol: float = 0.01,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Compute the imputation of the z-score using the RAISS model.\n\n        Args:\n            z_scores_known (np.ndarray): the vector of known Z scores\n            ld_matrix_known (np.ndarray) : the matrix of known LD correlations\n            ld_matrix_known_missing (np.ndarray): LD matrix of known SNPs with other unknown SNPs in large matrix (similar to `ld[unknowns, :][:,known]`)\n            lamb (float): size of the small value added to the diagonal of the covariance matrix before inversion. Defaults to 0.01.\n            rtol (float): threshold to filter eigenvectos by its eigenvalue. It makes an inversion biased but much more numerically robust. Default to 0.01.\n\n        Returns:\n            dict[str, Any]:\n                - var (np.ndarray): variance of the imputed SNPs\n                - mu (np.ndarray): the estimation of the zscore of the imputed SNPs\n                - ld_score (np.ndarray): the linkage disequilibrium score of the imputed SNPs\n                - condition_number (np.ndarray): the condition number of the correlation matrix\n                - correct_inversion (np.ndarray): a boolean array indicating if the inversion was successful\n                - imputation_r2 (np.ndarray): the R2 of the imputation\n        \"\"\"\n        sig_t_inv = SummaryStatisticsImputation._invert_sig_t(\n            ld_matrix_known, lamb, rtol\n        )\n        if sig_t_inv is None:\n            return {\n                \"var\": None,\n                \"mu\": None,\n                \"ld_score\": None,\n                \"condition_number\": None,\n                \"correct_inversion\": None,\n                \"imputation_r2\": None,\n            }\n        else:\n            condition_number = np.array(\n                [np.linalg.cond(ld_matrix_known)] * ld_matrix_known_missing.shape[0]\n            )\n            correct_inversion = np.array(\n                [\n                    SummaryStatisticsImputation._check_inversion(\n                        ld_matrix_known, sig_t_inv\n                    )\n                ]\n                * ld_matrix_known_missing.shape[0]\n            )\n\n            var, ld_score = SummaryStatisticsImputation._compute_var(\n                ld_matrix_known_missing, sig_t_inv, lamb\n            )\n\n            mu = SummaryStatisticsImputation._compute_mu(\n                ld_matrix_known_missing, sig_t_inv, z_scores_known\n            )\n            var_norm = SummaryStatisticsImputation._var_in_boundaries(var, lamb)\n\n            R2 = (1 + lamb) - var_norm\n\n            mu = mu / np.sqrt(R2)\n            return {\n                \"var\": var,\n                \"mu\": mu,\n                \"ld_score\": ld_score,\n                \"condition_number\": condition_number,\n                \"correct_inversion\": correct_inversion,\n                \"imputation_r2\": 1 - var,\n            }\n\n    @staticmethod\n    def _compute_mu(\n        sig_i_t: np.ndarray, sig_t_inv: np.ndarray, zt: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the estimation of z-score from neighborring snp.\n\n        Args:\n            sig_i_t (np.ndarray) : correlation matrix with line corresponding to unknown Snp (snp to impute) and column to known SNPs\n            sig_t_inv (np.ndarray): inverse of the correlation matrix of known matrix\n            zt (np.ndarray): Zscores of known snp\n        Returns:\n            np.ndarray: a vector of length i containing the estimate of zscore\n\n        \"\"\"\n        return np.dot(sig_i_t, np.dot(sig_t_inv, zt))\n\n    @staticmethod\n    def _compute_var(\n        sig_i_t: np.ndarray, sig_t_inv: np.ndarray, lamb: float\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Compute the expected variance of the imputed SNPs.\n\n        Args:\n            sig_i_t (np.ndarray) : correlation matrix with line corresponding to unknown Snp (snp to impute) and column to known SNPs\n            sig_t_inv (np.ndarray): inverse of the correlation matrix of known matrix\n            lamb (float): regularization term added to matrix\n\n        Returns:\n            tuple[np.ndarray, np.ndarray]: a tuple containing the variance and the ld score\n        \"\"\"\n        var = (1 + lamb) - np.einsum(\n            \"ij,jk,ki-&gt;i\", sig_i_t, sig_t_inv, sig_i_t.transpose()\n        )\n        ld_score = (sig_i_t**2).sum(1)\n\n        return var, ld_score\n\n    @staticmethod\n    def _check_inversion(sig_t: np.ndarray, sig_t_inv: np.ndarray) -&gt; bool:\n        \"\"\"Check if the inversion is correct.\n\n        Args:\n            sig_t (np.ndarray): the correlation matrix\n            sig_t_inv (np.ndarray): the inverse of the correlation matrix\n        Returns:\n            bool: True if the inversion is correct, False otherwise\n        \"\"\"\n        return np.allclose(sig_t, np.dot(sig_t, np.dot(sig_t_inv, sig_t)))\n\n    @staticmethod\n    def _var_in_boundaries(var: np.ndarray, lamb: float) -&gt; np.ndarray:\n        \"\"\"Forces the variance to be in the 0 to 1+lambda boundary. Theoritically we shouldn't have to do that.\n\n        Args:\n            var (np.ndarray): the variance of the imputed SNPs\n            lamb (float): regularization term added to the diagonal of the sig_t matrix\n\n        Returns:\n            np.ndarray: the variance of the imputed SNPs\n        \"\"\"\n        id_neg = np.where(var &lt; 0)\n        var[id_neg] = 0\n        id_inf = np.where(var &gt; (0.99999 + lamb))\n        var[id_inf] = 1\n\n        return var\n\n    @staticmethod\n    def _invert_sig_t(sig_t: np.ndarray, lamb: float, rtol: float) -&gt; np.ndarray:\n        \"\"\"Invert the correlation matrix. If the provided regularization values are not enough to stabilize the inversion process for the given matrix, the function calls itself recursively, increasing lamb and rtol by 10%.\n\n        Args:\n            sig_t (np.ndarray): the correlation matrix\n            lamb (float): regularization term added to the diagonal of the sig_t matrix\n            rtol (float): threshold to filter eigenvector with a eigenvalue under rtol make inversion biased but much more numerically robust\n\n        Returns:\n            np.ndarray: the inverse of the correlation matrix\n        \"\"\"\n        try:\n            np.fill_diagonal(sig_t, (1 + lamb))\n            sig_t_inv = scipy.linalg.pinv(sig_t, rtol=rtol, atol=0)\n            return sig_t_inv\n        except np.linalg.LinAlgError:\n            return SummaryStatisticsImputation._invert_sig_t(\n                sig_t, lamb * 1.1, rtol * 1.1\n            )\n</code></pre>"},{"location":"python_api/methods/sumstat_imputation/#gentropy.method.sumstat_imputation.SummaryStatisticsImputation.raiss_model","title":"<code>raiss_model(z_scores_known: np.ndarray, ld_matrix_known: np.ndarray, ld_matrix_known_missing: np.ndarray, lamb: float = 0.01, rtol: float = 0.01) -&gt; dict[str, Any]</code>  <code>staticmethod</code>","text":"<p>Compute the imputation of the z-score using the RAISS model.</p> <p>Parameters:</p> Name Type Description Default <code>z_scores_known</code> <code>ndarray</code> <p>the vector of known Z scores</p> required <code>ld_matrix_known (np.ndarray) </code> <p>the matrix of known LD correlations</p> required <code>ld_matrix_known_missing</code> <code>ndarray</code> <p>LD matrix of known SNPs with other unknown SNPs in large matrix (similar to <code>ld[unknowns, :][:,known]</code>)</p> required <code>lamb</code> <code>float</code> <p>size of the small value added to the diagonal of the covariance matrix before inversion. Defaults to 0.01.</p> <code>0.01</code> <code>rtol</code> <code>float</code> <p>threshold to filter eigenvectos by its eigenvalue. It makes an inversion biased but much more numerically robust. Default to 0.01.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: - var (np.ndarray): variance of the imputed SNPs - mu (np.ndarray): the estimation of the zscore of the imputed SNPs - ld_score (np.ndarray): the linkage disequilibrium score of the imputed SNPs - condition_number (np.ndarray): the condition number of the correlation matrix - correct_inversion (np.ndarray): a boolean array indicating if the inversion was successful - imputation_r2 (np.ndarray): the R2 of the imputation</p> Source code in <code>src/gentropy/method/sumstat_imputation.py</code> <pre><code>@staticmethod\ndef raiss_model(\n    z_scores_known: np.ndarray,\n    ld_matrix_known: np.ndarray,\n    ld_matrix_known_missing: np.ndarray,\n    lamb: float = 0.01,\n    rtol: float = 0.01,\n) -&gt; dict[str, Any]:\n    \"\"\"Compute the imputation of the z-score using the RAISS model.\n\n    Args:\n        z_scores_known (np.ndarray): the vector of known Z scores\n        ld_matrix_known (np.ndarray) : the matrix of known LD correlations\n        ld_matrix_known_missing (np.ndarray): LD matrix of known SNPs with other unknown SNPs in large matrix (similar to `ld[unknowns, :][:,known]`)\n        lamb (float): size of the small value added to the diagonal of the covariance matrix before inversion. Defaults to 0.01.\n        rtol (float): threshold to filter eigenvectos by its eigenvalue. It makes an inversion biased but much more numerically robust. Default to 0.01.\n\n    Returns:\n        dict[str, Any]:\n            - var (np.ndarray): variance of the imputed SNPs\n            - mu (np.ndarray): the estimation of the zscore of the imputed SNPs\n            - ld_score (np.ndarray): the linkage disequilibrium score of the imputed SNPs\n            - condition_number (np.ndarray): the condition number of the correlation matrix\n            - correct_inversion (np.ndarray): a boolean array indicating if the inversion was successful\n            - imputation_r2 (np.ndarray): the R2 of the imputation\n    \"\"\"\n    sig_t_inv = SummaryStatisticsImputation._invert_sig_t(\n        ld_matrix_known, lamb, rtol\n    )\n    if sig_t_inv is None:\n        return {\n            \"var\": None,\n            \"mu\": None,\n            \"ld_score\": None,\n            \"condition_number\": None,\n            \"correct_inversion\": None,\n            \"imputation_r2\": None,\n        }\n    else:\n        condition_number = np.array(\n            [np.linalg.cond(ld_matrix_known)] * ld_matrix_known_missing.shape[0]\n        )\n        correct_inversion = np.array(\n            [\n                SummaryStatisticsImputation._check_inversion(\n                    ld_matrix_known, sig_t_inv\n                )\n            ]\n            * ld_matrix_known_missing.shape[0]\n        )\n\n        var, ld_score = SummaryStatisticsImputation._compute_var(\n            ld_matrix_known_missing, sig_t_inv, lamb\n        )\n\n        mu = SummaryStatisticsImputation._compute_mu(\n            ld_matrix_known_missing, sig_t_inv, z_scores_known\n        )\n        var_norm = SummaryStatisticsImputation._var_in_boundaries(var, lamb)\n\n        R2 = (1 + lamb) - var_norm\n\n        mu = mu / np.sqrt(R2)\n        return {\n            \"var\": var,\n            \"mu\": mu,\n            \"ld_score\": ld_score,\n            \"condition_number\": condition_number,\n            \"correct_inversion\": correct_inversion,\n            \"imputation_r2\": 1 - var,\n        }\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/","title":"QC of GWAS Summary Statistics","text":"<p>This class consists of several general quality control checks for GWAS with full summary statistics. There are several checks included:</p> <ol> <li> <p>Genomic control lambda (median of the distribution of Chi2 statistics divided by expected for Chi2 with df=1). Lambda should be reasonably close to 1. Ideally not bigger than 2.</p> </li> <li> <p>P-Z check: the linear regression between log10 of reported p-values and log10 of p-values inferred from betas and standard errors. Intercept of the regression should be close to 0, slope close to 1.</p> </li> <li> <p>Mean beta check: mean of beta. Should be close to 0.</p> </li> <li> <p>The N_eff check: It estimates the ratio between effective sample size and the expected one and checks its distribution. It is possible to conduct only if the effective allele frequency is provided in the study. The median ratio is always close to 1, standard error should be close to 0.</p> </li> <li> <p>Number of SNPs and number of significant SNPs.</p> </li> </ol>"},{"location":"python_api/methods/sumstat_quality_controls/#summary-statistics-qc-checks","title":"Summary Statistics QC checks","text":""},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.gc_lambda_check","title":"<code>gentropy.method.sumstat_quality_controls.gc_lambda_check(gwas_for_qc: DataFrame) -&gt; DataFrame</code>","text":"<p>The genomic control lambda check for QC of GWAS summary statistics.</p> <p>The genomic control lambda is a measure of the inflation of test statistics in a GWAS. It is calculated as the ratio of the median of the squared test statistics to the expected median under the null hypothesis. The expected median under the null hypothesis is calculated using the chi-squared distribution with 1 degree of freedom.</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>DataFrame</code> <p>Dataframe with <code>studyId</code>, <code>beta</code>, and <code>standardError</code> columns.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the genomic control lambda for each study.</p> Warning <p>High lambda values indicate inflation of test statistics, which may be due to the population stratification or other confounding factors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s = 'studyId STRING, beta DOUBLE, standardError DOUBLE'\n&gt;&gt;&gt; d1 = [(\"S1\", 1.81, 0.2), (\"S1\", -0.1, 0.2)]\n&gt;&gt;&gt; d2 = [(\"S2\", 1.0, 0.1), (\"S2\", 1.0, 0.1)]\n&gt;&gt;&gt; df = spark.createDataFrame(d1 + d2, s)\n&gt;&gt;&gt; df.show()\n+-------+----+-------------+\n|studyId|beta|standardError|\n+-------+----+-------------+\n|     S1|1.81|          0.2|\n|     S1|-0.1|          0.2|\n|     S2| 1.0|          0.1|\n|     S2| 1.0|          0.1|\n+-------+----+-------------+\n</code></pre> <p>This method outputs one value per study</p> <pre><code>&gt;&gt;&gt; gc_lambda = f.round(\"gc_lambda\", 2).alias(\"gc_lambda\")\n&gt;&gt;&gt; gc_lambda_check(df).select(\"studyId\", gc_lambda).show()\n+-------+---------+\n|studyId|gc_lambda|\n+-------+---------+\n|     S1|     0.55|\n|     S2|   219.81|\n+-------+---------+\n</code></pre> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>def gc_lambda_check(gwas_for_qc: DataFrame) -&gt; DataFrame:\n    \"\"\"The genomic control lambda check for QC of GWAS summary statistics.\n\n    The genomic control lambda is a measure of the inflation of test statistics in a GWAS.\n    It is calculated as the ratio of the median of the squared test statistics to the expected median under the null hypothesis.\n    The expected median under the null hypothesis is calculated using the chi-squared distribution with 1 degree of freedom.\n\n    Args:\n        gwas_for_qc (DataFrame): Dataframe with `studyId`, `beta`, and `standardError` columns.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the genomic control lambda for each study.\n\n    Warning:\n        High lambda values indicate inflation of test statistics, which may be due to the population stratification or other confounding factors.\n\n    Examples:\n        &gt;&gt;&gt; s = 'studyId STRING, beta DOUBLE, standardError DOUBLE'\n        &gt;&gt;&gt; d1 = [(\"S1\", 1.81, 0.2), (\"S1\", -0.1, 0.2)]\n        &gt;&gt;&gt; d2 = [(\"S2\", 1.0, 0.1), (\"S2\", 1.0, 0.1)]\n        &gt;&gt;&gt; df = spark.createDataFrame(d1 + d2, s)\n        &gt;&gt;&gt; df.show()\n        +-------+----+-------------+\n        |studyId|beta|standardError|\n        +-------+----+-------------+\n        |     S1|1.81|          0.2|\n        |     S1|-0.1|          0.2|\n        |     S2| 1.0|          0.1|\n        |     S2| 1.0|          0.1|\n        +-------+----+-------------+\n        &lt;BLANKLINE&gt;\n\n        **This method outputs one value per study**\n\n        &gt;&gt;&gt; gc_lambda = f.round(\"gc_lambda\", 2).alias(\"gc_lambda\")\n        &gt;&gt;&gt; gc_lambda_check(df).select(\"studyId\", gc_lambda).show()\n        +-------+---------+\n        |studyId|gc_lambda|\n        +-------+---------+\n        |     S1|     0.55|\n        |     S2|   219.81|\n        +-------+---------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    z_score = f.col(\"beta\") / f.col(\"standardError\")\n    # NOTE! The statistic can be calculated using scipy.stats.chi2.isf(0.5, df=1)\n    # as well, since both functions are equal at the 0.5 quantile.\n    stat = chi2.ppf(0.5, df=1)\n    qc_c = (\n        gwas_for_qc.select(\"studyId\", \"beta\", \"standardError\")\n        .withColumn(\"Z2\", z_score**2)\n        .groupBy(\"studyId\")\n        .agg(f.percentile_approx(\"Z2\", 0.5).alias(\"z2_median\"))\n        .withColumn(\"gc_lambda\", f.col(\"z2_median\") / stat)\n        .select(\"studyId\", \"gc_lambda\")\n    )\n\n    return qc_c\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.p_z_test","title":"<code>gentropy.method.sumstat_quality_controls.p_z_test(gwas_for_qc: DataFrame) -&gt; DataFrame</code>","text":"<p>The P-Z test for QC of GWAS summary statistics.</p> <p>This function expects to have a dataframe with <code>studyId</code>, <code>beta</code>, <code>standardError</code>, <code>pValueMantissa</code> and <code>pValueExponent</code> columns</p> <p>It runs linear regression between reported p-values and p-values inferred from z-scores.</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>DataFrame</code> <p>Dataframe with <code>studyId</code>, <code>beta</code>, <code>standardError</code>, <code>pValueMantissa</code> and <code>pValueExponent</code> columns.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the results of the linear regression for each study.</p> Note <p>The algorithm does following things: 1. recalculates the negative logarithm of p-value from the square z-score 2. calculates the mean and se difference between the sum of logarithms derived form reported p-value mantissa and exponent and value recalculated from z-score.</p> Warning <p>The function requires the calculation of the chi-squared survival function to obtain the p-value from the z-score. which is not available in PySpark. The function uses scipy instead, thus it is not optimized for large datasets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s = 'studyId STRING, beta DOUBLE, standardError DOUBLE, pValueMantissa FLOAT, pValueExponent INTEGER'\n&gt;&gt;&gt; # Example where the variant reaches upper and lower boundaries for mantissa and upper bound for exponent\n&gt;&gt;&gt; d1 = [(\"S1\", 1.81, 0.2, 9.9, -20), (\"S1\", -0.1, 0.2, 1.0, -1)]\n&gt;&gt;&gt; # Example where z-score^2 (beta / se) &gt; 100\n&gt;&gt;&gt; d2 = [(\"S2\", 101.0, 10.0, 1.0, -1), (\"S2\", 1.0, 0.1, 1.0, -1), (\"S2\", 1.0, 0.1, 2.0, -2)]\n&gt;&gt;&gt; df = spark.createDataFrame(d1 + d2, s)\n&gt;&gt;&gt; df.show()\n+-------+-----+-------------+--------------+--------------+\n|studyId| beta|standardError|pValueMantissa|pValueExponent|\n+-------+-----+-------------+--------------+--------------+\n|     S1| 1.81|          0.2|           9.9|           -20|\n|     S1| -0.1|          0.2|           1.0|            -1|\n|     S2|101.0|         10.0|           1.0|            -1|\n|     S2|  1.0|          0.1|           1.0|            -1|\n|     S2|  1.0|          0.1|           2.0|            -2|\n+-------+-----+-------------+--------------+--------------+\n</code></pre> <p>This method outputs two values per study, mean and standard deviation of the difference between log p-value(s)</p> <pre><code>&gt;&gt;&gt; mean_diff_pz = f.round(\"mean_diff_pz\", 2).alias(\"mean_diff_pz\")\n&gt;&gt;&gt; se_diff_pz = f.round(\"se_diff_pz\", 2).alias(\"se_diff_pz\")\n&gt;&gt;&gt; p_z_test(df).select('studyId', mean_diff_pz, se_diff_pz).show()\n+-------+------------+----------+\n|studyId|mean_diff_pz|se_diff_pz|\n+-------+------------+----------+\n|     S1|        0.47|      0.45|\n|     S2|      -21.47|      0.49|\n+-------+------------+----------+\n</code></pre> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>def p_z_test(gwas_for_qc: DataFrame) -&gt; DataFrame:\n    \"\"\"The P-Z test for QC of GWAS summary statistics.\n\n    This function expects to have a dataframe with `studyId`, `beta`, `standardError`, `pValueMantissa` and `pValueExponent` columns\n\n    It runs linear regression between reported p-values and p-values inferred from z-scores.\n\n    Args:\n        gwas_for_qc (DataFrame): Dataframe with `studyId`, `beta`, `standardError`, `pValueMantissa` and `pValueExponent` columns.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the results of the linear regression for each study.\n\n    Note:\n        The algorithm does following things:\n        1. recalculates the negative logarithm of p-value from the square z-score\n        2. calculates the mean and se difference between the sum of logarithms derived form reported p-value mantissa and exponent and value recalculated from z-score.\n\n    Warning:\n         The function requires the calculation of the **chi-squared survival function** to obtain the p-value from the z-score.\n         which is not available in PySpark. The function uses scipy instead, thus **it is not optimized for large datasets**.\n\n    Examples:\n        &gt;&gt;&gt; s = 'studyId STRING, beta DOUBLE, standardError DOUBLE, pValueMantissa FLOAT, pValueExponent INTEGER'\n        &gt;&gt;&gt; # Example where the variant reaches upper and lower boundaries for mantissa and upper bound for exponent\n        &gt;&gt;&gt; d1 = [(\"S1\", 1.81, 0.2, 9.9, -20), (\"S1\", -0.1, 0.2, 1.0, -1)]\n        &gt;&gt;&gt; # Example where z-score^2 (beta / se) &gt; 100\n        &gt;&gt;&gt; d2 = [(\"S2\", 101.0, 10.0, 1.0, -1), (\"S2\", 1.0, 0.1, 1.0, -1), (\"S2\", 1.0, 0.1, 2.0, -2)]\n        &gt;&gt;&gt; df = spark.createDataFrame(d1 + d2, s)\n        &gt;&gt;&gt; df.show()\n        +-------+-----+-------------+--------------+--------------+\n        |studyId| beta|standardError|pValueMantissa|pValueExponent|\n        +-------+-----+-------------+--------------+--------------+\n        |     S1| 1.81|          0.2|           9.9|           -20|\n        |     S1| -0.1|          0.2|           1.0|            -1|\n        |     S2|101.0|         10.0|           1.0|            -1|\n        |     S2|  1.0|          0.1|           1.0|            -1|\n        |     S2|  1.0|          0.1|           2.0|            -2|\n        +-------+-----+-------------+--------------+--------------+\n        &lt;BLANKLINE&gt;\n\n        **This method outputs two values per study, mean and standard deviation of the difference between log p-value(s)**\n\n        &gt;&gt;&gt; mean_diff_pz = f.round(\"mean_diff_pz\", 2).alias(\"mean_diff_pz\")\n        &gt;&gt;&gt; se_diff_pz = f.round(\"se_diff_pz\", 2).alias(\"se_diff_pz\")\n        &gt;&gt;&gt; p_z_test(df).select('studyId', mean_diff_pz, se_diff_pz).show()\n        +-------+------------+----------+\n        |studyId|mean_diff_pz|se_diff_pz|\n        +-------+------------+----------+\n        |     S1|        0.47|      0.45|\n        |     S2|      -21.47|      0.49|\n        +-------+------------+----------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    qc_c = (\n        gwas_for_qc.withColumn(\"Z2\", (f.col(\"beta\") / f.col(\"standardError\")) ** 2)\n        .filter(f.col(\"Z2\") &lt;= 100)\n        .withColumn(\"neglogpValFromZScore\", neglogpval_from_z2(f.col(\"Z2\")))\n        .withColumn(\n            \"neglogpVal\", -1 * (f.log10(\"pValueMantissa\") + f.col(\"pValueExponent\"))\n        )\n        .withColumn(\"diffpval\", f.col(\"neglogpVal\") - f.col(\"neglogpValfromZScore\"))\n        .groupBy(\"studyId\")\n        .agg(\n            f.mean(\"diffpval\").alias(\"mean_diff_pz\"),\n            f.stddev(\"diffpval\").alias(\"se_diff_pz\"),\n        )\n        .select(\"studyId\", \"mean_diff_pz\", \"se_diff_pz\")\n    )\n\n    return qc_c\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.mean_beta_check","title":"<code>gentropy.method.sumstat_quality_controls.mean_beta_check(gwas_for_qc: DataFrame) -&gt; DataFrame</code>","text":"<p>The mean beta check for QC of GWAS summary statistics.</p> <p>This function expects to have a dataframe with <code>studyId</code> and <code>beta</code> columns and outputs the dataframe with mean beta aggregated over the studyId.</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>DataFrame</code> <p>Dataframe with <code>studyId</code> and <code>beta</code> columns.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the mean beta for each study.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s = \"studyId STRING, variantId STRING, beta DOUBLE\"\n&gt;&gt;&gt; df = spark.createDataFrame([('S1', '1_10000_A_T', 1.0), ('S1', '1_10001_C_T', 1.0), ('S2', '1_10001_C_T', 0.028)], schema=s)\n&gt;&gt;&gt; df.show()\n+-------+-----------+-----+\n|studyId|  variantId| beta|\n+-------+-----------+-----+\n|     S1|1_10000_A_T|  1.0|\n|     S1|1_10001_C_T|  1.0|\n|     S2|1_10001_C_T|0.028|\n+-------+-----------+-----+\n</code></pre> <p>This method outputs one value per study</p> <pre><code>&gt;&gt;&gt; mean_beta = f.round(\"mean_beta\", 3).alias(\"mean_beta\")\n&gt;&gt;&gt; mean_beta_check(df).select('studyId', mean_beta).show()\n+-------+---------+\n|studyId|mean_beta|\n+-------+---------+\n|     S1|      1.0|\n|     S2|    0.028|\n+-------+---------+\n</code></pre> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>def mean_beta_check(\n    gwas_for_qc: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"The mean beta check for QC of GWAS summary statistics.\n\n    This function expects to have a dataframe with `studyId` and `beta` columns and\n    outputs the dataframe with mean beta aggregated over the studyId.\n\n    Args:\n        gwas_for_qc (DataFrame): Dataframe with `studyId` and `beta` columns.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the mean beta for each study.\n\n    Examples:\n        &gt;&gt;&gt; s = \"studyId STRING, variantId STRING, beta DOUBLE\"\n        &gt;&gt;&gt; df = spark.createDataFrame([('S1', '1_10000_A_T', 1.0), ('S1', '1_10001_C_T', 1.0), ('S2', '1_10001_C_T', 0.028)], schema=s)\n        &gt;&gt;&gt; df.show()\n        +-------+-----------+-----+\n        |studyId|  variantId| beta|\n        +-------+-----------+-----+\n        |     S1|1_10000_A_T|  1.0|\n        |     S1|1_10001_C_T|  1.0|\n        |     S2|1_10001_C_T|0.028|\n        +-------+-----------+-----+\n        &lt;BLANKLINE&gt;\n\n        **This method outputs one value per study**\n\n        &gt;&gt;&gt; mean_beta = f.round(\"mean_beta\", 3).alias(\"mean_beta\")\n        &gt;&gt;&gt; mean_beta_check(df).select('studyId', mean_beta).show()\n        +-------+---------+\n        |studyId|mean_beta|\n        +-------+---------+\n        |     S1|      1.0|\n        |     S2|    0.028|\n        +-------+---------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    study_id = f.col(\"studyId\")\n    beta = f.col(\"beta\")\n    qc_c = gwas_for_qc.groupBy(study_id).agg(\n        f.mean(beta).alias(\"mean_beta\"),\n    )\n    return qc_c\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.sumstat_n_eff_check","title":"<code>gentropy.method.sumstat_quality_controls.sumstat_n_eff_check(gwas_for_qc: DataFrame, n_total: int = 100000, limit: int = 10000000, min_count: int = 100) -&gt; DataFrame</code>","text":"<p>The effective sample size check for QC of GWAS summary statistics.</p> <p>It estimates the ratio between effective sample size and the expected one and checks it's distribution. It is possible to conduct only if the effective allele frequency is provided in the study. The median ratio is always close to 1, but standard error could be inflated.</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>DataFrame</code> <p>Dataframe with <code>studyId</code>, <code>beta</code>, <code>standardError</code>, and <code>effectAlleleFrequencyFromSource</code> columns.</p> required <code>n_total</code> <code>int</code> <p>The reported sample size of the study. The QC metrics is robust toward the sample size.</p> <code>100000</code> <code>limit</code> <code>int</code> <p>The limit for the number of variants to be used for the estimation.</p> <code>10000000</code> <code>min_count</code> <code>int</code> <p>The minimum number of variants to be used for the estimation.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the effective sample size ratio for each study.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s = 'studyId STRING, beta DOUBLE, standardError DOUBLE, effectAlleleFrequencyFromSource FLOAT'\n&gt;&gt;&gt; # Example where we have a very common and very rare variant\n&gt;&gt;&gt; d1 = [(\"S1\", 1.81, 0.2, 0.999), (\"S1\", -0.1, 0.2, 0.001), (\"S1\", 1.0, 0.1, 0.5)]\n&gt;&gt;&gt; # Example where z-score^2 (beta / se) &gt; 100\n&gt;&gt;&gt; d2 = [(\"S2\", 1.81, 0.2, None), (\"S2\", 1.0, 0.1, 0.5), (\"S2\", 1.0, 0.1, 0.5)]\n&gt;&gt;&gt; df = spark.createDataFrame(d1 + d2, s)\n&gt;&gt;&gt; df.show()\n+-------+----+-------------+-------------------------------+\n|studyId|beta|standardError|effectAlleleFrequencyFromSource|\n+-------+----+-------------+-------------------------------+\n|     S1|1.81|          0.2|                          0.999|\n|     S1|-0.1|          0.2|                          0.001|\n|     S1| 1.0|          0.1|                            0.5|\n|     S2|1.81|          0.2|                           NULL|\n|     S2| 1.0|          0.1|                            0.5|\n|     S2| 1.0|          0.1|                            0.5|\n+-------+----+-------------+-------------------------------+\n</code></pre> <p>This method outputs one value per study</p> <pre><code>&gt;&gt;&gt; se_n = f.round(\"se_N\", 2)\n&gt;&gt;&gt; sumstat_n_eff_check(df, min_count=2, limit=2).select(\"studyId\", se_n).show()\n+-------+--------------+\n|studyId|round(se_N, 2)|\n+-------+--------------+\n|     S1|           0.0|\n|     S2|           0.0|\n+-------+--------------+\n</code></pre> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>def sumstat_n_eff_check(\n    gwas_for_qc: DataFrame,\n    n_total: int = 100_000,\n    limit: int = 10_000_000,\n    min_count: int = 100,\n) -&gt; DataFrame:\n    \"\"\"The effective sample size check for QC of GWAS summary statistics.\n\n    It estimates the ratio between effective sample size and the expected one and checks it's distribution.\n    It is possible to conduct only if the effective allele frequency is provided in the study.\n    The median ratio is always close to 1, but standard error could be inflated.\n\n    Args:\n        gwas_for_qc (DataFrame): Dataframe with `studyId`, `beta`, `standardError`, and `effectAlleleFrequencyFromSource` columns.\n        n_total (int): The reported sample size of the study. The QC metrics is robust toward the sample size.\n        limit (int): The limit for the number of variants to be used for the estimation.\n        min_count (int): The minimum number of variants to be used for the estimation.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the effective sample size ratio for each study.\n\n    Examples:\n        &gt;&gt;&gt; s = 'studyId STRING, beta DOUBLE, standardError DOUBLE, effectAlleleFrequencyFromSource FLOAT'\n        &gt;&gt;&gt; # Example where we have a very common and very rare variant\n        &gt;&gt;&gt; d1 = [(\"S1\", 1.81, 0.2, 0.999), (\"S1\", -0.1, 0.2, 0.001), (\"S1\", 1.0, 0.1, 0.5)]\n        &gt;&gt;&gt; # Example where z-score^2 (beta / se) &gt; 100\n        &gt;&gt;&gt; d2 = [(\"S2\", 1.81, 0.2, None), (\"S2\", 1.0, 0.1, 0.5), (\"S2\", 1.0, 0.1, 0.5)]\n        &gt;&gt;&gt; df = spark.createDataFrame(d1 + d2, s)\n        &gt;&gt;&gt; df.show()\n        +-------+----+-------------+-------------------------------+\n        |studyId|beta|standardError|effectAlleleFrequencyFromSource|\n        +-------+----+-------------+-------------------------------+\n        |     S1|1.81|          0.2|                          0.999|\n        |     S1|-0.1|          0.2|                          0.001|\n        |     S1| 1.0|          0.1|                            0.5|\n        |     S2|1.81|          0.2|                           NULL|\n        |     S2| 1.0|          0.1|                            0.5|\n        |     S2| 1.0|          0.1|                            0.5|\n        +-------+----+-------------+-------------------------------+\n        &lt;BLANKLINE&gt;\n\n        This method outputs one value per study\n\n        &gt;&gt;&gt; se_n = f.round(\"se_N\", 2)\n        &gt;&gt;&gt; sumstat_n_eff_check(df, min_count=2, limit=2).select(\"studyId\", se_n).show()\n        +-------+--------------+\n        |studyId|round(se_N, 2)|\n        +-------+--------------+\n        |     S1|           0.0|\n        |     S2|           0.0|\n        +-------+--------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    af = f.col(\"effectAlleleFrequencyFromSource\")\n    se = f.col(\"standardError\")\n    beta = f.col(\"beta\")\n    varG = genotypic_variance(af)\n    pheno_var = (se**2) * n_total * varG + ((f.col(\"beta\") ** 2) * varG)\n\n    window = Window.partitionBy(\"studyId\").orderBy(\"studyId\")\n    pheno_median = f.percentile_approx(pheno_var, 0.5).over(window)\n    N_hat_ratio = (pheno_median - (beta**2 * varG)) / (se**2 * varG * n_total)\n\n    df_with_counts = gwas_for_qc.dropna(\n        subset=[\"effectAlleleFrequencyFromSource\"]\n    ).withColumn(\"count\", f.count(\"studyId\").over(window))\n\n    # Filter the DataFrame to keep only the groups with count greater than or equal to min_count\n    filtered_df = df_with_counts.filter(f.col(\"count\") &gt;= min_count).drop(\"count\")\n\n    # Keep the number of variants up to the limit\n    gwas_df = (\n        filtered_df.withColumn(\"row_num\", row_number().over(window))\n        .filter(f.col(\"row_num\") &lt;= limit)\n        .drop(\"row_num\")\n    )\n    # Calculate the genotypic variance following the formula 2 * AlleleFrequency * (1 - AlleleFrequency)\n    qc_c = (\n        gwas_df.withColumn(\"pheno_var\", pheno_var)\n        .withColumn(\"pheno_median\", pheno_median)\n        .withColumn(\"N_hat_ratio\", N_hat_ratio)\n        .groupBy(\"studyId\")\n        .agg(f.stddev(\"N_hat_ratio\").alias(\"se_N\"))\n        .select(\"studyId\", \"se_N\")\n    )\n\n    return qc_c\n</code></pre>"},{"location":"python_api/methods/sumstat_quality_controls/#gentropy.method.sumstat_quality_controls.number_of_variants","title":"<code>gentropy.method.sumstat_quality_controls.number_of_variants(gwas_for_qc: DataFrame, pval_threshold: float = 5e-08) -&gt; DataFrame</code>","text":"<p>The function calculates number of SNPs and number of SNPs with p-value less than the threshold (default to 5e-8).</p> <p>Parameters:</p> Name Type Description Default <code>gwas_for_qc</code> <code>DataFrame</code> <p>Dataframe with <code>studyId</code>, <code>variantId</code>, <code>pValueMantissa</code> and <code>pValueExponent</code> columns.</p> required <code>pval_threshold</code> <code>float</code> <p>The threshold for the p-value.</p> <code>5e-08</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>PySpark DataFrame with the number of SNPs and number of SNPs with p-value less than threshold.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s = 'studyId STRING, variantId STRING, pValueMantissa FLOAT, pValueExponent INTEGER'\n&gt;&gt;&gt; d1 = [(\"S1\", \"1_10000_A_T\", 9.9, -20), (\"S1\", \"1_10001_C_T\", 1.0, -1), (\"S1\", \"1_10002_G_C\", 5.0, -8)]\n&gt;&gt;&gt; d2 = [(\"S2\", \"1_10001_C_T\", 1.0, -1), (\"S2\", \"1_10002_G_C\", 2.0, -2)]\n&gt;&gt;&gt; df = spark.createDataFrame(d1 + d2, s)\n&gt;&gt;&gt; df.show()\n+-------+-----------+--------------+--------------+\n|studyId|  variantId|pValueMantissa|pValueExponent|\n+-------+-----------+--------------+--------------+\n|     S1|1_10000_A_T|           9.9|           -20|\n|     S1|1_10001_C_T|           1.0|            -1|\n|     S1|1_10002_G_C|           5.0|            -8|\n|     S2|1_10001_C_T|           1.0|            -1|\n|     S2|1_10002_G_C|           2.0|            -2|\n+-------+-----------+--------------+--------------+\n</code></pre> <p>This method outputs two values per study <code>n_variants_sig</code> and <code>n_variants</code></p> <pre><code>&gt;&gt;&gt; number_of_variants(df).show()\n+-------+----------+--------------+\n|studyId|n_variants|n_variants_sig|\n+-------+----------+--------------+\n|     S1|         3|             2|\n|     S2|         2|             0|\n+-------+----------+--------------+\n</code></pre> Source code in <code>src/gentropy/method/sumstat_quality_controls.py</code> <pre><code>def number_of_variants(\n    gwas_for_qc: DataFrame,\n    pval_threshold: float = 5e-8,\n) -&gt; DataFrame:\n    \"\"\"The function calculates number of SNPs and number of SNPs with p-value less than the threshold (default to 5e-8).\n\n    Args:\n        gwas_for_qc (DataFrame): Dataframe with `studyId`, `variantId`, `pValueMantissa` and `pValueExponent` columns.\n        pval_threshold (float): The threshold for the p-value.\n\n    Returns:\n        DataFrame: PySpark DataFrame with the number of SNPs and number of SNPs with p-value less than threshold.\n\n    Examples:\n        &gt;&gt;&gt; s = 'studyId STRING, variantId STRING, pValueMantissa FLOAT, pValueExponent INTEGER'\n        &gt;&gt;&gt; d1 = [(\"S1\", \"1_10000_A_T\", 9.9, -20), (\"S1\", \"1_10001_C_T\", 1.0, -1), (\"S1\", \"1_10002_G_C\", 5.0, -8)]\n        &gt;&gt;&gt; d2 = [(\"S2\", \"1_10001_C_T\", 1.0, -1), (\"S2\", \"1_10002_G_C\", 2.0, -2)]\n        &gt;&gt;&gt; df = spark.createDataFrame(d1 + d2, s)\n        &gt;&gt;&gt; df.show()\n        +-------+-----------+--------------+--------------+\n        |studyId|  variantId|pValueMantissa|pValueExponent|\n        +-------+-----------+--------------+--------------+\n        |     S1|1_10000_A_T|           9.9|           -20|\n        |     S1|1_10001_C_T|           1.0|            -1|\n        |     S1|1_10002_G_C|           5.0|            -8|\n        |     S2|1_10001_C_T|           1.0|            -1|\n        |     S2|1_10002_G_C|           2.0|            -2|\n        +-------+-----------+--------------+--------------+\n        &lt;BLANKLINE&gt;\n\n        **This method outputs two values per study `n_variants_sig` and `n_variants`**\n\n        &gt;&gt;&gt; number_of_variants(df).show()\n        +-------+----------+--------------+\n        |studyId|n_variants|n_variants_sig|\n        +-------+----------+--------------+\n        |     S1|         3|             2|\n        |     S2|         2|             0|\n        +-------+----------+--------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    log10pval = f.log10(f.col(\"pValueMantissa\")) + f.col(\"pValueExponent\")\n    threshold = f.log10(f.lit(pval_threshold))\n    snp_counts = gwas_for_qc.groupBy(\"studyId\").agg(\n        f.count(\"*\").alias(\"n_variants\"),\n        f.sum((log10pval &lt;= threshold).cast(t.IntegerType())).alias(\"n_variants_sig\"),\n    )\n    return snp_counts\n</code></pre>"},{"location":"python_api/methods/susie_inf/","title":"SuSiE-inf - Fine-mapping with infinitesimal effects v1.1","text":"<p>This is an implementation of the SuSiE-inf method found here: https://github.com/FinucaneLab/fine-mapping-inf https://www.nature.com/articles/s41588-023-01597-3</p> <p>This fine-mapping approach has two approaches for updating estimates of the variance components - Method of Moments and Maximum Likelihood Estimator ('MoM' / 'MLE') The function takes an array of Z-scores and a numpy array matrix of variant LD to perform finemapping.</p>"},{"location":"python_api/methods/susie_inf/#gentropy.method.susie_inf.SUSIE_inf","title":"<code>gentropy.method.susie_inf.SUSIE_inf</code>  <code>dataclass</code>","text":"<p>SuSiE fine-mapping of a study locus from fine-mapping-inf package.</p> <p>Note: code copied from fine-mapping-inf package as a placeholder https://github.com/FinucaneLab/fine-mapping-inf</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if missing LD or if unsupported variance estimation</p> Source code in <code>src/gentropy/method/susie_inf.py</code> <pre><code>@dataclass\nclass SUSIE_inf:\n    \"\"\"SuSiE fine-mapping of a study locus from fine-mapping-inf package.\n\n    Note: code copied from fine-mapping-inf package as a placeholder\n    https://github.com/FinucaneLab/fine-mapping-inf\n\n    Raises:\n        RuntimeError: if missing LD or if unsupported variance estimation\n    \"\"\"\n\n    @staticmethod\n    def susie_inf(  # noqa: C901\n        z: np.ndarray,\n        meansq: float = 1,\n        n: int = 100000,\n        L: int = 10,\n        LD: np.ndarray | None = None,\n        V: np.ndarray | None = None,\n        Dsq: np.ndarray | None = None,\n        est_ssq: bool = True,\n        ssq: np.ndarray | None = None,\n        ssq_range: tuple[float, float] = (0, 1),\n        pi0: np.ndarray | None = None,\n        est_sigmasq: bool = True,\n        est_tausq: bool = False,\n        sigmasq: float = 1,\n        tausq: float = 0,\n        sigmasq_range: tuple[float, float] | None = None,\n        tausq_range: tuple[float, float] | None = None,\n        PIP: np.ndarray | None = None,\n        mu: np.ndarray | None = None,\n        method: str = \"moments\",\n        maxiter: int = 100,\n        PIP_tol: float = 0.001,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Susie with random effects.\n\n        Args:\n            z (np.ndarray): vector of z-scores (equal to X'y/sqrt(n))\n            meansq (float): average squared magnitude of y (equal to ||y||^2/n)\n            n (int): sample size\n            L (int): number of modeled causal effects\n            LD (np.ndarray | None): LD matrix (equal to X'X/n)\n            V (np.ndarray | None): precomputed p x p matrix of eigenvectors of X'X\n            Dsq (np.ndarray | None): precomputed length-p vector of eigenvalues of X'X\n            est_ssq (bool): estimate prior effect size variances s^2 using MLE\n            ssq (np.ndarray | None): length-L initialization s^2 for each effect\n            ssq_range (tuple[float, float]): lower and upper bounds for each s^2, if estimated\n            pi0 (np.ndarray | None): length-p vector of prior causal probability for each SNP; must sum to 1\n            est_sigmasq (bool): estimate variance sigma^2\n            est_tausq (bool): estimate both variances sigma^2 and tau^2\n            sigmasq (float): initial value for sigma^2\n            tausq (float): initial value for tau^2\n            sigmasq_range (tuple[float, float] | None): lower and upper bounds for sigma^2, if estimated using MLE\n            tausq_range (tuple[float, float] | None): lower and upper bounds for tau^2, if estimated using MLE\n            PIP (np.ndarray | None): p x L initializations of PIPs\n            mu (np.ndarray | None): p x L initializations of mu\n            method (str): one of {'moments','MLE'}\n            maxiter (int): maximum number of SuSiE iterations\n            PIP_tol (float): convergence threshold for PIP difference between iterations\n\n        Returns:\n            dict[str, Any]: Dictionary with keys:\n                PIP -- p x L matrix of PIPs, individually for each effect\n                mu -- p x L matrix of posterior means conditional on causal\n                omega -- p x L matrix of posterior precisions conditional on causal\n                lbf_variable -- p x L matrix of log-Bayes-factors, for each effect\n                ssq -- length-L array of final effect size variances s^2\n                sigmasq -- final value of sigma^2\n                tausq -- final value of tau^2\n                alpha -- length-p array of posterior means of infinitesimal effects\n                lbf -- length-p array of log-Bayes-factors for each CS\n\n        Raises:\n            RuntimeError: if missing LD or if unsupported variance estimation method\n        \"\"\"\n        p = len(z)\n        # Precompute V,D^2 in the SVD X=UDV', and V'X'y and y'y\n        if (V is None or Dsq is None) and LD is None:\n            raise RuntimeError(\"Missing LD\")\n        elif V is None or Dsq is None:\n            eigvals, V = scipy.linalg.eigh(LD)\n            Dsq = np.maximum(n * eigvals, 0)\n        else:\n            Dsq = np.maximum(Dsq, 0)\n        Xty = np.sqrt(n) * z\n        VtXty = V.T.dot(Xty)\n        yty = n * meansq\n        # Initialize diagonal variances, diag(X' Omega X), X' Omega y\n        var = tausq * Dsq + sigmasq\n        diagXtOmegaX = np.sum(V**2 * (Dsq / var), axis=1)\n        XtOmegay = V.dot(VtXty / var)\n        # Initialize s_l^2, PIP_j, mu_j, omega_j\n        if ssq is None:\n            ssq = np.ones(L) * 0.2\n        if PIP is None:\n            PIP = np.ones((p, L)) / p\n        if mu is None:\n            mu = np.zeros((p, L))\n        lbf_variable = np.zeros((p, L))\n        omega = diagXtOmegaX[:, np.newaxis] + 1 / ssq\n        # Initialize prior causal probabilities\n        if pi0 is None:\n            logpi0 = np.ones(p) * np.log(1.0 / p)\n        else:\n            logpi0 = -np.ones(p) * np.inf\n            inds = np.nonzero(pi0 &gt; 0)[0]\n            logpi0[inds] = np.log(pi0[inds])\n\n        ####### Main SuSiE iteration loop ######\n        def f(x: float) -&gt; float:\n            \"\"\"Negative ELBO as function of x = sigma_e^2.\n\n            Args:\n                x (float): sigma_e^2\n\n            Returns:\n                float: negative ELBO as function of x = sigma_e^2\n            \"\"\"\n            return -scipy.special.logsumexp(\n                -0.5 * np.log(1 + x * diagXtOmegaX)\n                + x * XtOmegar**2 / (2 * (1 + x * diagXtOmegaX))\n                + logpi0\n            )\n\n        for it in range(maxiter):\n            PIP_prev = PIP.copy()\n            # Single effect regression for each effect l = 1,...,L\n            for _l in range(L):\n                # Compute X' Omega r_l for residual r_l\n                b = np.sum(mu * PIP, axis=1) - mu[:, _l] * PIP[:, _l]\n                XtOmegaXb = V.dot(V.T.dot(b) * Dsq / var)\n                XtOmegar = XtOmegay - XtOmegaXb\n                if est_ssq:\n                    # Update prior variance ssq[l]\n                    res = minimize_scalar(f, bounds=ssq_range, method=\"bounded\")\n                    if res.success:\n                        ssq[_l] = res.x\n                # Update omega, mu, and PIP\n                omega[:, _l] = diagXtOmegaX + 1 / ssq[_l]\n                mu[:, _l] = XtOmegar / omega[:, _l]\n                lbf_variable[:, _l] = XtOmegar**2 / (2 * omega[:, _l]) - 0.5 * np.log(\n                    omega[:, _l] * ssq[_l]\n                )\n                logPIP = lbf_variable[:, _l] + logpi0\n                PIP[:, _l] = np.exp(logPIP - scipy.special.logsumexp(logPIP))\n            # Update variance components\n            if est_sigmasq or est_tausq:\n                if method == \"moments\":\n                    (sigmasq, tausq) = SUSIE_inf._MoM(\n                        PIP,\n                        mu,\n                        omega,\n                        sigmasq,\n                        tausq,\n                        n,\n                        V,\n                        Dsq,\n                        VtXty,\n                        Xty,\n                        yty,\n                        est_sigmasq,\n                        est_tausq,\n                    )\n                elif method == \"MLE\":\n                    (sigmasq, tausq) = SUSIE_inf._MLE(\n                        PIP,\n                        mu,\n                        omega,\n                        sigmasq,\n                        tausq,\n                        n,\n                        V,\n                        Dsq,\n                        VtXty,\n                        yty,\n                        est_sigmasq,\n                        est_tausq,\n                        it,\n                        sigmasq_range,\n                        tausq_range,\n                    )\n                else:\n                    raise RuntimeError(\"Unsupported variance estimation method\")\n                # Update X' Omega X, X' Omega y\n                var = tausq * Dsq + sigmasq\n                diagXtOmegaX = np.sum(V**2 * (Dsq / var), axis=1)\n                XtOmegay = V.dot(VtXty / var)\n            # Determine convergence from PIP differences\n            PIP_diff = np.max(np.abs(PIP_prev - PIP))\n            if PIP_diff &lt; PIP_tol:\n                break\n        # Compute posterior means of b and alpha\n        b = np.sum(mu * PIP, axis=1)\n        XtOmegaXb = V.dot(V.T.dot(b) * Dsq / var)\n        XtOmegar = XtOmegay - XtOmegaXb\n        alpha = tausq * XtOmegar\n\n        priors = np.log(np.repeat(1 / p, p))\n        lbf_cs = np.apply_along_axis(\n            lambda x: logsumexp(x + priors), axis=0, arr=lbf_variable\n        )\n        return {\n            \"PIP\": PIP,\n            \"mu\": mu,\n            \"omega\": omega,\n            \"lbf_variable\": lbf_variable,\n            \"ssq\": ssq,\n            \"sigmasq\": sigmasq,\n            \"tausq\": tausq,\n            \"alpha\": alpha,\n            \"lbf\": lbf_cs,\n        }\n\n    @staticmethod\n    def _MoM(\n        PIP: np.ndarray,\n        mu: np.ndarray,\n        omega: np.ndarray,\n        sigmasq: float,\n        tausq: float,\n        n: int,\n        V: np.ndarray,\n        Dsq: np.ndarray,\n        VtXty: np.ndarray,\n        Xty: np.ndarray,\n        yty: float,\n        est_sigmasq: bool,\n        est_tausq: bool,\n    ) -&gt; tuple[float, float]:\n        \"\"\"Subroutine to estimate sigma^2, tau^2 using method-of-moments.\n\n        Args:\n            PIP (np.ndarray): p x L matrix of PIPs\n            mu (np.ndarray): p x L matrix of posterior means conditional on causal\n            omega (np.ndarray): p x L matrix of posterior precisions conditional on causal\n            sigmasq (float): initial value for sigma^2\n            tausq (float): initial value for tau^2\n            n (int): sample size\n            V (np.ndarray): precomputed p x p matrix of eigenvectors of X'X\n            Dsq (np.ndarray): precomputed length-p vector of eigenvalues of X'X\n            VtXty (np.ndarray): precomputed length-p vector V'X'y\n            Xty (np.ndarray): precomputed length-p vector X'y\n            yty (float): precomputed y'y\n            est_sigmasq (bool): estimate variance sigma^2\n            est_tausq (bool): estimate both variances sigma^2 and tau^2\n\n        Returns:\n            tuple[float, float]: (sigmasq,tausq) tuple of updated variances\n        \"\"\"\n        (p, L) = mu.shape\n        # Compute A\n        A = np.array([[n, sum(Dsq)], [0, sum(Dsq**2)]])\n        A[1, 0] = A[0, 1]\n        # Compute diag(V'MV)\n        b = np.sum(mu * PIP, axis=1)\n        Vtb = V.T.dot(b)\n        diagVtMV = Vtb**2\n        tmpD = np.zeros(p)\n        for _l in range(L):\n            bl = mu[:, _l] * PIP[:, _l]\n            Vtbl = V.T.dot(bl)\n            diagVtMV -= Vtbl**2\n            tmpD += PIP[:, _l] * (mu[:, _l] ** 2 + 1 / omega[:, _l])\n        diagVtMV += np.sum((V.T) ** 2 * tmpD, axis=1)\n        # Compute x\n        x = np.zeros(2)\n        x[0] = yty - 2 * sum(b * Xty) + sum(Dsq * diagVtMV)\n        x[1] = sum(Xty**2) - 2 * sum(Vtb * VtXty * Dsq) + sum(Dsq**2 * diagVtMV)\n        if est_tausq:\n            sol = scipy.linalg.solve(A, x)\n            if sol[0] &gt; 0 and sol[1] &gt; 0:\n                (sigmasq, tausq) = sol\n            else:\n                (sigmasq, tausq) = (x[0] / n, 0)\n        elif est_sigmasq:\n            sigmasq = (x[0] - A[0, 1] * tausq) / n\n        return sigmasq, tausq\n\n    @staticmethod\n    def _MLE(\n        PIP: np.ndarray,\n        mu: np.ndarray,\n        omega: np.ndarray,\n        sigmasq: float,\n        tausq: float,\n        n: int,\n        V: np.ndarray,\n        Dsq: np.ndarray,\n        VtXty: np.ndarray,\n        yty: float,\n        est_sigmasq: bool,\n        est_tausq: bool,\n        it: int,\n        sigmasq_range: tuple[float, float] | None = None,\n        tausq_range: tuple[float, float] | None = None,\n    ) -&gt; tuple[float, float]:\n        \"\"\"Subroutine to estimate sigma^2, tau^2 using MLE.\n\n        Args:\n            PIP (np.ndarray): p x L matrix of PIPs\n            mu (np.ndarray): p x L matrix of posterior means conditional on causal\n            omega (np.ndarray): p x L matrix of posterior precisions conditional on causal\n            sigmasq (float): initial value for sigma^2\n            tausq (float): initial value for tau^2\n            n (int): sample size\n            V (np.ndarray): precomputed p x p matrix of eigenvectors of X'X\n            Dsq (np.ndarray): precomputed length-p vector of eigenvalues of X'X\n            VtXty (np.ndarray): precomputed length-p vector V'X'y\n            yty (float): precomputed y'y\n            est_sigmasq (bool): estimate variance sigma^2\n            est_tausq (bool): estimate both variances sigma^2 and tau^2\n            it (int): iteration number\n            sigmasq_range (tuple[float, float] | None): lower and upper bounds for sigma^2, if estimated using MLE\n            tausq_range (tuple[float, float] | None): lower and upper bounds for tau^2, if estimated using MLE\n\n        Returns:\n            tuple[float, float]: (sigmasq,tausq) tuple of updated variances\n        \"\"\"\n        (p, L) = mu.shape\n        if sigmasq_range is None:\n            sigmasq_range = (0.2 * yty / n, 1.2 * yty / n)\n        if tausq_range is None:\n            tausq_range = (1e-12, 1.2 * yty / (n * p))\n        # Compute diag(V'MV)\n        b = np.sum(mu * PIP, axis=1)\n        Vtb = V.T.dot(b)\n        diagVtMV = Vtb**2\n        tmpD = np.zeros(p)\n        for _l in range(L):\n            bl = mu[:, _l] * PIP[:, _l]\n            Vtbl = V.T.dot(bl)\n            diagVtMV -= Vtbl**2\n            tmpD += PIP[:, _l] * (mu[:, _l] ** 2 + 1 / omega[:, _l])\n        diagVtMV += np.sum((V.T) ** 2 * tmpD, axis=1)\n\n        # negative ELBO as function of x = (sigma_e^2,sigma_g^2)\n        def f(x: tuple[float, float]) -&gt; float:\n            \"\"\"Negative ELBO as function of x = (sigma_e^2,sigma_g^2).\n\n            Args:\n                x (tuple[float, float]): (sigma_e^2,sigma_g^2)\n\n            Returns:\n                float: negative ELBO as function of x = (sigma_e^2,sigma_g^2)\n            \"\"\"\n            return (\n                0.5 * (n - p) * np.log(x[0])\n                + 0.5 / x[0] * yty\n                + np.sum(\n                    0.5 * np.log(x[1] * Dsq + x[0])\n                    - 0.5 * x[1] / x[0] * VtXty**2 / (x[1] * Dsq + x[0])\n                    - Vtb * VtXty / (x[1] * Dsq + x[0])\n                    + 0.5 * Dsq / (x[1] * Dsq + x[0]) * diagVtMV\n                )\n            )\n\n        if est_tausq:\n            res = minimize(\n                f,\n                (sigmasq, tausq),\n                method=\"L-BFGS-B\",\n                bounds=(sigmasq_range, tausq_range),\n            )\n            if res.success:\n                sigmasq, tausq = res.x\n        elif est_sigmasq:\n\n            def g(x: float) -&gt; float:\n                \"\"\"Negative ELBO as function of x = sigma_e^2.\n\n                Args:\n                    x (float): sigma_e^2\n\n                Returns:\n                    float: negative ELBO as function of x = sigma_e^2\n                \"\"\"\n                return f((x, tausq))\n\n            res = minimize(g, sigmasq, method=\"L-BFGS-B\", bounds=(sigmasq_range,))\n            if res.success:\n                sigmasq = res.x\n        return sigmasq, tausq\n\n    @staticmethod\n    def cred_inf(\n        PIP: np.ndarray,\n        n: int = 100_000,\n        coverage: float = 0.99,\n        purity: float = 0.5,\n        LD: np.ndarray | None = None,\n        V: np.ndarray | None = None,\n        Dsq: np.ndarray | None = None,\n        dedup: bool = True,\n    ) -&gt; list[Any]:\n        \"\"\"Compute credible sets from single-effect PIPs.\n\n        Args:\n            PIP (np.ndarray): p x L matrix of PIPs\n            n (int): sample size\n            coverage (float): coverage of credible sets\n            purity (float): purity of credible sets\n            LD (np.ndarray | None): LD matrix (equal to X'X/n)\n            V (np.ndarray | None): precomputed p x p matrix of eigenvectors of X'X\n            Dsq (np.ndarray | None): precomputed length-p vector of eigenvalues of X'X\n            dedup (bool): whether to deduplicate credible sets\n\n        Returns:\n            list[Any]: list of L lists of SNP indices in each credible set\n\n        Raises:\n            RuntimeError: if missing inputs for purity filtering\n            ValueError: if either LD or V, Dsq are None\n        \"\"\"\n        if (V is None or Dsq is None or n is None) and LD is None:\n            raise RuntimeError(\"Missing inputs for purity filtering\")\n        # Compute credible sets\n        cred = []\n        for i in range(PIP.shape[1]):\n            sortinds = np.argsort(PIP[:, i])[::-1]\n            ind = min(np.nonzero(np.cumsum(PIP[sortinds, i]) &gt;= coverage)[0])\n            credset = sortinds[: (ind + 1)]\n            # Filter by purity\n            if len(credset) == 1:\n                cred.append(list(credset))\n                continue\n            if len(credset) &lt; 100:\n                rows = credset\n            else:\n                np.random.seed(123)\n                rows = np.random.choice(credset, size=100, replace=False)\n            if LD is not None:\n                LDloc = LD[np.ix_(rows, rows)]\n            elif V is not None and Dsq is not None:\n                LDloc = (V[rows, :] * Dsq).dot(V[rows, :].T) / n\n            else:\n                raise ValueError(\"Both LD and V, Dsq cannot be None\")\n            if np.min(np.abs(LDloc)) &gt; purity:\n                cred.append(sorted(credset))\n        if dedup:\n            cred = list(\n                map(\n                    list,\n                    sorted(set(map(tuple, cred)), key=list(map(tuple, cred)).index),\n                )\n            )\n        return cred\n\n    @staticmethod\n    def credible_set_qc(\n        cred_sets: StudyLocus,\n        p_value_threshold: float = 1e-5,\n        purity_min_r2: float = 0.01,\n        clump: bool = False,\n        ld_index: LDIndex | None = None,\n        study_index: StudyIndex | None = None,\n        ld_min_r2: float | None = 0.8,\n    ) -&gt; StudyLocus:\n        \"\"\"Filter credible sets by lead P-value and min-R2 purity, and performs LD clumping.\n\n        In case of duplicated loci, the filtering retains the loci wth the highest credibleSetlog10BF.\n\n\n        Args:\n            cred_sets (StudyLocus): StudyLocus object with credible sets to filter/clump\n            p_value_threshold (float): p-value threshold for filtering credible sets, default is 1e-5\n            purity_min_r2 (float): min-R2 purity threshold for filtering credible sets, default is 0.01\n            clump (bool): Whether to clump the credible sets by LD, default is False\n            ld_index (LDIndex | None): LDIndex object\n            study_index (StudyIndex | None): StudyIndex object\n            ld_min_r2 (float | None): LD R2 threshold for clumping, default is 0.8\n\n        Returns:\n            StudyLocus: Credible sets which pass filters and LD clumping.\n\n        Raises:\n            AssertionError: When running in clump mode, but no study study_index or ld_index or ld_min_r2 were provided.\n        \"\"\"\n        cred_sets.df = (\n            cred_sets.df.withColumn(\n                \"pValue\", f.col(\"pValueMantissa\") * f.pow(10, f.col(\"pValueExponent\"))\n            )\n            .filter(f.col(\"pValue\") &lt;= p_value_threshold)\n            .filter(f.col(\"purityMinR2\") &gt;= purity_min_r2)\n            .drop(\"pValue\")\n            .withColumn(\n                \"rn\",\n                f.row_number().over(\n                    Window.partitionBy(\"studyLocusId\").orderBy(\n                        f.desc(\"credibleSetLog10BF\")\n                    )\n                ),\n            )\n            .filter(f.col(\"rn\") == 1)\n            .drop(\"rn\")\n        )\n        if clump:\n            assert study_index, \"Running in clump mode, which requires study_index.\"\n            assert ld_index, \"Running in clump mode, which requires ld_index.\"\n            assert ld_min_r2, \"Running in clump mode, which requires ld_min_r2 value.\"\n            cred_sets = (\n                cred_sets.annotate_ld(study_index, ld_index, ld_min_r2)\n                .clump()\n                .filter(\n                    ~f.array_contains(\n                        f.col(\"qualityControls\"),\n                        StudyLocusQualityCheck.LD_CLUMPED.value,\n                    )\n                )\n            )\n\n        return cred_sets\n</code></pre>"},{"location":"python_api/methods/susie_inf/#gentropy.method.susie_inf.SUSIE_inf.cred_inf","title":"<code>cred_inf(PIP: np.ndarray, n: int = 100000, coverage: float = 0.99, purity: float = 0.5, LD: np.ndarray | None = None, V: np.ndarray | None = None, Dsq: np.ndarray | None = None, dedup: bool = True) -&gt; list[Any]</code>  <code>staticmethod</code>","text":"<p>Compute credible sets from single-effect PIPs.</p> <p>Parameters:</p> Name Type Description Default <code>PIP</code> <code>ndarray</code> <p>p x L matrix of PIPs</p> required <code>n</code> <code>int</code> <p>sample size</p> <code>100000</code> <code>coverage</code> <code>float</code> <p>coverage of credible sets</p> <code>0.99</code> <code>purity</code> <code>float</code> <p>purity of credible sets</p> <code>0.5</code> <code>LD</code> <code>ndarray | None</code> <p>LD matrix (equal to X'X/n)</p> <code>None</code> <code>V</code> <code>ndarray | None</code> <p>precomputed p x p matrix of eigenvectors of X'X</p> <code>None</code> <code>Dsq</code> <code>ndarray | None</code> <p>precomputed length-p vector of eigenvalues of X'X</p> <code>None</code> <code>dedup</code> <code>bool</code> <p>whether to deduplicate credible sets</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: list of L lists of SNP indices in each credible set</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if missing inputs for purity filtering</p> <code>ValueError</code> <p>if either LD or V, Dsq are None</p> Source code in <code>src/gentropy/method/susie_inf.py</code> <pre><code>@staticmethod\ndef cred_inf(\n    PIP: np.ndarray,\n    n: int = 100_000,\n    coverage: float = 0.99,\n    purity: float = 0.5,\n    LD: np.ndarray | None = None,\n    V: np.ndarray | None = None,\n    Dsq: np.ndarray | None = None,\n    dedup: bool = True,\n) -&gt; list[Any]:\n    \"\"\"Compute credible sets from single-effect PIPs.\n\n    Args:\n        PIP (np.ndarray): p x L matrix of PIPs\n        n (int): sample size\n        coverage (float): coverage of credible sets\n        purity (float): purity of credible sets\n        LD (np.ndarray | None): LD matrix (equal to X'X/n)\n        V (np.ndarray | None): precomputed p x p matrix of eigenvectors of X'X\n        Dsq (np.ndarray | None): precomputed length-p vector of eigenvalues of X'X\n        dedup (bool): whether to deduplicate credible sets\n\n    Returns:\n        list[Any]: list of L lists of SNP indices in each credible set\n\n    Raises:\n        RuntimeError: if missing inputs for purity filtering\n        ValueError: if either LD or V, Dsq are None\n    \"\"\"\n    if (V is None or Dsq is None or n is None) and LD is None:\n        raise RuntimeError(\"Missing inputs for purity filtering\")\n    # Compute credible sets\n    cred = []\n    for i in range(PIP.shape[1]):\n        sortinds = np.argsort(PIP[:, i])[::-1]\n        ind = min(np.nonzero(np.cumsum(PIP[sortinds, i]) &gt;= coverage)[0])\n        credset = sortinds[: (ind + 1)]\n        # Filter by purity\n        if len(credset) == 1:\n            cred.append(list(credset))\n            continue\n        if len(credset) &lt; 100:\n            rows = credset\n        else:\n            np.random.seed(123)\n            rows = np.random.choice(credset, size=100, replace=False)\n        if LD is not None:\n            LDloc = LD[np.ix_(rows, rows)]\n        elif V is not None and Dsq is not None:\n            LDloc = (V[rows, :] * Dsq).dot(V[rows, :].T) / n\n        else:\n            raise ValueError(\"Both LD and V, Dsq cannot be None\")\n        if np.min(np.abs(LDloc)) &gt; purity:\n            cred.append(sorted(credset))\n    if dedup:\n        cred = list(\n            map(\n                list,\n                sorted(set(map(tuple, cred)), key=list(map(tuple, cred)).index),\n            )\n        )\n    return cred\n</code></pre>"},{"location":"python_api/methods/susie_inf/#gentropy.method.susie_inf.SUSIE_inf.credible_set_qc","title":"<code>credible_set_qc(cred_sets: StudyLocus, p_value_threshold: float = 1e-05, purity_min_r2: float = 0.01, clump: bool = False, ld_index: LDIndex | None = None, study_index: StudyIndex | None = None, ld_min_r2: float | None = 0.8) -&gt; StudyLocus</code>  <code>staticmethod</code>","text":"<p>Filter credible sets by lead P-value and min-R2 purity, and performs LD clumping.</p> <p>In case of duplicated loci, the filtering retains the loci wth the highest credibleSetlog10BF.</p> <p>Parameters:</p> Name Type Description Default <code>cred_sets</code> <code>StudyLocus</code> <p>StudyLocus object with credible sets to filter/clump</p> required <code>p_value_threshold</code> <code>float</code> <p>p-value threshold for filtering credible sets, default is 1e-5</p> <code>1e-05</code> <code>purity_min_r2</code> <code>float</code> <p>min-R2 purity threshold for filtering credible sets, default is 0.01</p> <code>0.01</code> <code>clump</code> <code>bool</code> <p>Whether to clump the credible sets by LD, default is False</p> <code>False</code> <code>ld_index</code> <code>LDIndex | None</code> <p>LDIndex object</p> <code>None</code> <code>study_index</code> <code>StudyIndex | None</code> <p>StudyIndex object</p> <code>None</code> <code>ld_min_r2</code> <code>float | None</code> <p>LD R2 threshold for clumping, default is 0.8</p> <code>0.8</code> <p>Returns:</p> Name Type Description <code>StudyLocus</code> <code>StudyLocus</code> <p>Credible sets which pass filters and LD clumping.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>When running in clump mode, but no study study_index or ld_index or ld_min_r2 were provided.</p> Source code in <code>src/gentropy/method/susie_inf.py</code> <pre><code>@staticmethod\ndef credible_set_qc(\n    cred_sets: StudyLocus,\n    p_value_threshold: float = 1e-5,\n    purity_min_r2: float = 0.01,\n    clump: bool = False,\n    ld_index: LDIndex | None = None,\n    study_index: StudyIndex | None = None,\n    ld_min_r2: float | None = 0.8,\n) -&gt; StudyLocus:\n    \"\"\"Filter credible sets by lead P-value and min-R2 purity, and performs LD clumping.\n\n    In case of duplicated loci, the filtering retains the loci wth the highest credibleSetlog10BF.\n\n\n    Args:\n        cred_sets (StudyLocus): StudyLocus object with credible sets to filter/clump\n        p_value_threshold (float): p-value threshold for filtering credible sets, default is 1e-5\n        purity_min_r2 (float): min-R2 purity threshold for filtering credible sets, default is 0.01\n        clump (bool): Whether to clump the credible sets by LD, default is False\n        ld_index (LDIndex | None): LDIndex object\n        study_index (StudyIndex | None): StudyIndex object\n        ld_min_r2 (float | None): LD R2 threshold for clumping, default is 0.8\n\n    Returns:\n        StudyLocus: Credible sets which pass filters and LD clumping.\n\n    Raises:\n        AssertionError: When running in clump mode, but no study study_index or ld_index or ld_min_r2 were provided.\n    \"\"\"\n    cred_sets.df = (\n        cred_sets.df.withColumn(\n            \"pValue\", f.col(\"pValueMantissa\") * f.pow(10, f.col(\"pValueExponent\"))\n        )\n        .filter(f.col(\"pValue\") &lt;= p_value_threshold)\n        .filter(f.col(\"purityMinR2\") &gt;= purity_min_r2)\n        .drop(\"pValue\")\n        .withColumn(\n            \"rn\",\n            f.row_number().over(\n                Window.partitionBy(\"studyLocusId\").orderBy(\n                    f.desc(\"credibleSetLog10BF\")\n                )\n            ),\n        )\n        .filter(f.col(\"rn\") == 1)\n        .drop(\"rn\")\n    )\n    if clump:\n        assert study_index, \"Running in clump mode, which requires study_index.\"\n        assert ld_index, \"Running in clump mode, which requires ld_index.\"\n        assert ld_min_r2, \"Running in clump mode, which requires ld_min_r2 value.\"\n        cred_sets = (\n            cred_sets.annotate_ld(study_index, ld_index, ld_min_r2)\n            .clump()\n            .filter(\n                ~f.array_contains(\n                    f.col(\"qualityControls\"),\n                    StudyLocusQualityCheck.LD_CLUMPED.value,\n                )\n            )\n        )\n\n    return cred_sets\n</code></pre>"},{"location":"python_api/methods/susie_inf/#gentropy.method.susie_inf.SUSIE_inf.susie_inf","title":"<code>susie_inf(z: np.ndarray, meansq: float = 1, n: int = 100000, L: int = 10, LD: np.ndarray | None = None, V: np.ndarray | None = None, Dsq: np.ndarray | None = None, est_ssq: bool = True, ssq: np.ndarray | None = None, ssq_range: tuple[float, float] = (0, 1), pi0: np.ndarray | None = None, est_sigmasq: bool = True, est_tausq: bool = False, sigmasq: float = 1, tausq: float = 0, sigmasq_range: tuple[float, float] | None = None, tausq_range: tuple[float, float] | None = None, PIP: np.ndarray | None = None, mu: np.ndarray | None = None, method: str = 'moments', maxiter: int = 100, PIP_tol: float = 0.001) -&gt; dict[str, Any]</code>  <code>staticmethod</code>","text":"<p>Susie with random effects.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>ndarray</code> <p>vector of z-scores (equal to X'y/sqrt(n))</p> required <code>meansq</code> <code>float</code> <p>average squared magnitude of y (equal to ||y||^2/n)</p> <code>1</code> <code>n</code> <code>int</code> <p>sample size</p> <code>100000</code> <code>L</code> <code>int</code> <p>number of modeled causal effects</p> <code>10</code> <code>LD</code> <code>ndarray | None</code> <p>LD matrix (equal to X'X/n)</p> <code>None</code> <code>V</code> <code>ndarray | None</code> <p>precomputed p x p matrix of eigenvectors of X'X</p> <code>None</code> <code>Dsq</code> <code>ndarray | None</code> <p>precomputed length-p vector of eigenvalues of X'X</p> <code>None</code> <code>est_ssq</code> <code>bool</code> <p>estimate prior effect size variances s^2 using MLE</p> <code>True</code> <code>ssq</code> <code>ndarray | None</code> <p>length-L initialization s^2 for each effect</p> <code>None</code> <code>ssq_range</code> <code>tuple[float, float]</code> <p>lower and upper bounds for each s^2, if estimated</p> <code>(0, 1)</code> <code>pi0</code> <code>ndarray | None</code> <p>length-p vector of prior causal probability for each SNP; must sum to 1</p> <code>None</code> <code>est_sigmasq</code> <code>bool</code> <p>estimate variance sigma^2</p> <code>True</code> <code>est_tausq</code> <code>bool</code> <p>estimate both variances sigma^2 and tau^2</p> <code>False</code> <code>sigmasq</code> <code>float</code> <p>initial value for sigma^2</p> <code>1</code> <code>tausq</code> <code>float</code> <p>initial value for tau^2</p> <code>0</code> <code>sigmasq_range</code> <code>tuple[float, float] | None</code> <p>lower and upper bounds for sigma^2, if estimated using MLE</p> <code>None</code> <code>tausq_range</code> <code>tuple[float, float] | None</code> <p>lower and upper bounds for tau^2, if estimated using MLE</p> <code>None</code> <code>PIP</code> <code>ndarray | None</code> <p>p x L initializations of PIPs</p> <code>None</code> <code>mu</code> <code>ndarray | None</code> <p>p x L initializations of mu</p> <code>None</code> <code>method</code> <code>str</code> <p>one of {'moments','MLE'}</p> <code>'moments'</code> <code>maxiter</code> <code>int</code> <p>maximum number of SuSiE iterations</p> <code>100</code> <code>PIP_tol</code> <code>float</code> <p>convergence threshold for PIP difference between iterations</p> <code>0.001</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary with keys: PIP -- p x L matrix of PIPs, individually for each effect mu -- p x L matrix of posterior means conditional on causal omega -- p x L matrix of posterior precisions conditional on causal lbf_variable -- p x L matrix of log-Bayes-factors, for each effect ssq -- length-L array of final effect size variances s^2 sigmasq -- final value of sigma^2 tausq -- final value of tau^2 alpha -- length-p array of posterior means of infinitesimal effects lbf -- length-p array of log-Bayes-factors for each CS</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if missing LD or if unsupported variance estimation method</p> Source code in <code>src/gentropy/method/susie_inf.py</code> <pre><code>@staticmethod\ndef susie_inf(  # noqa: C901\n    z: np.ndarray,\n    meansq: float = 1,\n    n: int = 100000,\n    L: int = 10,\n    LD: np.ndarray | None = None,\n    V: np.ndarray | None = None,\n    Dsq: np.ndarray | None = None,\n    est_ssq: bool = True,\n    ssq: np.ndarray | None = None,\n    ssq_range: tuple[float, float] = (0, 1),\n    pi0: np.ndarray | None = None,\n    est_sigmasq: bool = True,\n    est_tausq: bool = False,\n    sigmasq: float = 1,\n    tausq: float = 0,\n    sigmasq_range: tuple[float, float] | None = None,\n    tausq_range: tuple[float, float] | None = None,\n    PIP: np.ndarray | None = None,\n    mu: np.ndarray | None = None,\n    method: str = \"moments\",\n    maxiter: int = 100,\n    PIP_tol: float = 0.001,\n) -&gt; dict[str, Any]:\n    \"\"\"Susie with random effects.\n\n    Args:\n        z (np.ndarray): vector of z-scores (equal to X'y/sqrt(n))\n        meansq (float): average squared magnitude of y (equal to ||y||^2/n)\n        n (int): sample size\n        L (int): number of modeled causal effects\n        LD (np.ndarray | None): LD matrix (equal to X'X/n)\n        V (np.ndarray | None): precomputed p x p matrix of eigenvectors of X'X\n        Dsq (np.ndarray | None): precomputed length-p vector of eigenvalues of X'X\n        est_ssq (bool): estimate prior effect size variances s^2 using MLE\n        ssq (np.ndarray | None): length-L initialization s^2 for each effect\n        ssq_range (tuple[float, float]): lower and upper bounds for each s^2, if estimated\n        pi0 (np.ndarray | None): length-p vector of prior causal probability for each SNP; must sum to 1\n        est_sigmasq (bool): estimate variance sigma^2\n        est_tausq (bool): estimate both variances sigma^2 and tau^2\n        sigmasq (float): initial value for sigma^2\n        tausq (float): initial value for tau^2\n        sigmasq_range (tuple[float, float] | None): lower and upper bounds for sigma^2, if estimated using MLE\n        tausq_range (tuple[float, float] | None): lower and upper bounds for tau^2, if estimated using MLE\n        PIP (np.ndarray | None): p x L initializations of PIPs\n        mu (np.ndarray | None): p x L initializations of mu\n        method (str): one of {'moments','MLE'}\n        maxiter (int): maximum number of SuSiE iterations\n        PIP_tol (float): convergence threshold for PIP difference between iterations\n\n    Returns:\n        dict[str, Any]: Dictionary with keys:\n            PIP -- p x L matrix of PIPs, individually for each effect\n            mu -- p x L matrix of posterior means conditional on causal\n            omega -- p x L matrix of posterior precisions conditional on causal\n            lbf_variable -- p x L matrix of log-Bayes-factors, for each effect\n            ssq -- length-L array of final effect size variances s^2\n            sigmasq -- final value of sigma^2\n            tausq -- final value of tau^2\n            alpha -- length-p array of posterior means of infinitesimal effects\n            lbf -- length-p array of log-Bayes-factors for each CS\n\n    Raises:\n        RuntimeError: if missing LD or if unsupported variance estimation method\n    \"\"\"\n    p = len(z)\n    # Precompute V,D^2 in the SVD X=UDV', and V'X'y and y'y\n    if (V is None or Dsq is None) and LD is None:\n        raise RuntimeError(\"Missing LD\")\n    elif V is None or Dsq is None:\n        eigvals, V = scipy.linalg.eigh(LD)\n        Dsq = np.maximum(n * eigvals, 0)\n    else:\n        Dsq = np.maximum(Dsq, 0)\n    Xty = np.sqrt(n) * z\n    VtXty = V.T.dot(Xty)\n    yty = n * meansq\n    # Initialize diagonal variances, diag(X' Omega X), X' Omega y\n    var = tausq * Dsq + sigmasq\n    diagXtOmegaX = np.sum(V**2 * (Dsq / var), axis=1)\n    XtOmegay = V.dot(VtXty / var)\n    # Initialize s_l^2, PIP_j, mu_j, omega_j\n    if ssq is None:\n        ssq = np.ones(L) * 0.2\n    if PIP is None:\n        PIP = np.ones((p, L)) / p\n    if mu is None:\n        mu = np.zeros((p, L))\n    lbf_variable = np.zeros((p, L))\n    omega = diagXtOmegaX[:, np.newaxis] + 1 / ssq\n    # Initialize prior causal probabilities\n    if pi0 is None:\n        logpi0 = np.ones(p) * np.log(1.0 / p)\n    else:\n        logpi0 = -np.ones(p) * np.inf\n        inds = np.nonzero(pi0 &gt; 0)[0]\n        logpi0[inds] = np.log(pi0[inds])\n\n    ####### Main SuSiE iteration loop ######\n    def f(x: float) -&gt; float:\n        \"\"\"Negative ELBO as function of x = sigma_e^2.\n\n        Args:\n            x (float): sigma_e^2\n\n        Returns:\n            float: negative ELBO as function of x = sigma_e^2\n        \"\"\"\n        return -scipy.special.logsumexp(\n            -0.5 * np.log(1 + x * diagXtOmegaX)\n            + x * XtOmegar**2 / (2 * (1 + x * diagXtOmegaX))\n            + logpi0\n        )\n\n    for it in range(maxiter):\n        PIP_prev = PIP.copy()\n        # Single effect regression for each effect l = 1,...,L\n        for _l in range(L):\n            # Compute X' Omega r_l for residual r_l\n            b = np.sum(mu * PIP, axis=1) - mu[:, _l] * PIP[:, _l]\n            XtOmegaXb = V.dot(V.T.dot(b) * Dsq / var)\n            XtOmegar = XtOmegay - XtOmegaXb\n            if est_ssq:\n                # Update prior variance ssq[l]\n                res = minimize_scalar(f, bounds=ssq_range, method=\"bounded\")\n                if res.success:\n                    ssq[_l] = res.x\n            # Update omega, mu, and PIP\n            omega[:, _l] = diagXtOmegaX + 1 / ssq[_l]\n            mu[:, _l] = XtOmegar / omega[:, _l]\n            lbf_variable[:, _l] = XtOmegar**2 / (2 * omega[:, _l]) - 0.5 * np.log(\n                omega[:, _l] * ssq[_l]\n            )\n            logPIP = lbf_variable[:, _l] + logpi0\n            PIP[:, _l] = np.exp(logPIP - scipy.special.logsumexp(logPIP))\n        # Update variance components\n        if est_sigmasq or est_tausq:\n            if method == \"moments\":\n                (sigmasq, tausq) = SUSIE_inf._MoM(\n                    PIP,\n                    mu,\n                    omega,\n                    sigmasq,\n                    tausq,\n                    n,\n                    V,\n                    Dsq,\n                    VtXty,\n                    Xty,\n                    yty,\n                    est_sigmasq,\n                    est_tausq,\n                )\n            elif method == \"MLE\":\n                (sigmasq, tausq) = SUSIE_inf._MLE(\n                    PIP,\n                    mu,\n                    omega,\n                    sigmasq,\n                    tausq,\n                    n,\n                    V,\n                    Dsq,\n                    VtXty,\n                    yty,\n                    est_sigmasq,\n                    est_tausq,\n                    it,\n                    sigmasq_range,\n                    tausq_range,\n                )\n            else:\n                raise RuntimeError(\"Unsupported variance estimation method\")\n            # Update X' Omega X, X' Omega y\n            var = tausq * Dsq + sigmasq\n            diagXtOmegaX = np.sum(V**2 * (Dsq / var), axis=1)\n            XtOmegay = V.dot(VtXty / var)\n        # Determine convergence from PIP differences\n        PIP_diff = np.max(np.abs(PIP_prev - PIP))\n        if PIP_diff &lt; PIP_tol:\n            break\n    # Compute posterior means of b and alpha\n    b = np.sum(mu * PIP, axis=1)\n    XtOmegaXb = V.dot(V.T.dot(b) * Dsq / var)\n    XtOmegar = XtOmegay - XtOmegaXb\n    alpha = tausq * XtOmegar\n\n    priors = np.log(np.repeat(1 / p, p))\n    lbf_cs = np.apply_along_axis(\n        lambda x: logsumexp(x + priors), axis=0, arr=lbf_variable\n    )\n    return {\n        \"PIP\": PIP,\n        \"mu\": mu,\n        \"omega\": omega,\n        \"lbf_variable\": lbf_variable,\n        \"ssq\": ssq,\n        \"sigmasq\": sigmasq,\n        \"tausq\": tausq,\n        \"alpha\": alpha,\n        \"lbf\": lbf_cs,\n    }\n</code></pre>"},{"location":"python_api/methods/l2g/_l2g/","title":"Locus to Gene (L2G) model","text":"<p>The \u201clocus-to-gene\u201d (L2G) model derives features to prioritize likely causal genes at each GWAS locus based on genetic and functional genomics features. The main categories of predictive features are:</p> <ul> <li>Distance: (from credible set variants to gene)</li> <li>Molecular QTL Colocalization</li> <li>Chromatin Interaction: (e.g., promoter-capture Hi-C)</li> <li>Variant Pathogenicity: (from VEP)</li> </ul> <p>Some of the predictive features weight variant-to-gene (or genomic region-to-gene) evidence based on the posterior probability that the variant is causal, determined through fine-mapping of the GWAS association.</p> <p>For a more detailed description of how each feature is computed, see the L2G Feature documentation.</p> <p>Details of the L2G model are provided in our Nature Genetics publication (ref - Nature Genetics Publication):</p> <ul> <li>Title: An open approach to systematically prioritize causal variants and genes at all published human GWAS trait-associated loci.</li> <li>Authors: Mountjoy, E., Schmidt, E.M., Carmona, M. et al.</li> <li>Journal: Nat Genet 53, 1527\u20131533 (2021).</li> <li>DOI: 10.1038/s41588-021-00945-5</li> </ul>"},{"location":"python_api/methods/l2g/feature_factory/","title":"L2G Feature Factory","text":""},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.FeatureFactory","title":"<code>gentropy.method.l2g.feature_factory.FeatureFactory</code>","text":"<p>Factory class for creating features.</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>class FeatureFactory:\n    \"\"\"Factory class for creating features.\"\"\"\n\n    feature_mapper: Mapping[str, type[L2GFeature]] = {\n        \"distanceSentinelTss\": DistanceSentinelTssFeature,\n        \"distanceSentinelTssNeighbourhood\": DistanceSentinelTssNeighbourhoodFeature,\n        \"distanceSentinelFootprint\": DistanceSentinelFootprintFeature,\n        \"distanceSentinelFootprintNeighbourhood\": DistanceSentinelFootprintNeighbourhoodFeature,\n        \"distanceTssMean\": DistanceTssMeanFeature,\n        \"distanceTssMeanNeighbourhood\": DistanceTssMeanNeighbourhoodFeature,\n        \"distanceFootprintMean\": DistanceFootprintMeanFeature,\n        \"distanceFootprintMeanNeighbourhood\": DistanceFootprintMeanNeighbourhoodFeature,\n        \"eQtlColocClppMaximum\": EQtlColocClppMaximumFeature,\n        \"eQtlColocClppMaximumNeighbourhood\": EQtlColocClppMaximumNeighbourhoodFeature,\n        \"pQtlColocClppMaximum\": PQtlColocClppMaximumFeature,\n        \"pQtlColocClppMaximumNeighbourhood\": PQtlColocClppMaximumNeighbourhoodFeature,\n        \"sQtlColocClppMaximum\": SQtlColocClppMaximumFeature,\n        \"sQtlColocClppMaximumNeighbourhood\": SQtlColocClppMaximumNeighbourhoodFeature,\n        \"eQtlColocH4Maximum\": EQtlColocH4MaximumFeature,\n        \"eQtlColocH4MaximumNeighbourhood\": EQtlColocH4MaximumNeighbourhoodFeature,\n        \"pQtlColocH4Maximum\": PQtlColocH4MaximumFeature,\n        \"pQtlColocH4MaximumNeighbourhood\": PQtlColocH4MaximumNeighbourhoodFeature,\n        \"sQtlColocH4Maximum\": SQtlColocH4MaximumFeature,\n        \"sQtlColocH4MaximumNeighbourhood\": SQtlColocH4MaximumNeighbourhoodFeature,\n        \"vepMean\": VepMeanFeature,\n        \"vepMeanNeighbourhood\": VepMeanNeighbourhoodFeature,\n        \"vepMaximum\": VepMaximumFeature,\n        \"vepMaximumNeighbourhood\": VepMaximumNeighbourhoodFeature,\n        \"geneCount500kb\": GeneCountFeature,\n        \"proteinGeneCount500kb\": ProteinGeneCountFeature,\n        \"isProteinCoding\": ProteinCodingFeature,\n        \"credibleSetConfidence\": CredibleSetConfidenceFeature,\n    }\n\n    def __init__(\n        self: FeatureFactory,\n        study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n        features_list: list[str],\n    ) -&gt; None:\n        \"\"\"Initializes the factory.\n\n        Args:\n            study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n            features_list (list[str]): list of features to compute.\n        \"\"\"\n        self.study_loci_to_annotate = study_loci_to_annotate\n        self.features_list = features_list\n\n    def generate_features(\n        self: FeatureFactory,\n        features_input_loader: L2GFeatureInputLoader,\n    ) -&gt; list[L2GFeature]:\n        \"\"\"Generates a feature matrix by reading an object with instructions on how to create the features.\n\n        Args:\n            features_input_loader (L2GFeatureInputLoader): object with required features dependencies.\n\n        Returns:\n            list[L2GFeature]: list of computed features.\n\n        Raises:\n            ValueError: If feature not found.\n        \"\"\"\n        computed_features = []\n        for feature in self.features_list:\n            if feature in self.feature_mapper:\n                computed_features.append(\n                    self.compute_feature(feature, features_input_loader)\n                )\n            else:\n                raise ValueError(f\"Feature {feature} not found.\")\n        return computed_features\n\n    def compute_feature(\n        self: FeatureFactory,\n        feature_name: str,\n        features_input_loader: L2GFeatureInputLoader,\n    ) -&gt; L2GFeature:\n        \"\"\"Instantiates feature class.\n\n        Args:\n            feature_name (str): name of the feature\n            features_input_loader (L2GFeatureInputLoader): Object that contais features input.\n\n        Returns:\n            L2GFeature: instantiated feature object\n        \"\"\"\n        # Extract feature class and dependency type\n        feature_cls = self.feature_mapper[feature_name]\n        feature_dependency_type = feature_cls.feature_dependency_type\n        return feature_cls.compute(\n            study_loci_to_annotate=self.study_loci_to_annotate,\n            feature_dependency=features_input_loader.get_dependency_by_type(\n                feature_dependency_type\n            ),\n        )\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.FeatureFactory.__init__","title":"<code>__init__(study_loci_to_annotate: StudyLocus | L2GGoldStandard, features_list: list[str]) -&gt; None</code>","text":"<p>Initializes the factory.</p> <p>Parameters:</p> Name Type Description Default <code>study_loci_to_annotate</code> <code>StudyLocus | L2GGoldStandard</code> <p>The dataset containing study loci that will be used for annotation</p> required <code>features_list</code> <code>list[str]</code> <p>list of features to compute.</p> required Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def __init__(\n    self: FeatureFactory,\n    study_loci_to_annotate: StudyLocus | L2GGoldStandard,\n    features_list: list[str],\n) -&gt; None:\n    \"\"\"Initializes the factory.\n\n    Args:\n        study_loci_to_annotate (StudyLocus | L2GGoldStandard): The dataset containing study loci that will be used for annotation\n        features_list (list[str]): list of features to compute.\n    \"\"\"\n    self.study_loci_to_annotate = study_loci_to_annotate\n    self.features_list = features_list\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.FeatureFactory.compute_feature","title":"<code>compute_feature(feature_name: str, features_input_loader: L2GFeatureInputLoader) -&gt; L2GFeature</code>","text":"<p>Instantiates feature class.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>name of the feature</p> required <code>features_input_loader</code> <code>L2GFeatureInputLoader</code> <p>Object that contais features input.</p> required <p>Returns:</p> Name Type Description <code>L2GFeature</code> <code>L2GFeature</code> <p>instantiated feature object</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def compute_feature(\n    self: FeatureFactory,\n    feature_name: str,\n    features_input_loader: L2GFeatureInputLoader,\n) -&gt; L2GFeature:\n    \"\"\"Instantiates feature class.\n\n    Args:\n        feature_name (str): name of the feature\n        features_input_loader (L2GFeatureInputLoader): Object that contais features input.\n\n    Returns:\n        L2GFeature: instantiated feature object\n    \"\"\"\n    # Extract feature class and dependency type\n    feature_cls = self.feature_mapper[feature_name]\n    feature_dependency_type = feature_cls.feature_dependency_type\n    return feature_cls.compute(\n        study_loci_to_annotate=self.study_loci_to_annotate,\n        feature_dependency=features_input_loader.get_dependency_by_type(\n            feature_dependency_type\n        ),\n    )\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.FeatureFactory.generate_features","title":"<code>generate_features(features_input_loader: L2GFeatureInputLoader) -&gt; list[L2GFeature]</code>","text":"<p>Generates a feature matrix by reading an object with instructions on how to create the features.</p> <p>Parameters:</p> Name Type Description Default <code>features_input_loader</code> <code>L2GFeatureInputLoader</code> <p>object with required features dependencies.</p> required <p>Returns:</p> Type Description <code>list[L2GFeature]</code> <p>list[L2GFeature]: list of computed features.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If feature not found.</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def generate_features(\n    self: FeatureFactory,\n    features_input_loader: L2GFeatureInputLoader,\n) -&gt; list[L2GFeature]:\n    \"\"\"Generates a feature matrix by reading an object with instructions on how to create the features.\n\n    Args:\n        features_input_loader (L2GFeatureInputLoader): object with required features dependencies.\n\n    Returns:\n        list[L2GFeature]: list of computed features.\n\n    Raises:\n        ValueError: If feature not found.\n    \"\"\"\n    computed_features = []\n    for feature in self.features_list:\n        if feature in self.feature_mapper:\n            computed_features.append(\n                self.compute_feature(feature, features_input_loader)\n            )\n        else:\n            raise ValueError(f\"Feature {feature} not found.\")\n    return computed_features\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.L2GFeatureInputLoader","title":"<code>gentropy.method.l2g.feature_factory.L2GFeatureInputLoader</code>","text":"<p>Loads all input datasets required for the L2GFeature dataset.</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>class L2GFeatureInputLoader:\n    \"\"\"Loads all input datasets required for the L2GFeature dataset.\"\"\"\n\n    def __init__(\n        self,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initializes L2GFeatureInputLoader with the provided inputs and returns loaded dependencies as a dictionary.\n\n        Args:\n            **kwargs (Any): keyword arguments with the name of the dependency and the dependency itself.\n        \"\"\"\n        self.input_dependencies = {k: v for k, v in kwargs.items() if v is not None}\n\n    def get_dependency_by_type(\n        self, dependency_type: list[Any] | Any\n    ) -&gt; dict[str, Any]:\n        \"\"\"Returns the dependency that matches the provided type.\n\n        Args:\n            dependency_type (list[Any] | Any): type(s) of the dependency to return.\n\n        Returns:\n            dict[str, Any]: dictionary of dependenci(es) that match the provided type(s).\n        \"\"\"\n        if not isinstance(dependency_type, list):\n            dependency_type = [dependency_type]\n        return {\n            k: v\n            for k, v in self.input_dependencies.items()\n            if isinstance(v, tuple(dependency_type))\n        }\n\n    def __iter__(self) -&gt; Iterator[tuple[str, Any]]:\n        \"\"\"Make the class iterable, returning an iterator over key-value pairs.\n\n        Returns:\n            Iterator[tuple[str, Any]]: iterator over the dictionary's key-value pairs.\n        \"\"\"\n        return iter(self.input_dependencies.items())\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the input dependencies.\n\n        Useful for understanding the loader content without having to print the object attribute.\n\n        Returns:\n            str: string representation of the input dependencies.\n        \"\"\"\n        return repr(self.input_dependencies)\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.L2GFeatureInputLoader.__init__","title":"<code>__init__(**kwargs: Any) -&gt; None</code>","text":"<p>Initializes L2GFeatureInputLoader with the provided inputs and returns loaded dependencies as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments with the name of the dependency and the dependency itself.</p> <code>{}</code> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def __init__(\n    self,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initializes L2GFeatureInputLoader with the provided inputs and returns loaded dependencies as a dictionary.\n\n    Args:\n        **kwargs (Any): keyword arguments with the name of the dependency and the dependency itself.\n    \"\"\"\n    self.input_dependencies = {k: v for k, v in kwargs.items() if v is not None}\n</code></pre>"},{"location":"python_api/methods/l2g/feature_factory/#gentropy.method.l2g.feature_factory.L2GFeatureInputLoader.get_dependency_by_type","title":"<code>get_dependency_by_type(dependency_type: list[Any] | Any) -&gt; dict[str, Any]</code>","text":"<p>Returns the dependency that matches the provided type.</p> <p>Parameters:</p> Name Type Description Default <code>dependency_type</code> <code>list[Any] | Any</code> <p>type(s) of the dependency to return.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: dictionary of dependenci(es) that match the provided type(s).</p> Source code in <code>src/gentropy/method/l2g/feature_factory.py</code> <pre><code>def get_dependency_by_type(\n    self, dependency_type: list[Any] | Any\n) -&gt; dict[str, Any]:\n    \"\"\"Returns the dependency that matches the provided type.\n\n    Args:\n        dependency_type (list[Any] | Any): type(s) of the dependency to return.\n\n    Returns:\n        dict[str, Any]: dictionary of dependenci(es) that match the provided type(s).\n    \"\"\"\n    if not isinstance(dependency_type, list):\n        dependency_type = [dependency_type]\n    return {\n        k: v\n        for k, v in self.input_dependencies.items()\n        if isinstance(v, tuple(dependency_type))\n    }\n</code></pre>"},{"location":"python_api/methods/l2g/model/","title":"L2G Model","text":""},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel","title":"<code>gentropy.method.l2g.model.LocusToGeneModel</code>  <code>dataclass</code>","text":"<p>Wrapper for the Locus to Gene classifier.</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>@dataclass\nclass LocusToGeneModel:\n    \"\"\"Wrapper for the Locus to Gene classifier.\"\"\"\n\n    model: Any = XGBClassifier(random_state=42, eval_metric=\"aucpr\")\n    features_list: list[str] = field(default_factory=list)\n    hyperparameters: dict[str, Any] = field(\n        default_factory=lambda: {\n            \"max_depth\": 5,\n            \"reg_alpha\": 1,  # L1 regularization\n            \"reg_lambda\": 1.0,  # L2 regularization\n            \"subsample\": 0.8,\n            \"colsample_bytree\": 0.8,\n            \"eta\": 0.05,\n            \"min_child_weight\": 10,\n            \"scale_pos_weight\": 0.8,\n        }\n    )\n    training_data: L2GFeatureMatrix | None = None\n    label_encoder: dict[str, int] = field(\n        default_factory=lambda: {\n            \"negative\": 0,\n            \"positive\": 1,\n        }\n    )\n\n    def __post_init__(self: LocusToGeneModel) -&gt; None:\n        \"\"\"Post-initialisation to fit the estimator with the provided params.\"\"\"\n        self.model.set_params(**self.hyperparameters_dict)\n\n    @classmethod\n    def load_from_disk(\n        cls: type[LocusToGeneModel],\n        session: Session,\n        path: str,\n        model_name: str = \"classifier.skops\",\n        **kwargs: Any,\n    ) -&gt; LocusToGeneModel:\n        \"\"\"Load a fitted model from disk.\n\n        Args:\n            session (Session): Session object that loads the training data\n            path (str): Path to the directory containing model and metadata\n            model_name (str): Name of the persisted model to load. Defaults to \"classifier.skops\".\n            **kwargs(Any): Keyword arguments to pass to the constructor\n\n        Returns:\n            LocusToGeneModel: L2G model loaded from disk\n\n        Raises:\n            ValueError: If the model has not been fitted yet\n        \"\"\"\n        model_path = (Path(path) / model_name).as_posix()\n        if model_path.startswith(\"gs://\"):\n            path = model_path.removeprefix(\"gs://\")\n            bucket_name = path.split(\"/\")[0]\n            blob_name = \"/\".join(path.split(\"/\")[1:])\n            from google.cloud import storage\n\n            client = storage.Client()\n            bucket = storage.Bucket(client=client, name=bucket_name)\n            blob = storage.Blob(name=blob_name, bucket=bucket)\n            data = blob.download_as_string(client=client)\n            loaded_model = sio.loads(data, trusted=sio.get_untrusted_types(data=data))\n        else:\n            loaded_model = sio.load(\n                model_path, trusted=sio.get_untrusted_types(file=model_path)\n            )\n            try:\n                # Try loading the training data if it is in the model directory\n                training_data = L2GFeatureMatrix(\n                    _df=session.spark.createDataFrame(\n                        # Parquets are read with Pandas to easily read local files\n                        pd.concat(\n                            [\n                                pd.read_parquet(\n                                    (Path(path) / \"train.parquet\").as_posix()\n                                ),\n                                pd.read_parquet(\n                                    (Path(path) / \"test.parquet\").as_posix()\n                                ),\n                            ]\n                        )\n                    ),\n                    features_list=kwargs.get(\"features_list\"),\n                )\n            except Exception as e:\n                logging.error(\"Training data set to none. Error: %s\", e)\n                training_data = None\n\n        if (\n            isinstance(loaded_model, GradientBoostingClassifier)\n            and not loaded_model._is_fitted()\n        ) or (\n            isinstance(loaded_model, XGBClassifier) and not loaded_model.get_booster()\n        ):\n            raise ValueError(\"Model has not been fitted yet.\")\n        return cls(model=loaded_model, training_data=training_data, **kwargs)\n\n    @classmethod\n    def load_from_hub(\n        cls: type[LocusToGeneModel],\n        session: Session,\n        hf_model_id: str,\n        hf_model_version: str | None = None,\n        hf_token: str | None = None,\n    ) -&gt; LocusToGeneModel:\n        \"\"\"Load a model from the Hugging Face Hub. This will download the model from the hub and load it from disk.\n\n        Args:\n            session (Session): Session object to load the training data\n            hf_model_id (str): Model ID on the Hugging Face Hub\n            hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n            hf_token (str | None): Hugging Face Hub token to download the model (only required if private)\n\n        Returns:\n            LocusToGeneModel: L2G model loaded from the Hugging Face Hub\n        \"\"\"\n        from huggingface_hub import snapshot_download\n\n        def get_features_list_from_metadata() -&gt; list[str]:\n            \"\"\"Get the features list (in the right order) from the metadata JSON file downloaded from the Hub.\n\n            Returns:\n                list[str]: Features list\n            \"\"\"\n            model_config_path = str(Path(local_path) / \"config.json\")\n            with open(model_config_path) as f:\n                model_config = json.load(f)\n            return [\n                column\n                for column in model_config[\"sklearn\"][\"columns\"]\n                if column\n                not in [\n                    \"studyLocusId\",\n                    \"geneId\",\n                    \"traitFromSourceMappedId\",\n                    \"goldStandardSet\",\n                ]\n            ]\n\n        local_path = snapshot_download(\n            repo_id=hf_model_id,\n            local_dir=hf_model_id,\n            revision=hf_model_version,\n            token=hf_token,\n        )\n        features_list = get_features_list_from_metadata()\n        return cls.load_from_disk(\n            session,\n            local_path,\n            features_list=features_list,\n        )\n\n    @property\n    def hyperparameters_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return hyperparameters as a dictionary.\n\n        Returns:\n            dict[str, Any]: Hyperparameters\n\n        Raises:\n            ValueError: If hyperparameters have not been set\n        \"\"\"\n        if not self.hyperparameters:\n            raise ValueError(\"Hyperparameters have not been set.\")\n        elif isinstance(self.hyperparameters, dict):\n            return self.hyperparameters\n        return self.hyperparameters.default_factory()\n\n    def predict(\n        self: LocusToGeneModel,\n        feature_matrix: L2GFeatureMatrix,\n        session: Session,\n    ) -&gt; L2GPrediction:\n        \"\"\"Apply the model to a given feature matrix dataframe. The feature matrix needs to be preprocessed first.\n\n        Args:\n            feature_matrix (L2GFeatureMatrix): Feature matrix to apply the model to.\n            session (Session): Session object to convert data to Spark\n\n        Returns:\n            L2GPrediction: Dataset containing credible sets and their L2G scores\n        \"\"\"\n        from gentropy.dataset.l2g_prediction import L2GPrediction\n\n        pd_dataframe.iteritems = pd_dataframe.items\n\n        feature_matrix_pdf = feature_matrix._df.toPandas()\n        # L2G score is the probability the classifier assigns to the positive class (the second element in the probability array)\n        feature_matrix_pdf[\"score\"] = self.model.predict_proba(\n            # We drop the fixed columns to only pass the feature values to the classifier\n            feature_matrix_pdf.drop(feature_matrix.fixed_cols, axis=1)\n            .apply(pd_to_numeric)\n            .values\n        )[:, 1]\n        output_cols = [field.name for field in L2GPrediction.get_schema().fields]\n        return L2GPrediction(\n            _df=session.spark.createDataFrame(feature_matrix_pdf.filter(output_cols)),\n            _schema=L2GPrediction.get_schema(),\n            model=self,\n        )\n\n    def save(self: LocusToGeneModel, path: str) -&gt; None:\n        \"\"\"Saves fitted model to disk using the skops persistence format.\n\n        Args:\n            path (str): Path to save the persisted model. Should end with .skops\n\n        Raises:\n            ValueError: If the model has not been fitted yet or if the path does not end with .skops\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model has not been fitted yet.\")\n        if not path.endswith(\".skops\"):\n            raise ValueError(\"Path should end with .skops\")\n        if path.startswith(\"gs://\"):\n            local_path = path.split(\"/\")[-1]\n            sio.dump(self.model, local_path)\n            copy_to_gcs(local_path, path)\n        else:\n            # create directory if path does not exist\n            Path(path).parent.mkdir(parents=True, exist_ok=True)\n            sio.dump(self.model, path)\n\n    def _create_hugging_face_model_card(\n        self: LocusToGeneModel,\n        local_repo: str,\n    ) -&gt; None:\n        \"\"\"Create a model card to document the model in the hub. The model card is saved in the local repo before pushing it to the hub.\n\n        Args:\n            local_repo (str): Path to the folder where the README file will be saved to be pushed to the Hugging Face Hub\n        \"\"\"\n        card_data = ModelCardData(\n            language=\"en\",\n            license=\"mit\",\n            library_name=\"sklearn\",\n            tags=[\n                \"sklearn\",\n                \"tabular-classification\",\n                \"genomics\",\n                \"gwas\",\n                \"gene-prioritization\",\n            ],\n        )\n\n        # Create model card with custom template\n        card = ModelCard.from_template(\n            card_data,\n            template_path=None,\n        )\n\n        card.text = \"\"\"# Locus-to-Gene (L2G) Model\n\nThe locus-to-gene (L2G) model prioritises likely causal genes at each GWAS locus based on genetic and functional genomics features.\n\n## Model Description\n\nThis is a **Gradient Boosting Classifier** (XGBoost) trained to predict causal genes at GWAS loci.\n\nLimited to protein-coding genes with available feature data.\n\n**Key Features:**\n- **Distance**: proximity from credible set variants to gene\n- **Molecular QTL Colocalization**: evidence from expression/protein QTL studies\n- **Variant Pathogenicity**: VEP (Variant Effect Predictor) scores\n\n## Usage\n\n```python\nfrom gentropy.method.l2g.model import LocusToGeneModel\nfrom gentropy.common.session import Session\n\n# Load model from Hugging Face Hub\nsession = Session()\nmodel = LocusToGeneModel.load_from_hub(\n    session=session,\n    hf_model_id=\"opentargets/locus_to_gene\"\n)\n\n# Make predictions on your L2G feature matrix\npredictions = model.predict(your_feature_matrix, session)\n```\n\n## Training\n\n- **Architecture**: XGBoost Gradient Boosting Classifier\n- **Training Data**: Curated positive/negative gene-locus pairs from Open Targets\n- **Evaluation Metric**: Area under precision-recall curve (AUCPR)\n\n## Citation\n\nIf you use this model, please cite:\n\n```bibtex\n@article{ghoussaini2021open,\ntitle={Open Targets Genetics: systematic identification of trait-associated genes using large-scale genetics and functional genomics},\nauthor={Ghoussaini, Maya and Mountjoy, Edward and Carmona, Maria and others},\njournal={Nature Genetics},\nvolume={53},\npages={1527--1533},\nyear={2021},\ndoi={10.1038/s41588-021-00945-5}\n}\n```\n\n## More Information\n\n- **Repository**: [opentargets/gentropy](https://github.com/opentargets/gentropy)\n- **Documentation**: [L2G Method Docs](https://opentargets.github.io/gentropy/python_api/methods/l2g/_l2g/)\n- **Developer**: Open Targets\n\"\"\"\n        card.save(Path(local_repo) / \"README.md\")\n\n    def export_to_hugging_face_hub(\n        self: LocusToGeneModel,\n        model_path: str,\n        hf_hub_token: str,\n        feature_matrix: L2GFeatureMatrix,\n        commit_message: str,\n        repo_id: str = \"opentargets/locus_to_gene\",\n        test_size: float = 0.15,\n    ) -&gt; None:\n        \"\"\"Share the model and training dataset on Hugging Face Hub.\n\n        This will save both the trained model and the train/test splits used for\n        training to enable full reproducibility.\n\n        Args:\n            model_path (str): The path to the L2G model file.\n            hf_hub_token (str): Hugging Face Hub token\n            feature_matrix (L2GFeatureMatrix): Data used to train the model. This is used to have an example input for the model and to store the column order.\n            commit_message (str): Commit message for the push\n            repo_id (str): The Hugging Face Hub repo id where the model will be stored.\n            test_size (float): Proportion of data to include in the test split. Defaults to 0.15\n\n        Raises:\n            RuntimeError: If the push to the Hugging Face Hub fails\n        \"\"\"\n        import shutil\n        import tempfile\n\n        from sklearn import __version__ as sklearn_version\n\n        # Create a temporary directory for all operations\n        with tempfile.TemporaryDirectory(prefix=\"l2g_hf_hub_\") as temp_dir:\n            temp_dir_path = Path(temp_dir)\n\n            try:\n                # Create train/test split\n                train_df, test_df = feature_matrix.generate_train_test_split(\n                    test_size=test_size,\n                    verbose=True,\n                    label_encoder=self.label_encoder,\n                    label_col=feature_matrix.label_col,\n                )\n                train_df.to_parquet(temp_dir_path / \"train.parquet\")\n                test_df.to_parquet(temp_dir_path / \"test.parquet\")\n\n                shutil.copy(model_path, temp_dir_path / \"classifier.skops\")\n\n                with open(temp_dir_path / \"config.json\", \"w\") as f:\n                    config = {\n                        \"sklearn\": {\n                            \"columns\": train_df.columns.tolist(),\n                            \"sklearn_version\": sklearn_version,\n                        },\n                        \"task\": \"tabular-classification\",\n                    }\n                    json.dump(config, f, indent=2)\n\n                with open(temp_dir_path / \"requirements.txt\", \"w\") as f:\n                    f.write(f\"scikit-learn=={sklearn_version}\\n\")\n                    f.write(\"skops\\n\")\n\n                # Create model card\n                self._create_hugging_face_model_card(str(temp_dir_path))\n\n                # Create repo if it doesn't exist and upload\n                api = HfApi(token=hf_hub_token)\n                create_repo(repo_id, exist_ok=True, token=hf_hub_token)\n\n                api.upload_folder(\n                    folder_path=str(temp_dir_path),\n                    repo_id=repo_id,\n                    commit_message=commit_message,\n                )\n\n            except Exception as e:\n                raise RuntimeError(f\"Failed to push to Hugging Face Hub: {e}\") from e\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.hyperparameters_dict","title":"<code>hyperparameters_dict: dict[str, Any]</code>  <code>property</code>","text":"<p>Return hyperparameters as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Hyperparameters</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If hyperparameters have not been set</p>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.export_to_hugging_face_hub","title":"<code>export_to_hugging_face_hub(model_path: str, hf_hub_token: str, feature_matrix: L2GFeatureMatrix, commit_message: str, repo_id: str = 'opentargets/locus_to_gene', test_size: float = 0.15) -&gt; None</code>","text":"<p>Share the model and training dataset on Hugging Face Hub.</p> <p>This will save both the trained model and the train/test splits used for training to enable full reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the L2G model file.</p> required <code>hf_hub_token</code> <code>str</code> <p>Hugging Face Hub token</p> required <code>feature_matrix</code> <code>L2GFeatureMatrix</code> <p>Data used to train the model. This is used to have an example input for the model and to store the column order.</p> required <code>commit_message</code> <code>str</code> <p>Commit message for the push</p> required <code>repo_id</code> <code>str</code> <p>The Hugging Face Hub repo id where the model will be stored.</p> <code>'opentargets/locus_to_gene'</code> <code>test_size</code> <code>float</code> <p>Proportion of data to include in the test split. Defaults to 0.15</p> <code>0.15</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the push to the Hugging Face Hub fails</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>def export_to_hugging_face_hub(\n    self: LocusToGeneModel,\n    model_path: str,\n    hf_hub_token: str,\n    feature_matrix: L2GFeatureMatrix,\n    commit_message: str,\n    repo_id: str = \"opentargets/locus_to_gene\",\n    test_size: float = 0.15,\n) -&gt; None:\n    \"\"\"Share the model and training dataset on Hugging Face Hub.\n\n    This will save both the trained model and the train/test splits used for\n    training to enable full reproducibility.\n\n    Args:\n        model_path (str): The path to the L2G model file.\n        hf_hub_token (str): Hugging Face Hub token\n        feature_matrix (L2GFeatureMatrix): Data used to train the model. This is used to have an example input for the model and to store the column order.\n        commit_message (str): Commit message for the push\n        repo_id (str): The Hugging Face Hub repo id where the model will be stored.\n        test_size (float): Proportion of data to include in the test split. Defaults to 0.15\n\n    Raises:\n        RuntimeError: If the push to the Hugging Face Hub fails\n    \"\"\"\n    import shutil\n    import tempfile\n\n    from sklearn import __version__ as sklearn_version\n\n    # Create a temporary directory for all operations\n    with tempfile.TemporaryDirectory(prefix=\"l2g_hf_hub_\") as temp_dir:\n        temp_dir_path = Path(temp_dir)\n\n        try:\n            # Create train/test split\n            train_df, test_df = feature_matrix.generate_train_test_split(\n                test_size=test_size,\n                verbose=True,\n                label_encoder=self.label_encoder,\n                label_col=feature_matrix.label_col,\n            )\n            train_df.to_parquet(temp_dir_path / \"train.parquet\")\n            test_df.to_parquet(temp_dir_path / \"test.parquet\")\n\n            shutil.copy(model_path, temp_dir_path / \"classifier.skops\")\n\n            with open(temp_dir_path / \"config.json\", \"w\") as f:\n                config = {\n                    \"sklearn\": {\n                        \"columns\": train_df.columns.tolist(),\n                        \"sklearn_version\": sklearn_version,\n                    },\n                    \"task\": \"tabular-classification\",\n                }\n                json.dump(config, f, indent=2)\n\n            with open(temp_dir_path / \"requirements.txt\", \"w\") as f:\n                f.write(f\"scikit-learn=={sklearn_version}\\n\")\n                f.write(\"skops\\n\")\n\n            # Create model card\n            self._create_hugging_face_model_card(str(temp_dir_path))\n\n            # Create repo if it doesn't exist and upload\n            api = HfApi(token=hf_hub_token)\n            create_repo(repo_id, exist_ok=True, token=hf_hub_token)\n\n            api.upload_folder(\n                folder_path=str(temp_dir_path),\n                repo_id=repo_id,\n                commit_message=commit_message,\n            )\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to push to Hugging Face Hub: {e}\") from e\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.load_from_disk","title":"<code>load_from_disk(session: Session, path: str, model_name: str = 'classifier.skops', **kwargs: Any) -&gt; LocusToGeneModel</code>  <code>classmethod</code>","text":"<p>Load a fitted model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that loads the training data</p> required <code>path</code> <code>str</code> <p>Path to the directory containing model and metadata</p> required <code>model_name</code> <code>str</code> <p>Name of the persisted model to load. Defaults to \"classifier.skops\".</p> <code>'classifier.skops'</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the constructor</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LocusToGeneModel</code> <code>LocusToGeneModel</code> <p>L2G model loaded from disk</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been fitted yet</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>@classmethod\ndef load_from_disk(\n    cls: type[LocusToGeneModel],\n    session: Session,\n    path: str,\n    model_name: str = \"classifier.skops\",\n    **kwargs: Any,\n) -&gt; LocusToGeneModel:\n    \"\"\"Load a fitted model from disk.\n\n    Args:\n        session (Session): Session object that loads the training data\n        path (str): Path to the directory containing model and metadata\n        model_name (str): Name of the persisted model to load. Defaults to \"classifier.skops\".\n        **kwargs(Any): Keyword arguments to pass to the constructor\n\n    Returns:\n        LocusToGeneModel: L2G model loaded from disk\n\n    Raises:\n        ValueError: If the model has not been fitted yet\n    \"\"\"\n    model_path = (Path(path) / model_name).as_posix()\n    if model_path.startswith(\"gs://\"):\n        path = model_path.removeprefix(\"gs://\")\n        bucket_name = path.split(\"/\")[0]\n        blob_name = \"/\".join(path.split(\"/\")[1:])\n        from google.cloud import storage\n\n        client = storage.Client()\n        bucket = storage.Bucket(client=client, name=bucket_name)\n        blob = storage.Blob(name=blob_name, bucket=bucket)\n        data = blob.download_as_string(client=client)\n        loaded_model = sio.loads(data, trusted=sio.get_untrusted_types(data=data))\n    else:\n        loaded_model = sio.load(\n            model_path, trusted=sio.get_untrusted_types(file=model_path)\n        )\n        try:\n            # Try loading the training data if it is in the model directory\n            training_data = L2GFeatureMatrix(\n                _df=session.spark.createDataFrame(\n                    # Parquets are read with Pandas to easily read local files\n                    pd.concat(\n                        [\n                            pd.read_parquet(\n                                (Path(path) / \"train.parquet\").as_posix()\n                            ),\n                            pd.read_parquet(\n                                (Path(path) / \"test.parquet\").as_posix()\n                            ),\n                        ]\n                    )\n                ),\n                features_list=kwargs.get(\"features_list\"),\n            )\n        except Exception as e:\n            logging.error(\"Training data set to none. Error: %s\", e)\n            training_data = None\n\n    if (\n        isinstance(loaded_model, GradientBoostingClassifier)\n        and not loaded_model._is_fitted()\n    ) or (\n        isinstance(loaded_model, XGBClassifier) and not loaded_model.get_booster()\n    ):\n        raise ValueError(\"Model has not been fitted yet.\")\n    return cls(model=loaded_model, training_data=training_data, **kwargs)\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.load_from_hub","title":"<code>load_from_hub(session: Session, hf_model_id: str, hf_model_version: str | None = None, hf_token: str | None = None) -&gt; LocusToGeneModel</code>  <code>classmethod</code>","text":"<p>Load a model from the Hugging Face Hub. This will download the model from the hub and load it from disk.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object to load the training data</p> required <code>hf_model_id</code> <code>str</code> <p>Model ID on the Hugging Face Hub</p> required <code>hf_model_version</code> <code>str | None</code> <p>Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.</p> <code>None</code> <code>hf_token</code> <code>str | None</code> <p>Hugging Face Hub token to download the model (only required if private)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LocusToGeneModel</code> <code>LocusToGeneModel</code> <p>L2G model loaded from the Hugging Face Hub</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>@classmethod\ndef load_from_hub(\n    cls: type[LocusToGeneModel],\n    session: Session,\n    hf_model_id: str,\n    hf_model_version: str | None = None,\n    hf_token: str | None = None,\n) -&gt; LocusToGeneModel:\n    \"\"\"Load a model from the Hugging Face Hub. This will download the model from the hub and load it from disk.\n\n    Args:\n        session (Session): Session object to load the training data\n        hf_model_id (str): Model ID on the Hugging Face Hub\n        hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n        hf_token (str | None): Hugging Face Hub token to download the model (only required if private)\n\n    Returns:\n        LocusToGeneModel: L2G model loaded from the Hugging Face Hub\n    \"\"\"\n    from huggingface_hub import snapshot_download\n\n    def get_features_list_from_metadata() -&gt; list[str]:\n        \"\"\"Get the features list (in the right order) from the metadata JSON file downloaded from the Hub.\n\n        Returns:\n            list[str]: Features list\n        \"\"\"\n        model_config_path = str(Path(local_path) / \"config.json\")\n        with open(model_config_path) as f:\n            model_config = json.load(f)\n        return [\n            column\n            for column in model_config[\"sklearn\"][\"columns\"]\n            if column\n            not in [\n                \"studyLocusId\",\n                \"geneId\",\n                \"traitFromSourceMappedId\",\n                \"goldStandardSet\",\n            ]\n        ]\n\n    local_path = snapshot_download(\n        repo_id=hf_model_id,\n        local_dir=hf_model_id,\n        revision=hf_model_version,\n        token=hf_token,\n    )\n    features_list = get_features_list_from_metadata()\n    return cls.load_from_disk(\n        session,\n        local_path,\n        features_list=features_list,\n    )\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.predict","title":"<code>predict(feature_matrix: L2GFeatureMatrix, session: Session) -&gt; L2GPrediction</code>","text":"<p>Apply the model to a given feature matrix dataframe. The feature matrix needs to be preprocessed first.</p> <p>Parameters:</p> Name Type Description Default <code>feature_matrix</code> <code>L2GFeatureMatrix</code> <p>Feature matrix to apply the model to.</p> required <code>session</code> <code>Session</code> <p>Session object to convert data to Spark</p> required <p>Returns:</p> Name Type Description <code>L2GPrediction</code> <code>L2GPrediction</code> <p>Dataset containing credible sets and their L2G scores</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>def predict(\n    self: LocusToGeneModel,\n    feature_matrix: L2GFeatureMatrix,\n    session: Session,\n) -&gt; L2GPrediction:\n    \"\"\"Apply the model to a given feature matrix dataframe. The feature matrix needs to be preprocessed first.\n\n    Args:\n        feature_matrix (L2GFeatureMatrix): Feature matrix to apply the model to.\n        session (Session): Session object to convert data to Spark\n\n    Returns:\n        L2GPrediction: Dataset containing credible sets and their L2G scores\n    \"\"\"\n    from gentropy.dataset.l2g_prediction import L2GPrediction\n\n    pd_dataframe.iteritems = pd_dataframe.items\n\n    feature_matrix_pdf = feature_matrix._df.toPandas()\n    # L2G score is the probability the classifier assigns to the positive class (the second element in the probability array)\n    feature_matrix_pdf[\"score\"] = self.model.predict_proba(\n        # We drop the fixed columns to only pass the feature values to the classifier\n        feature_matrix_pdf.drop(feature_matrix.fixed_cols, axis=1)\n        .apply(pd_to_numeric)\n        .values\n    )[:, 1]\n    output_cols = [field.name for field in L2GPrediction.get_schema().fields]\n    return L2GPrediction(\n        _df=session.spark.createDataFrame(feature_matrix_pdf.filter(output_cols)),\n        _schema=L2GPrediction.get_schema(),\n        model=self,\n    )\n</code></pre>"},{"location":"python_api/methods/l2g/model/#gentropy.method.l2g.model.LocusToGeneModel.save","title":"<code>save(path: str) -&gt; None</code>","text":"<p>Saves fitted model to disk using the skops persistence format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the persisted model. Should end with .skops</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been fitted yet or if the path does not end with .skops</p> Source code in <code>src/gentropy/method/l2g/model.py</code> <pre><code>def save(self: LocusToGeneModel, path: str) -&gt; None:\n    \"\"\"Saves fitted model to disk using the skops persistence format.\n\n    Args:\n        path (str): Path to save the persisted model. Should end with .skops\n\n    Raises:\n        ValueError: If the model has not been fitted yet or if the path does not end with .skops\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n    if not path.endswith(\".skops\"):\n        raise ValueError(\"Path should end with .skops\")\n    if path.startswith(\"gs://\"):\n        local_path = path.split(\"/\")[-1]\n        sio.dump(self.model, local_path)\n        copy_to_gcs(local_path, path)\n    else:\n        # create directory if path does not exist\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n        sio.dump(self.model, path)\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/","title":"L2G Trainer","text":""},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer","title":"<code>gentropy.method.l2g.trainer.LocusToGeneTrainer</code>  <code>dataclass</code>","text":"<p>Modelling of what is the most likely causal gene associated with a given locus.</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>@dataclass\nclass LocusToGeneTrainer:\n    \"\"\"Modelling of what is the most likely causal gene associated with a given locus.\"\"\"\n\n    model: LocusToGeneModel\n    feature_matrix: L2GFeatureMatrix\n\n    # Initialise vars\n    features_list: list[str] | None = None\n    train_df: pd.DataFrame | None = None\n    test_df: pd.DataFrame | None = None\n    x_train: np.ndarray | None = None\n    y_train: np.ndarray | None = None\n    x_test: np.ndarray | None = None\n    y_test: np.ndarray | None = None\n    run: Run | None = None\n    wandb_l2g_project_name: str = \"gentropy-locus-to-gene\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Set default features_list to feature_matrix's features_list if not provided.\"\"\"\n        self.features_list = (\n            self.feature_matrix.features_list\n            if self.features_list is None\n            else self.features_list\n        )\n\n    def fit(\n        self: LocusToGeneTrainer,\n    ) -&gt; LocusToGeneModel:\n        \"\"\"Fit the pipeline to the feature matrix dataframe.\n\n        Returns:\n            LocusToGeneModel: Fitted model\n\n        Raises:\n            ValueError: Train data not set, nothing to fit.\n            AssertionError: If x_train or y_train are empty matrices\n        \"\"\"\n        if (\n            self.x_train is not None\n            and self.y_train is not None\n            and self.features_list is not None\n        ):\n            assert (\n                self.x_train.size != 0 and self.y_train.size != 0\n            ), \"Train data not set, nothing to fit.\"\n            fitted_model = self.model.model.fit(X=self.x_train, y=self.y_train)\n            self.model = LocusToGeneModel(\n                model=fitted_model,\n                hyperparameters=fitted_model.get_params(),\n                training_data=self.feature_matrix,\n                features_list=self.features_list,\n            )\n            return self.model\n        raise ValueError(\"Train data not set, nothing to fit.\")\n\n    def _get_shap_explanation(\n        self: LocusToGeneTrainer,\n        model: LocusToGeneModel,\n    ) -&gt; Explanation:\n        \"\"\"Get the SHAP values for the given model and data. We sample the full X matrix (without the labels) to interpret their shap values.\n\n        Args:\n            model (LocusToGeneModel): Model to explain.\n\n        Returns:\n                Explanation: SHAP values for the given model and data.\n\n        Raises:\n            ValueError: Train data not set, cannot get SHAP values.\n            Exception: (ExplanationError) When the additivity check fails.\n        \"\"\"\n        if self.x_train is not None and self.x_test is not None:\n            training_data = pd.DataFrame(\n                np.vstack((self.x_train, self.x_test)),\n                columns=self.features_list,\n            )\n            explainer = shap.TreeExplainer(\n                model.model,\n                data=training_data,\n                feature_perturbation=\"interventional\",\n                model_output=\"probability\",\n            )\n            try:\n                return explainer(training_data.sample(n=1_000))\n            except Exception as e:\n                if \"Additivity check failed in TreeExplainer\" in repr(e):\n                    return explainer(\n                        training_data.sample(n=1_000), check_additivity=False\n                    )\n                else:\n                    raise\n\n        raise ValueError(\"Train data not set.\")\n\n    def log_plot_image_to_wandb(\n        self: LocusToGeneTrainer, title: str, plot: Axes\n    ) -&gt; None:\n        \"\"\"Accepts a plot object, and saves the fig to PNG to then log it in W&amp;B.\n\n        Args:\n            title (str): Title of the plot.\n            plot (Axes): Shap plot to log.\n\n        Raises:\n            ValueError: Run not set, cannot log to W&amp;B.\n        \"\"\"\n        if self.run is None:\n            raise ValueError(\"Run not set, cannot log to W&amp;B.\")\n        if not plot:\n            # Scatter plot returns none, so we need to handle this case\n            plt.savefig(\"tmp.png\", bbox_inches=\"tight\")\n        else:\n            plot.figure.savefig(\"tmp.png\", bbox_inches=\"tight\")\n        self.run.log({title: Image(\"tmp.png\")})\n        plt.close()\n        os.remove(\"tmp.png\")\n\n    def log_to_wandb(\n        self: LocusToGeneTrainer,\n        wandb_run_name: str,\n    ) -&gt; None:\n        \"\"\"Log evaluation results and feature importance to W&amp;B to compare between different L2G runs.\n\n        Dashboard is available at https://wandb.ai/open-targets/gentropy-locus-to-gene?nw=nwuseropentargets\n        Credentials to access W&amp;B are available at the OT central login sheet.\n\n        Args:\n            wandb_run_name (str): Name of the W&amp;B run\n\n        Raises:\n            RuntimeError: If dependencies are not available.\n            AssertionError: If x_train or y_train are empty matrices\n        \"\"\"\n        if (\n            self.x_train is None\n            or self.x_test is None\n            or self.y_train is None\n            or self.y_test is None\n            or self.features_list is None\n        ):\n            raise RuntimeError(\"Train data not set, we cannot log to W&amp;B.\")\n        assert (\n            self.x_train.size != 0 and self.y_train.size != 0\n        ), \"Train data not set, nothing to evaluate.\"\n        fitted_classifier = self.model.model\n        y_predicted = fitted_classifier.predict(self.x_test)\n        y_probas = fitted_classifier.predict_proba(self.x_test)\n        self.run = wandb_init(\n            project=self.wandb_l2g_project_name,\n            name=wandb_run_name,\n            config=fitted_classifier.get_params(),\n        )\n        # Track classification plots\n        plot_classifier(\n            self.model.model,\n            self.x_train,\n            self.x_test,\n            self.y_train,\n            self.y_test,\n            y_predicted,\n            y_probas,\n            labels=list(self.model.label_encoder.values()),\n            model_name=\"L2G-classifier\",\n            feature_names=self.features_list,\n            is_binary=True,\n        )\n        # Track evaluation metrics\n        metrics = self.evaluate(\n            y_true=self.y_test, y_pred=y_predicted, y_pred_proba=y_probas\n        )\n        self.run.log(metrics)\n        # Log feature missingness\n        self.run.log(\n            {\n                \"missingnessRates\": self.feature_matrix.calculate_feature_missingness_rate()\n            }\n        )\n        # Plot marginal contribution of each feature\n        explanation = self._get_shap_explanation(self.model)\n        self.log_plot_image_to_wandb(\n            \"Feature Contribution\",\n            shap.plots.bar(\n                explanation, max_display=len(self.features_list), show=False\n            ),\n        )\n        self.log_plot_image_to_wandb(\n            \"Beeswarm Plot\",\n            shap.plots.beeswarm(\n                explanation, max_display=len(self.features_list), show=False\n            ),\n        )\n        # Plot correlation between feature values and their importance\n        for feature in self.features_list:\n            self.log_plot_image_to_wandb(\n                f\"Effect of {feature} on the predictions\",\n                shap.plots.scatter(\n                    explanation[:, feature],\n                    show=False,\n                ),\n            )\n        wandb_termlog(\"Logged Shapley contributions.\")\n        self.run.finish()\n\n    def log_to_terminal(\n        self: LocusToGeneTrainer, eval_id: str, metrics: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"Log metrics to terminal.\n\n        Args:\n            eval_id (str): Name of the evaluation set\n            metrics (dict[str, Any]): Model metrics\n        \"\"\"\n        for metric, value in metrics.items():\n            logging.info(\"(%s) %s: %s\", eval_id, metric, value)\n\n    def train(\n        self: LocusToGeneTrainer,\n        wandb_run_name: str | None = None,\n        test_size: float = 0.15,\n        cross_validate: bool = True,\n        n_splits: int = 5,\n        hyperparameter_grid: dict[str, Any] | None = None,\n    ) -&gt; LocusToGeneModel:\n        \"\"\"Train the Locus to Gene model.\n\n        If cross_validation is set to True, we implement the following strategy:\n            1. Create held-out test set\n            2. Perform cross-validation on training set\n            3. Train final model on full training set\n            4. Evaluate once on test set\n\n        Args:\n            wandb_run_name (str | None): Name of the W&amp;B run. Unless this is provided, the model will not be logged to W&amp;B.\n            test_size (float): Proportion of the test set\n            cross_validate (bool): Whether to run cross-validation. Defaults to True.\n            n_splits(int): Number of folds the data is splitted in. The model is trained and evaluated `k - 1` times. Defaults to 5.\n            hyperparameter_grid (dict[str, Any] | None): Hyperparameter grid to sweep over. Defaults to None.\n\n        Returns:\n            LocusToGeneModel: Fitted model\n        \"\"\"\n        # Create held-out test set using hierarchical splitting\n        self.train_df, self.test_df = self.feature_matrix.generate_train_test_split(\n            test_size=test_size,\n            verbose=True,\n            label_encoder=self.model.label_encoder,\n            label_col=self.feature_matrix.label_col,\n        )\n        self.x_train = self.train_df[self.features_list].apply(pd.to_numeric).values\n        self.y_train = (\n            self.train_df[self.feature_matrix.label_col].apply(pd.to_numeric).values\n        )\n        self.x_test = self.test_df[self.features_list].apply(pd.to_numeric).values\n        self.y_test = (\n            self.test_df[self.feature_matrix.label_col].apply(pd.to_numeric).values\n        )\n\n        # Cross-validation\n        if cross_validate:\n            wandb_run_name = f\"{wandb_run_name}-cv\" if wandb_run_name else None\n            self.cross_validate(\n                wandb_run_name=wandb_run_name,\n                parameter_grid=hyperparameter_grid,\n                n_splits=n_splits,\n            )\n\n        # Train final model on full training set\n        self.fit()\n\n        # Evaluate once on hold out test set\n        if wandb_run_name:\n            wandb_run_name = f\"{wandb_run_name}-holdout\"\n            self.log_to_wandb(wandb_run_name)\n        else:\n            self.log_to_terminal(\n                eval_id=\"Hold-out\",\n                metrics=self.evaluate(\n                    y_true=self.y_test,\n                    y_pred=self.model.model.predict(self.x_test),\n                    y_pred_proba=self.model.model.predict_proba(self.x_test),\n                ),\n            )\n\n        return self.model\n\n    def cross_validate(\n        self: LocusToGeneTrainer,\n        wandb_run_name: str | None = None,\n        parameter_grid: dict[str, Any] | None = None,\n        n_splits: int = 5,\n        random_state: int = 42,\n    ) -&gt; None:\n        \"\"\"Log results of cross validation and hyperparameter tuning with W&amp;B Sweeps. Metrics for every combination of hyperparameters will be logged to W&amp;B for comparison.\n\n        Args:\n            wandb_run_name (str | None): Name of the W&amp;B run. Unless this is provided, the model will not be logged to W&amp;B.\n            parameter_grid (dict[str, Any] | None): Dictionary containing the hyperparameters to sweep over. The keys are the hyperparameter names, and the values are dictionaries containing the values to sweep over.\n            n_splits (int): Number of folds the data is splitted in. The model is trained and evaluated `k - 1` times. Defaults to 5.\n            random_state (int): Random seed for reproducibility. Defaults to 42.\n        \"\"\"\n        # If no grid is provided, use default ones set in the model\n        parameter_grid = parameter_grid or {\n            param: {\"values\": [value]}\n            for param, value in self.model.hyperparameters.items()\n        }\n\n        def cross_validate_single_fold(\n            fold_index: int,\n            fold_train_df: pd.DataFrame,\n            fold_val_df: pd.DataFrame,\n            sweep_id: str | None,\n            sweep_run_name: str | None,\n            config: dict[str, Any] | None,\n        ) -&gt; None:\n            \"\"\"Run cross-validation for a single fold.\n\n            Args:\n                fold_index (int): Index of the fold\n                fold_train_df (pd.DataFrame): Training data for the fold\n                fold_val_df (pd.DataFrame): Validation data for the fold\n                sweep_id (str | None): ID of the sweep, if logging to W&amp;B is enabled\n                sweep_run_name (str | None): Name of the sweep run, if logging to W&amp;B is enabled\n                config (dict[str, Any] | None): Configuration from the sweep, if logging to W&amp;B is enabled\n            \"\"\"\n            reset_wandb_env()\n\n            x_fold_train, x_fold_val = (\n                fold_train_df[self.features_list].values,\n                fold_val_df[self.features_list].values,\n            )\n            y_fold_train, y_fold_val = (\n                fold_train_df[self.feature_matrix.label_col].values,\n                fold_val_df[self.feature_matrix.label_col].values,\n            )\n\n            fold_model = clone(self.model.model)\n            fold_model.fit(x_fold_train, y_fold_train)\n            y_pred_proba = fold_model.predict_proba(x_fold_val)\n            y_pred = fold_model.predict(x_fold_val)\n\n            # Log metrics\n            metrics = self.evaluate(\n                y_true=y_fold_val, y_pred=y_pred, y_pred_proba=y_pred_proba\n            )\n            if sweep_id and sweep_run_name and config:\n                fold_model.set_params(**config)\n                # Initialize a new run for this fold\n                os.environ[\"WANDB_SWEEP_ID\"] = sweep_id\n                run = wandb_init(\n                    project=self.wandb_l2g_project_name,\n                    name=sweep_run_name,\n                    config=config,\n                    group=sweep_run_name,\n                    job_type=\"fold\",\n                    reinit=True,\n                )\n                run.log(metrics)\n                wandb_termlog(f\"Logged metrics for fold {fold_index}.\")\n                run.finish()\n            else:\n                self.log_to_terminal(eval_id=f\"Fold {fold_index}\", metrics=metrics)\n\n        def run_all_folds() -&gt; None:\n            \"\"\"Run cross-validation for all folds.\"\"\"\n            # Initialise vars\n            sweep_run = None\n            sweep_id = None\n            sweep_url = None\n            sweep_group_url = None\n            config = None\n            if wandb_run_name:\n                # Initialize the sweep run and get metadata\n                sweep_run = wandb_init(name=wandb_run_name)\n                sweep_id = sweep_run.sweep_id\n                sweep_url = sweep_run.get_sweep_url()\n                sweep_group_url = f\"{sweep_run.get_project_url()}/groups/{sweep_id}\"\n                sweep_run.notes = sweep_group_url\n                sweep_run.save()\n                config = dict(sweep_run.config)\n\n                # Reset wandb setup to ensure clean state\n                _setup(_reset=True)\n\n                wandb_termlog(f\"Sweep URL: {sweep_url}\")\n                wandb_termlog(f\"Sweep Group URL: {sweep_group_url}\")\n\n            # Split training data hierarchically for this fold and run all folds\n            for fold_index in range(n_splits):\n                fold_seed = random_state + fold_index\n                fold_train_df, fold_val_df = LocusToGeneTrainer.hierarchical_split(\n                    self.train_df,\n                    verbose=False,\n                    random_state=fold_seed,\n                )\n                cross_validate_single_fold(\n                    fold_index=fold_index + 1,\n                    fold_train_df=fold_train_df,\n                    fold_val_df=fold_val_df,\n                    sweep_id=sweep_id,\n                    sweep_run_name=f\"{wandb_run_name}-fold{fold_index + 1}\"\n                    if wandb_run_name\n                    else None,\n                    config=config if config else None,\n                )\n\n        if wandb_run_name:\n            # Evaluate with cross validation in a W&amp;B Sweep\n            sweep_config = {\n                \"method\": \"grid\",\n                \"name\": wandb_run_name,\n                \"metric\": {\"name\": \"areaUnderROC\", \"goal\": \"maximize\"},\n                \"parameters\": parameter_grid,\n            }\n            sweep_id = wandb_sweep(sweep_config, project=self.wandb_l2g_project_name)\n            wandb_agent(sweep_id, run_all_folds)\n        else:\n            # Evaluate with cross validation to the terminal\n            run_all_folds()\n\n    @staticmethod\n    def evaluate(\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        y_pred_proba: np.ndarray,\n    ) -&gt; dict[str, float]:\n        \"\"\"Evaluate the model on a test set.\n\n        Args:\n            y_true (np.ndarray): True labels\n            y_pred (np.ndarray): Predicted labels\n            y_pred_proba (np.ndarray): Predicted probabilities for the positive class\n\n        Returns:\n            dict[str, float]: Dictionary of evaluation metrics\n        \"\"\"\n        return {\n            \"areaUnderROC\": roc_auc_score(\n                y_true, y_pred_proba[:, 1], average=\"weighted\"\n            ),\n            \"accuracy\": accuracy_score(y_true, y_pred),\n            \"weightedPrecision\": precision_score(y_true, y_pred, average=\"weighted\"),\n            \"averagePrecision\": average_precision_score(\n                y_true, y_pred, average=\"weighted\"\n            ),\n            \"weightedRecall\": recall_score(y_true, y_pred, average=\"weighted\"),\n            \"f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n        }\n\n    @staticmethod\n    def hierarchical_split(\n        data_df: pd.DataFrame,\n        test_size: float = 0.15,\n        verbose: bool = True,\n        random_state: int = 777,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Implements hierarchical splitting strategy to prevent data leakage.\n\n        Strategy:\n        1. Split positives by geneId groups\n        2. Further split by studyLocusId within each gene group\n        3. Augment splits with corresponding negatives based on studyLocusId\n\n        Args:\n            data_df (pd.DataFrame): Input dataframe with goldStandardSet column (1=positive, 0=negative)\n            test_size (float): Proportion of data for test set. Defaults to 0.15\n            verbose (bool): Print splitting statistics\n            random_state (int): Random seed for reproducibility. Defaults to 777\n\n        Returns:\n            tuple[pd.DataFrame, pd.DataFrame]: Training and test dataframes\n        \"\"\"\n        positives = data_df[data_df[\"goldStandardSet\"] == 1].copy()\n        negatives = data_df[data_df[\"goldStandardSet\"] == 0].copy()\n\n        # 1: Group positives by geneId and split genes between train/test by prioritising larger groups\n        gene_groups = positives.groupby(\"geneId\").size().reset_index(name=\"count\")\n        gene_groups = gene_groups.sort_values(\"count\", ascending=False)\n\n        genes_train, genes_test = train_test_split(\n            gene_groups[\"geneId\"].tolist(),\n            test_size=test_size,\n            shuffle=True,\n            random_state=random_state,\n        )\n\n        # 2: Split by studyLocusId within each gene group\n        train_study_loci = set()\n        test_study_loci = set()\n        train_gene_positives = positives[positives[\"geneId\"].isin(genes_train)]\n        train_study_loci.update(train_gene_positives[\"studyLocusId\"].unique())\n\n        test_gene_positives = positives[positives[\"geneId\"].isin(genes_test)]\n        test_study_loci.update(test_gene_positives[\"studyLocusId\"].unique())\n\n        # If we have overlapping loci, we assign them to train set after controlling that the overlap is not too large\n        overlapping_loci = train_study_loci.intersection(test_study_loci)\n        if overlapping_loci:\n            test_study_loci = test_study_loci - overlapping_loci\n            test_gene_positives = test_gene_positives[\n                ~test_gene_positives[\"studyLocusId\"].isin(overlapping_loci)\n            ]\n        if len(overlapping_loci) / len(test_study_loci) &gt; 0.1:\n            logging.warning(\n                \"Abundant overlap between train and test sets: %d\",\n                len(overlapping_loci),\n            )\n\n        # Final positive splits\n        train_positives = positives[positives[\"studyLocusId\"].isin(train_study_loci)]\n        test_positives = positives[positives[\"studyLocusId\"].isin(test_study_loci)]\n\n        if verbose:\n            logging.info(\"Total samples: %d\", len(data_df))\n            logging.info(\"Positives: %d\", len(positives))\n            logging.info(\"Negatives: %d\", len(negatives))\n            logging.info(\"Unique genes in positives: %d\", positives[\"geneId\"].nunique())\n            logging.info(\n                \"Unique studyLocusIds in positives: %d\",\n                positives[\"studyLocusId\"].nunique(),\n            )\n            logging.info(\"\\nGene-level split:\")\n            logging.info(\"Genes in train: %d\", len(genes_train))\n            logging.info(\"Genes in test: %d\", len(genes_test))\n            logging.info(\"\\nStudyLocusId-level split:\")\n            logging.info(\"StudyLocusIds in train: %d\", len(train_study_loci))\n            logging.info(\"StudyLocusIds in test: %d\", len(test_study_loci))\n            logging.info(\"Positive samples in train: %d\", len(train_positives))\n            logging.info(\"Positive samples in test: %d\", len(test_positives))\n\n        # 3: Expand splits by bringing negatives to the loci\n        train_negatives = negatives[negatives[\"studyLocusId\"].isin(train_study_loci)]\n        test_negatives = negatives[negatives[\"studyLocusId\"].isin(test_study_loci)]\n\n        # 4: Final splits\n        train_df = pd.concat([train_positives, train_negatives], ignore_index=True)\n        test_df = pd.concat([test_positives, test_negatives], ignore_index=True)\n\n        train_genes = set(train_df[\"geneId\"].unique())\n        test_genes = set(test_df[\"geneId\"].unique())\n        train_loci = set(train_df[\"studyLocusId\"].unique())\n        test_loci = set(test_df[\"studyLocusId\"].unique())\n        loci_overlap = train_loci.intersection(test_loci)\n        if loci_overlap:\n            logging.warning(\n                \"Data leakage detected! Overlapping studyLocusIds between splits.\"\n            )\n        if verbose:\n            gene_overlap = train_genes.intersection(test_genes)\n            logging.info(\"\\nFinal split statistics:\")\n            logging.info(\n                \"Train set: %d samples (%d positives)\",\n                len(train_df),\n                train_df[\"goldStandardSet\"].sum(),\n            )\n            logging.info(\n                \"Test set: %d samples (%d positives)\",\n                len(test_df),\n                test_df[\"goldStandardSet\"].sum(),\n            )\n            logging.info(\n                \"Gene overlap between splits (expected): %d\", len(gene_overlap)\n            )\n            logging.info(\n                \"StudyLocusId overlap between splits (not expected): %d\",\n                len(loci_overlap),\n            )\n\n        return train_df, test_df\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.cross_validate","title":"<code>cross_validate(wandb_run_name: str | None = None, parameter_grid: dict[str, Any] | None = None, n_splits: int = 5, random_state: int = 42) -&gt; None</code>","text":"<p>Log results of cross validation and hyperparameter tuning with W&amp;B Sweeps. Metrics for every combination of hyperparameters will be logged to W&amp;B for comparison.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_run_name</code> <code>str | None</code> <p>Name of the W&amp;B run. Unless this is provided, the model will not be logged to W&amp;B.</p> <code>None</code> <code>parameter_grid</code> <code>dict[str, Any] | None</code> <p>Dictionary containing the hyperparameters to sweep over. The keys are the hyperparameter names, and the values are dictionaries containing the values to sweep over.</p> <code>None</code> <code>n_splits</code> <code>int</code> <p>Number of folds the data is splitted in. The model is trained and evaluated <code>k - 1</code> times. Defaults to 5.</p> <code>5</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def cross_validate(\n    self: LocusToGeneTrainer,\n    wandb_run_name: str | None = None,\n    parameter_grid: dict[str, Any] | None = None,\n    n_splits: int = 5,\n    random_state: int = 42,\n) -&gt; None:\n    \"\"\"Log results of cross validation and hyperparameter tuning with W&amp;B Sweeps. Metrics for every combination of hyperparameters will be logged to W&amp;B for comparison.\n\n    Args:\n        wandb_run_name (str | None): Name of the W&amp;B run. Unless this is provided, the model will not be logged to W&amp;B.\n        parameter_grid (dict[str, Any] | None): Dictionary containing the hyperparameters to sweep over. The keys are the hyperparameter names, and the values are dictionaries containing the values to sweep over.\n        n_splits (int): Number of folds the data is splitted in. The model is trained and evaluated `k - 1` times. Defaults to 5.\n        random_state (int): Random seed for reproducibility. Defaults to 42.\n    \"\"\"\n    # If no grid is provided, use default ones set in the model\n    parameter_grid = parameter_grid or {\n        param: {\"values\": [value]}\n        for param, value in self.model.hyperparameters.items()\n    }\n\n    def cross_validate_single_fold(\n        fold_index: int,\n        fold_train_df: pd.DataFrame,\n        fold_val_df: pd.DataFrame,\n        sweep_id: str | None,\n        sweep_run_name: str | None,\n        config: dict[str, Any] | None,\n    ) -&gt; None:\n        \"\"\"Run cross-validation for a single fold.\n\n        Args:\n            fold_index (int): Index of the fold\n            fold_train_df (pd.DataFrame): Training data for the fold\n            fold_val_df (pd.DataFrame): Validation data for the fold\n            sweep_id (str | None): ID of the sweep, if logging to W&amp;B is enabled\n            sweep_run_name (str | None): Name of the sweep run, if logging to W&amp;B is enabled\n            config (dict[str, Any] | None): Configuration from the sweep, if logging to W&amp;B is enabled\n        \"\"\"\n        reset_wandb_env()\n\n        x_fold_train, x_fold_val = (\n            fold_train_df[self.features_list].values,\n            fold_val_df[self.features_list].values,\n        )\n        y_fold_train, y_fold_val = (\n            fold_train_df[self.feature_matrix.label_col].values,\n            fold_val_df[self.feature_matrix.label_col].values,\n        )\n\n        fold_model = clone(self.model.model)\n        fold_model.fit(x_fold_train, y_fold_train)\n        y_pred_proba = fold_model.predict_proba(x_fold_val)\n        y_pred = fold_model.predict(x_fold_val)\n\n        # Log metrics\n        metrics = self.evaluate(\n            y_true=y_fold_val, y_pred=y_pred, y_pred_proba=y_pred_proba\n        )\n        if sweep_id and sweep_run_name and config:\n            fold_model.set_params(**config)\n            # Initialize a new run for this fold\n            os.environ[\"WANDB_SWEEP_ID\"] = sweep_id\n            run = wandb_init(\n                project=self.wandb_l2g_project_name,\n                name=sweep_run_name,\n                config=config,\n                group=sweep_run_name,\n                job_type=\"fold\",\n                reinit=True,\n            )\n            run.log(metrics)\n            wandb_termlog(f\"Logged metrics for fold {fold_index}.\")\n            run.finish()\n        else:\n            self.log_to_terminal(eval_id=f\"Fold {fold_index}\", metrics=metrics)\n\n    def run_all_folds() -&gt; None:\n        \"\"\"Run cross-validation for all folds.\"\"\"\n        # Initialise vars\n        sweep_run = None\n        sweep_id = None\n        sweep_url = None\n        sweep_group_url = None\n        config = None\n        if wandb_run_name:\n            # Initialize the sweep run and get metadata\n            sweep_run = wandb_init(name=wandb_run_name)\n            sweep_id = sweep_run.sweep_id\n            sweep_url = sweep_run.get_sweep_url()\n            sweep_group_url = f\"{sweep_run.get_project_url()}/groups/{sweep_id}\"\n            sweep_run.notes = sweep_group_url\n            sweep_run.save()\n            config = dict(sweep_run.config)\n\n            # Reset wandb setup to ensure clean state\n            _setup(_reset=True)\n\n            wandb_termlog(f\"Sweep URL: {sweep_url}\")\n            wandb_termlog(f\"Sweep Group URL: {sweep_group_url}\")\n\n        # Split training data hierarchically for this fold and run all folds\n        for fold_index in range(n_splits):\n            fold_seed = random_state + fold_index\n            fold_train_df, fold_val_df = LocusToGeneTrainer.hierarchical_split(\n                self.train_df,\n                verbose=False,\n                random_state=fold_seed,\n            )\n            cross_validate_single_fold(\n                fold_index=fold_index + 1,\n                fold_train_df=fold_train_df,\n                fold_val_df=fold_val_df,\n                sweep_id=sweep_id,\n                sweep_run_name=f\"{wandb_run_name}-fold{fold_index + 1}\"\n                if wandb_run_name\n                else None,\n                config=config if config else None,\n            )\n\n    if wandb_run_name:\n        # Evaluate with cross validation in a W&amp;B Sweep\n        sweep_config = {\n            \"method\": \"grid\",\n            \"name\": wandb_run_name,\n            \"metric\": {\"name\": \"areaUnderROC\", \"goal\": \"maximize\"},\n            \"parameters\": parameter_grid,\n        }\n        sweep_id = wandb_sweep(sweep_config, project=self.wandb_l2g_project_name)\n        wandb_agent(sweep_id, run_all_folds)\n    else:\n        # Evaluate with cross validation to the terminal\n        run_all_folds()\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.evaluate","title":"<code>evaluate(y_true: np.ndarray, y_pred: np.ndarray, y_pred_proba: np.ndarray) -&gt; dict[str, float]</code>  <code>staticmethod</code>","text":"<p>Evaluate the model on a test set.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>True labels</p> required <code>y_pred</code> <code>ndarray</code> <p>Predicted labels</p> required <code>y_pred_proba</code> <code>ndarray</code> <p>Predicted probabilities for the positive class</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Dictionary of evaluation metrics</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>@staticmethod\ndef evaluate(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    y_pred_proba: np.ndarray,\n) -&gt; dict[str, float]:\n    \"\"\"Evaluate the model on a test set.\n\n    Args:\n        y_true (np.ndarray): True labels\n        y_pred (np.ndarray): Predicted labels\n        y_pred_proba (np.ndarray): Predicted probabilities for the positive class\n\n    Returns:\n        dict[str, float]: Dictionary of evaluation metrics\n    \"\"\"\n    return {\n        \"areaUnderROC\": roc_auc_score(\n            y_true, y_pred_proba[:, 1], average=\"weighted\"\n        ),\n        \"accuracy\": accuracy_score(y_true, y_pred),\n        \"weightedPrecision\": precision_score(y_true, y_pred, average=\"weighted\"),\n        \"averagePrecision\": average_precision_score(\n            y_true, y_pred, average=\"weighted\"\n        ),\n        \"weightedRecall\": recall_score(y_true, y_pred, average=\"weighted\"),\n        \"f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n    }\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.fit","title":"<code>fit() -&gt; LocusToGeneModel</code>","text":"<p>Fit the pipeline to the feature matrix dataframe.</p> <p>Returns:</p> Name Type Description <code>LocusToGeneModel</code> <code>LocusToGeneModel</code> <p>Fitted model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Train data not set, nothing to fit.</p> <code>AssertionError</code> <p>If x_train or y_train are empty matrices</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def fit(\n    self: LocusToGeneTrainer,\n) -&gt; LocusToGeneModel:\n    \"\"\"Fit the pipeline to the feature matrix dataframe.\n\n    Returns:\n        LocusToGeneModel: Fitted model\n\n    Raises:\n        ValueError: Train data not set, nothing to fit.\n        AssertionError: If x_train or y_train are empty matrices\n    \"\"\"\n    if (\n        self.x_train is not None\n        and self.y_train is not None\n        and self.features_list is not None\n    ):\n        assert (\n            self.x_train.size != 0 and self.y_train.size != 0\n        ), \"Train data not set, nothing to fit.\"\n        fitted_model = self.model.model.fit(X=self.x_train, y=self.y_train)\n        self.model = LocusToGeneModel(\n            model=fitted_model,\n            hyperparameters=fitted_model.get_params(),\n            training_data=self.feature_matrix,\n            features_list=self.features_list,\n        )\n        return self.model\n    raise ValueError(\"Train data not set, nothing to fit.\")\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.hierarchical_split","title":"<code>hierarchical_split(data_df: pd.DataFrame, test_size: float = 0.15, verbose: bool = True, random_state: int = 777) -&gt; tuple[pd.DataFrame, pd.DataFrame]</code>  <code>staticmethod</code>","text":"<p>Implements hierarchical splitting strategy to prevent data leakage.</p> <p>Strategy: 1. Split positives by geneId groups 2. Further split by studyLocusId within each gene group 3. Augment splits with corresponding negatives based on studyLocusId</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>Input dataframe with goldStandardSet column (1=positive, 0=negative)</p> required <code>test_size</code> <code>float</code> <p>Proportion of data for test set. Defaults to 0.15</p> <code>0.15</code> <code>verbose</code> <code>bool</code> <p>Print splitting statistics</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 777</p> <code>777</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: Training and test dataframes</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>@staticmethod\ndef hierarchical_split(\n    data_df: pd.DataFrame,\n    test_size: float = 0.15,\n    verbose: bool = True,\n    random_state: int = 777,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Implements hierarchical splitting strategy to prevent data leakage.\n\n    Strategy:\n    1. Split positives by geneId groups\n    2. Further split by studyLocusId within each gene group\n    3. Augment splits with corresponding negatives based on studyLocusId\n\n    Args:\n        data_df (pd.DataFrame): Input dataframe with goldStandardSet column (1=positive, 0=negative)\n        test_size (float): Proportion of data for test set. Defaults to 0.15\n        verbose (bool): Print splitting statistics\n        random_state (int): Random seed for reproducibility. Defaults to 777\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]: Training and test dataframes\n    \"\"\"\n    positives = data_df[data_df[\"goldStandardSet\"] == 1].copy()\n    negatives = data_df[data_df[\"goldStandardSet\"] == 0].copy()\n\n    # 1: Group positives by geneId and split genes between train/test by prioritising larger groups\n    gene_groups = positives.groupby(\"geneId\").size().reset_index(name=\"count\")\n    gene_groups = gene_groups.sort_values(\"count\", ascending=False)\n\n    genes_train, genes_test = train_test_split(\n        gene_groups[\"geneId\"].tolist(),\n        test_size=test_size,\n        shuffle=True,\n        random_state=random_state,\n    )\n\n    # 2: Split by studyLocusId within each gene group\n    train_study_loci = set()\n    test_study_loci = set()\n    train_gene_positives = positives[positives[\"geneId\"].isin(genes_train)]\n    train_study_loci.update(train_gene_positives[\"studyLocusId\"].unique())\n\n    test_gene_positives = positives[positives[\"geneId\"].isin(genes_test)]\n    test_study_loci.update(test_gene_positives[\"studyLocusId\"].unique())\n\n    # If we have overlapping loci, we assign them to train set after controlling that the overlap is not too large\n    overlapping_loci = train_study_loci.intersection(test_study_loci)\n    if overlapping_loci:\n        test_study_loci = test_study_loci - overlapping_loci\n        test_gene_positives = test_gene_positives[\n            ~test_gene_positives[\"studyLocusId\"].isin(overlapping_loci)\n        ]\n    if len(overlapping_loci) / len(test_study_loci) &gt; 0.1:\n        logging.warning(\n            \"Abundant overlap between train and test sets: %d\",\n            len(overlapping_loci),\n        )\n\n    # Final positive splits\n    train_positives = positives[positives[\"studyLocusId\"].isin(train_study_loci)]\n    test_positives = positives[positives[\"studyLocusId\"].isin(test_study_loci)]\n\n    if verbose:\n        logging.info(\"Total samples: %d\", len(data_df))\n        logging.info(\"Positives: %d\", len(positives))\n        logging.info(\"Negatives: %d\", len(negatives))\n        logging.info(\"Unique genes in positives: %d\", positives[\"geneId\"].nunique())\n        logging.info(\n            \"Unique studyLocusIds in positives: %d\",\n            positives[\"studyLocusId\"].nunique(),\n        )\n        logging.info(\"\\nGene-level split:\")\n        logging.info(\"Genes in train: %d\", len(genes_train))\n        logging.info(\"Genes in test: %d\", len(genes_test))\n        logging.info(\"\\nStudyLocusId-level split:\")\n        logging.info(\"StudyLocusIds in train: %d\", len(train_study_loci))\n        logging.info(\"StudyLocusIds in test: %d\", len(test_study_loci))\n        logging.info(\"Positive samples in train: %d\", len(train_positives))\n        logging.info(\"Positive samples in test: %d\", len(test_positives))\n\n    # 3: Expand splits by bringing negatives to the loci\n    train_negatives = negatives[negatives[\"studyLocusId\"].isin(train_study_loci)]\n    test_negatives = negatives[negatives[\"studyLocusId\"].isin(test_study_loci)]\n\n    # 4: Final splits\n    train_df = pd.concat([train_positives, train_negatives], ignore_index=True)\n    test_df = pd.concat([test_positives, test_negatives], ignore_index=True)\n\n    train_genes = set(train_df[\"geneId\"].unique())\n    test_genes = set(test_df[\"geneId\"].unique())\n    train_loci = set(train_df[\"studyLocusId\"].unique())\n    test_loci = set(test_df[\"studyLocusId\"].unique())\n    loci_overlap = train_loci.intersection(test_loci)\n    if loci_overlap:\n        logging.warning(\n            \"Data leakage detected! Overlapping studyLocusIds between splits.\"\n        )\n    if verbose:\n        gene_overlap = train_genes.intersection(test_genes)\n        logging.info(\"\\nFinal split statistics:\")\n        logging.info(\n            \"Train set: %d samples (%d positives)\",\n            len(train_df),\n            train_df[\"goldStandardSet\"].sum(),\n        )\n        logging.info(\n            \"Test set: %d samples (%d positives)\",\n            len(test_df),\n            test_df[\"goldStandardSet\"].sum(),\n        )\n        logging.info(\n            \"Gene overlap between splits (expected): %d\", len(gene_overlap)\n        )\n        logging.info(\n            \"StudyLocusId overlap between splits (not expected): %d\",\n            len(loci_overlap),\n        )\n\n    return train_df, test_df\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.log_plot_image_to_wandb","title":"<code>log_plot_image_to_wandb(title: str, plot: Axes) -&gt; None</code>","text":"<p>Accepts a plot object, and saves the fig to PNG to then log it in W&amp;B.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the plot.</p> required <code>plot</code> <code>Axes</code> <p>Shap plot to log.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Run not set, cannot log to W&amp;B.</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def log_plot_image_to_wandb(\n    self: LocusToGeneTrainer, title: str, plot: Axes\n) -&gt; None:\n    \"\"\"Accepts a plot object, and saves the fig to PNG to then log it in W&amp;B.\n\n    Args:\n        title (str): Title of the plot.\n        plot (Axes): Shap plot to log.\n\n    Raises:\n        ValueError: Run not set, cannot log to W&amp;B.\n    \"\"\"\n    if self.run is None:\n        raise ValueError(\"Run not set, cannot log to W&amp;B.\")\n    if not plot:\n        # Scatter plot returns none, so we need to handle this case\n        plt.savefig(\"tmp.png\", bbox_inches=\"tight\")\n    else:\n        plot.figure.savefig(\"tmp.png\", bbox_inches=\"tight\")\n    self.run.log({title: Image(\"tmp.png\")})\n    plt.close()\n    os.remove(\"tmp.png\")\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.log_to_terminal","title":"<code>log_to_terminal(eval_id: str, metrics: dict[str, Any]) -&gt; None</code>","text":"<p>Log metrics to terminal.</p> <p>Parameters:</p> Name Type Description Default <code>eval_id</code> <code>str</code> <p>Name of the evaluation set</p> required <code>metrics</code> <code>dict[str, Any]</code> <p>Model metrics</p> required Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def log_to_terminal(\n    self: LocusToGeneTrainer, eval_id: str, metrics: dict[str, Any]\n) -&gt; None:\n    \"\"\"Log metrics to terminal.\n\n    Args:\n        eval_id (str): Name of the evaluation set\n        metrics (dict[str, Any]): Model metrics\n    \"\"\"\n    for metric, value in metrics.items():\n        logging.info(\"(%s) %s: %s\", eval_id, metric, value)\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.log_to_wandb","title":"<code>log_to_wandb(wandb_run_name: str) -&gt; None</code>","text":"<p>Log evaluation results and feature importance to W&amp;B to compare between different L2G runs.</p> <p>Dashboard is available at https://wandb.ai/open-targets/gentropy-locus-to-gene?nw=nwuseropentargets Credentials to access W&amp;B are available at the OT central login sheet.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_run_name</code> <code>str</code> <p>Name of the W&amp;B run</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If dependencies are not available.</p> <code>AssertionError</code> <p>If x_train or y_train are empty matrices</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def log_to_wandb(\n    self: LocusToGeneTrainer,\n    wandb_run_name: str,\n) -&gt; None:\n    \"\"\"Log evaluation results and feature importance to W&amp;B to compare between different L2G runs.\n\n    Dashboard is available at https://wandb.ai/open-targets/gentropy-locus-to-gene?nw=nwuseropentargets\n    Credentials to access W&amp;B are available at the OT central login sheet.\n\n    Args:\n        wandb_run_name (str): Name of the W&amp;B run\n\n    Raises:\n        RuntimeError: If dependencies are not available.\n        AssertionError: If x_train or y_train are empty matrices\n    \"\"\"\n    if (\n        self.x_train is None\n        or self.x_test is None\n        or self.y_train is None\n        or self.y_test is None\n        or self.features_list is None\n    ):\n        raise RuntimeError(\"Train data not set, we cannot log to W&amp;B.\")\n    assert (\n        self.x_train.size != 0 and self.y_train.size != 0\n    ), \"Train data not set, nothing to evaluate.\"\n    fitted_classifier = self.model.model\n    y_predicted = fitted_classifier.predict(self.x_test)\n    y_probas = fitted_classifier.predict_proba(self.x_test)\n    self.run = wandb_init(\n        project=self.wandb_l2g_project_name,\n        name=wandb_run_name,\n        config=fitted_classifier.get_params(),\n    )\n    # Track classification plots\n    plot_classifier(\n        self.model.model,\n        self.x_train,\n        self.x_test,\n        self.y_train,\n        self.y_test,\n        y_predicted,\n        y_probas,\n        labels=list(self.model.label_encoder.values()),\n        model_name=\"L2G-classifier\",\n        feature_names=self.features_list,\n        is_binary=True,\n    )\n    # Track evaluation metrics\n    metrics = self.evaluate(\n        y_true=self.y_test, y_pred=y_predicted, y_pred_proba=y_probas\n    )\n    self.run.log(metrics)\n    # Log feature missingness\n    self.run.log(\n        {\n            \"missingnessRates\": self.feature_matrix.calculate_feature_missingness_rate()\n        }\n    )\n    # Plot marginal contribution of each feature\n    explanation = self._get_shap_explanation(self.model)\n    self.log_plot_image_to_wandb(\n        \"Feature Contribution\",\n        shap.plots.bar(\n            explanation, max_display=len(self.features_list), show=False\n        ),\n    )\n    self.log_plot_image_to_wandb(\n        \"Beeswarm Plot\",\n        shap.plots.beeswarm(\n            explanation, max_display=len(self.features_list), show=False\n        ),\n    )\n    # Plot correlation between feature values and their importance\n    for feature in self.features_list:\n        self.log_plot_image_to_wandb(\n            f\"Effect of {feature} on the predictions\",\n            shap.plots.scatter(\n                explanation[:, feature],\n                show=False,\n            ),\n        )\n    wandb_termlog(\"Logged Shapley contributions.\")\n    self.run.finish()\n</code></pre>"},{"location":"python_api/methods/l2g/trainer/#gentropy.method.l2g.trainer.LocusToGeneTrainer.train","title":"<code>train(wandb_run_name: str | None = None, test_size: float = 0.15, cross_validate: bool = True, n_splits: int = 5, hyperparameter_grid: dict[str, Any] | None = None) -&gt; LocusToGeneModel</code>","text":"<p>Train the Locus to Gene model.</p> <p>If cross_validation is set to True, we implement the following strategy:     1. Create held-out test set     2. Perform cross-validation on training set     3. Train final model on full training set     4. Evaluate once on test set</p> <p>Parameters:</p> Name Type Description Default <code>wandb_run_name</code> <code>str | None</code> <p>Name of the W&amp;B run. Unless this is provided, the model will not be logged to W&amp;B.</p> <code>None</code> <code>test_size</code> <code>float</code> <p>Proportion of the test set</p> <code>0.15</code> <code>cross_validate</code> <code>bool</code> <p>Whether to run cross-validation. Defaults to True.</p> <code>True</code> <code>n_splits</code> <code>int</code> <p>Number of folds the data is splitted in. The model is trained and evaluated <code>k - 1</code> times. Defaults to 5.</p> <code>5</code> <code>hyperparameter_grid</code> <code>dict[str, Any] | None</code> <p>Hyperparameter grid to sweep over. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LocusToGeneModel</code> <code>LocusToGeneModel</code> <p>Fitted model</p> Source code in <code>src/gentropy/method/l2g/trainer.py</code> <pre><code>def train(\n    self: LocusToGeneTrainer,\n    wandb_run_name: str | None = None,\n    test_size: float = 0.15,\n    cross_validate: bool = True,\n    n_splits: int = 5,\n    hyperparameter_grid: dict[str, Any] | None = None,\n) -&gt; LocusToGeneModel:\n    \"\"\"Train the Locus to Gene model.\n\n    If cross_validation is set to True, we implement the following strategy:\n        1. Create held-out test set\n        2. Perform cross-validation on training set\n        3. Train final model on full training set\n        4. Evaluate once on test set\n\n    Args:\n        wandb_run_name (str | None): Name of the W&amp;B run. Unless this is provided, the model will not be logged to W&amp;B.\n        test_size (float): Proportion of the test set\n        cross_validate (bool): Whether to run cross-validation. Defaults to True.\n        n_splits(int): Number of folds the data is splitted in. The model is trained and evaluated `k - 1` times. Defaults to 5.\n        hyperparameter_grid (dict[str, Any] | None): Hyperparameter grid to sweep over. Defaults to None.\n\n    Returns:\n        LocusToGeneModel: Fitted model\n    \"\"\"\n    # Create held-out test set using hierarchical splitting\n    self.train_df, self.test_df = self.feature_matrix.generate_train_test_split(\n        test_size=test_size,\n        verbose=True,\n        label_encoder=self.model.label_encoder,\n        label_col=self.feature_matrix.label_col,\n    )\n    self.x_train = self.train_df[self.features_list].apply(pd.to_numeric).values\n    self.y_train = (\n        self.train_df[self.feature_matrix.label_col].apply(pd.to_numeric).values\n    )\n    self.x_test = self.test_df[self.features_list].apply(pd.to_numeric).values\n    self.y_test = (\n        self.test_df[self.feature_matrix.label_col].apply(pd.to_numeric).values\n    )\n\n    # Cross-validation\n    if cross_validate:\n        wandb_run_name = f\"{wandb_run_name}-cv\" if wandb_run_name else None\n        self.cross_validate(\n            wandb_run_name=wandb_run_name,\n            parameter_grid=hyperparameter_grid,\n            n_splits=n_splits,\n        )\n\n    # Train final model on full training set\n    self.fit()\n\n    # Evaluate once on hold out test set\n    if wandb_run_name:\n        wandb_run_name = f\"{wandb_run_name}-holdout\"\n        self.log_to_wandb(wandb_run_name)\n    else:\n        self.log_to_terminal(\n            eval_id=\"Hold-out\",\n            metrics=self.evaluate(\n                y_true=self.y_test,\n                y_pred=self.model.model.predict(self.x_test),\n                y_pred_proba=self.model.model.predict_proba(self.x_test),\n            ),\n        )\n\n    return self.model\n</code></pre>"},{"location":"python_api/steps/_steps/","title":"Step","text":"<p>This section provides description for the <code>Step</code> class. Each <code>Step</code> uses its own set of Methods and Datasets and implements the logic necessary to read a set of inputs, perform the transformation and write the outputs. All steps are available through the command line interface when running the <code>gentropy</code> command.</p>"},{"location":"python_api/steps/biosample_index_step/","title":"biosample_index","text":""},{"location":"python_api/steps/biosample_index_step/#gentropy.biosample_index.BiosampleIndexStep","title":"<code>gentropy.biosample_index.BiosampleIndexStep</code>","text":"<p>Biosample index step.</p> <p>This step generates a Biosample index dataset from the various ontology sources. Currently Cell Ontology and Uberon are supported.</p> Source code in <code>src/gentropy/biosample_index.py</code> <pre><code>class BiosampleIndexStep:\n    \"\"\"Biosample index step.\n\n    This step generates a Biosample index dataset from the various ontology sources. Currently Cell Ontology and Uberon are supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        cell_ontology_input_path: str,\n        uberon_input_path: str,\n        efo_input_path: str,\n        biosample_index_path: str,\n    ) -&gt; None:\n        \"\"\"Run Biosample index generation step.\n\n        Args:\n            session (Session): Session object.\n            cell_ontology_input_path (str): Input cell ontology dataset path.\n            uberon_input_path (str): Input uberon dataset path.\n            efo_input_path (str): Input efo dataset path.\n            biosample_index_path (str): Output biosample index dataset path.\n        \"\"\"\n        cell_ontology_index = extract_ontology_from_json(\n            cell_ontology_input_path, session.spark\n        )\n        uberon_index = extract_ontology_from_json(uberon_input_path, session.spark)\n        efo_index = extract_ontology_from_json(\n            efo_input_path, session.spark\n        ).retain_rows_with_ancestor_id([\"CL_0000000\"])\n\n        biosample_index = cell_ontology_index.merge_indices([uberon_index, efo_index])\n\n        biosample_index.df.coalesce(session.output_partitions).write.mode(\n            session.write_mode\n        ).parquet(biosample_index_path)\n</code></pre>"},{"location":"python_api/steps/biosample_index_step/#gentropy.biosample_index.BiosampleIndexStep.__init__","title":"<code>__init__(session: Session, cell_ontology_input_path: str, uberon_input_path: str, efo_input_path: str, biosample_index_path: str) -&gt; None</code>","text":"<p>Run Biosample index generation step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>cell_ontology_input_path</code> <code>str</code> <p>Input cell ontology dataset path.</p> required <code>uberon_input_path</code> <code>str</code> <p>Input uberon dataset path.</p> required <code>efo_input_path</code> <code>str</code> <p>Input efo dataset path.</p> required <code>biosample_index_path</code> <code>str</code> <p>Output biosample index dataset path.</p> required Source code in <code>src/gentropy/biosample_index.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    cell_ontology_input_path: str,\n    uberon_input_path: str,\n    efo_input_path: str,\n    biosample_index_path: str,\n) -&gt; None:\n    \"\"\"Run Biosample index generation step.\n\n    Args:\n        session (Session): Session object.\n        cell_ontology_input_path (str): Input cell ontology dataset path.\n        uberon_input_path (str): Input uberon dataset path.\n        efo_input_path (str): Input efo dataset path.\n        biosample_index_path (str): Output biosample index dataset path.\n    \"\"\"\n    cell_ontology_index = extract_ontology_from_json(\n        cell_ontology_input_path, session.spark\n    )\n    uberon_index = extract_ontology_from_json(uberon_input_path, session.spark)\n    efo_index = extract_ontology_from_json(\n        efo_input_path, session.spark\n    ).retain_rows_with_ancestor_id([\"CL_0000000\"])\n\n    biosample_index = cell_ontology_index.merge_indices([uberon_index, efo_index])\n\n    biosample_index.df.coalesce(session.output_partitions).write.mode(\n        session.write_mode\n    ).parquet(biosample_index_path)\n</code></pre>"},{"location":"python_api/steps/colocalisation/","title":"colocalisation","text":""},{"location":"python_api/steps/colocalisation/#gentropy.colocalisation.ColocalisationStep","title":"<code>gentropy.colocalisation.ColocalisationStep</code>","text":"<p>Colocalisation step.</p> <p>This workflow runs colocalisation analyses that assess the degree to which independent signals of the association share the same causal variant in a region of the genome, typically limited by linkage disequilibrium (LD).</p> Source code in <code>src/gentropy/colocalisation.py</code> <pre><code>class ColocalisationStep:\n    \"\"\"Colocalisation step.\n\n    This workflow runs colocalisation analyses that assess the degree to which independent signals of the association share the same causal variant in a region of the genome, typically limited by linkage disequilibrium (LD).\n    \"\"\"\n\n    __coloc_methods__ = {\n        method.METHOD_NAME.lower(): method\n        for method in ColocalisationMethodInterface.__subclasses__()\n    }\n\n    def __init__(\n        self,\n        session: Session,\n        credible_set_path: str,\n        coloc_path: str,\n        colocalisation_method: str,\n        restrict_right_studies: list[str] | None = None,\n        gwas_v_qtl_overlap_only: bool = False,\n        colocalisation_method_params: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"Run Colocalisation step.\n\n        This step allows for running two colocalisation methods: ecaviar and coloc. The default behaviour is all gwas vs all gwas plus all gwas vs all molecular-QTLs.\n\n        Args:\n            session (Session): Session object.\n            credible_set_path (str): Input credible sets path.\n            coloc_path (str): Output path.\n            colocalisation_method (str): Colocalisation method. Use 'coloc_pip_ecaviar' to run both ColocPIP and eCAVIAR and merge results.\n            restrict_right_studies (list[str] | None): List of study IDs to restrict the right side of the colocalisation overlaps to, e.g. all gwas vs a single studyId. Defaults to None.\n            gwas_v_qtl_overlap_only (bool): If True, restricts the right side of colocalisation overlaps to only molecular-QTL studies, e.g. all gwas vs all molQTLs. Defaults to False.\n            colocalisation_method_params (dict[str, Any] | None): Keyword arguments passed to the colocalise method of Colocalisation class. Defaults to None\n\n        Keyword Args:\n            priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4. For coloc method only.\n            priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4. For coloc method only.\n            priorc12 (float): Prior on variant being causal for both traits. Defaults to 1e-5. For coloc method only.\n            overlap_size_cutoff (int): Minimum number of overlapping variants bfore filtering. Defaults to 0.\n            posterior_cutoff (float): Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.0.\n        \"\"\"\n        colocalisation_method = colocalisation_method.lower()\n\n        # Extract\n        credible_set = StudyLocus.from_parquet(\n            session, credible_set_path, recusiveFileLookup=True\n        )\n\n        if colocalisation_method == \"coloc_pip_ecaviar\":\n            # Transform - find overlaps once\n            overlaps = credible_set.find_overlaps(\n                restrict_right_studies=restrict_right_studies,\n                gwas_v_qtl_overlap_only=gwas_v_qtl_overlap_only,\n            )\n\n            # Run ColocPIP\n\n            coloc_pip = ColocPIP.colocalise\n            if colocalisation_method_params:\n                coloc_pip = partial(coloc_pip, **colocalisation_method_params)\n            coloc_pip_results = coloc_pip(overlaps)\n\n            # Run eCAVIAR\n\n            ecaviar_results = ECaviar.colocalise(overlaps)\n\n            # Merge results: join on key columns and combine metrics\n            join_keys = [\n                \"leftStudyLocusId\",\n                \"rightStudyLocusId\",\n                \"chromosome\",\n                \"rightStudyType\",\n            ]\n\n            colocalisation_results = Colocalisation(\n                _df=coloc_pip_results.df.alias(\"pip\")\n                .join(\n                    ecaviar_results.df.alias(\"ecav\").select(\n                        *join_keys,\n                        f.col(\"clpp\").alias(\"clpp_ecaviar\"),\n                        f.col(\"numberColocalisingVariants\").alias(\n                            \"numberColocalisingVariants_ecaviar\"\n                        ),\n                    ),\n                    on=join_keys,\n                    how=\"inner\",\n                )\n                .select(\n                    f.col(\"pip.leftStudyLocusId\"),\n                    f.col(\"pip.rightStudyLocusId\"),\n                    f.col(\"pip.rightStudyType\"),\n                    f.col(\"pip.chromosome\"),\n                    # Use a combined method name\n                    f.lit(\"COLOC_PIP_ECAVIAR\").alias(\"colocalisationMethod\"),\n                    # Use the max number of colocalising variants from both methods\n                    f.greatest(\n                        f.col(\"pip.numberColocalisingVariants\"),\n                        f.col(\"numberColocalisingVariants_ecaviar\"),\n                    ).alias(\"numberColocalisingVariants\"),\n                    # Keep h3 and h4 from ColocPIP\n                    f.col(\"pip.h3\"),\n                    f.col(\"pip.h4\"),\n                    # Add clpp from eCAVIAR\n                    f.col(\"clpp_ecaviar\").alias(\"clpp\"),\n                    # Keep beta ratio from ColocPIP\n                    f.col(\"pip.betaRatioSignAverage\"),\n                ),\n                _schema=Colocalisation.get_schema(),\n            )\n        else:\n            colocalisation_class = self._get_colocalisation_class(colocalisation_method)\n\n            if colocalisation_method == Coloc.METHOD_NAME.lower():\n                credible_set = credible_set.filter(\n                    f.col(\"finemappingMethod\").isin(\n                        FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n                    )\n                )\n\n            # Transform\n            overlaps = credible_set.find_overlaps(\n                restrict_right_studies=restrict_right_studies,\n                gwas_v_qtl_overlap_only=gwas_v_qtl_overlap_only,\n            )\n\n            # Make a partial caller to ensure that colocalisation_method_params are added to the call only when dict is not empty\n            coloc = colocalisation_class.colocalise\n            if colocalisation_method_params:\n                coloc = partial(coloc, **colocalisation_method_params)\n            colocalisation_results = coloc(overlaps)\n\n        # Load\n        colocalisation_results.df.coalesce(session.output_partitions).write.mode(\n            session.write_mode\n        ).parquet(coloc_path)\n\n    @classmethod\n    def _get_colocalisation_class(\n        cls, method: str\n    ) -&gt; type[ColocalisationMethodInterface]:\n        \"\"\"Get colocalisation class.\n\n        Args:\n            method (str): Colocalisation method.\n\n        Returns:\n            type[ColocalisationMethodInterface]: Class that implements the ColocalisationMethodInterface.\n\n        Raises:\n            ValueError: if method not available.\n\n        Examples:\n            &gt;&gt;&gt; ColocalisationStep._get_colocalisation_class(\"ECaviar\")\n            &lt;class 'gentropy.method.colocalisation.ECaviar'&gt;\n        \"\"\"\n        method = method.lower()\n        if method not in cls.__coloc_methods__:\n            raise ValueError(f\"Colocalisation method {method} not available.\")\n        return cls.__coloc_methods__[method]\n</code></pre>"},{"location":"python_api/steps/colocalisation/#gentropy.colocalisation.ColocalisationStep.__init__","title":"<code>__init__(session: Session, credible_set_path: str, coloc_path: str, colocalisation_method: str, restrict_right_studies: list[str] | None = None, gwas_v_qtl_overlap_only: bool = False, colocalisation_method_params: dict[str, Any] | None = None) -&gt; None</code>","text":"<p>Run Colocalisation step.</p> <p>This step allows for running two colocalisation methods: ecaviar and coloc. The default behaviour is all gwas vs all gwas plus all gwas vs all molecular-QTLs.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>credible_set_path</code> <code>str</code> <p>Input credible sets path.</p> required <code>coloc_path</code> <code>str</code> <p>Output path.</p> required <code>colocalisation_method</code> <code>str</code> <p>Colocalisation method. Use 'coloc_pip_ecaviar' to run both ColocPIP and eCAVIAR and merge results.</p> required <code>restrict_right_studies</code> <code>list[str] | None</code> <p>List of study IDs to restrict the right side of the colocalisation overlaps to, e.g. all gwas vs a single studyId. Defaults to None.</p> <code>None</code> <code>gwas_v_qtl_overlap_only</code> <code>bool</code> <p>If True, restricts the right side of colocalisation overlaps to only molecular-QTL studies, e.g. all gwas vs all molQTLs. Defaults to False.</p> <code>False</code> <code>colocalisation_method_params</code> <code>dict[str, Any] | None</code> <p>Keyword arguments passed to the colocalise method of Colocalisation class. Defaults to None</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>priorc1</code> <code>float</code> <p>Prior on variant being causal for trait 1. Defaults to 1e-4. For coloc method only.</p> <code>priorc2</code> <code>float</code> <p>Prior on variant being causal for trait 2. Defaults to 1e-4. For coloc method only.</p> <code>priorc12</code> <code>float</code> <p>Prior on variant being causal for both traits. Defaults to 1e-5. For coloc method only.</p> <code>overlap_size_cutoff</code> <code>int</code> <p>Minimum number of overlapping variants bfore filtering. Defaults to 0.</p> <code>posterior_cutoff</code> <code>float</code> <p>Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.0.</p> Source code in <code>src/gentropy/colocalisation.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    credible_set_path: str,\n    coloc_path: str,\n    colocalisation_method: str,\n    restrict_right_studies: list[str] | None = None,\n    gwas_v_qtl_overlap_only: bool = False,\n    colocalisation_method_params: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Run Colocalisation step.\n\n    This step allows for running two colocalisation methods: ecaviar and coloc. The default behaviour is all gwas vs all gwas plus all gwas vs all molecular-QTLs.\n\n    Args:\n        session (Session): Session object.\n        credible_set_path (str): Input credible sets path.\n        coloc_path (str): Output path.\n        colocalisation_method (str): Colocalisation method. Use 'coloc_pip_ecaviar' to run both ColocPIP and eCAVIAR and merge results.\n        restrict_right_studies (list[str] | None): List of study IDs to restrict the right side of the colocalisation overlaps to, e.g. all gwas vs a single studyId. Defaults to None.\n        gwas_v_qtl_overlap_only (bool): If True, restricts the right side of colocalisation overlaps to only molecular-QTL studies, e.g. all gwas vs all molQTLs. Defaults to False.\n        colocalisation_method_params (dict[str, Any] | None): Keyword arguments passed to the colocalise method of Colocalisation class. Defaults to None\n\n    Keyword Args:\n        priorc1 (float): Prior on variant being causal for trait 1. Defaults to 1e-4. For coloc method only.\n        priorc2 (float): Prior on variant being causal for trait 2. Defaults to 1e-4. For coloc method only.\n        priorc12 (float): Prior on variant being causal for both traits. Defaults to 1e-5. For coloc method only.\n        overlap_size_cutoff (int): Minimum number of overlapping variants bfore filtering. Defaults to 0.\n        posterior_cutoff (float): Minimum overlapping Posterior probability cutoff for small overlaps. Defaults to 0.0.\n    \"\"\"\n    colocalisation_method = colocalisation_method.lower()\n\n    # Extract\n    credible_set = StudyLocus.from_parquet(\n        session, credible_set_path, recusiveFileLookup=True\n    )\n\n    if colocalisation_method == \"coloc_pip_ecaviar\":\n        # Transform - find overlaps once\n        overlaps = credible_set.find_overlaps(\n            restrict_right_studies=restrict_right_studies,\n            gwas_v_qtl_overlap_only=gwas_v_qtl_overlap_only,\n        )\n\n        # Run ColocPIP\n\n        coloc_pip = ColocPIP.colocalise\n        if colocalisation_method_params:\n            coloc_pip = partial(coloc_pip, **colocalisation_method_params)\n        coloc_pip_results = coloc_pip(overlaps)\n\n        # Run eCAVIAR\n\n        ecaviar_results = ECaviar.colocalise(overlaps)\n\n        # Merge results: join on key columns and combine metrics\n        join_keys = [\n            \"leftStudyLocusId\",\n            \"rightStudyLocusId\",\n            \"chromosome\",\n            \"rightStudyType\",\n        ]\n\n        colocalisation_results = Colocalisation(\n            _df=coloc_pip_results.df.alias(\"pip\")\n            .join(\n                ecaviar_results.df.alias(\"ecav\").select(\n                    *join_keys,\n                    f.col(\"clpp\").alias(\"clpp_ecaviar\"),\n                    f.col(\"numberColocalisingVariants\").alias(\n                        \"numberColocalisingVariants_ecaviar\"\n                    ),\n                ),\n                on=join_keys,\n                how=\"inner\",\n            )\n            .select(\n                f.col(\"pip.leftStudyLocusId\"),\n                f.col(\"pip.rightStudyLocusId\"),\n                f.col(\"pip.rightStudyType\"),\n                f.col(\"pip.chromosome\"),\n                # Use a combined method name\n                f.lit(\"COLOC_PIP_ECAVIAR\").alias(\"colocalisationMethod\"),\n                # Use the max number of colocalising variants from both methods\n                f.greatest(\n                    f.col(\"pip.numberColocalisingVariants\"),\n                    f.col(\"numberColocalisingVariants_ecaviar\"),\n                ).alias(\"numberColocalisingVariants\"),\n                # Keep h3 and h4 from ColocPIP\n                f.col(\"pip.h3\"),\n                f.col(\"pip.h4\"),\n                # Add clpp from eCAVIAR\n                f.col(\"clpp_ecaviar\").alias(\"clpp\"),\n                # Keep beta ratio from ColocPIP\n                f.col(\"pip.betaRatioSignAverage\"),\n            ),\n            _schema=Colocalisation.get_schema(),\n        )\n    else:\n        colocalisation_class = self._get_colocalisation_class(colocalisation_method)\n\n        if colocalisation_method == Coloc.METHOD_NAME.lower():\n            credible_set = credible_set.filter(\n                f.col(\"finemappingMethod\").isin(\n                    FinemappingMethod.SUSIE.value, FinemappingMethod.SUSIE_INF.value\n                )\n            )\n\n        # Transform\n        overlaps = credible_set.find_overlaps(\n            restrict_right_studies=restrict_right_studies,\n            gwas_v_qtl_overlap_only=gwas_v_qtl_overlap_only,\n        )\n\n        # Make a partial caller to ensure that colocalisation_method_params are added to the call only when dict is not empty\n        coloc = colocalisation_class.colocalise\n        if colocalisation_method_params:\n            coloc = partial(coloc, **colocalisation_method_params)\n        colocalisation_results = coloc(overlaps)\n\n    # Load\n    colocalisation_results.df.coalesce(session.output_partitions).write.mode(\n        session.write_mode\n    ).parquet(coloc_path)\n</code></pre>"},{"location":"python_api/steps/credible_set_qc_step/","title":"credible_set_qc","text":""},{"location":"python_api/steps/credible_set_qc_step/#gentropy.credible_set_qc.CredibleSetQCStep","title":"<code>gentropy.credible_set_qc.CredibleSetQCStep</code>","text":"<p>Credible set quality control step for fine mapped StudyLoci.</p> Source code in <code>src/gentropy/credible_set_qc.py</code> <pre><code>class CredibleSetQCStep:\n    \"\"\"Credible set quality control step for fine mapped StudyLoci.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        credible_sets_path: str,\n        output_path: str,\n        p_value_threshold: float,\n        purity_min_r2: float,\n        clump: bool,\n        ld_index_path: str | None,\n        study_index_path: str | None,\n        ld_min_r2: float | None,\n        n_partitions: int | None,\n    ) -&gt; None:\n        \"\"\"Run credible set quality control step.\n\n        Check defaults used by steps in hydra configuration `gentropy.config.CredibleSetQCStepConfig`\n\n        Due to the large number of partitions at the input credible_set_path after finemapping, the\n        best strategy it is to repartition and save the dataset after deduplication.\n\n        The `clump` mode will perform additional LD based clumping on the input credible sets.\n        Enabling `clump` mode requires providing `ld_index_path`, `study_index_path` and `ld_min_r2`.\n\n        Args:\n            session (Session): Session object.\n            credible_sets_path (str): Path to credible sets file.\n            output_path (str): Path to write the output file.\n            p_value_threshold (float): P-value threshold for credible set quality control.\n            purity_min_r2 (float): Minimum R2 for purity estimation.\n            clump (bool): Whether to clump the credible sets by LD.\n            ld_index_path (str | None): Path to LD index file.\n            study_index_path (str | None): Path to study index file.\n            ld_min_r2 (float | None): Minimum R2 for LD estimation.\n            n_partitions (int | None): Number of partitions to coalesce the dataset after reading. Defaults to 200\n        \"\"\"\n        n_partitions = n_partitions or 200\n\n        ld_index = (\n            LDIndex.from_parquet(session, ld_index_path) if ld_index_path else None\n        )\n        study_index = (\n            StudyIndex.from_parquet(session, study_index_path)\n            if study_index_path\n            else None\n        )\n\n        cred_sets = StudyLocus.from_parquet(\n            session, credible_sets_path, recursiveFileLookup=True\n        ).coalesce(n_partitions)\n\n        cred_sets_clean = SUSIE_inf.credible_set_qc(\n            cred_sets,\n            p_value_threshold,\n            purity_min_r2,\n            clump,\n            ld_index,\n            study_index,\n            ld_min_r2,\n        )\n        # ensure the saved object is still a valid StudyLocus\n        StudyLocus(\n            _df=cred_sets_clean.df, _schema=StudyLocus.get_schema()\n        ).df.write.mode(session.write_mode).parquet(output_path)\n</code></pre>"},{"location":"python_api/steps/credible_set_qc_step/#gentropy.credible_set_qc.CredibleSetQCStep.__init__","title":"<code>__init__(session: Session, credible_sets_path: str, output_path: str, p_value_threshold: float, purity_min_r2: float, clump: bool, ld_index_path: str | None, study_index_path: str | None, ld_min_r2: float | None, n_partitions: int | None) -&gt; None</code>","text":"<p>Run credible set quality control step.</p> <p>Check defaults used by steps in hydra configuration <code>gentropy.config.CredibleSetQCStepConfig</code></p> <p>Due to the large number of partitions at the input credible_set_path after finemapping, the best strategy it is to repartition and save the dataset after deduplication.</p> <p>The <code>clump</code> mode will perform additional LD based clumping on the input credible sets. Enabling <code>clump</code> mode requires providing <code>ld_index_path</code>, <code>study_index_path</code> and <code>ld_min_r2</code>.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>credible_sets_path</code> <code>str</code> <p>Path to credible sets file.</p> required <code>output_path</code> <code>str</code> <p>Path to write the output file.</p> required <code>p_value_threshold</code> <code>float</code> <p>P-value threshold for credible set quality control.</p> required <code>purity_min_r2</code> <code>float</code> <p>Minimum R2 for purity estimation.</p> required <code>clump</code> <code>bool</code> <p>Whether to clump the credible sets by LD.</p> required <code>ld_index_path</code> <code>str | None</code> <p>Path to LD index file.</p> required <code>study_index_path</code> <code>str | None</code> <p>Path to study index file.</p> required <code>ld_min_r2</code> <code>float | None</code> <p>Minimum R2 for LD estimation.</p> required <code>n_partitions</code> <code>int | None</code> <p>Number of partitions to coalesce the dataset after reading. Defaults to 200</p> required Source code in <code>src/gentropy/credible_set_qc.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    credible_sets_path: str,\n    output_path: str,\n    p_value_threshold: float,\n    purity_min_r2: float,\n    clump: bool,\n    ld_index_path: str | None,\n    study_index_path: str | None,\n    ld_min_r2: float | None,\n    n_partitions: int | None,\n) -&gt; None:\n    \"\"\"Run credible set quality control step.\n\n    Check defaults used by steps in hydra configuration `gentropy.config.CredibleSetQCStepConfig`\n\n    Due to the large number of partitions at the input credible_set_path after finemapping, the\n    best strategy it is to repartition and save the dataset after deduplication.\n\n    The `clump` mode will perform additional LD based clumping on the input credible sets.\n    Enabling `clump` mode requires providing `ld_index_path`, `study_index_path` and `ld_min_r2`.\n\n    Args:\n        session (Session): Session object.\n        credible_sets_path (str): Path to credible sets file.\n        output_path (str): Path to write the output file.\n        p_value_threshold (float): P-value threshold for credible set quality control.\n        purity_min_r2 (float): Minimum R2 for purity estimation.\n        clump (bool): Whether to clump the credible sets by LD.\n        ld_index_path (str | None): Path to LD index file.\n        study_index_path (str | None): Path to study index file.\n        ld_min_r2 (float | None): Minimum R2 for LD estimation.\n        n_partitions (int | None): Number of partitions to coalesce the dataset after reading. Defaults to 200\n    \"\"\"\n    n_partitions = n_partitions or 200\n\n    ld_index = (\n        LDIndex.from_parquet(session, ld_index_path) if ld_index_path else None\n    )\n    study_index = (\n        StudyIndex.from_parquet(session, study_index_path)\n        if study_index_path\n        else None\n    )\n\n    cred_sets = StudyLocus.from_parquet(\n        session, credible_sets_path, recursiveFileLookup=True\n    ).coalesce(n_partitions)\n\n    cred_sets_clean = SUSIE_inf.credible_set_qc(\n        cred_sets,\n        p_value_threshold,\n        purity_min_r2,\n        clump,\n        ld_index,\n        study_index,\n        ld_min_r2,\n    )\n    # ensure the saved object is still a valid StudyLocus\n    StudyLocus(\n        _df=cred_sets_clean.df, _schema=StudyLocus.get_schema()\n    ).df.write.mode(session.write_mode).parquet(output_path)\n</code></pre>"},{"location":"python_api/steps/credible_set_qc_step/#gentropy.config.CredibleSetQCStepConfig","title":"<code>gentropy.config.CredibleSetQCStepConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>StepConfig</code></p> <p>Credible set quality control step configuration.</p> Source code in <code>src/gentropy/config.py</code> <pre><code>@dataclass\nclass CredibleSetQCStepConfig(StepConfig):\n    \"\"\"Credible set quality control step configuration.\"\"\"\n\n    credible_sets_path: str = MISSING\n    output_path: str = MISSING\n    p_value_threshold: float = 1e-5\n    purity_min_r2: float = 0.01\n    clump: bool = False\n    ld_index_path: str | None = None\n    study_index_path: str | None = None\n    ld_min_r2: float | None = 0.8\n    n_partitions: int | None = 200\n    _target_: str = \"gentropy.credible_set_qc.CredibleSetQCStep\"\n</code></pre>"},{"location":"python_api/steps/eqtl_catalogue/","title":"eQTL Catalogue","text":""},{"location":"python_api/steps/eqtl_catalogue/#gentropy.eqtl_catalogue.EqtlCatalogueStep","title":"<code>gentropy.eqtl_catalogue.EqtlCatalogueStep</code>","text":"<p>eQTL Catalogue ingestion step.</p> <p>From SuSIE fine mapping results (available at their FTP ), we extract credible sets and study index datasets from gene expression QTL studies.</p> Source code in <code>src/gentropy/eqtl_catalogue.py</code> <pre><code>class EqtlCatalogueStep:\n    \"\"\"eQTL Catalogue ingestion step.\n\n    From SuSIE fine mapping results (available at [their FTP](https://ftp.ebi.ac.uk/pub/databases/spot/eQTL/susie/) ), we extract credible sets and study index datasets from gene expression QTL studies.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        mqtl_quantification_methods_blacklist: list[str],\n        eqtl_catalogue_paths_imported: str,\n        eqtl_catalogue_study_index_out: str,\n        eqtl_catalogue_credible_sets_out: str,\n        eqtl_lead_pvalue_threshold: float = EqtlCatalogueConfig().eqtl_lead_pvalue_threshold,\n    ) -&gt; None:\n        \"\"\"Run eQTL Catalogue ingestion step.\n\n        Args:\n            session (Session): Session object.\n            mqtl_quantification_methods_blacklist (list[str]): Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv\n            eqtl_catalogue_paths_imported (str): Input eQTL Catalogue fine mapping results path.\n            eqtl_catalogue_study_index_out (str): Output eQTL Catalogue study index path.\n            eqtl_catalogue_credible_sets_out (str): Output eQTL Catalogue credible sets path.\n            eqtl_lead_pvalue_threshold (float, optional): Lead p-value threshold. Defaults to EqtlCatalogueConfig().eqtl_lead_pvalue_threshold.\n        \"\"\"\n        # Extract\n        studies_metadata = EqtlCatalogueStudyIndex.read_studies_from_source(\n            session, list(mqtl_quantification_methods_blacklist)\n        )\n\n        # Load raw data only for the studies we are interested in ingestion. This makes the proces much lighter.\n        studies_to_ingest = EqtlCatalogueStudyIndex.get_studies_of_interest(\n            studies_metadata\n        )\n        credible_sets_df = EqtlCatalogueFinemapping.read_credible_set_from_source(\n            session,\n            credible_set_path=[\n                f\"{eqtl_catalogue_paths_imported}/{qtd_id}.credible_sets.tsv\"\n                for qtd_id in studies_to_ingest\n            ],\n        )\n        lbf_df = EqtlCatalogueFinemapping.read_lbf_from_source(\n            session,\n            lbf_path=[\n                f\"{eqtl_catalogue_paths_imported}/{qtd_id}.lbf_variable.txt\"\n                for qtd_id in studies_to_ingest\n            ],\n        )\n\n        # Transform\n        processed_susie_df = EqtlCatalogueFinemapping.parse_susie_results(\n            credible_sets_df, lbf_df, studies_metadata\n        )\n\n        (\n            EqtlCatalogueStudyIndex.from_susie_results(processed_susie_df)\n            # Writing the output:\n            .df.write.mode(session.write_mode)\n            .parquet(eqtl_catalogue_study_index_out)\n        )\n\n        (\n            EqtlCatalogueFinemapping.from_susie_results(processed_susie_df)\n            # Flagging sub-significnat loci:\n            .validate_lead_pvalue(pvalue_cutoff=eqtl_lead_pvalue_threshold)\n            # Writing the output:\n            .df.write.mode(session.write_mode)\n            .parquet(eqtl_catalogue_credible_sets_out)\n        )\n</code></pre>"},{"location":"python_api/steps/eqtl_catalogue/#gentropy.eqtl_catalogue.EqtlCatalogueStep.__init__","title":"<code>__init__(session: Session, mqtl_quantification_methods_blacklist: list[str], eqtl_catalogue_paths_imported: str, eqtl_catalogue_study_index_out: str, eqtl_catalogue_credible_sets_out: str, eqtl_lead_pvalue_threshold: float = EqtlCatalogueConfig().eqtl_lead_pvalue_threshold) -&gt; None</code>","text":"<p>Run eQTL Catalogue ingestion step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>mqtl_quantification_methods_blacklist</code> <code>list[str]</code> <p>Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv</p> required <code>eqtl_catalogue_paths_imported</code> <code>str</code> <p>Input eQTL Catalogue fine mapping results path.</p> required <code>eqtl_catalogue_study_index_out</code> <code>str</code> <p>Output eQTL Catalogue study index path.</p> required <code>eqtl_catalogue_credible_sets_out</code> <code>str</code> <p>Output eQTL Catalogue credible sets path.</p> required <code>eqtl_lead_pvalue_threshold</code> <code>float</code> <p>Lead p-value threshold. Defaults to EqtlCatalogueConfig().eqtl_lead_pvalue_threshold.</p> <code>eqtl_lead_pvalue_threshold</code> Source code in <code>src/gentropy/eqtl_catalogue.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    mqtl_quantification_methods_blacklist: list[str],\n    eqtl_catalogue_paths_imported: str,\n    eqtl_catalogue_study_index_out: str,\n    eqtl_catalogue_credible_sets_out: str,\n    eqtl_lead_pvalue_threshold: float = EqtlCatalogueConfig().eqtl_lead_pvalue_threshold,\n) -&gt; None:\n    \"\"\"Run eQTL Catalogue ingestion step.\n\n    Args:\n        session (Session): Session object.\n        mqtl_quantification_methods_blacklist (list[str]): Molecular trait quantification methods that we don't want to ingest. Available options in https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/data_tables/dataset_metadata.tsv\n        eqtl_catalogue_paths_imported (str): Input eQTL Catalogue fine mapping results path.\n        eqtl_catalogue_study_index_out (str): Output eQTL Catalogue study index path.\n        eqtl_catalogue_credible_sets_out (str): Output eQTL Catalogue credible sets path.\n        eqtl_lead_pvalue_threshold (float, optional): Lead p-value threshold. Defaults to EqtlCatalogueConfig().eqtl_lead_pvalue_threshold.\n    \"\"\"\n    # Extract\n    studies_metadata = EqtlCatalogueStudyIndex.read_studies_from_source(\n        session, list(mqtl_quantification_methods_blacklist)\n    )\n\n    # Load raw data only for the studies we are interested in ingestion. This makes the proces much lighter.\n    studies_to_ingest = EqtlCatalogueStudyIndex.get_studies_of_interest(\n        studies_metadata\n    )\n    credible_sets_df = EqtlCatalogueFinemapping.read_credible_set_from_source(\n        session,\n        credible_set_path=[\n            f\"{eqtl_catalogue_paths_imported}/{qtd_id}.credible_sets.tsv\"\n            for qtd_id in studies_to_ingest\n        ],\n    )\n    lbf_df = EqtlCatalogueFinemapping.read_lbf_from_source(\n        session,\n        lbf_path=[\n            f\"{eqtl_catalogue_paths_imported}/{qtd_id}.lbf_variable.txt\"\n            for qtd_id in studies_to_ingest\n        ],\n    )\n\n    # Transform\n    processed_susie_df = EqtlCatalogueFinemapping.parse_susie_results(\n        credible_sets_df, lbf_df, studies_metadata\n    )\n\n    (\n        EqtlCatalogueStudyIndex.from_susie_results(processed_susie_df)\n        # Writing the output:\n        .df.write.mode(session.write_mode)\n        .parquet(eqtl_catalogue_study_index_out)\n    )\n\n    (\n        EqtlCatalogueFinemapping.from_susie_results(processed_susie_df)\n        # Flagging sub-significnat loci:\n        .validate_lead_pvalue(pvalue_cutoff=eqtl_lead_pvalue_threshold)\n        # Writing the output:\n        .df.write.mode(session.write_mode)\n        .parquet(eqtl_catalogue_credible_sets_out)\n    )\n</code></pre>"},{"location":"python_api/steps/finngen_studies/","title":"finngen_studies","text":""},{"location":"python_api/steps/finngen_studies/#gentropy.finngen_studies.FinnGenStudiesStep","title":"<code>gentropy.finngen_studies.FinnGenStudiesStep</code>","text":"<p>FinnGen study index generation step.</p> Source code in <code>src/gentropy/finngen_studies.py</code> <pre><code>class FinnGenStudiesStep:\n    \"\"\"FinnGen study index generation step.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        finngen_study_index_out: str,\n        finngen_phenotype_table_url: str = FinngenStudiesConfig().finngen_phenotype_table_url,\n        finngen_release_prefix: str = FinngenStudiesConfig().finngen_release_prefix,\n        finngen_summary_stats_url_prefix: str = FinngenStudiesConfig().finngen_summary_stats_url_prefix,\n        finngen_summary_stats_url_suffix: str = FinngenStudiesConfig().finngen_summary_stats_url_suffix,\n        efo_curation_mapping_url: str = FinngenStudiesConfig().efo_curation_mapping_url,\n        sample_size: int = FinngenStudiesConfig().sample_size,\n    ) -&gt; None:\n        \"\"\"Run FinnGen study index generation step.\n\n        Args:\n            session (Session): Session object.\n            finngen_study_index_out (str): Output FinnGen study index path.\n            finngen_phenotype_table_url (str): URL to the FinnGen phenotype table.\n            finngen_release_prefix (str): FinnGen release prefix.\n            finngen_summary_stats_url_prefix (str): FinnGen summary stats URL prefix.\n            finngen_summary_stats_url_suffix (str): FinnGen summary stats URL suffix.\n            efo_curation_mapping_url (str): URL to the EFO curation mapping file\n            sample_size (int): Number of individuals that participated in sample collection, derived from finngen release metadata.\n        \"\"\"\n        _match = FinnGenStudyIndex.validate_release_prefix(finngen_release_prefix)\n        release_prefix = _match[\"prefix\"]\n        release = _match[\"release\"]\n\n        efo_mapping = EFOMapping.from_path(session, efo_curation_mapping_url)\n        study_index = FinnGenStudyIndex.from_source(\n            session.spark,\n            finngen_phenotype_table_url,\n            release_prefix,\n            finngen_summary_stats_url_prefix,\n            finngen_summary_stats_url_suffix,\n            sample_size,\n        )\n        study_index_with_efo = efo_mapping.annotate_study_index(study_index, release)\n        study_index_with_efo.df.coalesce(session.output_partitions).write.mode(\n            session.write_mode\n        ).parquet(finngen_study_index_out)\n</code></pre>"},{"location":"python_api/steps/finngen_studies/#gentropy.finngen_studies.FinnGenStudiesStep.__init__","title":"<code>__init__(session: Session, finngen_study_index_out: str, finngen_phenotype_table_url: str = FinngenStudiesConfig().finngen_phenotype_table_url, finngen_release_prefix: str = FinngenStudiesConfig().finngen_release_prefix, finngen_summary_stats_url_prefix: str = FinngenStudiesConfig().finngen_summary_stats_url_prefix, finngen_summary_stats_url_suffix: str = FinngenStudiesConfig().finngen_summary_stats_url_suffix, efo_curation_mapping_url: str = FinngenStudiesConfig().efo_curation_mapping_url, sample_size: int = FinngenStudiesConfig().sample_size) -&gt; None</code>","text":"<p>Run FinnGen study index generation step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>finngen_study_index_out</code> <code>str</code> <p>Output FinnGen study index path.</p> required <code>finngen_phenotype_table_url</code> <code>str</code> <p>URL to the FinnGen phenotype table.</p> <code>finngen_phenotype_table_url</code> <code>finngen_release_prefix</code> <code>str</code> <p>FinnGen release prefix.</p> <code>finngen_release_prefix</code> <code>finngen_summary_stats_url_prefix</code> <code>str</code> <p>FinnGen summary stats URL prefix.</p> <code>finngen_summary_stats_url_prefix</code> <code>finngen_summary_stats_url_suffix</code> <code>str</code> <p>FinnGen summary stats URL suffix.</p> <code>finngen_summary_stats_url_suffix</code> <code>efo_curation_mapping_url</code> <code>str</code> <p>URL to the EFO curation mapping file</p> <code>efo_curation_mapping_url</code> <code>sample_size</code> <code>int</code> <p>Number of individuals that participated in sample collection, derived from finngen release metadata.</p> <code>sample_size</code> Source code in <code>src/gentropy/finngen_studies.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    finngen_study_index_out: str,\n    finngen_phenotype_table_url: str = FinngenStudiesConfig().finngen_phenotype_table_url,\n    finngen_release_prefix: str = FinngenStudiesConfig().finngen_release_prefix,\n    finngen_summary_stats_url_prefix: str = FinngenStudiesConfig().finngen_summary_stats_url_prefix,\n    finngen_summary_stats_url_suffix: str = FinngenStudiesConfig().finngen_summary_stats_url_suffix,\n    efo_curation_mapping_url: str = FinngenStudiesConfig().efo_curation_mapping_url,\n    sample_size: int = FinngenStudiesConfig().sample_size,\n) -&gt; None:\n    \"\"\"Run FinnGen study index generation step.\n\n    Args:\n        session (Session): Session object.\n        finngen_study_index_out (str): Output FinnGen study index path.\n        finngen_phenotype_table_url (str): URL to the FinnGen phenotype table.\n        finngen_release_prefix (str): FinnGen release prefix.\n        finngen_summary_stats_url_prefix (str): FinnGen summary stats URL prefix.\n        finngen_summary_stats_url_suffix (str): FinnGen summary stats URL suffix.\n        efo_curation_mapping_url (str): URL to the EFO curation mapping file\n        sample_size (int): Number of individuals that participated in sample collection, derived from finngen release metadata.\n    \"\"\"\n    _match = FinnGenStudyIndex.validate_release_prefix(finngen_release_prefix)\n    release_prefix = _match[\"prefix\"]\n    release = _match[\"release\"]\n\n    efo_mapping = EFOMapping.from_path(session, efo_curation_mapping_url)\n    study_index = FinnGenStudyIndex.from_source(\n        session.spark,\n        finngen_phenotype_table_url,\n        release_prefix,\n        finngen_summary_stats_url_prefix,\n        finngen_summary_stats_url_suffix,\n        sample_size,\n    )\n    study_index_with_efo = efo_mapping.annotate_study_index(study_index, release)\n    study_index_with_efo.df.coalesce(session.output_partitions).write.mode(\n        session.write_mode\n    ).parquet(finngen_study_index_out)\n</code></pre>"},{"location":"python_api/steps/finngen_sumstat_preprocess/","title":"finngen_sumstat_preprocess","text":""},{"location":"python_api/steps/finngen_sumstat_preprocess/#gentropy.finngen_sumstat_preprocess.FinnGenSumstatPreprocessStep","title":"<code>gentropy.finngen_sumstat_preprocess.FinnGenSumstatPreprocessStep</code>","text":"<p>FinnGen sumstats preprocessing.</p> Source code in <code>src/gentropy/finngen_sumstat_preprocess.py</code> <pre><code>class FinnGenSumstatPreprocessStep:\n    \"\"\"FinnGen sumstats preprocessing.\"\"\"\n\n    def __init__(\n        self, session: Session, raw_sumstats_path: str, out_sumstats_path: str\n    ) -&gt; None:\n        \"\"\"Run FinnGen summary stats preprocessing step.\n\n        Args:\n            session (Session): Session object.\n            raw_sumstats_path (str): Input raw summary stats path.\n            out_sumstats_path (str): Output summary stats path.\n        \"\"\"\n        # Process summary stats.\n        (\n            FinnGenSummaryStats.from_source(session.spark, raw_file=raw_sumstats_path)\n            .df.write.mode(session.write_mode)\n            .parquet(out_sumstats_path)\n        )\n</code></pre>"},{"location":"python_api/steps/finngen_sumstat_preprocess/#gentropy.finngen_sumstat_preprocess.FinnGenSumstatPreprocessStep.__init__","title":"<code>__init__(session: Session, raw_sumstats_path: str, out_sumstats_path: str) -&gt; None</code>","text":"<p>Run FinnGen summary stats preprocessing step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>raw_sumstats_path</code> <code>str</code> <p>Input raw summary stats path.</p> required <code>out_sumstats_path</code> <code>str</code> <p>Output summary stats path.</p> required Source code in <code>src/gentropy/finngen_sumstat_preprocess.py</code> <pre><code>def __init__(\n    self, session: Session, raw_sumstats_path: str, out_sumstats_path: str\n) -&gt; None:\n    \"\"\"Run FinnGen summary stats preprocessing step.\n\n    Args:\n        session (Session): Session object.\n        raw_sumstats_path (str): Input raw summary stats path.\n        out_sumstats_path (str): Output summary stats path.\n    \"\"\"\n    # Process summary stats.\n    (\n        FinnGenSummaryStats.from_source(session.spark, raw_file=raw_sumstats_path)\n        .df.write.mode(session.write_mode)\n        .parquet(out_sumstats_path)\n    )\n</code></pre>"},{"location":"python_api/steps/finngen_ukbb_mvp_meta_step/","title":"FinnGen UKBB MVP Meta Analysis Step","text":""},{"location":"python_api/steps/finngen_ukbb_mvp_meta_step/#gentropy.finngen_ukb_mvp_meta.FinngenUkbMvpMetaSummaryStatisticsIngestionStep","title":"<code>gentropy.finngen_ukb_mvp_meta.FinngenUkbMvpMetaSummaryStatisticsIngestionStep</code>","text":"<p>FinnGen UK Biobank and Million Veteran Program meta-analysis summary statistics ingestion step.</p>"},{"location":"python_api/steps/finngen_ukbb_mvp_meta_step/#gentropy.finngen_ukb_mvp_meta.FinngenUkbMvpMetaSummaryStatisticsIngestionStep--process-overview","title":"Process overview","text":"<p>The step performs the following operations:</p> <ol> <li>Prepares <code>FinnGenManifest</code> and <code>EFOCuration</code>.</li> <li>Builds the <code>StudyIndex</code>.</li> <li>Reads the raw summary statistics paths from <code>StudyIndex</code>.</li> <li>Converts source summary statistics from BGZIP into Parquet.</li> <li>Prepares <code>VariantDirection</code> for allele flipping.</li> <li>Harmonises <code>SummaryStatistics</code>.</li> <li>Performs quality control on harmonised <code>SummaryStatistics</code>.</li> <li>Updates <code>StudyIndex</code> with QC results.</li> </ol> <pre><code>graph TD\n    %% --- INPUTS ---\n    A1([source_manifest_path]) --&gt; B1\n    A2([efo_curation_path]) --&gt; B2\n    A3([gnomad_variant_index_path]) --&gt; G1\n    A4([Source Summary Statistics in BGZIP format]) --&gt; C3\n\n    %% --- STEP 1: StudyIndex ---\n    subgraph \"Building studyIndex\"\n    B1[\"FinnGenMetaManifest\"] --&gt; C1[\"StudyIndex\"]\n    B2[\"EFOMapping\"] --&gt; C1\n    end\n\n    %% --- STEP 2: Raw Summary Statistics ---\n    subgraph \"Downloading summary statistics\"\n    C1 --&gt; C2[\"List of summary statistics paths\"]\n    C2 --&gt; C3[\"Raw summary statistics in parquet format\"]\n    end\n\n    %% --- STEP 3: Quality Control ---\n    subgraph \"Variant Annotations\"\n    G1[\"VariantIndex\"] --&gt; G2[\"VariantDirection\"]\n    end\n\n    %% --- STEP 4: Harmonised Summary Statistics ---\n    subgraph \"Harmonising summary statistics\"\n    C3 --&gt; D1[\"Allele flipping\"]\n    B1 --&gt; D1\n    G2 --&gt; D1\n    D1 --&gt; D2[\"Removal of not meta-analysed variants\"]\n    D2 --&gt; D3[\"Removal of low imputation score variants\"]\n    D3 --&gt; D4[\"Removal of low allele count variants\"]\n    D4 --&gt; E1[\"Harmonised summary statistics in parquet format\"]\n    end\n\n    %% --- STEP 5: QC ---\n    subgraph \"Summary Statistics QC\"\n    E1 --&gt; Q1[\"SummaryStatistics QC\"]\n    Q1 --&gt; Q2[\"StudyIndex annotated with QC\"]\n    C1 --&gt; Q2\n    end\n\n    %% --- STYLING ---\n    classDef input fill:#f8f8ff,stroke:#555,stroke-width:1px,color:#000;\n    classDef output fill:#e7ffe7,stroke:#555,stroke-width:1px,color:#000;\n\n    class A1,A2,A3,A4 input;\n    class Q2,E1,Q1 output;</code></pre> Inputs <ul> <li> This step requires the gnomAD variant index to perform the allele flipping during harmonisation.</li> <li> The <code>source_manifest_path</code> should point to a manifest that includes paths to the summary statistics files.</li> </ul> Outputs <p>This step outputs 4 artifacts:</p> <ul> <li> Raw summary statistics in Parquet format.</li> <li> Harmonised summary statistics in Parquet format.</li> <li> Summary statistics QC results in Parquet format.</li> <li> Study Index in parquet format (updated with QC results).</li> </ul> Source code in <code>src/gentropy/finngen_ukb_mvp_meta.py</code> <pre><code>class FinngenUkbMvpMetaSummaryStatisticsIngestionStep:\n    \"\"\"FinnGen UK Biobank and Million Veteran Program meta-analysis summary statistics ingestion step.\n\n    # Process overview\n\n    The step performs the following operations:\n\n    1. Prepares `FinnGenManifest` and `EFOCuration`.\n    2. Builds the `StudyIndex`.\n    3. Reads the raw summary statistics paths from `StudyIndex`.\n    3. Converts **source summary statistics** from _BGZIP_ into _Parquet_.\n    4. Prepares `VariantDirection` for allele flipping.\n    5. Harmonises `SummaryStatistics`.\n    6. Performs quality control on harmonised `SummaryStatistics`.\n    7. Updates `StudyIndex` with QC results.\n\n    ``` mermaid\n    graph TD\n        %% --- INPUTS ---\n        A1([source_manifest_path]) --&gt; B1\n        A2([efo_curation_path]) --&gt; B2\n        A3([gnomad_variant_index_path]) --&gt; G1\n        A4([Source Summary Statistics in BGZIP format]) --&gt; C3\n\n        %% --- STEP 1: StudyIndex ---\n        subgraph \"Building studyIndex\"\n        B1[\"FinnGenMetaManifest\"] --&gt; C1[\"StudyIndex\"]\n        B2[\"EFOMapping\"] --&gt; C1\n        end\n\n        %% --- STEP 2: Raw Summary Statistics ---\n        subgraph \"Downloading summary statistics\"\n        C1 --&gt; C2[\"List of summary statistics paths\"]\n        C2 --&gt; C3[\"Raw summary statistics in parquet format\"]\n        end\n\n        %% --- STEP 3: Quality Control ---\n        subgraph \"Variant Annotations\"\n        G1[\"VariantIndex\"] --&gt; G2[\"VariantDirection\"]\n        end\n\n        %% --- STEP 4: Harmonised Summary Statistics ---\n        subgraph \"Harmonising summary statistics\"\n        C3 --&gt; D1[\"Allele flipping\"]\n        B1 --&gt; D1\n        G2 --&gt; D1\n        D1 --&gt; D2[\"Removal of not meta-analysed variants\"]\n        D2 --&gt; D3[\"Removal of low imputation score variants\"]\n        D3 --&gt; D4[\"Removal of low allele count variants\"]\n        D4 --&gt; E1[\"Harmonised summary statistics in parquet format\"]\n        end\n\n        %% --- STEP 5: QC ---\n        subgraph \"Summary Statistics QC\"\n        E1 --&gt; Q1[\"SummaryStatistics QC\"]\n        Q1 --&gt; Q2[\"StudyIndex annotated with QC\"]\n        C1 --&gt; Q2\n        end\n\n        %% --- STYLING ---\n        classDef input fill:#f8f8ff,stroke:#555,stroke-width:1px,color:#000;\n        classDef output fill:#e7ffe7,stroke:#555,stroke-width:1px,color:#000;\n\n        class A1,A2,A3,A4 input;\n        class Q2,E1,Q1 output;\n    ```\n\n    ??? tip \"Inputs\"\n        - [x] This step requires the gnomAD variant index to perform the allele flipping during harmonisation.\n        - [x] The `source_manifest_path` should point to a manifest that includes paths to the summary statistics files.\n\n    ??? tip \"Outputs\"\n        This step outputs 4 artifacts:\n\n        - [x] Raw summary statistics in Parquet format.\n        - [x] Harmonised summary statistics in Parquet format.\n        - [x] Summary statistics QC results in Parquet format.\n        - [x] Study Index in parquet format (updated with QC results).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        # Inputs\n        source_manifest_path: str,\n        efo_curation_path: str,\n        gnomad_variant_index_path: str,\n        # Outputs\n        study_index_output_path: str,\n        raw_summary_statistics_output_path: str,\n        harmonised_summary_statistics_output_path: str,\n        harmonised_summary_statistics_qc_output_path: str,\n        # Harmonisation config\n        perform_meta_analysis_filter: bool = True,\n        imputation_score_threshold: float = 0.8,\n        perform_imputation_score_filter: bool = True,\n        min_allele_count_threshold: int = 20,\n        perform_min_allele_count_filter: bool = True,\n        min_allele_frequency_threshold: float = 1e-4,\n        perform_min_allele_frequency_filter: bool = False,\n        filter_out_ambiguous_variants: bool = False,\n        # QC config\n        qc_threshold: float = 1e-8,\n    ) -&gt; None:\n        \"\"\"Data ingestion and harmonisation step for FinnGen UKB meta-analysis.\n\n        Args:\n            session (Session): Session object.\n            source_manifest_path (str): Path to the manifest file.\n            efo_curation_path (str): Path to the EFO curation file.\n            gnomad_variant_index_path (str): Path to the gnomAD variant index file.\n            study_index_output_path (str): Output path for the study index.\n            raw_summary_statistics_output_path (str): Output path for raw summary statistics.\n            harmonised_summary_statistics_output_path (str): Output path for harmonised summary statistics.\n            harmonised_summary_statistics_qc_output_path (str): Output path for harmonised summary statistics QC results.\n            perform_meta_analysis_filter (bool, optional): Whether to filter non-meta analyzed variants.\n            imputation_score_threshold (float, optional): Imputation score threshold.\n            perform_imputation_score_filter (bool, optional): Whether to filter low imputation scores.\n            min_allele_count_threshold (int, optional): Minimum allele count threshold.\n            perform_min_allele_count_filter (bool, optional): Whether to filter low allele counts.\n            min_allele_frequency_threshold (float, optional): Minimum allele frequency threshold.\n            perform_min_allele_frequency_filter (bool, optional): Whether to filter low allele frequencies.\n            filter_out_ambiguous_variants (bool, optional): Whether to filter out ambiguous variants.\n            qc_threshold (float, optional): P-value threshold for QC.\n\n        Raises:\n            AssertionError: If no summary statistics paths are found in the study index.\n        \"\"\"\n        assert qc_threshold &lt; 1.0, \"QC threshold should be a p-value less than 1.0.\"\n        sumstat_harmonisation_config: dict[str, Any] = {\n            \"perform_meta_analysis_filter\": perform_meta_analysis_filter,\n            \"imputation_score_threshold\": imputation_score_threshold,\n            \"perform_imputation_score_filter\": perform_imputation_score_filter,\n            \"min_allele_count_threshold\": min_allele_count_threshold,\n            \"perform_min_allele_count_filter\": perform_min_allele_count_filter,\n            \"min_allele_frequency_threshold\": min_allele_frequency_threshold,\n            \"perform_min_allele_frequency_filter\": perform_min_allele_frequency_filter,\n            \"filter_out_ambiguous_variants\": filter_out_ambiguous_variants,\n        }\n\n        session.logger.info(f\"Reading Finngen manifest from {source_manifest_path}.\")\n        finngen_manifest = FinnGenMetaManifest.from_path(\n            session=session, manifest_path=source_manifest_path\n        )\n        session.logger.info(f\"Building study index for: {finngen_manifest.meta.value}\")\n        session.logger.info(f\"Reading EFO curation from {efo_curation_path}.\")\n        efo_mapping = EFOMapping.from_path(\n            session=session, efo_curation_path=efo_curation_path\n        )\n\n        session.logger.info(\"Creating study index.\")\n        study_index = FinnGenMetaStudyIndex.from_finngen_manifest(\n            manifest=finngen_manifest, efo_mapping=efo_mapping\n        )\n\n        session.logger.info(\"Writing study index.\")\n        study_index.df.write.mode(session.write_mode).parquet(study_index_output_path)\n        session.logger.info(f\"Study index written to {study_index_output_path}.\")\n\n        session.logger.info(\"Reading summary statistics paths from manifest.\")\n        # NOTE: we can rely on the study index to extract the raw summary statistics paths\n        # to make sure to only process these summary statistics which are part of the study index.\n        # this may not be accurate if the summary statistics source paths were not found in the\n        # source manifest.\n        source_summary_statistics_paths = study_index.get_summary_statistics_paths()\n        assert (\n            len(source_summary_statistics_paths) &gt; 0\n        ), \"No summary statistics paths found in study index.\"\n        session.logger.info(\n            f\"Found {len(source_summary_statistics_paths)} summary statistics files.\"\n        )\n\n        session.logger.info(\"Converting raw summary statistics to Parquet format.\")\n        FinnGenUkbMvpMetaSummaryStatistics.bgzip_to_parquet(\n            session=session,\n            summary_statistics_list=source_summary_statistics_paths,\n            datasource=finngen_manifest.meta,\n            raw_summary_statistics_output_path=raw_summary_statistics_output_path,\n            n_threads=FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_OPTIMAL,\n        )\n        session.logger.info(\"Raw summary statistics conversion completed.\")\n        session.logger.info(f\"Output path: {raw_summary_statistics_output_path}.\")\n\n        session.logger.info(\"Reading gnomAD variant index.\")\n        gnomad_variant_index = VariantIndex.from_parquet(\n            session=session, path=gnomad_variant_index_path\n        )\n\n        session.logger.info(\"Building variant direction annotations.\")\n        variant_direction = VariantDirection.from_variant_index(\n            variant_index=gnomad_variant_index\n        )\n\n        session.logger.info(\"Reading raw summary statistics.\")\n        raw_summary_statistics = session.spark.read.parquet(\n            raw_summary_statistics_output_path\n        )\n\n        session.logger.info(\"Harmonising summary statistics.\")\n        session.logger.info(\"Applying the following harmonisation configuration:\")\n        for key, value in sumstat_harmonisation_config.items():\n            session.logger.info(f\"  - {key}: {value}\")\n        harmonised_summary_statistics = FinnGenUkbMvpMetaSummaryStatistics.from_source(\n            raw_summary_statistics=raw_summary_statistics,\n            finngen_manifest=finngen_manifest,\n            variant_annotations=variant_direction,\n            **sumstat_harmonisation_config,\n        )\n\n        session.logger.info(\"Writing harmonised summary statistics.\")\n        harmonised_summary_statistics.df.write.mode(session.write_mode).parquet(\n            harmonised_summary_statistics_output_path\n        )\n        session.logger.info(\n            f\"Harmonised summary statistics written to {harmonised_summary_statistics_output_path}.\"\n        )\n\n        session.logger.info(\"Reading harmonised summary statistics for QC.\")\n        harmonised_summary_statistics = SummaryStatistics.from_parquet(\n            session=session, path=harmonised_summary_statistics_output_path\n        )\n        session.logger.info(\"Running summary statistics QC.\")\n        summary_statistics_qc = SummaryStatisticsQC.from_summary_statistics(\n            gwas=harmonised_summary_statistics,\n            pval_threshold=qc_threshold,\n        )\n\n        session.logger.info(\"Writing summary statistics QC results.\")\n        summary_statistics_qc.df.repartition(1).write.mode(session.write_mode).parquet(\n            harmonised_summary_statistics_qc_output_path\n        )\n        session.logger.info(\n            f\"Summary statistics QC results written to {harmonised_summary_statistics_qc_output_path}.\"\n        )\n\n        session.logger.info(\"Adding qc to the study index.\")\n        study_index = StudyIndex.from_parquet(\n            session=session, path=study_index_output_path\n        )\n        study_index = study_index.annotate_sumstats_qc(summary_statistics_qc)\n\n        session.logger.info(\"Writing updated study index.\")\n        study_index.df.repartition(1).write.mode(\"overwrite\").parquet(\n            study_index_output_path\n        )\n        session.logger.info(\"Updated study index with qc flags.\")\n</code></pre>"},{"location":"python_api/steps/finngen_ukbb_mvp_meta_step/#gentropy.finngen_ukb_mvp_meta.FinngenUkbMvpMetaSummaryStatisticsIngestionStep.__init__","title":"<code>__init__(session: Session, source_manifest_path: str, efo_curation_path: str, gnomad_variant_index_path: str, study_index_output_path: str, raw_summary_statistics_output_path: str, harmonised_summary_statistics_output_path: str, harmonised_summary_statistics_qc_output_path: str, perform_meta_analysis_filter: bool = True, imputation_score_threshold: float = 0.8, perform_imputation_score_filter: bool = True, min_allele_count_threshold: int = 20, perform_min_allele_count_filter: bool = True, min_allele_frequency_threshold: float = 0.0001, perform_min_allele_frequency_filter: bool = False, filter_out_ambiguous_variants: bool = False, qc_threshold: float = 1e-08) -&gt; None</code>","text":"<p>Data ingestion and harmonisation step for FinnGen UKB meta-analysis.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>source_manifest_path</code> <code>str</code> <p>Path to the manifest file.</p> required <code>efo_curation_path</code> <code>str</code> <p>Path to the EFO curation file.</p> required <code>gnomad_variant_index_path</code> <code>str</code> <p>Path to the gnomAD variant index file.</p> required <code>study_index_output_path</code> <code>str</code> <p>Output path for the study index.</p> required <code>raw_summary_statistics_output_path</code> <code>str</code> <p>Output path for raw summary statistics.</p> required <code>harmonised_summary_statistics_output_path</code> <code>str</code> <p>Output path for harmonised summary statistics.</p> required <code>harmonised_summary_statistics_qc_output_path</code> <code>str</code> <p>Output path for harmonised summary statistics QC results.</p> required <code>perform_meta_analysis_filter</code> <code>bool</code> <p>Whether to filter non-meta analyzed variants.</p> <code>True</code> <code>imputation_score_threshold</code> <code>float</code> <p>Imputation score threshold.</p> <code>0.8</code> <code>perform_imputation_score_filter</code> <code>bool</code> <p>Whether to filter low imputation scores.</p> <code>True</code> <code>min_allele_count_threshold</code> <code>int</code> <p>Minimum allele count threshold.</p> <code>20</code> <code>perform_min_allele_count_filter</code> <code>bool</code> <p>Whether to filter low allele counts.</p> <code>True</code> <code>min_allele_frequency_threshold</code> <code>float</code> <p>Minimum allele frequency threshold.</p> <code>0.0001</code> <code>perform_min_allele_frequency_filter</code> <code>bool</code> <p>Whether to filter low allele frequencies.</p> <code>False</code> <code>filter_out_ambiguous_variants</code> <code>bool</code> <p>Whether to filter out ambiguous variants.</p> <code>False</code> <code>qc_threshold</code> <code>float</code> <p>P-value threshold for QC.</p> <code>1e-08</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If no summary statistics paths are found in the study index.</p> Source code in <code>src/gentropy/finngen_ukb_mvp_meta.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    # Inputs\n    source_manifest_path: str,\n    efo_curation_path: str,\n    gnomad_variant_index_path: str,\n    # Outputs\n    study_index_output_path: str,\n    raw_summary_statistics_output_path: str,\n    harmonised_summary_statistics_output_path: str,\n    harmonised_summary_statistics_qc_output_path: str,\n    # Harmonisation config\n    perform_meta_analysis_filter: bool = True,\n    imputation_score_threshold: float = 0.8,\n    perform_imputation_score_filter: bool = True,\n    min_allele_count_threshold: int = 20,\n    perform_min_allele_count_filter: bool = True,\n    min_allele_frequency_threshold: float = 1e-4,\n    perform_min_allele_frequency_filter: bool = False,\n    filter_out_ambiguous_variants: bool = False,\n    # QC config\n    qc_threshold: float = 1e-8,\n) -&gt; None:\n    \"\"\"Data ingestion and harmonisation step for FinnGen UKB meta-analysis.\n\n    Args:\n        session (Session): Session object.\n        source_manifest_path (str): Path to the manifest file.\n        efo_curation_path (str): Path to the EFO curation file.\n        gnomad_variant_index_path (str): Path to the gnomAD variant index file.\n        study_index_output_path (str): Output path for the study index.\n        raw_summary_statistics_output_path (str): Output path for raw summary statistics.\n        harmonised_summary_statistics_output_path (str): Output path for harmonised summary statistics.\n        harmonised_summary_statistics_qc_output_path (str): Output path for harmonised summary statistics QC results.\n        perform_meta_analysis_filter (bool, optional): Whether to filter non-meta analyzed variants.\n        imputation_score_threshold (float, optional): Imputation score threshold.\n        perform_imputation_score_filter (bool, optional): Whether to filter low imputation scores.\n        min_allele_count_threshold (int, optional): Minimum allele count threshold.\n        perform_min_allele_count_filter (bool, optional): Whether to filter low allele counts.\n        min_allele_frequency_threshold (float, optional): Minimum allele frequency threshold.\n        perform_min_allele_frequency_filter (bool, optional): Whether to filter low allele frequencies.\n        filter_out_ambiguous_variants (bool, optional): Whether to filter out ambiguous variants.\n        qc_threshold (float, optional): P-value threshold for QC.\n\n    Raises:\n        AssertionError: If no summary statistics paths are found in the study index.\n    \"\"\"\n    assert qc_threshold &lt; 1.0, \"QC threshold should be a p-value less than 1.0.\"\n    sumstat_harmonisation_config: dict[str, Any] = {\n        \"perform_meta_analysis_filter\": perform_meta_analysis_filter,\n        \"imputation_score_threshold\": imputation_score_threshold,\n        \"perform_imputation_score_filter\": perform_imputation_score_filter,\n        \"min_allele_count_threshold\": min_allele_count_threshold,\n        \"perform_min_allele_count_filter\": perform_min_allele_count_filter,\n        \"min_allele_frequency_threshold\": min_allele_frequency_threshold,\n        \"perform_min_allele_frequency_filter\": perform_min_allele_frequency_filter,\n        \"filter_out_ambiguous_variants\": filter_out_ambiguous_variants,\n    }\n\n    session.logger.info(f\"Reading Finngen manifest from {source_manifest_path}.\")\n    finngen_manifest = FinnGenMetaManifest.from_path(\n        session=session, manifest_path=source_manifest_path\n    )\n    session.logger.info(f\"Building study index for: {finngen_manifest.meta.value}\")\n    session.logger.info(f\"Reading EFO curation from {efo_curation_path}.\")\n    efo_mapping = EFOMapping.from_path(\n        session=session, efo_curation_path=efo_curation_path\n    )\n\n    session.logger.info(\"Creating study index.\")\n    study_index = FinnGenMetaStudyIndex.from_finngen_manifest(\n        manifest=finngen_manifest, efo_mapping=efo_mapping\n    )\n\n    session.logger.info(\"Writing study index.\")\n    study_index.df.write.mode(session.write_mode).parquet(study_index_output_path)\n    session.logger.info(f\"Study index written to {study_index_output_path}.\")\n\n    session.logger.info(\"Reading summary statistics paths from manifest.\")\n    # NOTE: we can rely on the study index to extract the raw summary statistics paths\n    # to make sure to only process these summary statistics which are part of the study index.\n    # this may not be accurate if the summary statistics source paths were not found in the\n    # source manifest.\n    source_summary_statistics_paths = study_index.get_summary_statistics_paths()\n    assert (\n        len(source_summary_statistics_paths) &gt; 0\n    ), \"No summary statistics paths found in study index.\"\n    session.logger.info(\n        f\"Found {len(source_summary_statistics_paths)} summary statistics files.\"\n    )\n\n    session.logger.info(\"Converting raw summary statistics to Parquet format.\")\n    FinnGenUkbMvpMetaSummaryStatistics.bgzip_to_parquet(\n        session=session,\n        summary_statistics_list=source_summary_statistics_paths,\n        datasource=finngen_manifest.meta,\n        raw_summary_statistics_output_path=raw_summary_statistics_output_path,\n        n_threads=FinnGenUkbMvpMetaSummaryStatistics.N_THREAD_OPTIMAL,\n    )\n    session.logger.info(\"Raw summary statistics conversion completed.\")\n    session.logger.info(f\"Output path: {raw_summary_statistics_output_path}.\")\n\n    session.logger.info(\"Reading gnomAD variant index.\")\n    gnomad_variant_index = VariantIndex.from_parquet(\n        session=session, path=gnomad_variant_index_path\n    )\n\n    session.logger.info(\"Building variant direction annotations.\")\n    variant_direction = VariantDirection.from_variant_index(\n        variant_index=gnomad_variant_index\n    )\n\n    session.logger.info(\"Reading raw summary statistics.\")\n    raw_summary_statistics = session.spark.read.parquet(\n        raw_summary_statistics_output_path\n    )\n\n    session.logger.info(\"Harmonising summary statistics.\")\n    session.logger.info(\"Applying the following harmonisation configuration:\")\n    for key, value in sumstat_harmonisation_config.items():\n        session.logger.info(f\"  - {key}: {value}\")\n    harmonised_summary_statistics = FinnGenUkbMvpMetaSummaryStatistics.from_source(\n        raw_summary_statistics=raw_summary_statistics,\n        finngen_manifest=finngen_manifest,\n        variant_annotations=variant_direction,\n        **sumstat_harmonisation_config,\n    )\n\n    session.logger.info(\"Writing harmonised summary statistics.\")\n    harmonised_summary_statistics.df.write.mode(session.write_mode).parquet(\n        harmonised_summary_statistics_output_path\n    )\n    session.logger.info(\n        f\"Harmonised summary statistics written to {harmonised_summary_statistics_output_path}.\"\n    )\n\n    session.logger.info(\"Reading harmonised summary statistics for QC.\")\n    harmonised_summary_statistics = SummaryStatistics.from_parquet(\n        session=session, path=harmonised_summary_statistics_output_path\n    )\n    session.logger.info(\"Running summary statistics QC.\")\n    summary_statistics_qc = SummaryStatisticsQC.from_summary_statistics(\n        gwas=harmonised_summary_statistics,\n        pval_threshold=qc_threshold,\n    )\n\n    session.logger.info(\"Writing summary statistics QC results.\")\n    summary_statistics_qc.df.repartition(1).write.mode(session.write_mode).parquet(\n        harmonised_summary_statistics_qc_output_path\n    )\n    session.logger.info(\n        f\"Summary statistics QC results written to {harmonised_summary_statistics_qc_output_path}.\"\n    )\n\n    session.logger.info(\"Adding qc to the study index.\")\n    study_index = StudyIndex.from_parquet(\n        session=session, path=study_index_output_path\n    )\n    study_index = study_index.annotate_sumstats_qc(summary_statistics_qc)\n\n    session.logger.info(\"Writing updated study index.\")\n    study_index.df.repartition(1).write.mode(\"overwrite\").parquet(\n        study_index_output_path\n    )\n    session.logger.info(\"Updated study index with qc flags.\")\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_curation/","title":"gwas_catalog_study_curation","text":""},{"location":"python_api/steps/gwas_catalog_curation/#gentropy.gwas_catalog_study_curation.GWASCatalogStudyCurationStep","title":"<code>gentropy.gwas_catalog_study_curation.GWASCatalogStudyCurationStep</code>","text":"<p>Annotate GWAS Catalog studies with additional curation and create a curation backlog.</p> Source code in <code>src/gentropy/gwas_catalog_study_curation.py</code> <pre><code>class GWASCatalogStudyCurationStep:\n    \"\"\"Annotate GWAS Catalog studies with additional curation and create a curation backlog.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        catalog_study_files: list[str],\n        catalog_ancestry_files: list[str],\n        gwas_catalog_study_curation_out: str,\n        gwas_catalog_study_curation_file: str | None,\n    ) -&gt; None:\n        \"\"\"Run step to annotate and create backlog.\n\n        Args:\n            session (Session): Session object.\n            catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n            catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n            gwas_catalog_study_curation_out (str): Path for the updated curation table.\n            gwas_catalog_study_curation_file (str | None): Path to the original curation table. Optinal\n\n        Raises:\n            ValueError: If the curation file is provided but not a CSV file or URL.\n        \"\"\"\n        catalog_studies = session.spark.read.csv(\n            list(catalog_study_files), sep=\"\\t\", header=True\n        )\n        ancestry_lut = session.spark.read.csv(\n            list(catalog_ancestry_files), sep=\"\\t\", header=True\n        )\n\n        if gwas_catalog_study_curation_file:\n            if gwas_catalog_study_curation_file.endswith(\".csv\"):\n                gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_csv(\n                    session, gwas_catalog_study_curation_file\n                )\n            elif gwas_catalog_study_curation_file.startswith(\"http\"):\n                gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_url(\n                    session, gwas_catalog_study_curation_file\n                )\n            else:\n                raise ValueError(\n                    \"Only CSV files or URLs are accepted as curation file.\"\n                )\n\n        # Process GWAS Catalog studies and get list of studies for curation:\n        (\n            StudyIndexGWASCatalogParser.from_source(catalog_studies, ancestry_lut)\n            # Adding existing curation:\n            .annotate_from_study_curation(gwas_catalog_study_curation)\n            # Extract new studies for curation:\n            .extract_studies_for_curation(gwas_catalog_study_curation)\n            # Save table:\n            .toPandas()\n            .to_csv(gwas_catalog_study_curation_out, sep=\"\\t\", index=False)\n        )\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_curation/#gentropy.gwas_catalog_study_curation.GWASCatalogStudyCurationStep.__init__","title":"<code>__init__(session: Session, catalog_study_files: list[str], catalog_ancestry_files: list[str], gwas_catalog_study_curation_out: str, gwas_catalog_study_curation_file: str | None) -&gt; None</code>","text":"<p>Run step to annotate and create backlog.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>catalog_study_files</code> <code>list[str]</code> <p>List of raw GWAS catalog studies file.</p> required <code>catalog_ancestry_files</code> <code>list[str]</code> <p>List of raw ancestry annotations files from GWAS Catalog.</p> required <code>gwas_catalog_study_curation_out</code> <code>str</code> <p>Path for the updated curation table.</p> required <code>gwas_catalog_study_curation_file</code> <code>str | None</code> <p>Path to the original curation table. Optinal</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the curation file is provided but not a CSV file or URL.</p> Source code in <code>src/gentropy/gwas_catalog_study_curation.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    catalog_study_files: list[str],\n    catalog_ancestry_files: list[str],\n    gwas_catalog_study_curation_out: str,\n    gwas_catalog_study_curation_file: str | None,\n) -&gt; None:\n    \"\"\"Run step to annotate and create backlog.\n\n    Args:\n        session (Session): Session object.\n        catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n        catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n        gwas_catalog_study_curation_out (str): Path for the updated curation table.\n        gwas_catalog_study_curation_file (str | None): Path to the original curation table. Optinal\n\n    Raises:\n        ValueError: If the curation file is provided but not a CSV file or URL.\n    \"\"\"\n    catalog_studies = session.spark.read.csv(\n        list(catalog_study_files), sep=\"\\t\", header=True\n    )\n    ancestry_lut = session.spark.read.csv(\n        list(catalog_ancestry_files), sep=\"\\t\", header=True\n    )\n\n    if gwas_catalog_study_curation_file:\n        if gwas_catalog_study_curation_file.endswith(\".csv\"):\n            gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_csv(\n                session, gwas_catalog_study_curation_file\n            )\n        elif gwas_catalog_study_curation_file.startswith(\"http\"):\n            gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_url(\n                session, gwas_catalog_study_curation_file\n            )\n        else:\n            raise ValueError(\n                \"Only CSV files or URLs are accepted as curation file.\"\n            )\n\n    # Process GWAS Catalog studies and get list of studies for curation:\n    (\n        StudyIndexGWASCatalogParser.from_source(catalog_studies, ancestry_lut)\n        # Adding existing curation:\n        .annotate_from_study_curation(gwas_catalog_study_curation)\n        # Extract new studies for curation:\n        .extract_studies_for_curation(gwas_catalog_study_curation)\n        # Save table:\n        .toPandas()\n        .to_csv(gwas_catalog_study_curation_out, sep=\"\\t\", index=False)\n    )\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_study_index/","title":"gwas_catalog_study_inclusion","text":""},{"location":"python_api/steps/gwas_catalog_study_index/#gentropy.gwas_catalog_study_index.GWASCatalogStudyIndexGenerationStep","title":"<code>gentropy.gwas_catalog_study_index.GWASCatalogStudyIndexGenerationStep</code>","text":"<p>GWAS Catalog study index generation.</p> <p>This step generates a study index from the GWAS Catalog studies and ancestry files. It can also add additional curation information and summary statistics QC information when available.</p> <p>''' warning This step does not generate study index for gwas catalog top hits.</p> <p>This step provides several optional arguments to add additional information to the study index:</p> <ul> <li>gwas_catalog_study_curation_file: csv file or URL containing the curation table. If provided it annotates the study index with the additional curation information performed by the Open Targets team.</li> <li>sumstats_qc_path: Path to the summary statistics QC table. If provided it annotates the study index with the summary statistics QC information in the <code>sumstatQCValues</code> columns (e.g. <code>n_variants</code>, <code>n_variants_sig</code> etc.).</li> </ul> Source code in <code>src/gentropy/gwas_catalog_study_index.py</code> <pre><code>class GWASCatalogStudyIndexGenerationStep:\n    \"\"\"GWAS Catalog study index generation.\n\n    This step generates a study index from the GWAS Catalog studies and ancestry files. It can also add additional curation information and summary statistics QC information when available.\n\n    ''' warning\n    This step does not generate study index for gwas catalog top hits.\n\n    This step provides several optional arguments to add additional information to the study index:\n\n    - gwas_catalog_study_curation_file: csv file or URL containing the curation table. If provided it annotates the study index with the additional curation information performed by the Open Targets team.\n    - sumstats_qc_path: Path to the summary statistics QC table. If provided it annotates the study index with the summary statistics QC information in the `sumstatQCValues` columns (e.g. `n_variants`, `n_variants_sig` etc.).\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        catalog_study_files: list[str],\n        catalog_ancestry_files: list[str],\n        study_index_path: str,\n        gwas_catalog_study_curation_file: str | None = None,\n        sumstats_qc_path: str | None = None,\n    ) -&gt; None:\n        \"\"\"Run step.\n\n        Args:\n            session (Session): Session objecct.\n            catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n            catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n            study_index_path (str): Output GWAS catalog studies path.\n            gwas_catalog_study_curation_file (str | None): csv file or URL containing the curation table. Optional.\n            sumstats_qc_path (str | None): Path to the summary statistics QC table. Optional.\n\n        Raises:\n            ValueError: If the curation file is provided but not a CSV file or URL.\n        \"\"\"\n        # Core Study Index Generation:\n        study_index = StudyIndexGWASCatalogParser.from_source(\n            session.spark.read.csv(list(catalog_study_files), sep=\"\\t\", header=True),\n            session.spark.read.csv(list(catalog_ancestry_files), sep=\"\\t\", header=True),\n        )\n\n        # Annotate with curation if provided:\n        if gwas_catalog_study_curation_file:\n            if gwas_catalog_study_curation_file.endswith(\n                \".tsv\"\n            ) | gwas_catalog_study_curation_file.endswith(\".tsv\"):\n                gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_csv(\n                    session, gwas_catalog_study_curation_file\n                )\n            elif gwas_catalog_study_curation_file.startswith(\"http\"):\n                gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_url(\n                    session, gwas_catalog_study_curation_file\n                )\n            else:\n                raise ValueError(\n                    \"Only CSV/TSV files or URLs are accepted as curation file.\"\n                )\n            study_index = study_index.annotate_from_study_curation(\n                gwas_catalog_study_curation\n            )\n\n        # Annotate with sumstats QC if provided:\n        if sumstats_qc_path:\n            sumstats_qc = SummaryStatisticsQC.from_parquet(\n                session=session,\n                path=sumstats_qc_path,\n                recursiveFileLookup=True,\n            )\n            study_index_with_qc = study_index.annotate_sumstats_qc(sumstats_qc)\n            # Write the study\n            study_index_with_qc.df.coalesce(session.output_partitions).write.mode(\n                session.write_mode\n            ).parquet(study_index_path)\n        else:\n            study_index.df.coalesce(session.output_partitions).write.mode(\n                session.write_mode\n            ).parquet(study_index_path)\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_study_index/#gentropy.gwas_catalog_study_index.GWASCatalogStudyIndexGenerationStep.__init__","title":"<code>__init__(session: Session, catalog_study_files: list[str], catalog_ancestry_files: list[str], study_index_path: str, gwas_catalog_study_curation_file: str | None = None, sumstats_qc_path: str | None = None) -&gt; None</code>","text":"<p>Run step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session objecct.</p> required <code>catalog_study_files</code> <code>list[str]</code> <p>List of raw GWAS catalog studies file.</p> required <code>catalog_ancestry_files</code> <code>list[str]</code> <p>List of raw ancestry annotations files from GWAS Catalog.</p> required <code>study_index_path</code> <code>str</code> <p>Output GWAS catalog studies path.</p> required <code>gwas_catalog_study_curation_file</code> <code>str | None</code> <p>csv file or URL containing the curation table. Optional.</p> <code>None</code> <code>sumstats_qc_path</code> <code>str | None</code> <p>Path to the summary statistics QC table. Optional.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the curation file is provided but not a CSV file or URL.</p> Source code in <code>src/gentropy/gwas_catalog_study_index.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    catalog_study_files: list[str],\n    catalog_ancestry_files: list[str],\n    study_index_path: str,\n    gwas_catalog_study_curation_file: str | None = None,\n    sumstats_qc_path: str | None = None,\n) -&gt; None:\n    \"\"\"Run step.\n\n    Args:\n        session (Session): Session objecct.\n        catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n        catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n        study_index_path (str): Output GWAS catalog studies path.\n        gwas_catalog_study_curation_file (str | None): csv file or URL containing the curation table. Optional.\n        sumstats_qc_path (str | None): Path to the summary statistics QC table. Optional.\n\n    Raises:\n        ValueError: If the curation file is provided but not a CSV file or URL.\n    \"\"\"\n    # Core Study Index Generation:\n    study_index = StudyIndexGWASCatalogParser.from_source(\n        session.spark.read.csv(list(catalog_study_files), sep=\"\\t\", header=True),\n        session.spark.read.csv(list(catalog_ancestry_files), sep=\"\\t\", header=True),\n    )\n\n    # Annotate with curation if provided:\n    if gwas_catalog_study_curation_file:\n        if gwas_catalog_study_curation_file.endswith(\n            \".tsv\"\n        ) | gwas_catalog_study_curation_file.endswith(\".tsv\"):\n            gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_csv(\n                session, gwas_catalog_study_curation_file\n            )\n        elif gwas_catalog_study_curation_file.startswith(\"http\"):\n            gwas_catalog_study_curation = StudyIndexGWASCatalogOTCuration.from_url(\n                session, gwas_catalog_study_curation_file\n            )\n        else:\n            raise ValueError(\n                \"Only CSV/TSV files or URLs are accepted as curation file.\"\n            )\n        study_index = study_index.annotate_from_study_curation(\n            gwas_catalog_study_curation\n        )\n\n    # Annotate with sumstats QC if provided:\n    if sumstats_qc_path:\n        sumstats_qc = SummaryStatisticsQC.from_parquet(\n            session=session,\n            path=sumstats_qc_path,\n            recursiveFileLookup=True,\n        )\n        study_index_with_qc = study_index.annotate_sumstats_qc(sumstats_qc)\n        # Write the study\n        study_index_with_qc.df.coalesce(session.output_partitions).write.mode(\n            session.write_mode\n        ).parquet(study_index_path)\n    else:\n        study_index.df.coalesce(session.output_partitions).write.mode(\n            session.write_mode\n        ).parquet(study_index_path)\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_sumstat_preprocess/","title":"gwas_catalog_sumstat_preprocess","text":""},{"location":"python_api/steps/gwas_catalog_sumstat_preprocess/#gentropy.gwas_catalog_sumstat_preprocess.GWASCatalogSumstatsPreprocessStep","title":"<code>gentropy.gwas_catalog_sumstat_preprocess.GWASCatalogSumstatsPreprocessStep</code>","text":"<p>Step to preprocess GWAS Catalog harmonised summary stats.</p> <p>It additionally performs sanity filter of GWAS before saving it.</p> Source code in <code>src/gentropy/gwas_catalog_sumstat_preprocess.py</code> <pre><code>class GWASCatalogSumstatsPreprocessStep:\n    \"\"\"Step to preprocess GWAS Catalog harmonised summary stats.\n\n    It additionally performs sanity filter of GWAS before saving it.\n    \"\"\"\n\n    def __init__(\n        self, session: Session, raw_sumstats_path: str, out_sumstats_path: str\n    ) -&gt; None:\n        \"\"\"Run step to preprocess GWAS Catalog harmonised summary stats and produce SummaryStatistics dataset.\n\n        Args:\n            session (Session): Session object.\n            raw_sumstats_path (str): Input GWAS Catalog harmonised summary stats path.\n            out_sumstats_path (str): Output SummaryStatistics dataset path.\n        \"\"\"\n        # Processing dataset:\n        GWASCatalogSummaryStatistics.from_gwas_harmonized_summary_stats(\n            session.spark, raw_sumstats_path\n        ).sanity_filter().df.write.mode(session.write_mode).parquet(out_sumstats_path)\n        session.logger.info(\"Processing dataset successfully completed.\")\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_sumstat_preprocess/#gentropy.gwas_catalog_sumstat_preprocess.GWASCatalogSumstatsPreprocessStep.__init__","title":"<code>__init__(session: Session, raw_sumstats_path: str, out_sumstats_path: str) -&gt; None</code>","text":"<p>Run step to preprocess GWAS Catalog harmonised summary stats and produce SummaryStatistics dataset.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>raw_sumstats_path</code> <code>str</code> <p>Input GWAS Catalog harmonised summary stats path.</p> required <code>out_sumstats_path</code> <code>str</code> <p>Output SummaryStatistics dataset path.</p> required Source code in <code>src/gentropy/gwas_catalog_sumstat_preprocess.py</code> <pre><code>def __init__(\n    self, session: Session, raw_sumstats_path: str, out_sumstats_path: str\n) -&gt; None:\n    \"\"\"Run step to preprocess GWAS Catalog harmonised summary stats and produce SummaryStatistics dataset.\n\n    Args:\n        session (Session): Session object.\n        raw_sumstats_path (str): Input GWAS Catalog harmonised summary stats path.\n        out_sumstats_path (str): Output SummaryStatistics dataset path.\n    \"\"\"\n    # Processing dataset:\n    GWASCatalogSummaryStatistics.from_gwas_harmonized_summary_stats(\n        session.spark, raw_sumstats_path\n    ).sanity_filter().df.write.mode(session.write_mode).parquet(out_sumstats_path)\n    session.logger.info(\"Processing dataset successfully completed.\")\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_top_hits/","title":"GWAS Catalog Top Hits Ingestion Step","text":""},{"location":"python_api/steps/gwas_catalog_top_hits/#gentropy.gwas_catalog_top_hits.GWASCatalogTopHitIngestionStep","title":"<code>gentropy.gwas_catalog_top_hits.GWASCatalogTopHitIngestionStep</code>","text":"<p>GWAS Catalog ingestion step to extract GWASCatalog top hits.</p> Source code in <code>src/gentropy/gwas_catalog_top_hits.py</code> <pre><code>class GWASCatalogTopHitIngestionStep:\n    \"\"\"GWAS Catalog ingestion step to extract GWASCatalog top hits.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        catalog_study_files: list[str],\n        catalog_ancestry_files: list[str],\n        catalog_associations_file: str,\n        variant_annotation_path: str,\n        catalog_studies_out: str,\n        catalog_associations_out: str,\n        distance: int = WindowBasedClumpingStepConfig().distance,\n    ) -&gt; None:\n        \"\"\"Run step.\n\n        Args:\n            session (Session): Session object.\n            catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n            catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n            catalog_associations_file (str): Raw GWAS catalog associations file.\n            variant_annotation_path (str): Path to GnomAD variants.\n            catalog_studies_out (str): Output GWAS catalog studies path.\n            catalog_associations_out (str): Output GWAS catalog associations path.\n            distance (int): Distance, within which tagging variants are collected around the semi-index.\n        \"\"\"\n        # Extract\n        gnomad_variants = VariantIndex.from_parquet(session, variant_annotation_path)\n        catalog_studies = session.spark.read.csv(\n            list(catalog_study_files), sep=\"\\t\", header=True\n        )\n        ancestry_lut = session.spark.read.csv(\n            list(catalog_ancestry_files), sep=\"\\t\", header=True\n        )\n        catalog_associations = session.spark.read.csv(\n            catalog_associations_file, sep=\"\\t\", header=True\n        ).persist()\n\n        # Transform\n        study_index, study_locus = GWASCatalogStudySplitter.split(\n            StudyIndexGWASCatalogParser.from_source(catalog_studies, ancestry_lut),\n            GWASCatalogCuratedAssociationsParser.from_source(\n                catalog_associations, gnomad_variants\n            ),\n        )\n        # Load\n        (\n            study_index\n            # Flag all studies without sumstats\n            .add_no_sumstats_flag()\n            # Save dataset:\n            .df.write.mode(session.write_mode)\n            .parquet(catalog_studies_out)\n        )\n\n        (\n            study_locus.window_based_clumping(distance)\n            .df.write.mode(session.write_mode)\n            .parquet(catalog_associations_out)\n        )\n</code></pre>"},{"location":"python_api/steps/gwas_catalog_top_hits/#gentropy.gwas_catalog_top_hits.GWASCatalogTopHitIngestionStep.__init__","title":"<code>__init__(session: Session, catalog_study_files: list[str], catalog_ancestry_files: list[str], catalog_associations_file: str, variant_annotation_path: str, catalog_studies_out: str, catalog_associations_out: str, distance: int = WindowBasedClumpingStepConfig().distance) -&gt; None</code>","text":"<p>Run step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>catalog_study_files</code> <code>list[str]</code> <p>List of raw GWAS catalog studies file.</p> required <code>catalog_ancestry_files</code> <code>list[str]</code> <p>List of raw ancestry annotations files from GWAS Catalog.</p> required <code>catalog_associations_file</code> <code>str</code> <p>Raw GWAS catalog associations file.</p> required <code>variant_annotation_path</code> <code>str</code> <p>Path to GnomAD variants.</p> required <code>catalog_studies_out</code> <code>str</code> <p>Output GWAS catalog studies path.</p> required <code>catalog_associations_out</code> <code>str</code> <p>Output GWAS catalog associations path.</p> required <code>distance</code> <code>int</code> <p>Distance, within which tagging variants are collected around the semi-index.</p> <code>distance</code> Source code in <code>src/gentropy/gwas_catalog_top_hits.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    catalog_study_files: list[str],\n    catalog_ancestry_files: list[str],\n    catalog_associations_file: str,\n    variant_annotation_path: str,\n    catalog_studies_out: str,\n    catalog_associations_out: str,\n    distance: int = WindowBasedClumpingStepConfig().distance,\n) -&gt; None:\n    \"\"\"Run step.\n\n    Args:\n        session (Session): Session object.\n        catalog_study_files (list[str]): List of raw GWAS catalog studies file.\n        catalog_ancestry_files (list[str]): List of raw ancestry annotations files from GWAS Catalog.\n        catalog_associations_file (str): Raw GWAS catalog associations file.\n        variant_annotation_path (str): Path to GnomAD variants.\n        catalog_studies_out (str): Output GWAS catalog studies path.\n        catalog_associations_out (str): Output GWAS catalog associations path.\n        distance (int): Distance, within which tagging variants are collected around the semi-index.\n    \"\"\"\n    # Extract\n    gnomad_variants = VariantIndex.from_parquet(session, variant_annotation_path)\n    catalog_studies = session.spark.read.csv(\n        list(catalog_study_files), sep=\"\\t\", header=True\n    )\n    ancestry_lut = session.spark.read.csv(\n        list(catalog_ancestry_files), sep=\"\\t\", header=True\n    )\n    catalog_associations = session.spark.read.csv(\n        catalog_associations_file, sep=\"\\t\", header=True\n    ).persist()\n\n    # Transform\n    study_index, study_locus = GWASCatalogStudySplitter.split(\n        StudyIndexGWASCatalogParser.from_source(catalog_studies, ancestry_lut),\n        GWASCatalogCuratedAssociationsParser.from_source(\n            catalog_associations, gnomad_variants\n        ),\n    )\n    # Load\n    (\n        study_index\n        # Flag all studies without sumstats\n        .add_no_sumstats_flag()\n        # Save dataset:\n        .df.write.mode(session.write_mode)\n        .parquet(catalog_studies_out)\n    )\n\n    (\n        study_locus.window_based_clumping(distance)\n        .df.write.mode(session.write_mode)\n        .parquet(catalog_associations_out)\n    )\n</code></pre>"},{"location":"python_api/steps/l2g/","title":"Locus to Gene (L2G)","text":""},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneFeatureMatrixStep","title":"<code>gentropy.l2g.LocusToGeneFeatureMatrixStep</code>","text":"<p>Annotate credible set with functional genomics features.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>class LocusToGeneFeatureMatrixStep:\n    \"\"\"Annotate credible set with functional genomics features.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        *,\n        features_list: list[str],\n        credible_set_path: str,\n        variant_index_path: str | None = None,\n        colocalisation_path: str | None = None,\n        study_index_path: str | None = None,\n        target_index_path: str | None = None,\n        feature_matrix_path: str,\n        append_null_features: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialise the step and run the logic based on mode.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            features_list (list[str]): List of features to use for the model\n            credible_set_path (str): Path to the credible set dataset necessary to build the feature matrix\n            variant_index_path (str | None): Path to the variant index dataset\n            colocalisation_path (str | None): Path to the colocalisation dataset\n            study_index_path (str | None): Path to the study index dataset\n            target_index_path (str | None): Path to the target index dataset\n            feature_matrix_path (str): Path to the L2G feature matrix output dataset\n            append_null_features (bool): Whether to append null features to the feature matrix. Defaults to False.\n        \"\"\"\n        credible_set = StudyLocus.from_parquet(\n            session, credible_set_path, recursiveFileLookup=True\n        )\n        studies = (\n            StudyIndex.from_parquet(session, study_index_path, recursiveFileLookup=True)\n            if study_index_path\n            else None\n        )\n        variant_index = (\n            VariantIndex.from_parquet(session, variant_index_path)\n            if variant_index_path\n            else None\n        )\n        coloc = (\n            Colocalisation.from_parquet(\n                session, colocalisation_path, recursiveFileLookup=True\n            )\n            if colocalisation_path\n            else None\n        )\n\n        target_index = (\n            TargetIndex.from_parquet(\n                session, target_index_path, recursiveFileLookup=True\n            )\n            if target_index_path\n            else None\n        )\n        features_input_loader = L2GFeatureInputLoader(\n            variant_index=variant_index,\n            colocalisation=coloc,\n            study_index=studies,\n            study_locus=credible_set,\n            target_index=target_index,\n        )\n\n        fm = credible_set.filter(f.col(\"studyType\") == \"gwas\").build_feature_matrix(\n            features_list,\n            features_input_loader,\n            append_null_features=append_null_features,\n        )\n\n        if target_index is not None:\n            target_index_df = target_index.df.select(\"id\", \"biotype\").withColumnRenamed(\n                \"id\", \"geneId\"\n            )\n\n            target_index_df = target_index_df.withColumn(\n                \"isProteinCoding\",\n                f.when(f.col(\"biotype\") == \"protein_coding\", 1).otherwise(0),\n            ).drop(\"biotype\")\n\n            fm._df = fm._df.drop(\"isProteinCoding\").join(\n                target_index_df, on=\"geneId\", how=\"inner\"\n            )\n\n        fm._df.coalesce(session.output_partitions).write.mode(\n            session.write_mode\n        ).parquet(feature_matrix_path)\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneFeatureMatrixStep.__init__","title":"<code>__init__(session: Session, *, features_list: list[str], credible_set_path: str, variant_index_path: str | None = None, colocalisation_path: str | None = None, study_index_path: str | None = None, target_index_path: str | None = None, feature_matrix_path: str, append_null_features: bool = False) -&gt; None</code>","text":"<p>Initialise the step and run the logic based on mode.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>features_list</code> <code>list[str]</code> <p>List of features to use for the model</p> required <code>credible_set_path</code> <code>str</code> <p>Path to the credible set dataset necessary to build the feature matrix</p> required <code>variant_index_path</code> <code>str | None</code> <p>Path to the variant index dataset</p> <code>None</code> <code>colocalisation_path</code> <code>str | None</code> <p>Path to the colocalisation dataset</p> <code>None</code> <code>study_index_path</code> <code>str | None</code> <p>Path to the study index dataset</p> <code>None</code> <code>target_index_path</code> <code>str | None</code> <p>Path to the target index dataset</p> <code>None</code> <code>feature_matrix_path</code> <code>str</code> <p>Path to the L2G feature matrix output dataset</p> required <code>append_null_features</code> <code>bool</code> <p>Whether to append null features to the feature matrix. Defaults to False.</p> <code>False</code> Source code in <code>src/gentropy/l2g.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    *,\n    features_list: list[str],\n    credible_set_path: str,\n    variant_index_path: str | None = None,\n    colocalisation_path: str | None = None,\n    study_index_path: str | None = None,\n    target_index_path: str | None = None,\n    feature_matrix_path: str,\n    append_null_features: bool = False,\n) -&gt; None:\n    \"\"\"Initialise the step and run the logic based on mode.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        features_list (list[str]): List of features to use for the model\n        credible_set_path (str): Path to the credible set dataset necessary to build the feature matrix\n        variant_index_path (str | None): Path to the variant index dataset\n        colocalisation_path (str | None): Path to the colocalisation dataset\n        study_index_path (str | None): Path to the study index dataset\n        target_index_path (str | None): Path to the target index dataset\n        feature_matrix_path (str): Path to the L2G feature matrix output dataset\n        append_null_features (bool): Whether to append null features to the feature matrix. Defaults to False.\n    \"\"\"\n    credible_set = StudyLocus.from_parquet(\n        session, credible_set_path, recursiveFileLookup=True\n    )\n    studies = (\n        StudyIndex.from_parquet(session, study_index_path, recursiveFileLookup=True)\n        if study_index_path\n        else None\n    )\n    variant_index = (\n        VariantIndex.from_parquet(session, variant_index_path)\n        if variant_index_path\n        else None\n    )\n    coloc = (\n        Colocalisation.from_parquet(\n            session, colocalisation_path, recursiveFileLookup=True\n        )\n        if colocalisation_path\n        else None\n    )\n\n    target_index = (\n        TargetIndex.from_parquet(\n            session, target_index_path, recursiveFileLookup=True\n        )\n        if target_index_path\n        else None\n    )\n    features_input_loader = L2GFeatureInputLoader(\n        variant_index=variant_index,\n        colocalisation=coloc,\n        study_index=studies,\n        study_locus=credible_set,\n        target_index=target_index,\n    )\n\n    fm = credible_set.filter(f.col(\"studyType\") == \"gwas\").build_feature_matrix(\n        features_list,\n        features_input_loader,\n        append_null_features=append_null_features,\n    )\n\n    if target_index is not None:\n        target_index_df = target_index.df.select(\"id\", \"biotype\").withColumnRenamed(\n            \"id\", \"geneId\"\n        )\n\n        target_index_df = target_index_df.withColumn(\n            \"isProteinCoding\",\n            f.when(f.col(\"biotype\") == \"protein_coding\", 1).otherwise(0),\n        ).drop(\"biotype\")\n\n        fm._df = fm._df.drop(\"isProteinCoding\").join(\n            target_index_df, on=\"geneId\", how=\"inner\"\n        )\n\n    fm._df.coalesce(session.output_partitions).write.mode(\n        session.write_mode\n    ).parquet(feature_matrix_path)\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep","title":"<code>gentropy.l2g.LocusToGeneStep</code>","text":"<p>Locus to gene step.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>class LocusToGeneStep:\n    \"\"\"Locus to gene step.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        *,\n        run_mode: str,\n        hyperparameters: dict[str, Any],\n        download_from_hub: bool,\n        cross_validate: bool,\n        credible_set_path: str,\n        feature_matrix_path: str,\n        wandb_run_name: str | None = None,\n        model_path: str | None = None,\n        features_list: list[str] | None = None,\n        gold_standard_curation_path: str | None = None,\n        variant_index_path: str | None = None,\n        gene_interactions_path: str | None = None,\n        predictions_path: str | None = None,\n        l2g_threshold: float | None = None,\n        hf_hub_repo_id: str | None = None,\n        hf_model_commit_message: str | None = \"chore: update model\",\n        hf_model_version: str | None = None,\n        explain_predictions: bool | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the step and run the logic based on mode.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            run_mode (str): Run mode, either 'train' or 'predict'\n            hyperparameters (dict[str, Any]): Hyperparameters for the model\n            download_from_hub (bool): Whether to download the model from Hugging Face Hub\n            cross_validate (bool): Whether to run cross validation (5-fold by default) to train the model.\n            credible_set_path (str): Path to the credible set dataset necessary to build the feature matrix\n            feature_matrix_path (str): Path to the L2G feature matrix input dataset\n            wandb_run_name (str | None): Name of the run to track model training in Weights and Biases\n            model_path (str | None): Path to the model. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).\n            features_list (list[str] | None): List of features to use to train the model\n            gold_standard_curation_path (str | None): Path to the gold standard curation file\n            variant_index_path (str | None): Path to the variant index\n            gene_interactions_path (str | None): Path to the gene interactions dataset\n            predictions_path (str | None): Path to the L2G predictions output dataset\n            l2g_threshold (float | None): An optional threshold for the L2G score to filter predictions. A threshold of 0.05 is recommended.\n            hf_hub_repo_id (str | None): Hugging Face Hub repository ID. If provided, the model will be uploaded to Hugging Face.\n            hf_model_commit_message (str | None): Commit message when we upload the model to the Hugging Face Hub\n            hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n            explain_predictions (bool | None): Whether to extract SHAP importances for the L2G predictions. This is computationally expensive.\n\n        Raises:\n            ValueError: If run_mode is not 'train' or 'predict'\n        \"\"\"\n        if run_mode not in [\"train\", \"predict\"]:\n            raise ValueError(\n                f\"run_mode must be one of 'train' or 'predict', got {run_mode}\"\n            )\n\n        self.session = session\n        self.run_mode = run_mode\n        self.predictions_path = predictions_path\n        self.features_list = list(features_list) if features_list else None\n        self.hyperparameters = dict(hyperparameters)\n        self.wandb_run_name = wandb_run_name\n        self.cross_validate = cross_validate\n        self.hf_hub_repo_id = hf_hub_repo_id\n        self.download_from_hub = download_from_hub\n        self.hf_model_commit_message = hf_model_commit_message\n        self.l2g_threshold = l2g_threshold or 0.0\n        self.gold_standard_curation_path = gold_standard_curation_path\n        self.gene_interactions_path = gene_interactions_path\n        self.variant_index_path = variant_index_path\n        self.model_path = (\n            hf_hub_repo_id\n            if not model_path and download_from_hub and hf_hub_repo_id\n            else model_path\n        )\n        self.hf_model_version = hf_model_version\n        self.explain_predictions = explain_predictions\n\n        # Load common inputs\n        self.credible_set = StudyLocus.from_parquet(\n            session, credible_set_path, recursiveFileLookup=True\n        )\n        self.feature_matrix = L2GFeatureMatrix(\n            _df=session.load_data(feature_matrix_path),\n        )\n\n        if run_mode == \"predict\":\n            self.run_predict()\n        elif run_mode == \"train\":\n            self.gold_standard = self.prepare_gold_standard()\n            self.run_train()\n\n    def prepare_gold_standard(self) -&gt; L2GGoldStandard:\n        \"\"\"Prepare the gold standard for training.\n\n        Returns:\n            L2GGoldStandard: training dataset.\n\n        Raises:\n            ValueError: When gold standard path, is not provided, or when\n                parsing OTG gold standard but missing interactions and variant index paths.\n            TypeError: When gold standard is not OTG gold standard nor L2GGoldStandard.\n\n        \"\"\"\n        if self.gold_standard_curation_path is None:\n            raise ValueError(\"Gold Standard is required for model training.\")\n        # Read the gold standard either from json or parquet, default to parquet if can not infer the format from extension.\n        ext = self.gold_standard_curation_path.split(\".\")[-1]\n        ext = \"parquet\" if ext not in [\"parquet\", \"json\"] else ext\n        gold_standard = self.session.load_data(self.gold_standard_curation_path, ext)\n        schema_issues = compare_struct_schemas(\n            gold_standard.schema, L2GGoldStandard.get_schema()\n        )\n        # Parse the gold standard depending on the input schema\n        match schema_issues:\n            case {**extra} if not extra:\n                # Schema is the same as L2GGoldStandard - load the GS\n                # NOTE: match to empty dict will be non-selective\n                # see https://stackoverflow.com/questions/75389166/how-to-match-an-empty-dictionary                logging.info(\"Successfully parsed gold standard.\")\n                return L2GGoldStandard(\n                    _df=gold_standard,\n                    _schema=L2GGoldStandard.get_schema(),\n                )\n            case {\"unexpected_columns\": extra_columns}:\n                # All mandatory columns present, extra columns are allowed but not passed to the L2GGoldStandard object\n                logging.info(\"Successfully parsed gold standard with extra columns.\")\n                return L2GGoldStandard(\n                    _df=gold_standard.drop(*extra_columns),\n                    _schema=L2GGoldStandard.get_schema(),\n                )\n            case {\n                \"missing_mandatory_columns\": [\n                    \"studyLocusId\",\n                    \"variantId\",\n                    \"studyId\",\n                    \"geneId\",\n                    \"goldStandardSet\",\n                ],\n                \"unexpected_columns\": [\n                    \"association_info\",\n                    \"gold_standard_info\",\n                    \"metadata\",\n                    \"sentinel_variant\",\n                    \"trait_info\",\n                ],\n            }:\n                # There are schema mismatches, this would mean that we have\n                logging.info(\"Detected OTG Gold Standard. Attempting to parse it.\")\n                otg_curation = gold_standard\n                if self.gene_interactions_path is None:\n                    raise ValueError(\"Interactions are required for parsing curation.\")\n                if self.variant_index_path is None:\n                    raise ValueError(\"Variant Index are required for parsing curation.\")\n\n                interactions = self.session.load_data(\n                    self.gene_interactions_path, \"parquet\"\n                )\n                variant_index = VariantIndex.from_parquet(\n                    self.session, self.variant_index_path\n                )\n                study_locus_overlap = StudyLocus(\n                    _df=self.credible_set.df.join(\n                        otg_curation.select(\n                            f.concat_ws(\n                                \"_\",\n                                f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n                                f.col(\"sentinel_variant.locus_GRCh38.position\"),\n                                f.col(\"sentinel_variant.alleles.reference\"),\n                                f.col(\"sentinel_variant.alleles.alternative\"),\n                            ).alias(\"variantId\"),\n                            f.col(\"association_info.otg_id\").alias(\"studyId\"),\n                        ),\n                        on=[\n                            \"studyId\",\n                            \"variantId\",\n                        ],\n                        how=\"inner\",\n                    ),\n                    _schema=StudyLocus.get_schema(),\n                ).find_overlaps()\n\n                return L2GGoldStandard.from_otg_curation(\n                    gold_standard_curation=otg_curation,\n                    variant_index=variant_index,\n                    study_locus_overlap=study_locus_overlap,\n                    interactions=interactions,\n                )\n            case _:\n                raise TypeError(\"Incorrect gold standard dataset provided.\")\n\n    def run_predict(self) -&gt; None:\n        \"\"\"Run the prediction step.\n\n        Raises:\n            ValueError: If predictions_path is not provided for prediction mode\n        \"\"\"\n        if not self.predictions_path:\n            raise ValueError(\"predictions_path must be provided for prediction mode\")\n        predictions = (\n            L2GPrediction.from_credible_set(\n                self.session,\n                self.credible_set,\n                self.feature_matrix,\n                model_path=self.model_path,\n                features_list=self.features_list,\n                hf_token=self._get_hf_token(),\n                hf_model_version=self.hf_model_version,\n                download_from_hub=self.download_from_hub,\n            )\n            .filter(f.col(\"score\") &gt;= self.l2g_threshold)\n            .add_features(\n                self.feature_matrix,\n            )\n        )\n        if self.explain_predictions:\n            predictions = predictions.explain()\n        predictions.df.coalesce(self.session.output_partitions).write.mode(\n            self.session.write_mode\n        ).parquet(self.predictions_path)\n        self.session.logger.info(\"L2G predictions saved successfully.\")\n\n    def _get_hf_token(self) -&gt; str | None:\n        if self.download_from_hub:\n            return access_gcp_secret(\"hfhub-key\", \"open-targets-genetics-dev\")\n        return None\n\n    def run_train(self) -&gt; None:\n        \"\"\"Run the training step.\n\n        Raises:\n            ValueError: If features list is not provided for model training.\n        \"\"\"\n        if self.features_list is None:\n            raise ValueError(\"Features list is required for model training.\")\n        # Initialize access to weights and biases\n        if self.wandb_run_name:\n            wandb_key = access_gcp_secret(\"wandb-key\", \"open-targets-genetics-dev\")\n            wandb_login(key=wandb_key)\n\n        # Instantiate classifier and train model\n        l2g_model = LocusToGeneModel(\n            model=XGBClassifier(random_state=777, eval_metric=\"aucpr\"),\n            hyperparameters=self.hyperparameters,\n            features_list=self.features_list,\n        )\n\n        # Calculate the gold standard features\n        feature_matrix = self._annotate_gold_standards_w_feature_matrix()\n\n        # Run the training\n        trained_model = LocusToGeneTrainer(\n            model=l2g_model, feature_matrix=feature_matrix\n        ).train(wandb_run_name=self.wandb_run_name, cross_validate=self.cross_validate)\n\n        # Export the model\n        if trained_model.training_data and trained_model.model and self.model_path:\n            trained_model.save(self.model_path)\n            if self.hf_hub_repo_id and self.hf_model_commit_message:\n                hf_hub_token = access_gcp_secret(\n                    \"hfhub-key\", \"open-targets-genetics-dev\"\n                )\n                trained_model.export_to_hugging_face_hub(\n                    # we upload the model saved in the filesystem\n                    self.model_path.split(\"/\")[-1],\n                    hf_hub_token,\n                    feature_matrix=trained_model.training_data,\n                    repo_id=self.hf_hub_repo_id,\n                    commit_message=self.hf_model_commit_message,\n                )\n\n    def _annotate_gold_standards_w_feature_matrix(self) -&gt; L2GFeatureMatrix:\n        \"\"\"Generate the feature matrix of annotated gold standards.\n\n        Returns:\n            L2GFeatureMatrix: Feature matrix with gold standards annotated with features.\n        \"\"\"\n        return (\n            self.gold_standard.build_feature_matrix(\n                self.feature_matrix, self.credible_set\n            )\n            .select_features(self.features_list)\n            .persist()\n        )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep.__init__","title":"<code>__init__(session: Session, *, run_mode: str, hyperparameters: dict[str, Any], download_from_hub: bool, cross_validate: bool, credible_set_path: str, feature_matrix_path: str, wandb_run_name: str | None = None, model_path: str | None = None, features_list: list[str] | None = None, gold_standard_curation_path: str | None = None, variant_index_path: str | None = None, gene_interactions_path: str | None = None, predictions_path: str | None = None, l2g_threshold: float | None = None, hf_hub_repo_id: str | None = None, hf_model_commit_message: str | None = 'chore: update model', hf_model_version: str | None = None, explain_predictions: bool | None = None) -&gt; None</code>","text":"<p>Initialise the step and run the logic based on mode.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>run_mode</code> <code>str</code> <p>Run mode, either 'train' or 'predict'</p> required <code>hyperparameters</code> <code>dict[str, Any]</code> <p>Hyperparameters for the model</p> required <code>download_from_hub</code> <code>bool</code> <p>Whether to download the model from Hugging Face Hub</p> required <code>cross_validate</code> <code>bool</code> <p>Whether to run cross validation (5-fold by default) to train the model.</p> required <code>credible_set_path</code> <code>str</code> <p>Path to the credible set dataset necessary to build the feature matrix</p> required <code>feature_matrix_path</code> <code>str</code> <p>Path to the L2G feature matrix input dataset</p> required <code>wandb_run_name</code> <code>str | None</code> <p>Name of the run to track model training in Weights and Biases</p> <code>None</code> <code>model_path</code> <code>str | None</code> <p>Path to the model. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).</p> <code>None</code> <code>features_list</code> <code>list[str] | None</code> <p>List of features to use to train the model</p> <code>None</code> <code>gold_standard_curation_path</code> <code>str | None</code> <p>Path to the gold standard curation file</p> <code>None</code> <code>variant_index_path</code> <code>str | None</code> <p>Path to the variant index</p> <code>None</code> <code>gene_interactions_path</code> <code>str | None</code> <p>Path to the gene interactions dataset</p> <code>None</code> <code>predictions_path</code> <code>str | None</code> <p>Path to the L2G predictions output dataset</p> <code>None</code> <code>l2g_threshold</code> <code>float | None</code> <p>An optional threshold for the L2G score to filter predictions. A threshold of 0.05 is recommended.</p> <code>None</code> <code>hf_hub_repo_id</code> <code>str | None</code> <p>Hugging Face Hub repository ID. If provided, the model will be uploaded to Hugging Face.</p> <code>None</code> <code>hf_model_commit_message</code> <code>str | None</code> <p>Commit message when we upload the model to the Hugging Face Hub</p> <code>'chore: update model'</code> <code>hf_model_version</code> <code>str | None</code> <p>Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.</p> <code>None</code> <code>explain_predictions</code> <code>bool | None</code> <p>Whether to extract SHAP importances for the L2G predictions. This is computationally expensive.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run_mode is not 'train' or 'predict'</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    *,\n    run_mode: str,\n    hyperparameters: dict[str, Any],\n    download_from_hub: bool,\n    cross_validate: bool,\n    credible_set_path: str,\n    feature_matrix_path: str,\n    wandb_run_name: str | None = None,\n    model_path: str | None = None,\n    features_list: list[str] | None = None,\n    gold_standard_curation_path: str | None = None,\n    variant_index_path: str | None = None,\n    gene_interactions_path: str | None = None,\n    predictions_path: str | None = None,\n    l2g_threshold: float | None = None,\n    hf_hub_repo_id: str | None = None,\n    hf_model_commit_message: str | None = \"chore: update model\",\n    hf_model_version: str | None = None,\n    explain_predictions: bool | None = None,\n) -&gt; None:\n    \"\"\"Initialise the step and run the logic based on mode.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        run_mode (str): Run mode, either 'train' or 'predict'\n        hyperparameters (dict[str, Any]): Hyperparameters for the model\n        download_from_hub (bool): Whether to download the model from Hugging Face Hub\n        cross_validate (bool): Whether to run cross validation (5-fold by default) to train the model.\n        credible_set_path (str): Path to the credible set dataset necessary to build the feature matrix\n        feature_matrix_path (str): Path to the L2G feature matrix input dataset\n        wandb_run_name (str | None): Name of the run to track model training in Weights and Biases\n        model_path (str | None): Path to the model. It can be either in the filesystem or the name on the Hugging Face Hub (in the form of username/repo_name).\n        features_list (list[str] | None): List of features to use to train the model\n        gold_standard_curation_path (str | None): Path to the gold standard curation file\n        variant_index_path (str | None): Path to the variant index\n        gene_interactions_path (str | None): Path to the gene interactions dataset\n        predictions_path (str | None): Path to the L2G predictions output dataset\n        l2g_threshold (float | None): An optional threshold for the L2G score to filter predictions. A threshold of 0.05 is recommended.\n        hf_hub_repo_id (str | None): Hugging Face Hub repository ID. If provided, the model will be uploaded to Hugging Face.\n        hf_model_commit_message (str | None): Commit message when we upload the model to the Hugging Face Hub\n        hf_model_version (str | None): Tag, branch, or commit hash to download the model from the Hub. If None, the latest commit is downloaded.\n        explain_predictions (bool | None): Whether to extract SHAP importances for the L2G predictions. This is computationally expensive.\n\n    Raises:\n        ValueError: If run_mode is not 'train' or 'predict'\n    \"\"\"\n    if run_mode not in [\"train\", \"predict\"]:\n        raise ValueError(\n            f\"run_mode must be one of 'train' or 'predict', got {run_mode}\"\n        )\n\n    self.session = session\n    self.run_mode = run_mode\n    self.predictions_path = predictions_path\n    self.features_list = list(features_list) if features_list else None\n    self.hyperparameters = dict(hyperparameters)\n    self.wandb_run_name = wandb_run_name\n    self.cross_validate = cross_validate\n    self.hf_hub_repo_id = hf_hub_repo_id\n    self.download_from_hub = download_from_hub\n    self.hf_model_commit_message = hf_model_commit_message\n    self.l2g_threshold = l2g_threshold or 0.0\n    self.gold_standard_curation_path = gold_standard_curation_path\n    self.gene_interactions_path = gene_interactions_path\n    self.variant_index_path = variant_index_path\n    self.model_path = (\n        hf_hub_repo_id\n        if not model_path and download_from_hub and hf_hub_repo_id\n        else model_path\n    )\n    self.hf_model_version = hf_model_version\n    self.explain_predictions = explain_predictions\n\n    # Load common inputs\n    self.credible_set = StudyLocus.from_parquet(\n        session, credible_set_path, recursiveFileLookup=True\n    )\n    self.feature_matrix = L2GFeatureMatrix(\n        _df=session.load_data(feature_matrix_path),\n    )\n\n    if run_mode == \"predict\":\n        self.run_predict()\n    elif run_mode == \"train\":\n        self.gold_standard = self.prepare_gold_standard()\n        self.run_train()\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep.prepare_gold_standard","title":"<code>prepare_gold_standard() -&gt; L2GGoldStandard</code>","text":"<p>Prepare the gold standard for training.</p> <p>Returns:</p> Name Type Description <code>L2GGoldStandard</code> <code>L2GGoldStandard</code> <p>training dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When gold standard path, is not provided, or when parsing OTG gold standard but missing interactions and variant index paths.</p> <code>TypeError</code> <p>When gold standard is not OTG gold standard nor L2GGoldStandard.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>def prepare_gold_standard(self) -&gt; L2GGoldStandard:\n    \"\"\"Prepare the gold standard for training.\n\n    Returns:\n        L2GGoldStandard: training dataset.\n\n    Raises:\n        ValueError: When gold standard path, is not provided, or when\n            parsing OTG gold standard but missing interactions and variant index paths.\n        TypeError: When gold standard is not OTG gold standard nor L2GGoldStandard.\n\n    \"\"\"\n    if self.gold_standard_curation_path is None:\n        raise ValueError(\"Gold Standard is required for model training.\")\n    # Read the gold standard either from json or parquet, default to parquet if can not infer the format from extension.\n    ext = self.gold_standard_curation_path.split(\".\")[-1]\n    ext = \"parquet\" if ext not in [\"parquet\", \"json\"] else ext\n    gold_standard = self.session.load_data(self.gold_standard_curation_path, ext)\n    schema_issues = compare_struct_schemas(\n        gold_standard.schema, L2GGoldStandard.get_schema()\n    )\n    # Parse the gold standard depending on the input schema\n    match schema_issues:\n        case {**extra} if not extra:\n            # Schema is the same as L2GGoldStandard - load the GS\n            # NOTE: match to empty dict will be non-selective\n            # see https://stackoverflow.com/questions/75389166/how-to-match-an-empty-dictionary                logging.info(\"Successfully parsed gold standard.\")\n            return L2GGoldStandard(\n                _df=gold_standard,\n                _schema=L2GGoldStandard.get_schema(),\n            )\n        case {\"unexpected_columns\": extra_columns}:\n            # All mandatory columns present, extra columns are allowed but not passed to the L2GGoldStandard object\n            logging.info(\"Successfully parsed gold standard with extra columns.\")\n            return L2GGoldStandard(\n                _df=gold_standard.drop(*extra_columns),\n                _schema=L2GGoldStandard.get_schema(),\n            )\n        case {\n            \"missing_mandatory_columns\": [\n                \"studyLocusId\",\n                \"variantId\",\n                \"studyId\",\n                \"geneId\",\n                \"goldStandardSet\",\n            ],\n            \"unexpected_columns\": [\n                \"association_info\",\n                \"gold_standard_info\",\n                \"metadata\",\n                \"sentinel_variant\",\n                \"trait_info\",\n            ],\n        }:\n            # There are schema mismatches, this would mean that we have\n            logging.info(\"Detected OTG Gold Standard. Attempting to parse it.\")\n            otg_curation = gold_standard\n            if self.gene_interactions_path is None:\n                raise ValueError(\"Interactions are required for parsing curation.\")\n            if self.variant_index_path is None:\n                raise ValueError(\"Variant Index are required for parsing curation.\")\n\n            interactions = self.session.load_data(\n                self.gene_interactions_path, \"parquet\"\n            )\n            variant_index = VariantIndex.from_parquet(\n                self.session, self.variant_index_path\n            )\n            study_locus_overlap = StudyLocus(\n                _df=self.credible_set.df.join(\n                    otg_curation.select(\n                        f.concat_ws(\n                            \"_\",\n                            f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n                            f.col(\"sentinel_variant.locus_GRCh38.position\"),\n                            f.col(\"sentinel_variant.alleles.reference\"),\n                            f.col(\"sentinel_variant.alleles.alternative\"),\n                        ).alias(\"variantId\"),\n                        f.col(\"association_info.otg_id\").alias(\"studyId\"),\n                    ),\n                    on=[\n                        \"studyId\",\n                        \"variantId\",\n                    ],\n                    how=\"inner\",\n                ),\n                _schema=StudyLocus.get_schema(),\n            ).find_overlaps()\n\n            return L2GGoldStandard.from_otg_curation(\n                gold_standard_curation=otg_curation,\n                variant_index=variant_index,\n                study_locus_overlap=study_locus_overlap,\n                interactions=interactions,\n            )\n        case _:\n            raise TypeError(\"Incorrect gold standard dataset provided.\")\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep.run_predict","title":"<code>run_predict() -&gt; None</code>","text":"<p>Run the prediction step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If predictions_path is not provided for prediction mode</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>def run_predict(self) -&gt; None:\n    \"\"\"Run the prediction step.\n\n    Raises:\n        ValueError: If predictions_path is not provided for prediction mode\n    \"\"\"\n    if not self.predictions_path:\n        raise ValueError(\"predictions_path must be provided for prediction mode\")\n    predictions = (\n        L2GPrediction.from_credible_set(\n            self.session,\n            self.credible_set,\n            self.feature_matrix,\n            model_path=self.model_path,\n            features_list=self.features_list,\n            hf_token=self._get_hf_token(),\n            hf_model_version=self.hf_model_version,\n            download_from_hub=self.download_from_hub,\n        )\n        .filter(f.col(\"score\") &gt;= self.l2g_threshold)\n        .add_features(\n            self.feature_matrix,\n        )\n    )\n    if self.explain_predictions:\n        predictions = predictions.explain()\n    predictions.df.coalesce(self.session.output_partitions).write.mode(\n        self.session.write_mode\n    ).parquet(self.predictions_path)\n    self.session.logger.info(\"L2G predictions saved successfully.\")\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneStep.run_train","title":"<code>run_train() -&gt; None</code>","text":"<p>Run the training step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If features list is not provided for model training.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>def run_train(self) -&gt; None:\n    \"\"\"Run the training step.\n\n    Raises:\n        ValueError: If features list is not provided for model training.\n    \"\"\"\n    if self.features_list is None:\n        raise ValueError(\"Features list is required for model training.\")\n    # Initialize access to weights and biases\n    if self.wandb_run_name:\n        wandb_key = access_gcp_secret(\"wandb-key\", \"open-targets-genetics-dev\")\n        wandb_login(key=wandb_key)\n\n    # Instantiate classifier and train model\n    l2g_model = LocusToGeneModel(\n        model=XGBClassifier(random_state=777, eval_metric=\"aucpr\"),\n        hyperparameters=self.hyperparameters,\n        features_list=self.features_list,\n    )\n\n    # Calculate the gold standard features\n    feature_matrix = self._annotate_gold_standards_w_feature_matrix()\n\n    # Run the training\n    trained_model = LocusToGeneTrainer(\n        model=l2g_model, feature_matrix=feature_matrix\n    ).train(wandb_run_name=self.wandb_run_name, cross_validate=self.cross_validate)\n\n    # Export the model\n    if trained_model.training_data and trained_model.model and self.model_path:\n        trained_model.save(self.model_path)\n        if self.hf_hub_repo_id and self.hf_model_commit_message:\n            hf_hub_token = access_gcp_secret(\n                \"hfhub-key\", \"open-targets-genetics-dev\"\n            )\n            trained_model.export_to_hugging_face_hub(\n                # we upload the model saved in the filesystem\n                self.model_path.split(\"/\")[-1],\n                hf_hub_token,\n                feature_matrix=trained_model.training_data,\n                repo_id=self.hf_hub_repo_id,\n                commit_message=self.hf_model_commit_message,\n            )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneEvidenceStep","title":"<code>gentropy.l2g.LocusToGeneEvidenceStep</code>","text":"<p>Locus to gene evidence step.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>class LocusToGeneEvidenceStep:\n    \"\"\"Locus to gene evidence step.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        locus_to_gene_predictions_path: str,\n        credible_set_path: str,\n        study_index_path: str,\n        evidence_output_path: str,\n        locus_to_gene_threshold: float,\n    ) -&gt; None:\n        \"\"\"Initialise the step and generate disease/target evidence.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            locus_to_gene_predictions_path (str): Path to the L2G predictions dataset\n            credible_set_path (str): Path to the credible set dataset\n            study_index_path (str): Path to the study index dataset\n            evidence_output_path (str): Path to the L2G evidence output dataset. The output format is ndjson gzipped.\n            locus_to_gene_threshold (float, optional): Threshold to consider a gene as a target. Defaults to 0.05.\n        \"\"\"\n        # Reading the predictions\n        locus_to_gene_prediction = L2GPrediction.from_parquet(\n            session, locus_to_gene_predictions_path\n        )\n        # Reading the credible set\n        credible_sets = StudyLocus.from_parquet(session, credible_set_path)\n\n        # Reading the study index\n        study_index = StudyIndex.from_parquet(session, study_index_path)\n\n        # Generate evidence and save file:\n        (\n            locus_to_gene_prediction.to_disease_target_evidence(\n                credible_sets, study_index, locus_to_gene_threshold\n            )\n            .coalesce(session.output_partitions)\n            .write.mode(session.write_mode)\n            .parquet(evidence_output_path)\n        )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneEvidenceStep.__init__","title":"<code>__init__(session: Session, locus_to_gene_predictions_path: str, credible_set_path: str, study_index_path: str, evidence_output_path: str, locus_to_gene_threshold: float) -&gt; None</code>","text":"<p>Initialise the step and generate disease/target evidence.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>locus_to_gene_predictions_path</code> <code>str</code> <p>Path to the L2G predictions dataset</p> required <code>credible_set_path</code> <code>str</code> <p>Path to the credible set dataset</p> required <code>study_index_path</code> <code>str</code> <p>Path to the study index dataset</p> required <code>evidence_output_path</code> <code>str</code> <p>Path to the L2G evidence output dataset. The output format is ndjson gzipped.</p> required <code>locus_to_gene_threshold</code> <code>float</code> <p>Threshold to consider a gene as a target. Defaults to 0.05.</p> required Source code in <code>src/gentropy/l2g.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    locus_to_gene_predictions_path: str,\n    credible_set_path: str,\n    study_index_path: str,\n    evidence_output_path: str,\n    locus_to_gene_threshold: float,\n) -&gt; None:\n    \"\"\"Initialise the step and generate disease/target evidence.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        locus_to_gene_predictions_path (str): Path to the L2G predictions dataset\n        credible_set_path (str): Path to the credible set dataset\n        study_index_path (str): Path to the study index dataset\n        evidence_output_path (str): Path to the L2G evidence output dataset. The output format is ndjson gzipped.\n        locus_to_gene_threshold (float, optional): Threshold to consider a gene as a target. Defaults to 0.05.\n    \"\"\"\n    # Reading the predictions\n    locus_to_gene_prediction = L2GPrediction.from_parquet(\n        session, locus_to_gene_predictions_path\n    )\n    # Reading the credible set\n    credible_sets = StudyLocus.from_parquet(session, credible_set_path)\n\n    # Reading the study index\n    study_index = StudyIndex.from_parquet(session, study_index_path)\n\n    # Generate evidence and save file:\n    (\n        locus_to_gene_prediction.to_disease_target_evidence(\n            credible_sets, study_index, locus_to_gene_threshold\n        )\n        .coalesce(session.output_partitions)\n        .write.mode(session.write_mode)\n        .parquet(evidence_output_path)\n    )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneAssociationsStep","title":"<code>gentropy.l2g.LocusToGeneAssociationsStep</code>","text":"<p>Locus to gene associations step.</p> Source code in <code>src/gentropy/l2g.py</code> <pre><code>class LocusToGeneAssociationsStep:\n    \"\"\"Locus to gene associations step.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        evidence_input_path: str,\n        disease_index_path: str,\n        direct_associations_output_path: str,\n        indirect_associations_output_path: str,\n    ) -&gt; None:\n        \"\"\"Create direct and indirect association datasets.\n\n        Args:\n            session (Session): Session object that contains the Spark session\n            evidence_input_path (str): Path to the L2G evidence input dataset\n            disease_index_path (str): Path to disease index file\n            direct_associations_output_path (str): Path to the direct associations output dataset\n            indirect_associations_output_path (str): Path to the indirect associations output dataset\n        \"\"\"\n        # Read in the disease index\n        disease_index = session.spark.read.parquet(disease_index_path).select(\n            f.col(\"id\").alias(\"diseaseId\"),\n            f.explode(\"ancestors\").alias(\"ancestorDiseaseId\"),\n        )\n\n        # Read in the L2G evidence\n        disease_target_evidence = session.spark.read.json(evidence_input_path).select(\n            f.col(\"targetFromSourceId\").alias(\"targetId\"),\n            f.col(\"diseaseFromSourceMappedId\").alias(\"diseaseId\"),\n            f.col(\"resourceScore\"),\n        )\n\n        # Generate direct assocations and save file\n        (\n            disease_target_evidence.groupBy(\"targetId\", \"diseaseId\")\n            .agg(f.collect_set(\"resourceScore\").alias(\"scores\"))\n            .select(\n                \"targetId\",\n                \"diseaseId\",\n                calculate_harmonic_sum(f.col(\"scores\")).alias(\"harmonicSum\"),\n            )\n            .write.mode(session.write_mode)\n            .parquet(direct_associations_output_path)\n        )\n\n        # Generate indirect assocations and save file\n        (\n            disease_target_evidence.join(disease_index, on=\"diseaseId\", how=\"inner\")\n            .groupBy(\"targetId\", \"ancestorDiseaseId\")\n            .agg(f.collect_set(\"resourceScore\").alias(\"scores\"))\n            .select(\n                \"targetId\",\n                \"ancestorDiseaseId\",\n                calculate_harmonic_sum(f.col(\"scores\")).alias(\"harmonicSum\"),\n            )\n            .write.mode(session.write_mode)\n            .parquet(indirect_associations_output_path)\n        )\n</code></pre>"},{"location":"python_api/steps/l2g/#gentropy.l2g.LocusToGeneAssociationsStep.__init__","title":"<code>__init__(session: Session, evidence_input_path: str, disease_index_path: str, direct_associations_output_path: str, indirect_associations_output_path: str) -&gt; None</code>","text":"<p>Create direct and indirect association datasets.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object that contains the Spark session</p> required <code>evidence_input_path</code> <code>str</code> <p>Path to the L2G evidence input dataset</p> required <code>disease_index_path</code> <code>str</code> <p>Path to disease index file</p> required <code>direct_associations_output_path</code> <code>str</code> <p>Path to the direct associations output dataset</p> required <code>indirect_associations_output_path</code> <code>str</code> <p>Path to the indirect associations output dataset</p> required Source code in <code>src/gentropy/l2g.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    evidence_input_path: str,\n    disease_index_path: str,\n    direct_associations_output_path: str,\n    indirect_associations_output_path: str,\n) -&gt; None:\n    \"\"\"Create direct and indirect association datasets.\n\n    Args:\n        session (Session): Session object that contains the Spark session\n        evidence_input_path (str): Path to the L2G evidence input dataset\n        disease_index_path (str): Path to disease index file\n        direct_associations_output_path (str): Path to the direct associations output dataset\n        indirect_associations_output_path (str): Path to the indirect associations output dataset\n    \"\"\"\n    # Read in the disease index\n    disease_index = session.spark.read.parquet(disease_index_path).select(\n        f.col(\"id\").alias(\"diseaseId\"),\n        f.explode(\"ancestors\").alias(\"ancestorDiseaseId\"),\n    )\n\n    # Read in the L2G evidence\n    disease_target_evidence = session.spark.read.json(evidence_input_path).select(\n        f.col(\"targetFromSourceId\").alias(\"targetId\"),\n        f.col(\"diseaseFromSourceMappedId\").alias(\"diseaseId\"),\n        f.col(\"resourceScore\"),\n    )\n\n    # Generate direct assocations and save file\n    (\n        disease_target_evidence.groupBy(\"targetId\", \"diseaseId\")\n        .agg(f.collect_set(\"resourceScore\").alias(\"scores\"))\n        .select(\n            \"targetId\",\n            \"diseaseId\",\n            calculate_harmonic_sum(f.col(\"scores\")).alias(\"harmonicSum\"),\n        )\n        .write.mode(session.write_mode)\n        .parquet(direct_associations_output_path)\n    )\n\n    # Generate indirect assocations and save file\n    (\n        disease_target_evidence.join(disease_index, on=\"diseaseId\", how=\"inner\")\n        .groupBy(\"targetId\", \"ancestorDiseaseId\")\n        .agg(f.collect_set(\"resourceScore\").alias(\"scores\"))\n        .select(\n            \"targetId\",\n            \"ancestorDiseaseId\",\n            calculate_harmonic_sum(f.col(\"scores\")).alias(\"harmonicSum\"),\n        )\n        .write.mode(session.write_mode)\n        .parquet(indirect_associations_output_path)\n    )\n</code></pre>"},{"location":"python_api/steps/ld_clump/","title":"ld_based_clumping","text":""},{"location":"python_api/steps/ld_clump/#gentropy.ld_based_clumping.LDBasedClumpingStep","title":"<code>gentropy.ld_based_clumping.LDBasedClumpingStep</code>","text":"<p>Step to perform LD-based clumping on study locus dataset.</p> <p>As a first step, study locus is enriched with population specific linked-variants. That's why the study index and the ld index is required for this step. Study loci are flaggged in the resulting dataset, which can be explained by a more significant association from the same study.</p> Source code in <code>src/gentropy/ld_based_clumping.py</code> <pre><code>class LDBasedClumpingStep:\n    \"\"\"Step to perform LD-based clumping on study locus dataset.\n\n    As a first step, study locus is enriched with population specific linked-variants.\n    That's why the study index and the ld index is required for this step. Study loci are flaggged\n    in the resulting dataset, which can be explained by a more significant association\n    from the same study.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        study_locus_input_path: str,\n        study_index_path: str,\n        ld_index_path: str,\n        clumped_study_locus_output_path: str,\n    ) -&gt; None:\n        \"\"\"Run LD-based clumping step.\n\n        Args:\n            session (Session): Session object.\n            study_locus_input_path (str): Path to the input study locus.\n            study_index_path (str): Path to the study index.\n            ld_index_path (str): Path to the LD index.\n            clumped_study_locus_output_path (str): path of the resulting, clumped study-locus dataset.\n        \"\"\"\n        study_locus = StudyLocus.from_parquet(session, study_locus_input_path)\n        ld_index = LDIndex.from_parquet(session, ld_index_path)\n        study_index = StudyIndex.from_parquet(session, study_index_path)\n\n        (\n            study_locus\n            # Annotating study locus with LD information:\n            .annotate_ld(study_index, ld_index)\n            .clump()\n            # Save result:\n            .df.write.mode(session.write_mode)\n            .parquet(clumped_study_locus_output_path)\n        )\n</code></pre>"},{"location":"python_api/steps/ld_clump/#gentropy.ld_based_clumping.LDBasedClumpingStep.__init__","title":"<code>__init__(session: Session, study_locus_input_path: str, study_index_path: str, ld_index_path: str, clumped_study_locus_output_path: str) -&gt; None</code>","text":"<p>Run LD-based clumping step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>study_locus_input_path</code> <code>str</code> <p>Path to the input study locus.</p> required <code>study_index_path</code> <code>str</code> <p>Path to the study index.</p> required <code>ld_index_path</code> <code>str</code> <p>Path to the LD index.</p> required <code>clumped_study_locus_output_path</code> <code>str</code> <p>path of the resulting, clumped study-locus dataset.</p> required Source code in <code>src/gentropy/ld_based_clumping.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    study_locus_input_path: str,\n    study_index_path: str,\n    ld_index_path: str,\n    clumped_study_locus_output_path: str,\n) -&gt; None:\n    \"\"\"Run LD-based clumping step.\n\n    Args:\n        session (Session): Session object.\n        study_locus_input_path (str): Path to the input study locus.\n        study_index_path (str): Path to the study index.\n        ld_index_path (str): Path to the LD index.\n        clumped_study_locus_output_path (str): path of the resulting, clumped study-locus dataset.\n    \"\"\"\n    study_locus = StudyLocus.from_parquet(session, study_locus_input_path)\n    ld_index = LDIndex.from_parquet(session, ld_index_path)\n    study_index = StudyIndex.from_parquet(session, study_index_path)\n\n    (\n        study_locus\n        # Annotating study locus with LD information:\n        .annotate_ld(study_index, ld_index)\n        .clump()\n        # Save result:\n        .df.write.mode(session.write_mode)\n        .parquet(clumped_study_locus_output_path)\n    )\n</code></pre>"},{"location":"python_api/steps/ld_index/","title":"GnomAD Linkage data ingestion","text":""},{"location":"python_api/steps/ld_index/#gentropy.gnomad_ingestion.LDIndexStep","title":"<code>gentropy.gnomad_ingestion.LDIndexStep</code>","text":"<p>LD index step.</p> <p>This step is resource intensive</p> <p>Suggested params: high memory machine, 5TB of boot disk, no SSDs.</p> Source code in <code>src/gentropy/gnomad_ingestion.py</code> <pre><code>class LDIndexStep:\n    \"\"\"LD index step.\n\n    !!! warning \"This step is resource intensive\"\n\n        Suggested params: high memory machine, 5TB of boot disk, no SSDs.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        ld_index_out: str,\n        min_r2: float = LDIndexConfig().min_r2,\n        ld_matrix_template: str = LDIndexConfig().ld_matrix_template,\n        ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template,\n        ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations,\n        liftover_ht_path: str = LDIndexConfig().liftover_ht_path,\n        grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path,\n    ) -&gt; None:\n        \"\"\"Run step.\n\n        Args:\n            session (Session): Session object.\n            ld_index_out (str): Output LD index path. (required)\n            min_r2 (float): Minimum r2 to consider when considering variants within a window.\n            ld_matrix_template (str): Input path to the gnomAD ld file with placeholder for population\n            ld_index_raw_template (str): Input path to the raw gnomAD LD indices file with placeholder for population string\n            ld_populations (list[LD_Population | str]): Population names derived from the ld file paths\n            liftover_ht_path (str): Path to the liftover ht file\n            grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates.\n\n        Default values are provided in LDIndexConfig.\n        \"\"\"\n        (\n            GnomADLDMatrix(\n                ld_matrix_template=ld_matrix_template,\n                ld_index_raw_template=ld_index_raw_template,\n                grch37_to_grch38_chain_path=grch37_to_grch38_chain_path,\n                ld_populations=ld_populations,\n                liftover_ht_path=liftover_ht_path,\n            )\n            .as_ld_index(min_r2)\n            .df.write.partitionBy(\"chromosome\")\n            .mode(session.write_mode)\n            .parquet(ld_index_out)\n        )\n        session.logger.info(ld_index_out)\n</code></pre>"},{"location":"python_api/steps/ld_index/#gentropy.gnomad_ingestion.LDIndexStep.__init__","title":"<code>__init__(session: Session, ld_index_out: str, min_r2: float = LDIndexConfig().min_r2, ld_matrix_template: str = LDIndexConfig().ld_matrix_template, ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template, ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations, liftover_ht_path: str = LDIndexConfig().liftover_ht_path, grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path) -&gt; None</code>","text":"<p>Run step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>ld_index_out</code> <code>str</code> <p>Output LD index path. (required)</p> required <code>min_r2</code> <code>float</code> <p>Minimum r2 to consider when considering variants within a window.</p> <code>min_r2</code> <code>ld_matrix_template</code> <code>str</code> <p>Input path to the gnomAD ld file with placeholder for population</p> <code>ld_matrix_template</code> <code>ld_index_raw_template</code> <code>str</code> <p>Input path to the raw gnomAD LD indices file with placeholder for population string</p> <code>ld_index_raw_template</code> <code>ld_populations</code> <code>list[LD_Population | str]</code> <p>Population names derived from the ld file paths</p> <code>ld_populations</code> <code>liftover_ht_path</code> <code>str</code> <p>Path to the liftover ht file</p> <code>liftover_ht_path</code> <code>grch37_to_grch38_chain_path</code> <code>str</code> <p>Path to the chain file used to lift over the coordinates.</p> <code>grch37_to_grch38_chain_path</code> <p>Default values are provided in LDIndexConfig.</p> Source code in <code>src/gentropy/gnomad_ingestion.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    ld_index_out: str,\n    min_r2: float = LDIndexConfig().min_r2,\n    ld_matrix_template: str = LDIndexConfig().ld_matrix_template,\n    ld_index_raw_template: str = LDIndexConfig().ld_index_raw_template,\n    ld_populations: list[LD_Population | str] = LDIndexConfig().ld_populations,\n    liftover_ht_path: str = LDIndexConfig().liftover_ht_path,\n    grch37_to_grch38_chain_path: str = LDIndexConfig().grch37_to_grch38_chain_path,\n) -&gt; None:\n    \"\"\"Run step.\n\n    Args:\n        session (Session): Session object.\n        ld_index_out (str): Output LD index path. (required)\n        min_r2 (float): Minimum r2 to consider when considering variants within a window.\n        ld_matrix_template (str): Input path to the gnomAD ld file with placeholder for population\n        ld_index_raw_template (str): Input path to the raw gnomAD LD indices file with placeholder for population string\n        ld_populations (list[LD_Population | str]): Population names derived from the ld file paths\n        liftover_ht_path (str): Path to the liftover ht file\n        grch37_to_grch38_chain_path (str): Path to the chain file used to lift over the coordinates.\n\n    Default values are provided in LDIndexConfig.\n    \"\"\"\n    (\n        GnomADLDMatrix(\n            ld_matrix_template=ld_matrix_template,\n            ld_index_raw_template=ld_index_raw_template,\n            grch37_to_grch38_chain_path=grch37_to_grch38_chain_path,\n            ld_populations=ld_populations,\n            liftover_ht_path=liftover_ht_path,\n        )\n        .as_ld_index(min_r2)\n        .df.write.partitionBy(\"chromosome\")\n        .mode(session.write_mode)\n        .parquet(ld_index_out)\n    )\n    session.logger.info(ld_index_out)\n</code></pre>"},{"location":"python_api/steps/locus_breaker_clumping/","title":"locus_breaker_clumping","text":""},{"location":"python_api/steps/locus_breaker_clumping/#gentropy.locus_breaker_clumping.LocusBreakerClumpingStep","title":"<code>gentropy.locus_breaker_clumping.LocusBreakerClumpingStep</code>","text":"<p>Step to perform locus-breaker clumping on a study.</p> Source code in <code>src/gentropy/locus_breaker_clumping.py</code> <pre><code>class LocusBreakerClumpingStep:\n    \"\"\"Step to perform locus-breaker clumping on a study.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        summary_statistics_input_path: str,\n        clumped_study_locus_output_path: str,\n        lbc_baseline_pvalue: float,\n        lbc_distance_cutoff: int,\n        lbc_pvalue_threshold: float,\n        lbc_flanking_distance: int,\n        large_loci_size: int,\n        wbc_clump_distance: int,\n        wbc_pvalue_threshold: float,\n        collect_locus: bool = False,\n        remove_mhc: bool = True,\n    ) -&gt; None:\n        \"\"\"Run locus-breaker clumping step.\n\n        This step will perform locus-breaker clumping on the full set of summary statistics.\n        StudyLocus larger than the large_loci_size, by distance, will be further clumped with window-based\n        clumping.\n\n        Args:\n            session (Session): Session object.\n            summary_statistics_input_path (str): Path to the input study locus.\n            clumped_study_locus_output_path (str): path of the resulting, clumped study-locus dataset.\n            lbc_baseline_pvalue (float): Baseline p-value for locus breaker clumping.\n            lbc_distance_cutoff (int): Distance cutoff for locus breaker clumping.\n            lbc_pvalue_threshold (float): P-value threshold for locus breaker clumping.\n            lbc_flanking_distance (int): Flanking distance for locus breaker clumping.\n            large_loci_size (int): Threshold distance to define large loci for window-based clumping.\n            wbc_clump_distance (int): Clump distance for window breaker clumping.\n            wbc_pvalue_threshold (float): P-value threshold for window breaker clumping.\n            collect_locus (bool, optional): Whether to collect locus. Defaults to False.\n            remove_mhc (bool, optional): If true will use exclude_region() to remove the MHC region.\n        \"\"\"\n        sum_stats = SummaryStatistics.from_parquet(\n            session,\n            summary_statistics_input_path,\n        )\n        lbc = sum_stats.locus_breaker_clumping(\n            lbc_baseline_pvalue,\n            lbc_distance_cutoff,\n            lbc_pvalue_threshold,\n            lbc_flanking_distance,\n        )\n        wbc = sum_stats.window_based_clumping(wbc_clump_distance, wbc_pvalue_threshold)\n\n        clumped_result = LocusBreakerClumping.process_locus_breaker_output(\n            lbc,\n            wbc,\n            large_loci_size,\n        )\n        if remove_mhc:\n            clumped_result = clumped_result.exclude_region(\n                GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC),\n                exclude_overlap=True,\n            )\n\n        if collect_locus:\n            clumped_result = clumped_result.annotate_locus_statistics_boundaries(\n                sum_stats\n            )\n        clumped_result.df.write.partitionBy(\"studyLocusId\").mode(\n            session.write_mode\n        ).parquet(clumped_study_locus_output_path)\n</code></pre>"},{"location":"python_api/steps/locus_breaker_clumping/#gentropy.locus_breaker_clumping.LocusBreakerClumpingStep.__init__","title":"<code>__init__(session: Session, summary_statistics_input_path: str, clumped_study_locus_output_path: str, lbc_baseline_pvalue: float, lbc_distance_cutoff: int, lbc_pvalue_threshold: float, lbc_flanking_distance: int, large_loci_size: int, wbc_clump_distance: int, wbc_pvalue_threshold: float, collect_locus: bool = False, remove_mhc: bool = True) -&gt; None</code>","text":"<p>Run locus-breaker clumping step.</p> <p>This step will perform locus-breaker clumping on the full set of summary statistics. StudyLocus larger than the large_loci_size, by distance, will be further clumped with window-based clumping.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>summary_statistics_input_path</code> <code>str</code> <p>Path to the input study locus.</p> required <code>clumped_study_locus_output_path</code> <code>str</code> <p>path of the resulting, clumped study-locus dataset.</p> required <code>lbc_baseline_pvalue</code> <code>float</code> <p>Baseline p-value for locus breaker clumping.</p> required <code>lbc_distance_cutoff</code> <code>int</code> <p>Distance cutoff for locus breaker clumping.</p> required <code>lbc_pvalue_threshold</code> <code>float</code> <p>P-value threshold for locus breaker clumping.</p> required <code>lbc_flanking_distance</code> <code>int</code> <p>Flanking distance for locus breaker clumping.</p> required <code>large_loci_size</code> <code>int</code> <p>Threshold distance to define large loci for window-based clumping.</p> required <code>wbc_clump_distance</code> <code>int</code> <p>Clump distance for window breaker clumping.</p> required <code>wbc_pvalue_threshold</code> <code>float</code> <p>P-value threshold for window breaker clumping.</p> required <code>collect_locus</code> <code>bool</code> <p>Whether to collect locus. Defaults to False.</p> <code>False</code> <code>remove_mhc</code> <code>bool</code> <p>If true will use exclude_region() to remove the MHC region.</p> <code>True</code> Source code in <code>src/gentropy/locus_breaker_clumping.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    summary_statistics_input_path: str,\n    clumped_study_locus_output_path: str,\n    lbc_baseline_pvalue: float,\n    lbc_distance_cutoff: int,\n    lbc_pvalue_threshold: float,\n    lbc_flanking_distance: int,\n    large_loci_size: int,\n    wbc_clump_distance: int,\n    wbc_pvalue_threshold: float,\n    collect_locus: bool = False,\n    remove_mhc: bool = True,\n) -&gt; None:\n    \"\"\"Run locus-breaker clumping step.\n\n    This step will perform locus-breaker clumping on the full set of summary statistics.\n    StudyLocus larger than the large_loci_size, by distance, will be further clumped with window-based\n    clumping.\n\n    Args:\n        session (Session): Session object.\n        summary_statistics_input_path (str): Path to the input study locus.\n        clumped_study_locus_output_path (str): path of the resulting, clumped study-locus dataset.\n        lbc_baseline_pvalue (float): Baseline p-value for locus breaker clumping.\n        lbc_distance_cutoff (int): Distance cutoff for locus breaker clumping.\n        lbc_pvalue_threshold (float): P-value threshold for locus breaker clumping.\n        lbc_flanking_distance (int): Flanking distance for locus breaker clumping.\n        large_loci_size (int): Threshold distance to define large loci for window-based clumping.\n        wbc_clump_distance (int): Clump distance for window breaker clumping.\n        wbc_pvalue_threshold (float): P-value threshold for window breaker clumping.\n        collect_locus (bool, optional): Whether to collect locus. Defaults to False.\n        remove_mhc (bool, optional): If true will use exclude_region() to remove the MHC region.\n    \"\"\"\n    sum_stats = SummaryStatistics.from_parquet(\n        session,\n        summary_statistics_input_path,\n    )\n    lbc = sum_stats.locus_breaker_clumping(\n        lbc_baseline_pvalue,\n        lbc_distance_cutoff,\n        lbc_pvalue_threshold,\n        lbc_flanking_distance,\n    )\n    wbc = sum_stats.window_based_clumping(wbc_clump_distance, wbc_pvalue_threshold)\n\n    clumped_result = LocusBreakerClumping.process_locus_breaker_output(\n        lbc,\n        wbc,\n        large_loci_size,\n    )\n    if remove_mhc:\n        clumped_result = clumped_result.exclude_region(\n            GenomicRegion.from_known_genomic_region(KnownGenomicRegions.MHC),\n            exclude_overlap=True,\n        )\n\n    if collect_locus:\n        clumped_result = clumped_result.annotate_locus_statistics_boundaries(\n            sum_stats\n        )\n    clumped_result.df.write.partitionBy(\"studyLocusId\").mode(\n        session.write_mode\n    ).parquet(clumped_study_locus_output_path)\n</code></pre>"},{"location":"python_api/steps/pics/","title":"pics","text":""},{"location":"python_api/steps/pics/#gentropy.pics.PICSStep","title":"<code>gentropy.pics.PICSStep</code>","text":"<p>PICS finemapping of LD-annotated StudyLocus.</p> Source code in <code>src/gentropy/pics.py</code> <pre><code>class PICSStep:\n    \"\"\"PICS finemapping of LD-annotated StudyLocus.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        study_locus_ld_annotated_in: str,\n        picsed_study_locus_out: str,\n    ) -&gt; None:\n        \"\"\"Run PICS on LD annotated study-locus.\n\n        Args:\n            session (Session): Session object.\n            study_locus_ld_annotated_in (str): Input LD annotated study-locus path.\n            picsed_study_locus_out (str): Output PICSed study-locus path.\n        \"\"\"\n        # Extract\n        study_locus_ld_annotated = StudyLocus.from_parquet(\n            session, study_locus_ld_annotated_in\n        )\n        # PICS\n        (\n            PICS.finemap(study_locus_ld_annotated)\n            .filter_credible_set(credible_interval=CredibleInterval.IS99)\n            # Flagging sub-significnat loci:\n            .validate_lead_pvalue(\n                pvalue_cutoff=WindowBasedClumpingStepConfig().gwas_significance\n            )\n            # Writing the output:\n            .df.write.mode(session.write_mode)\n            .parquet(picsed_study_locus_out)\n        )\n</code></pre>"},{"location":"python_api/steps/pics/#gentropy.pics.PICSStep.__init__","title":"<code>__init__(session: Session, study_locus_ld_annotated_in: str, picsed_study_locus_out: str) -&gt; None</code>","text":"<p>Run PICS on LD annotated study-locus.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>study_locus_ld_annotated_in</code> <code>str</code> <p>Input LD annotated study-locus path.</p> required <code>picsed_study_locus_out</code> <code>str</code> <p>Output PICSed study-locus path.</p> required Source code in <code>src/gentropy/pics.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    study_locus_ld_annotated_in: str,\n    picsed_study_locus_out: str,\n) -&gt; None:\n    \"\"\"Run PICS on LD annotated study-locus.\n\n    Args:\n        session (Session): Session object.\n        study_locus_ld_annotated_in (str): Input LD annotated study-locus path.\n        picsed_study_locus_out (str): Output PICSed study-locus path.\n    \"\"\"\n    # Extract\n    study_locus_ld_annotated = StudyLocus.from_parquet(\n        session, study_locus_ld_annotated_in\n    )\n    # PICS\n    (\n        PICS.finemap(study_locus_ld_annotated)\n        .filter_credible_set(credible_interval=CredibleInterval.IS99)\n        # Flagging sub-significnat loci:\n        .validate_lead_pvalue(\n            pvalue_cutoff=WindowBasedClumpingStepConfig().gwas_significance\n        )\n        # Writing the output:\n        .df.write.mode(session.write_mode)\n        .parquet(picsed_study_locus_out)\n    )\n</code></pre>"},{"location":"python_api/steps/study_locus_validation/","title":"Study-Locus Validation","text":""},{"location":"python_api/steps/study_locus_validation/#gentropy.study_locus_validation.StudyLocusValidationStep","title":"<code>gentropy.study_locus_validation.StudyLocusValidationStep</code>","text":"<p>Study index validation step.</p> <p>This step reads and outputs a study index dataset with flagged studies when target of disease validation fails.</p> Source code in <code>src/gentropy/study_locus_validation.py</code> <pre><code>class StudyLocusValidationStep:\n    \"\"\"Study index validation step.\n\n    This step reads and outputs a study index dataset with flagged studies\n    when target of disease validation fails.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        study_locus_path: list[str],\n        study_index_path: str,\n        target_index_path: str,\n        valid_study_locus_path: str,\n        invalid_study_locus_path: str,\n        trans_qtl_threshold: int,\n        invalid_qc_reasons: list[str] = [],\n    ) -&gt; None:\n        \"\"\"Initialize step.\n\n        Args:\n            session (Session): Session object.\n            study_locus_path (list[str]): Path to study locus dataset.\n            study_index_path (str): Path to study index file.\n            target_index_path (str): path to the target index.\n            valid_study_locus_path (str): Path to write the valid records.\n            invalid_study_locus_path (str): Path to write the output file.\n            trans_qtl_threshold (int): genomic distance above which a QTL is considered trans.\n            invalid_qc_reasons (list[str]): List of invalid quality check reason names from `StudyLocusQualityCheck` (e.g. ['SUBSIGNIFICANT_FLAG']).\n        \"\"\"\n        # Reading datasets:\n        study_index = StudyIndex.from_parquet(session, study_index_path)\n        target_index = TargetIndex.from_parquet(session, target_index_path)\n\n        # Running validation then writing output:\n        study_locus_with_qc = (\n            StudyLocus.from_parquet(session, list(study_locus_path))\n            # Add flag for MHC region\n            .qc_MHC_region()\n            .validate_chromosome_label()  # Flagging credible sets with unsupported chromosomes\n            .validate_study(study_index)  # Flagging studies not in study index\n            .annotate_study_type(study_index)  # Add study type to study locus\n            .qc_redundant_top_hits_from_PICS()  # Flagging top hits from studies with PICS summary statistics\n            .qc_explained_by_SuSiE()  # Flagging credible sets in regions explained by SuSiE\n            # Annotates credible intervals and filter to only keep 95% credible sets\n            .filter_credible_set(credible_interval=CredibleInterval.IS95)\n            # Flagging credible sets with PIP &gt; 1 or PIP &lt; 0.95\n            .qc_abnormal_pips(\n                sum_pips_lower_threshold=0.95, sum_pips_upper_threshold=1.0001\n            )\n            # Annotate credible set confidence:\n            .assign_confidence()\n            # Flagging trans qtls:\n            .flag_trans_qtls(study_index, target_index, trans_qtl_threshold)\n        ).persist()  # we will need this for 2 types of outputs\n\n        # Valid study locus partitioned to simplify the finding of overlaps\n        study_locus_with_qc.valid_rows(invalid_qc_reasons).df.repartitionByRange(\n            session.output_partitions, \"chromosome\", \"position\"\n        ).sortWithinPartitions(\"chromosome\", \"position\").write.mode(\n            session.write_mode\n        ).parquet(valid_study_locus_path)\n\n        # Invalid study locus\n        study_locus_with_qc.valid_rows(invalid_qc_reasons, invalid=True).df.coalesce(\n            session.output_partitions\n        ).write.mode(session.write_mode).parquet(invalid_study_locus_path)\n</code></pre>"},{"location":"python_api/steps/study_locus_validation/#gentropy.study_locus_validation.StudyLocusValidationStep.__init__","title":"<code>__init__(session: Session, study_locus_path: list[str], study_index_path: str, target_index_path: str, valid_study_locus_path: str, invalid_study_locus_path: str, trans_qtl_threshold: int, invalid_qc_reasons: list[str] = []) -&gt; None</code>","text":"<p>Initialize step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>study_locus_path</code> <code>list[str]</code> <p>Path to study locus dataset.</p> required <code>study_index_path</code> <code>str</code> <p>Path to study index file.</p> required <code>target_index_path</code> <code>str</code> <p>path to the target index.</p> required <code>valid_study_locus_path</code> <code>str</code> <p>Path to write the valid records.</p> required <code>invalid_study_locus_path</code> <code>str</code> <p>Path to write the output file.</p> required <code>trans_qtl_threshold</code> <code>int</code> <p>genomic distance above which a QTL is considered trans.</p> required <code>invalid_qc_reasons</code> <code>list[str]</code> <p>List of invalid quality check reason names from <code>StudyLocusQualityCheck</code> (e.g. ['SUBSIGNIFICANT_FLAG']).</p> <code>[]</code> Source code in <code>src/gentropy/study_locus_validation.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    study_locus_path: list[str],\n    study_index_path: str,\n    target_index_path: str,\n    valid_study_locus_path: str,\n    invalid_study_locus_path: str,\n    trans_qtl_threshold: int,\n    invalid_qc_reasons: list[str] = [],\n) -&gt; None:\n    \"\"\"Initialize step.\n\n    Args:\n        session (Session): Session object.\n        study_locus_path (list[str]): Path to study locus dataset.\n        study_index_path (str): Path to study index file.\n        target_index_path (str): path to the target index.\n        valid_study_locus_path (str): Path to write the valid records.\n        invalid_study_locus_path (str): Path to write the output file.\n        trans_qtl_threshold (int): genomic distance above which a QTL is considered trans.\n        invalid_qc_reasons (list[str]): List of invalid quality check reason names from `StudyLocusQualityCheck` (e.g. ['SUBSIGNIFICANT_FLAG']).\n    \"\"\"\n    # Reading datasets:\n    study_index = StudyIndex.from_parquet(session, study_index_path)\n    target_index = TargetIndex.from_parquet(session, target_index_path)\n\n    # Running validation then writing output:\n    study_locus_with_qc = (\n        StudyLocus.from_parquet(session, list(study_locus_path))\n        # Add flag for MHC region\n        .qc_MHC_region()\n        .validate_chromosome_label()  # Flagging credible sets with unsupported chromosomes\n        .validate_study(study_index)  # Flagging studies not in study index\n        .annotate_study_type(study_index)  # Add study type to study locus\n        .qc_redundant_top_hits_from_PICS()  # Flagging top hits from studies with PICS summary statistics\n        .qc_explained_by_SuSiE()  # Flagging credible sets in regions explained by SuSiE\n        # Annotates credible intervals and filter to only keep 95% credible sets\n        .filter_credible_set(credible_interval=CredibleInterval.IS95)\n        # Flagging credible sets with PIP &gt; 1 or PIP &lt; 0.95\n        .qc_abnormal_pips(\n            sum_pips_lower_threshold=0.95, sum_pips_upper_threshold=1.0001\n        )\n        # Annotate credible set confidence:\n        .assign_confidence()\n        # Flagging trans qtls:\n        .flag_trans_qtls(study_index, target_index, trans_qtl_threshold)\n    ).persist()  # we will need this for 2 types of outputs\n\n    # Valid study locus partitioned to simplify the finding of overlaps\n    study_locus_with_qc.valid_rows(invalid_qc_reasons).df.repartitionByRange(\n        session.output_partitions, \"chromosome\", \"position\"\n    ).sortWithinPartitions(\"chromosome\", \"position\").write.mode(\n        session.write_mode\n    ).parquet(valid_study_locus_path)\n\n    # Invalid study locus\n    study_locus_with_qc.valid_rows(invalid_qc_reasons, invalid=True).df.coalesce(\n        session.output_partitions\n    ).write.mode(session.write_mode).parquet(invalid_study_locus_path)\n</code></pre>"},{"location":"python_api/steps/study_validation/","title":"Study Validation","text":""},{"location":"python_api/steps/study_validation/#gentropy.study_validation.StudyValidationStep","title":"<code>gentropy.study_validation.StudyValidationStep</code>","text":"<p>Study index validation step.</p> <p>This step reads and outputs a study index dataset with flagged studies when target of disease validation fails.</p> Source code in <code>src/gentropy/study_validation.py</code> <pre><code>class StudyValidationStep:\n    \"\"\"Study index validation step.\n\n    This step reads and outputs a study index dataset with flagged studies\n    when target of disease validation fails.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        study_index_path: list[str],\n        target_index_path: str,\n        disease_index_path: str,\n        biosample_index_path: str,\n        valid_study_index_path: str,\n        invalid_study_index_path: str,\n        invalid_qc_reasons: list[str] = [],\n    ) -&gt; None:\n        \"\"\"Initialize step.\n\n        Args:\n            session (Session): Session object.\n            study_index_path (list[str]): Path to study index file.\n            target_index_path (str): Path to target index file.\n            disease_index_path (str): Path to disease index file.\n            biosample_index_path (str): Path to biosample index file.\n            valid_study_index_path (str): Path to write the valid records.\n            invalid_study_index_path (str): Path to write the output file.\n            invalid_qc_reasons (list[str]): List of invalid quality check reason names from `StudyQualityCheck` (e.g. ['DUPLICATED_STUDY']).\n        \"\"\"\n        # Reading datasets:\n        target_index = TargetIndex.from_parquet(session, target_index_path)\n        biosample_index = BiosampleIndex.from_parquet(session, biosample_index_path)\n        # Reading disease index and pre-process.\n        # This logic does not belong anywhere, but gentorpy has no disease dataset yet.\n        disease_index = (\n            session.spark.read.parquet(disease_index_path)\n            .select(\n                f.col(\"id\").alias(\"diseaseId\"),\n                f.explode_outer(\n                    f.when(\n                        f.col(\"obsoleteTerms\").isNotNull(),\n                        f.array_union(f.array(\"id\"), f.col(\"obsoleteTerms\")),\n                    )\n                ).alias(\"efo\"),\n            )\n            .withColumn(\"efo\", f.coalesce(f.col(\"efo\"), f.col(\"diseaseId\")))\n        )\n        study_index = StudyIndex.from_parquet(session, list(study_index_path))\n\n        # Running validation:\n        study_index_with_qc = (\n            study_index.deconvolute_studies()  # Deconvolute studies where the same study is ingested from multiple sources\n            .validate_study_type()  # Flagging non-supported study types\n            .validate_target(target_index)  # Flagging QTL studies with invalid targets\n            .validate_disease(disease_index)  # Flagging invalid EFOs\n            .validate_biosample(\n                biosample_index\n            )  # Flagging QTL studies with invalid biosamples\n            .validate_analysis_flags()  # Flagging studies with case case design\n        ).persist()  # we will need this for 2 types of outputs\n\n        study_index_with_qc.valid_rows(invalid_qc_reasons, invalid=True).df.coalesce(\n            session.output_partitions\n        ).write.mode(session.write_mode).parquet(invalid_study_index_path)\n\n        study_index_with_qc.valid_rows(invalid_qc_reasons).df.coalesce(\n            session.output_partitions\n        ).write.mode(session.write_mode).parquet(valid_study_index_path)\n</code></pre>"},{"location":"python_api/steps/study_validation/#gentropy.study_validation.StudyValidationStep.__init__","title":"<code>__init__(session: Session, study_index_path: list[str], target_index_path: str, disease_index_path: str, biosample_index_path: str, valid_study_index_path: str, invalid_study_index_path: str, invalid_qc_reasons: list[str] = []) -&gt; None</code>","text":"<p>Initialize step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>study_index_path</code> <code>list[str]</code> <p>Path to study index file.</p> required <code>target_index_path</code> <code>str</code> <p>Path to target index file.</p> required <code>disease_index_path</code> <code>str</code> <p>Path to disease index file.</p> required <code>biosample_index_path</code> <code>str</code> <p>Path to biosample index file.</p> required <code>valid_study_index_path</code> <code>str</code> <p>Path to write the valid records.</p> required <code>invalid_study_index_path</code> <code>str</code> <p>Path to write the output file.</p> required <code>invalid_qc_reasons</code> <code>list[str]</code> <p>List of invalid quality check reason names from <code>StudyQualityCheck</code> (e.g. ['DUPLICATED_STUDY']).</p> <code>[]</code> Source code in <code>src/gentropy/study_validation.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    study_index_path: list[str],\n    target_index_path: str,\n    disease_index_path: str,\n    biosample_index_path: str,\n    valid_study_index_path: str,\n    invalid_study_index_path: str,\n    invalid_qc_reasons: list[str] = [],\n) -&gt; None:\n    \"\"\"Initialize step.\n\n    Args:\n        session (Session): Session object.\n        study_index_path (list[str]): Path to study index file.\n        target_index_path (str): Path to target index file.\n        disease_index_path (str): Path to disease index file.\n        biosample_index_path (str): Path to biosample index file.\n        valid_study_index_path (str): Path to write the valid records.\n        invalid_study_index_path (str): Path to write the output file.\n        invalid_qc_reasons (list[str]): List of invalid quality check reason names from `StudyQualityCheck` (e.g. ['DUPLICATED_STUDY']).\n    \"\"\"\n    # Reading datasets:\n    target_index = TargetIndex.from_parquet(session, target_index_path)\n    biosample_index = BiosampleIndex.from_parquet(session, biosample_index_path)\n    # Reading disease index and pre-process.\n    # This logic does not belong anywhere, but gentorpy has no disease dataset yet.\n    disease_index = (\n        session.spark.read.parquet(disease_index_path)\n        .select(\n            f.col(\"id\").alias(\"diseaseId\"),\n            f.explode_outer(\n                f.when(\n                    f.col(\"obsoleteTerms\").isNotNull(),\n                    f.array_union(f.array(\"id\"), f.col(\"obsoleteTerms\")),\n                )\n            ).alias(\"efo\"),\n        )\n        .withColumn(\"efo\", f.coalesce(f.col(\"efo\"), f.col(\"diseaseId\")))\n    )\n    study_index = StudyIndex.from_parquet(session, list(study_index_path))\n\n    # Running validation:\n    study_index_with_qc = (\n        study_index.deconvolute_studies()  # Deconvolute studies where the same study is ingested from multiple sources\n        .validate_study_type()  # Flagging non-supported study types\n        .validate_target(target_index)  # Flagging QTL studies with invalid targets\n        .validate_disease(disease_index)  # Flagging invalid EFOs\n        .validate_biosample(\n            biosample_index\n        )  # Flagging QTL studies with invalid biosamples\n        .validate_analysis_flags()  # Flagging studies with case case design\n    ).persist()  # we will need this for 2 types of outputs\n\n    study_index_with_qc.valid_rows(invalid_qc_reasons, invalid=True).df.coalesce(\n        session.output_partitions\n    ).write.mode(session.write_mode).parquet(invalid_study_index_path)\n\n    study_index_with_qc.valid_rows(invalid_qc_reasons).df.coalesce(\n        session.output_partitions\n    ).write.mode(session.write_mode).parquet(valid_study_index_path)\n</code></pre>"},{"location":"python_api/steps/summary_statistics_qc/","title":"summary_statistics_qc","text":""},{"location":"python_api/steps/summary_statistics_qc/#gentropy.sumstat_qc_step.SummaryStatisticsQCStep","title":"<code>gentropy.sumstat_qc_step.SummaryStatisticsQCStep</code>","text":"<p>Step to run GWAS QC.</p> Source code in <code>src/gentropy/sumstat_qc_step.py</code> <pre><code>class SummaryStatisticsQCStep:\n    \"\"\"Step to run GWAS QC.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        gwas_path: str,\n        output_path: str,\n        pval_threshold: float = 1e-8,\n    ) -&gt; None:\n        \"\"\"Calculating quality control metrics on the provided GWAS study.\n\n        Args:\n            session (Session): Spark session\n            gwas_path (str): Path to the GWAS summary statistics.\n            output_path (str): Output path for the QC results.\n            pval_threshold (float): P-value threshold for the QC. Default is 1e-8.\n        \"\"\"\n        gwas = SummaryStatistics.from_parquet(session, path=gwas_path)\n\n        (\n            SummaryStatisticsQC.from_summary_statistics(\n                gwas=gwas,\n                pval_threshold=pval_threshold,\n            )\n            .df.repartition(1)\n            .write.mode(session.write_mode)\n            .parquet(output_path)\n        )\n</code></pre>"},{"location":"python_api/steps/summary_statistics_qc/#gentropy.sumstat_qc_step.SummaryStatisticsQCStep.__init__","title":"<code>__init__(session: Session, gwas_path: str, output_path: str, pval_threshold: float = 1e-08) -&gt; None</code>","text":"<p>Calculating quality control metrics on the provided GWAS study.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Spark session</p> required <code>gwas_path</code> <code>str</code> <p>Path to the GWAS summary statistics.</p> required <code>output_path</code> <code>str</code> <p>Output path for the QC results.</p> required <code>pval_threshold</code> <code>float</code> <p>P-value threshold for the QC. Default is 1e-8.</p> <code>1e-08</code> Source code in <code>src/gentropy/sumstat_qc_step.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    gwas_path: str,\n    output_path: str,\n    pval_threshold: float = 1e-8,\n) -&gt; None:\n    \"\"\"Calculating quality control metrics on the provided GWAS study.\n\n    Args:\n        session (Session): Spark session\n        gwas_path (str): Path to the GWAS summary statistics.\n        output_path (str): Output path for the QC results.\n        pval_threshold (float): P-value threshold for the QC. Default is 1e-8.\n    \"\"\"\n    gwas = SummaryStatistics.from_parquet(session, path=gwas_path)\n\n    (\n        SummaryStatisticsQC.from_summary_statistics(\n            gwas=gwas,\n            pval_threshold=pval_threshold,\n        )\n        .df.repartition(1)\n        .write.mode(session.write_mode)\n        .parquet(output_path)\n    )\n</code></pre>"},{"location":"python_api/steps/ukb_ppp_eur_sumstat_preprocess/","title":"ukb_ppp_eur_sumstat_preprocess","text":""},{"location":"python_api/steps/ukb_ppp_eur_sumstat_preprocess/#gentropy.ukb_ppp_eur_sumstat_preprocess.UkbPppEurStep","title":"<code>gentropy.ukb_ppp_eur_sumstat_preprocess.UkbPppEurStep</code>","text":"<p>UKB PPP (EUR) data ingestion and harmonisation.</p> Source code in <code>src/gentropy/ukb_ppp_eur_sumstat_preprocess.py</code> <pre><code>class UkbPppEurStep:\n    \"\"\"UKB PPP (EUR) data ingestion and harmonisation.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        raw_study_index_path_from_tsv: str,\n        raw_summary_stats_path: str,\n        variant_annotation_path: str,\n        tmp_variant_annotation_path: str,\n        study_index_output_path: str,\n        summary_stats_output_path: str,\n    ) -&gt; None:\n        \"\"\"Run UKB PPP (EUR) data ingestion and harmonisation step.\n\n        Args:\n            session (Session): Session object.\n            raw_study_index_path_from_tsv (str): Input raw study index path.\n            raw_summary_stats_path (str): Input raw summary stats path.\n            variant_annotation_path (str): Input variant annotation dataset path.\n            tmp_variant_annotation_path (str): Temporary output path for variant annotation dataset.\n            study_index_output_path (str): Study index output path.\n            summary_stats_output_path (str): Summary stats output path.\n        \"\"\"\n        session.logger.info(\n            \"Pre-compute the direct and flipped variant annotation dataset.\"\n        )\n        prepare_va(session, variant_annotation_path, tmp_variant_annotation_path)\n\n        session.logger.info(\"Process study index.\")\n        (\n            UkbPppEurStudyIndex.from_source(\n                spark=session.spark,\n                raw_study_index_path_from_tsv=raw_study_index_path_from_tsv,\n                raw_summary_stats_path=raw_summary_stats_path,\n            )\n            .df.write.mode(\"overwrite\")\n            .parquet(study_index_output_path)\n        )\n\n        session.logger.info(\"Process and harmonise summary stats.\")\n        UkbPppEurSummaryStats.process_summary_stats_per_chromosome(\n            session,\n            raw_summary_stats_path,\n            tmp_variant_annotation_path,\n            summary_stats_output_path,\n            study_index_output_path,\n        )\n</code></pre>"},{"location":"python_api/steps/ukb_ppp_eur_sumstat_preprocess/#gentropy.ukb_ppp_eur_sumstat_preprocess.UkbPppEurStep.__init__","title":"<code>__init__(session: Session, raw_study_index_path_from_tsv: str, raw_summary_stats_path: str, variant_annotation_path: str, tmp_variant_annotation_path: str, study_index_output_path: str, summary_stats_output_path: str) -&gt; None</code>","text":"<p>Run UKB PPP (EUR) data ingestion and harmonisation step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>raw_study_index_path_from_tsv</code> <code>str</code> <p>Input raw study index path.</p> required <code>raw_summary_stats_path</code> <code>str</code> <p>Input raw summary stats path.</p> required <code>variant_annotation_path</code> <code>str</code> <p>Input variant annotation dataset path.</p> required <code>tmp_variant_annotation_path</code> <code>str</code> <p>Temporary output path for variant annotation dataset.</p> required <code>study_index_output_path</code> <code>str</code> <p>Study index output path.</p> required <code>summary_stats_output_path</code> <code>str</code> <p>Summary stats output path.</p> required Source code in <code>src/gentropy/ukb_ppp_eur_sumstat_preprocess.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    raw_study_index_path_from_tsv: str,\n    raw_summary_stats_path: str,\n    variant_annotation_path: str,\n    tmp_variant_annotation_path: str,\n    study_index_output_path: str,\n    summary_stats_output_path: str,\n) -&gt; None:\n    \"\"\"Run UKB PPP (EUR) data ingestion and harmonisation step.\n\n    Args:\n        session (Session): Session object.\n        raw_study_index_path_from_tsv (str): Input raw study index path.\n        raw_summary_stats_path (str): Input raw summary stats path.\n        variant_annotation_path (str): Input variant annotation dataset path.\n        tmp_variant_annotation_path (str): Temporary output path for variant annotation dataset.\n        study_index_output_path (str): Study index output path.\n        summary_stats_output_path (str): Summary stats output path.\n    \"\"\"\n    session.logger.info(\n        \"Pre-compute the direct and flipped variant annotation dataset.\"\n    )\n    prepare_va(session, variant_annotation_path, tmp_variant_annotation_path)\n\n    session.logger.info(\"Process study index.\")\n    (\n        UkbPppEurStudyIndex.from_source(\n            spark=session.spark,\n            raw_study_index_path_from_tsv=raw_study_index_path_from_tsv,\n            raw_summary_stats_path=raw_summary_stats_path,\n        )\n        .df.write.mode(\"overwrite\")\n        .parquet(study_index_output_path)\n    )\n\n    session.logger.info(\"Process and harmonise summary stats.\")\n    UkbPppEurSummaryStats.process_summary_stats_per_chromosome(\n        session,\n        raw_summary_stats_path,\n        tmp_variant_annotation_path,\n        summary_stats_output_path,\n        study_index_output_path,\n    )\n</code></pre>"},{"location":"python_api/steps/variant_annotation_step/","title":"GnomAD variant data ingestion","text":""},{"location":"python_api/steps/variant_annotation_step/#gentropy.gnomad_ingestion.GnomadVariantIndexStep","title":"<code>gentropy.gnomad_ingestion.GnomadVariantIndexStep</code>","text":"<p>A step to generate variant index dataset from gnomad data.</p> <p>Variant annotation step produces a dataset of the type <code>VariantIndex</code> derived from gnomADs <code>gnomad.genomes.vX.X.X.sites.ht</code> Hail's table. This dataset is used to validate variants and as a source of annotation.</p> Source code in <code>src/gentropy/gnomad_ingestion.py</code> <pre><code>class GnomadVariantIndexStep:\n    \"\"\"A step to generate variant index dataset from gnomad data.\n\n    Variant annotation step produces a dataset of the type `VariantIndex` derived from gnomADs `gnomad.genomes.vX.X.X.sites.ht` Hail's table.\n    This dataset is used to validate variants and as a source of annotation.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        variant_annotation_path: str = GnomadVariantConfig().variant_annotation_path,\n        gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path,\n        gnomad_joint_path: str = GnomadVariantConfig().gnomad_joint_path,\n        gnomad_variant_populations: list[\n            VariantPopulation | str\n        ] = GnomadVariantConfig().gnomad_variant_populations,\n    ) -&gt; None:\n        \"\"\"Run Variant Annotation step.\n\n        Args:\n            session (Session): Session object.\n            variant_annotation_path (str): Output path for the variant annotation dataset.\n            gnomad_genomes_path (str): Path to the gnomAD genomes hail table.\n            gnomad_joint_path (str): Path to the gnomAD joint hail table.\n            gnomad_variant_populations (list[VariantPopulation | str]): List of populations to include in the annotation.\n\n        All defaults are stored in the GnomadVariantConfig.\n        \"\"\"\n        # amend data source version to output path\n        session.logger.info(\"Gnomad variant annotation path:\")\n        session.logger.info(variant_annotation_path)\n\n        gnomad_rsids = GnomADVariantRsIds(\n            gnomad_genomes_path=gnomad_genomes_path,\n        ).as_variant_index()\n\n        gnomad_allele_frequencies = GnomADVariantFrequencies(\n            gnomad_joint_path=gnomad_joint_path,\n            gnomad_variant_populations=gnomad_variant_populations,\n        ).as_variant_index()\n\n        # Parse variant info from source.\n        (\n            gnomad_allele_frequencies.add_annotation(gnomad_rsids)\n            .df.repartitionByRange(\"chromosome\", \"position\")\n            .sortWithinPartitions(\"chromosome\", \"position\")\n            .write.mode(session.write_mode)\n            .parquet(variant_annotation_path)\n        )\n</code></pre>"},{"location":"python_api/steps/variant_annotation_step/#gentropy.gnomad_ingestion.GnomadVariantIndexStep.__init__","title":"<code>__init__(session: Session, variant_annotation_path: str = GnomadVariantConfig().variant_annotation_path, gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path, gnomad_joint_path: str = GnomadVariantConfig().gnomad_joint_path, gnomad_variant_populations: list[VariantPopulation | str] = GnomadVariantConfig().gnomad_variant_populations) -&gt; None</code>","text":"<p>Run Variant Annotation step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>variant_annotation_path</code> <code>str</code> <p>Output path for the variant annotation dataset.</p> <code>variant_annotation_path</code> <code>gnomad_genomes_path</code> <code>str</code> <p>Path to the gnomAD genomes hail table.</p> <code>gnomad_genomes_path</code> <code>gnomad_joint_path</code> <code>str</code> <p>Path to the gnomAD joint hail table.</p> <code>gnomad_joint_path</code> <code>gnomad_variant_populations</code> <code>list[VariantPopulation | str]</code> <p>List of populations to include in the annotation.</p> <code>gnomad_variant_populations</code> <p>All defaults are stored in the GnomadVariantConfig.</p> Source code in <code>src/gentropy/gnomad_ingestion.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    variant_annotation_path: str = GnomadVariantConfig().variant_annotation_path,\n    gnomad_genomes_path: str = GnomadVariantConfig().gnomad_genomes_path,\n    gnomad_joint_path: str = GnomadVariantConfig().gnomad_joint_path,\n    gnomad_variant_populations: list[\n        VariantPopulation | str\n    ] = GnomadVariantConfig().gnomad_variant_populations,\n) -&gt; None:\n    \"\"\"Run Variant Annotation step.\n\n    Args:\n        session (Session): Session object.\n        variant_annotation_path (str): Output path for the variant annotation dataset.\n        gnomad_genomes_path (str): Path to the gnomAD genomes hail table.\n        gnomad_joint_path (str): Path to the gnomAD joint hail table.\n        gnomad_variant_populations (list[VariantPopulation | str]): List of populations to include in the annotation.\n\n    All defaults are stored in the GnomadVariantConfig.\n    \"\"\"\n    # amend data source version to output path\n    session.logger.info(\"Gnomad variant annotation path:\")\n    session.logger.info(variant_annotation_path)\n\n    gnomad_rsids = GnomADVariantRsIds(\n        gnomad_genomes_path=gnomad_genomes_path,\n    ).as_variant_index()\n\n    gnomad_allele_frequencies = GnomADVariantFrequencies(\n        gnomad_joint_path=gnomad_joint_path,\n        gnomad_variant_populations=gnomad_variant_populations,\n    ).as_variant_index()\n\n    # Parse variant info from source.\n    (\n        gnomad_allele_frequencies.add_annotation(gnomad_rsids)\n        .df.repartitionByRange(\"chromosome\", \"position\")\n        .sortWithinPartitions(\"chromosome\", \"position\")\n        .write.mode(session.write_mode)\n        .parquet(variant_annotation_path)\n    )\n</code></pre>"},{"location":"python_api/steps/variant_index_step/","title":"variant_index","text":""},{"location":"python_api/steps/variant_index_step/#gentropy.variant_index.VariantIndexStep","title":"<code>gentropy.variant_index.VariantIndexStep</code>","text":"<p>Generate variant index based on a VEP output in json format.</p> <p>The variant index is a dataset that contains variant annotations extracted from VEP output. It is expected that all variants in the VEP output are present in the variant index. There's an option to provide extra variant annotations to be added to the variant index eg. allele frequencies from GnomAD.</p> Source code in <code>src/gentropy/variant_index.py</code> <pre><code>class VariantIndexStep:\n    \"\"\"Generate variant index based on a VEP output in json format.\n\n    The variant index is a dataset that contains variant annotations extracted from VEP output. It is expected that all variants in the VEP output are present in the variant index.\n    There's an option to provide extra variant annotations to be added to the variant index eg. allele frequencies from GnomAD.\n    \"\"\"\n\n    def __init__(\n        self: VariantIndexStep,\n        session: Session,\n        vep_output_json_path: str,\n        variant_index_path: str,\n        hash_threshold: int,\n        variant_annotations_path: list[str] | None = None,\n        amino_acid_change_annotations: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Run VariantIndex step.\n\n        Args:\n            session (Session): Session object.\n            vep_output_json_path (str): Variant effect predictor output path (in json format).\n            variant_index_path (str): Variant index dataset path to save resulting data.\n            hash_threshold (int): Hash threshold for variant identifier length.\n            variant_annotations_path (list[str] | None): List of paths to extra variant annotation datasets.\n            amino_acid_change_annotations (list[str] | None): list of paths to amino-acid based variant annotations.\n        \"\"\"\n        # Extract variant annotations from VEP output:\n        variant_index = VariantEffectPredictorParser.extract_variant_index_from_vep(\n            session.spark, vep_output_json_path, hash_threshold\n        )\n\n        # Process variant annotations if provided:\n        if variant_annotations_path:\n            for annotation_path in variant_annotations_path:\n                # Read variant annotations from parquet:\n                annotations = VariantIndex.from_parquet(\n                    session=session,\n                    path=annotation_path,\n                    recursiveFileLookup=True,\n                    id_threshold=hash_threshold,\n                )\n\n                # Update index with extra annotations:\n                variant_index = variant_index.add_annotation(annotations)\n\n        # If provided read amino-acid based annotation and enrich variant index:\n        if amino_acid_change_annotations:\n            for annotation_path in amino_acid_change_annotations:\n                annotation_data = AminoAcidVariants.from_parquet(\n                    session, annotation_path\n                )\n\n                # Update index with extra annotations:\n                variant_index = variant_index.annotate_with_amino_acid_consequences(\n                    annotation_data\n                )\n\n        (\n            variant_index.df.repartitionByRange(\n                session.output_partitions, \"chromosome\", \"position\"\n            )\n            .sortWithinPartitions(\"chromosome\", \"position\")\n            .write.mode(session.write_mode)\n            .parquet(variant_index_path)\n        )\n</code></pre>"},{"location":"python_api/steps/variant_index_step/#gentropy.variant_index.VariantIndexStep.__init__","title":"<code>__init__(session: Session, vep_output_json_path: str, variant_index_path: str, hash_threshold: int, variant_annotations_path: list[str] | None = None, amino_acid_change_annotations: list[str] | None = None) -&gt; None</code>","text":"<p>Run VariantIndex step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>vep_output_json_path</code> <code>str</code> <p>Variant effect predictor output path (in json format).</p> required <code>variant_index_path</code> <code>str</code> <p>Variant index dataset path to save resulting data.</p> required <code>hash_threshold</code> <code>int</code> <p>Hash threshold for variant identifier length.</p> required <code>variant_annotations_path</code> <code>list[str] | None</code> <p>List of paths to extra variant annotation datasets.</p> <code>None</code> <code>amino_acid_change_annotations</code> <code>list[str] | None</code> <p>list of paths to amino-acid based variant annotations.</p> <code>None</code> Source code in <code>src/gentropy/variant_index.py</code> <pre><code>def __init__(\n    self: VariantIndexStep,\n    session: Session,\n    vep_output_json_path: str,\n    variant_index_path: str,\n    hash_threshold: int,\n    variant_annotations_path: list[str] | None = None,\n    amino_acid_change_annotations: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Run VariantIndex step.\n\n    Args:\n        session (Session): Session object.\n        vep_output_json_path (str): Variant effect predictor output path (in json format).\n        variant_index_path (str): Variant index dataset path to save resulting data.\n        hash_threshold (int): Hash threshold for variant identifier length.\n        variant_annotations_path (list[str] | None): List of paths to extra variant annotation datasets.\n        amino_acid_change_annotations (list[str] | None): list of paths to amino-acid based variant annotations.\n    \"\"\"\n    # Extract variant annotations from VEP output:\n    variant_index = VariantEffectPredictorParser.extract_variant_index_from_vep(\n        session.spark, vep_output_json_path, hash_threshold\n    )\n\n    # Process variant annotations if provided:\n    if variant_annotations_path:\n        for annotation_path in variant_annotations_path:\n            # Read variant annotations from parquet:\n            annotations = VariantIndex.from_parquet(\n                session=session,\n                path=annotation_path,\n                recursiveFileLookup=True,\n                id_threshold=hash_threshold,\n            )\n\n            # Update index with extra annotations:\n            variant_index = variant_index.add_annotation(annotations)\n\n    # If provided read amino-acid based annotation and enrich variant index:\n    if amino_acid_change_annotations:\n        for annotation_path in amino_acid_change_annotations:\n            annotation_data = AminoAcidVariants.from_parquet(\n                session, annotation_path\n            )\n\n            # Update index with extra annotations:\n            variant_index = variant_index.annotate_with_amino_acid_consequences(\n                annotation_data\n            )\n\n    (\n        variant_index.df.repartitionByRange(\n            session.output_partitions, \"chromosome\", \"position\"\n        )\n        .sortWithinPartitions(\"chromosome\", \"position\")\n        .write.mode(session.write_mode)\n        .parquet(variant_index_path)\n    )\n</code></pre>"},{"location":"python_api/steps/window_based_clumping/","title":"window_based_clumping","text":""},{"location":"python_api/steps/window_based_clumping/#gentropy.window_based_clumping.WindowBasedClumpingStep","title":"<code>gentropy.window_based_clumping.WindowBasedClumpingStep</code>","text":"<p>Apply window based clumping on summary statistics datasets.</p> Source code in <code>src/gentropy/window_based_clumping.py</code> <pre><code>class WindowBasedClumpingStep:\n    \"\"\"Apply window based clumping on summary statistics datasets.\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        summary_statistics_input_path: str,\n        study_locus_output_path: str,\n        distance: int = WindowBasedClumpingStepConfig().distance,\n        gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance,\n        collect_locus: bool = WindowBasedClumpingStepConfig().collect_locus,\n        collect_locus_distance: int = WindowBasedClumpingStepConfig().collect_locus_distance,\n        inclusion_list_path: str\n        | None = WindowBasedClumpingStepConfig().inclusion_list_path,\n    ) -&gt; None:\n        \"\"\"Run window-based clumping step.\n\n        Args:\n            session (Session): Session object.\n            summary_statistics_input_path (str): Path to the harmonized summary statistics dataset.\n            study_locus_output_path (str): Output path for the resulting study locus dataset.\n            distance (int): Distance, within which tagging variants are collected around the semi-index. Optional.\n            gwas_significance (float): GWAS significance threshold. Defaults to 5e-8.\n            collect_locus (bool): Whether to collect locus around semi-indices. Optional.\n            collect_locus_distance (int): Distance, within which tagging variants are collected around the semi-index. Optional.\n            inclusion_list_path (str | None): Path to the inclusion list (list of white-listed study identifier). Optional.\n\n        Check WindowBasedClumpingStepConfig object for default values.\n        \"\"\"\n        # If inclusion list path is provided, only these studies will be read:\n        if inclusion_list_path:\n            study_ids_to_ingest = [\n                f'{summary_statistics_input_path}/{row[\"studyId\"]}.parquet'\n                for row in session.spark.read.parquet(inclusion_list_path).collect()\n            ]\n        else:\n            # If no inclusion list is provided, read all summary stats in folder:\n            study_ids_to_ingest = [summary_statistics_input_path]\n\n        ss = SummaryStatistics.from_parquet(\n            session,\n            study_ids_to_ingest,\n            recursiveFileLookup=True,\n        )\n\n        # Clumping:\n        study_locus = ss.window_based_clumping(\n            distance=distance, gwas_significance=gwas_significance\n        )\n\n        # Optional locus collection:\n        if collect_locus:\n            # Collecting locus around semi-indices:\n            study_locus = study_locus.annotate_locus_statistics(\n                ss, collect_locus_distance=collect_locus_distance\n            )\n\n        study_locus.df.write.mode(session.write_mode).parquet(study_locus_output_path)\n</code></pre>"},{"location":"python_api/steps/window_based_clumping/#gentropy.window_based_clumping.WindowBasedClumpingStep.__init__","title":"<code>__init__(session: Session, summary_statistics_input_path: str, study_locus_output_path: str, distance: int = WindowBasedClumpingStepConfig().distance, gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance, collect_locus: bool = WindowBasedClumpingStepConfig().collect_locus, collect_locus_distance: int = WindowBasedClumpingStepConfig().collect_locus_distance, inclusion_list_path: str | None = WindowBasedClumpingStepConfig().inclusion_list_path) -&gt; None</code>","text":"<p>Run window-based clumping step.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session object.</p> required <code>summary_statistics_input_path</code> <code>str</code> <p>Path to the harmonized summary statistics dataset.</p> required <code>study_locus_output_path</code> <code>str</code> <p>Output path for the resulting study locus dataset.</p> required <code>distance</code> <code>int</code> <p>Distance, within which tagging variants are collected around the semi-index. Optional.</p> <code>distance</code> <code>gwas_significance</code> <code>float</code> <p>GWAS significance threshold. Defaults to 5e-8.</p> <code>gwas_significance</code> <code>collect_locus</code> <code>bool</code> <p>Whether to collect locus around semi-indices. Optional.</p> <code>collect_locus</code> <code>collect_locus_distance</code> <code>int</code> <p>Distance, within which tagging variants are collected around the semi-index. Optional.</p> <code>collect_locus_distance</code> <code>inclusion_list_path</code> <code>str | None</code> <p>Path to the inclusion list (list of white-listed study identifier). Optional.</p> <code>inclusion_list_path</code> <p>Check WindowBasedClumpingStepConfig object for default values.</p> Source code in <code>src/gentropy/window_based_clumping.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    summary_statistics_input_path: str,\n    study_locus_output_path: str,\n    distance: int = WindowBasedClumpingStepConfig().distance,\n    gwas_significance: float = WindowBasedClumpingStepConfig().gwas_significance,\n    collect_locus: bool = WindowBasedClumpingStepConfig().collect_locus,\n    collect_locus_distance: int = WindowBasedClumpingStepConfig().collect_locus_distance,\n    inclusion_list_path: str\n    | None = WindowBasedClumpingStepConfig().inclusion_list_path,\n) -&gt; None:\n    \"\"\"Run window-based clumping step.\n\n    Args:\n        session (Session): Session object.\n        summary_statistics_input_path (str): Path to the harmonized summary statistics dataset.\n        study_locus_output_path (str): Output path for the resulting study locus dataset.\n        distance (int): Distance, within which tagging variants are collected around the semi-index. Optional.\n        gwas_significance (float): GWAS significance threshold. Defaults to 5e-8.\n        collect_locus (bool): Whether to collect locus around semi-indices. Optional.\n        collect_locus_distance (int): Distance, within which tagging variants are collected around the semi-index. Optional.\n        inclusion_list_path (str | None): Path to the inclusion list (list of white-listed study identifier). Optional.\n\n    Check WindowBasedClumpingStepConfig object for default values.\n    \"\"\"\n    # If inclusion list path is provided, only these studies will be read:\n    if inclusion_list_path:\n        study_ids_to_ingest = [\n            f'{summary_statistics_input_path}/{row[\"studyId\"]}.parquet'\n            for row in session.spark.read.parquet(inclusion_list_path).collect()\n        ]\n    else:\n        # If no inclusion list is provided, read all summary stats in folder:\n        study_ids_to_ingest = [summary_statistics_input_path]\n\n    ss = SummaryStatistics.from_parquet(\n        session,\n        study_ids_to_ingest,\n        recursiveFileLookup=True,\n    )\n\n    # Clumping:\n    study_locus = ss.window_based_clumping(\n        distance=distance, gwas_significance=gwas_significance\n    )\n\n    # Optional locus collection:\n    if collect_locus:\n        # Collecting locus around semi-indices:\n        study_locus = study_locus.annotate_locus_statistics(\n            ss, collect_locus_distance=collect_locus_distance\n        )\n\n    study_locus.df.write.mode(session.write_mode).parquet(study_locus_output_path)\n</code></pre>"}]}